# Comparing `tmp/trulens_eval-0.7.0a0-py3-none-any.whl.zip` & `tmp/trulens_eval-0.8.0a0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,44 +1,44 @@
-Zip file size: 266271 bytes, number of entries: 42
+Zip file size: 270791 bytes, number of entries: 42
 -rw-rw-r--  2.0 unx     5185 b- defN 23-Jul-27 21:06 trulens_eval/Example_TruBot.py
--rw-rw-r--  2.0 unx     3475 b- defN 23-Jul-27 21:06 trulens_eval/Leaderboard.py
--rw-rw-r--  2.0 unx     1424 b- defN 23-Jul-27 21:26 trulens_eval/__init__.py
--rw-rw-r--  2.0 unx    12040 b- defN 23-Jul-21 19:10 trulens_eval/app.py
--rw-rw-r--  2.0 unx    21224 b- defN 23-Jul-21 19:10 trulens_eval/db.py
+-rw-rw-r--  2.0 unx     3933 b- defN 23-Aug-03 01:06 trulens_eval/Leaderboard.py
+-rw-rw-r--  2.0 unx     1424 b- defN 23-Aug-03 01:17 trulens_eval/__init__.py
+-rw-rw-r--  2.0 unx    12654 b- defN 23-Aug-03 01:06 trulens_eval/app.py
+-rw-rw-r--  2.0 unx    21226 b- defN 23-Aug-03 01:06 trulens_eval/db.py
 -rw-rw-r--  2.0 unx    14061 b- defN 23-Jul-12 14:37 trulens_eval/db_migration.py
--rw-rw-r--  2.0 unx    69314 b- defN 23-Jul-27 21:06 trulens_eval/feedback.py
+-rw-rw-r--  2.0 unx    69337 b- defN 23-Aug-03 01:06 trulens_eval/feedback.py
 -rw-rw-r--  2.0 unx     7034 b- defN 23-Jul-27 21:06 trulens_eval/feedback_prompts.py
--rw-rw-r--  2.0 unx    20145 b- defN 23-Jul-12 18:13 trulens_eval/instruments.py
+-rw-rw-r--  2.0 unx    25142 b- defN 23-Aug-03 01:06 trulens_eval/instruments.py
 -rw-rw-r--  2.0 unx    13338 b- defN 23-Jul-27 21:06 trulens_eval/keys.py
--rw-rw-r--  2.0 unx    23228 b- defN 23-Jul-27 21:06 trulens_eval/provider_apis.py
--rw-rw-r--  2.0 unx    14134 b- defN 23-Jul-27 21:06 trulens_eval/schema.py
--rw-rw-r--  2.0 unx    16238 b- defN 23-Jul-27 21:06 trulens_eval/tru.py
+-rw-rw-r--  2.0 unx    35158 b- defN 23-Aug-03 01:06 trulens_eval/provider_apis.py
+-rw-rw-r--  2.0 unx    14256 b- defN 23-Aug-03 01:06 trulens_eval/schema.py
+-rw-rw-r--  2.0 unx    16348 b- defN 23-Aug-03 01:06 trulens_eval/tru.py
 -rw-rw-r--  2.0 unx      293 b- defN 23-Jul-12 14:37 trulens_eval/tru_app.py
--rw-rw-r--  2.0 unx     3545 b- defN 23-Jul-12 18:13 trulens_eval/tru_basic_app.py
--rw-rw-r--  2.0 unx     7819 b- defN 23-Jul-12 20:22 trulens_eval/tru_chain.py
+-rw-rw-r--  2.0 unx     3553 b- defN 23-Aug-03 01:06 trulens_eval/tru_basic_app.py
+-rw-rw-r--  2.0 unx    10920 b- defN 23-Aug-03 01:06 trulens_eval/tru_chain.py
 -rw-rw-r--  2.0 unx      288 b- defN 23-Jul-12 14:37 trulens_eval/tru_db.py
 -rw-rw-r--  2.0 unx      318 b- defN 23-Jul-12 14:37 trulens_eval/tru_feedback.py
--rw-rw-r--  2.0 unx     6321 b- defN 23-Jul-27 21:06 trulens_eval/tru_llama.py
--rw-rw-r--  2.0 unx    47761 b- defN 23-Jul-27 21:06 trulens_eval/util.py
+-rw-rw-r--  2.0 unx    18967 b- defN 23-Aug-03 01:06 trulens_eval/tru_llama.py
+-rw-rw-r--  2.0 unx    47842 b- defN 23-Aug-03 01:06 trulens_eval/util.py
 -rw-rw-r--  2.0 unx    13110 b- defN 23-Jul-27 21:06 trulens_eval/pages/Evaluations.py
 -rw-rw-r--  2.0 unx     1123 b- defN 23-Jul-27 21:06 trulens_eval/pages/Progress.py
 -rw-rw-r--  2.0 unx     3294 b- defN 23-Jul-12 20:22 trulens_eval/react_components/record_viewer/__init__.py
--rw-rw-r--  2.0 unx      348 b- defN 23-Jul-27 21:27 trulens_eval/react_components/record_viewer/dist/index.html
--rw-rw-r--  2.0 unx   500498 b- defN 23-Jul-27 21:27 trulens_eval/react_components/record_viewer/dist/assets/index-4e44137e.js
+-rw-rw-r--  2.0 unx      348 b- defN 23-Aug-03 01:18 trulens_eval/react_components/record_viewer/dist/index.html
+-rw-rw-r--  2.0 unx   500498 b- defN 23-Aug-03 01:18 trulens_eval/react_components/record_viewer/dist/assets/index-4e44137e.js
 -rw-rw-r--  2.0 unx        0 b- defN 23-Jul-21 19:10 trulens_eval/utils/__init__.py
 -rw-rw-r--  2.0 unx       83 b- defN 23-Jul-12 20:22 trulens_eval/utils/command_line.py
 -rw-rw-r--  2.0 unx     5892 b- defN 23-Jul-12 22:23 trulens_eval/utils/langchain.py
 -rw-rw-r--  2.0 unx     4831 b- defN 23-Jul-27 21:06 trulens_eval/utils/llama.py
 -rw-rw-r--  2.0 unx     1001 b- defN 23-Jul-12 14:37 trulens_eval/utils/notebook_utils.py
--rw-rw-r--  2.0 unx      151 b- defN 23-Jul-21 19:10 trulens_eval/utils/python.py
+-rw-rw-r--  2.0 unx     2512 b- defN 23-Aug-03 01:06 trulens_eval/utils/python.py
 -rw-rw-r--  2.0 unx      166 b- defN 23-Jul-21 19:10 trulens_eval/utils/text.py
 -rw-rw-r--  2.0 unx      927 b- defN 23-Jul-12 20:22 trulens_eval/utils/trulens.py
 -rw-rw-r--  2.0 unx     1212 b- defN 23-Jul-12 14:37 trulens_eval/ux/add_logo.py
 -rw-rw-r--  2.0 unx     6421 b- defN 23-Jul-27 21:06 trulens_eval/ux/components.py
 -rw-rw-r--  2.0 unx     2307 b- defN 23-Jul-27 21:06 trulens_eval/ux/styles.py
 -rw-rw-r--  2.0 unx    29567 b- defN 23-Jul-12 14:37 trulens_eval/ux/trulens_logo.svg
--rw-rw-r--  2.0 unx    18744 b- defN 23-Jul-27 21:27 trulens_eval-0.7.0a0.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 23-Jul-27 21:27 trulens_eval-0.7.0a0.dist-info/WHEEL
--rw-rw-r--  2.0 unx       70 b- defN 23-Jul-27 21:27 trulens_eval-0.7.0a0.dist-info/entry_points.txt
--rw-rw-r--  2.0 unx       13 b- defN 23-Jul-27 21:27 trulens_eval-0.7.0a0.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     3635 b- defN 23-Jul-27 21:27 trulens_eval-0.7.0a0.dist-info/RECORD
-42 files, 880670 bytes uncompressed, 260461 bytes compressed:  70.4%
+-rw-rw-r--  2.0 unx    18929 b- defN 23-Aug-03 01:18 trulens_eval-0.8.0a0.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-Aug-03 01:18 trulens_eval-0.8.0a0.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       70 b- defN 23-Aug-03 01:18 trulens_eval-0.8.0a0.dist-info/entry_points.txt
+-rw-rw-r--  2.0 unx       13 b- defN 23-Aug-03 01:18 trulens_eval-0.8.0a0.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     3638 b- defN 23-Aug-03 01:18 trulens_eval-0.8.0a0.dist-info/RECORD
+42 files, 917311 bytes uncompressed, 264981 bytes compressed:  71.1%
```

## zipnote {}

```diff
@@ -105,23 +105,23 @@
 
 Filename: trulens_eval/ux/styles.py
 Comment: 
 
 Filename: trulens_eval/ux/trulens_logo.svg
 Comment: 
 
-Filename: trulens_eval-0.7.0a0.dist-info/METADATA
+Filename: trulens_eval-0.8.0a0.dist-info/METADATA
 Comment: 
 
-Filename: trulens_eval-0.7.0a0.dist-info/WHEEL
+Filename: trulens_eval-0.8.0a0.dist-info/WHEEL
 Comment: 
 
-Filename: trulens_eval-0.7.0a0.dist-info/entry_points.txt
+Filename: trulens_eval-0.8.0a0.dist-info/entry_points.txt
 Comment: 
 
-Filename: trulens_eval-0.7.0a0.dist-info/top_level.txt
+Filename: trulens_eval-0.8.0a0.dist-info/top_level.txt
 Comment: 
 
-Filename: trulens_eval-0.7.0a0.dist-info/RECORD
+Filename: trulens_eval-0.8.0a0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## trulens_eval/Leaderboard.py

```diff
@@ -1,9 +1,12 @@
+import argparse
 import json
 import math
+import os
+import sys
 
 from millify import millify
 import numpy as np
 import streamlit as st
 from streamlit_extras.switch_page_button import switch_page
 
 from trulens_eval.db_migration import MIGRATION_UNKNOWN_STR
@@ -18,15 +21,26 @@
 
 st.set_page_config(page_title="Leaderboard", layout="wide")
 
 from trulens_eval.ux.add_logo import add_logo
 
 add_logo()
 
-tru = Tru()
+parser = argparse.ArgumentParser()
+parser.add_argument('--database-file', default=Tru.DEFAULT_DATABASE_FILE)
+
+try:
+    args = parser.parse_args()
+except SystemExit as e:
+    # This exception will be raised if --help or invalid command line arguments
+    # are used. Currently, streamlit prevents the program from exiting normally,
+    # so we have to do a hard exit.
+    sys.exit(e.code)
+
+tru = Tru(database_file=args.database_file)
 lms = tru.db
 
 
 def streamlit_app():
     # Set the title and subtitle of the app
     st.title('App Leaderboard')
     st.write(
```

## trulens_eval/__init__.py

```diff
@@ -33,15 +33,15 @@
     - `util.py` 
     
     - `keys.py`
 
     - `utils/python.py` `utils/text.py`
 """
 
-__version__ = "0.7.0a"
+__version__ = "0.8.0a"
 
 from trulens_eval.feedback import Feedback
 from trulens_eval.feedback import Huggingface
 from trulens_eval.feedback import OpenAI
 from trulens_eval.feedback import Provider
 from trulens_eval.schema import FeedbackMode
 from trulens_eval.schema import Query
```

## trulens_eval/app.py

```diff
@@ -2,17 +2,15 @@
 Generalized root type for various libraries like llama_index and langchain .
 """
 
 from abc import ABC
 from abc import abstractmethod
 import logging
 from pprint import PrettyPrinter
-from typing import (
-    Any, Callable, Dict, Iterable, List, Optional, Sequence, Set, Tuple
-)
+from typing import Any, Dict, Iterable, Optional, Sequence, Set, Tuple
 
 import pydantic
 from pydantic import Field
 
 from trulens_eval.db import DB
 from trulens_eval.feedback import Feedback
 from trulens_eval.instruments import Instrument
@@ -95,60 +93,76 @@
 
         for k, v in self.json.items():
             if k not in skip and isinstance(v, JSON_BASES):
                 ret[k] = v
 
         return ret
 
+    @staticmethod
+    def innermost_base(
+        bases: Sequence[Class],
+        among_modules=set(["langchain", "llama_index", "trulens_eval"])
+    ) -> str:
+        """
+        Given a sequence of classes, return the first one which comes from one
+        of the `among_modules`. You can use this to determine where ultimately
+        the encoded class comes from in terms of langchain, llama_index, or
+        trulens_eval even in cases they extend each other's classes. Returns
+        None if no module from `among_modules` is named in `bases`.
+        """
+
+        for base in bases:
+            if "." in base.module.module_name:
+                root_module = base.module.module_name.split(".")[0]
+            else:
+                root_module = base.module.module_name
+
+            if root_module in among_modules:
+                return root_module
+
+        return None
+
 
 class LangChainComponent(ComponentView):
 
     @staticmethod
     def class_is(cls: Class) -> bool:
-        if cls.module.module_name.startswith("langchain."):
+        if ComponentView.innermost_base(cls.bases) == "langchain":
             return True
 
-        #if any(base.module.module_name.startswith("langchain.")
-        #       for base in cls.bases):
-        #    return True
-
         return False
 
     @staticmethod
     def of_json(json: JSON) -> 'LangChainComponent':
         from trulens_eval.utils.langchain import component_of_json
         return component_of_json(json)
 
 
 class LlamaIndexComponent(ComponentView):
 
     @staticmethod
     def class_is(cls: Class) -> bool:
-        if cls.module.module_name.startswith("llama_index."):
+        if ComponentView.innermost_base(cls.bases) == "llama_index":
             return True
 
-        #if any(base.module.module_name.startswith("llama_index.")
-        #       for base in cls.bases):
-        #    return True
-
         return False
 
     @staticmethod
     def of_json(json: JSON) -> 'LlamaIndexComponent':
         from trulens_eval.utils.llama import component_of_json
         return component_of_json(json)
 
 
 class TrulensComponent(ComponentView):
     """
     Components provided in trulens.
     """
 
     def class_is(cls: Class) -> bool:
-        if cls.module.module_name.startswith("trulens_eval."):
+        if ComponentView.innermost_base(cls.bases) == "trulens_eval":
             return True
 
         #if any(base.module.module_name.startswith("trulens.") for base in cls.bases):
         #    return True
 
         return False
```

## trulens_eval/db.py

```diff
@@ -449,15 +449,15 @@
         last_ts_before: Optional[datetime] = None
     ) -> pd.DataFrame:
 
         clauses = []
         vars = []
 
         if record_id is not None:
-            clauses.append("record_id=?")
+            clauses.append("f.record_id=?")
             vars.append(record_id)
 
         if feedback_result_id is not None:
             clauses.append("f.feedback_result_id=?")
             vars.append(feedback_result_id)
 
         if feedback_definition_id is not None:
```

## trulens_eval/feedback.py

```diff
@@ -1039,14 +1039,15 @@
           "gpt-3.5-turbo".
 
         - All other args/kwargs passed to OpenAIEndpoint constructor.
         """
 
         # TODO: why was self_kwargs required here independently of kwargs?
         self_kwargs = dict()
+        self_kwargs.update(**kwargs)
         self_kwargs['model_engine'] = model_engine
         self_kwargs['endpoint'] = OpenAIEndpoint(*args, **kwargs)
 
         super().__init__(
             **self_kwargs
         )  # need to include pydantic.BaseModel.__init__
 
@@ -1568,15 +1569,15 @@
                 )
                 groundedness_scores[f"statement_{i}"] = score
         return groundedness_scores, {"reason": reason}
 
     def grounded_statements_aggregator(
         self, source_statements_matrix: np.ndarray
     ) -> float:
-        """Aggregates multi-input, mulit-output information from the _groundedness_measure_experimental methods.
+        """Aggregates multi-input, mulit-output information from the groundedness_measure methods.
 
 
         Args:
             source_statements_matrix (np.ndarray): a 2D array with the first dimension corresponding to a source text,
                 and the second dimension corresponding to each sentence in a statement; it's groundedness score
 
         Returns:
```

## trulens_eval/instruments.py

```diff
@@ -42,18 +42,14 @@
 
     - This allows us to deserialize some objects. Pydantic models can be
       deserialized once we know their class and fields, for example.
     - This information is also used to determine component types without having
       to deserialize them first. 
     - See `schema.py:Class` for details.
 
-#### Tricky
-
-#### Limitations
-
 ### Functions/Methods
 
 Methods and functions are instrumented by overwriting choice attributes in
 various classes. 
 
 #### class/system specific
 
@@ -80,18 +76,14 @@
     intercepted and result in a callback.
 
 - langchain/llama_index callbacks. Each of these packages come with some
   callback system that lets one get various intermediate app results. The
   drawbacks is the need to handle different callback systems for each system and
   potentially missing information not exposed by them.
 
-### Tricky
-
-- 
-
 ### Calls
 
 The instrumented versions of functions/methods record the inputs/outputs and
 some additional data (see `schema.py:RecordAppCall`). As more then one
 instrumented call may take place as part of a app invokation, they are collected
 and returned together in the `calls` field of `schema.py:Record`.
 
@@ -113,21 +105,46 @@
   distinguish the different versions of the method.
 
 - Thread-safety -- it is tricky to use global data to keep track of instrumented
   method calls in presence of multiple threads. For this reason we do not use
   global data and instead hide instrumenting data in the call stack frames of
   the instrumentation methods. See `util.py:get_local_in_call_stack.py`.
 
-#### Limitations
+#### Threads
+
+Threads do not inherit call stacks from their creator. This is a problem due to
+our reliance on info stored on the stack. Therefore we have a limitation:
 
-- Threads need to be started using the utility class TP in order for
-  instrumented methods called in a thread to be tracked. As we rely on call
+- **Limitation**: Threads need to be started using the utility class TP in order
+  for instrumented methods called in a thread to be tracked. As we rely on call
   stack for call instrumentation we need to preserve the stack before a thread
   start which python does not do.  See `util.py:TP._thread_starter`.
 
+#### Async
+
+Similar to threads, code run as part of a `asyncio.Task` does not inherit the
+stack of the creator. Our current solution instruments `asyncio.new_event_loop`
+to make sure all tasks that get created in `async` track the stack of their
+creator. This is done in `utils/python.py:_new_event_loop` . The function
+`stack_with_tasks` is then used to integrate this information with the normal
+caller stack when needed. This may cause incompatibility issues when other tools
+use their own event loops or interfere with this instrumentation in other ways.
+Note that some async functions that seem to not involve `Task` do use tasks,
+such as `gather`.
+
+- **Limitation**: `async.Tasks` must be created via our `task_factory` as per
+  `utils/python.py:task_factory_with_stack`. This includes tasks created by
+  function such as `gather`. This limitation is not expected to be a problem
+  given our instrumentation except if other tools are used that modify `async`
+  in some ways.
+
+#### Limitations
+
+- Threading and async limitations. See **Threads** and **Async** .
+
 - If the same wrapped sub-app is called multiple times within a single call to
   the root app, the record of this execution will not be exact with regards to
   the path to the call information. All call paths will address the last subapp
   (by order in which it is instrumented). For example, in a sequential app
   containing two of the same app, call records will be addressed to the second
   of the (same) apps and contain a list describing calls of both the first and
   second.
@@ -146,25 +163,25 @@
 
 ## To Decide / To discuss
 
 ### Mirroring wrapped app behaviour and disabling instrumentation
 
 Should our wrappers behave like the wrapped apps? Current design is like this:
 
-```python chain = ... # some langchain chain
+```python
+chain = ... # some langchain chain
 
 tru = Tru() truchain = tru.Chain(chain, ...)
 
 plain_result = chain(...) # will not be recorded
 
 plain_result = truchain(...) # will be recorded
 
 plain_result, record = truchain.call_with_record(...) # will be recorded, and
 you get the record too
-
 ```
 
 The problem with the above is that "call_" part of "call_with_record" is
 langchain specific and implicitly so is __call__ whose behaviour we are
 replicating in TruChaib. Other wrapped apps may not implement their core
 functionality in "_call" or "__call__".
 
@@ -220,30 +237,32 @@
 
 - We require a root method to be placed on the stack to indicate the start of
   tracking. We therefore cannot implement something like a context-manager-based
   setup of the tracking system as suggested in the "To discuss" above.
 
 #### Alternatives
 
-- contextvars -- langchain uses these to manage contexts such as those used for
+- `contextvars` -- langchain uses these to manage contexts such as those used for
   instrumenting/tracking LLM usage. These can be used to manage call stack
   information like we do. The drawback is that these are not threadsafe or at
   least need instrumenting thread creation. We have to do a similar thing by
   requiring threads created by our utility package which does stack management
   instead of contextvar management.
 
 """
 
 from datetime import datetime
+import inspect
 from inspect import BoundArguments
 from inspect import signature
 import logging
 import os
 from pprint import PrettyPrinter
 import threading as th
+import traceback
 from typing import Callable, Dict, Iterable, Optional, Sequence, Set
 
 from pydantic import BaseModel
 
 from trulens_eval.feedback import Feedback
 from trulens_eval.schema import Perf
 from trulens_eval.schema import Query
@@ -303,20 +322,20 @@
         instrumented.
         """
 
         return any(module_name.startswith(mod2) for mod2 in self.modules)
 
     def __init__(
         self,
-        root_method: Optional[Callable] = None,
+        root_methods: Optional[Set[Callable]] = None,
         modules: Iterable[str] = [],
         classes: Iterable[type] = [],
         methods: Dict[str, Callable] = {},
     ):
-        self.root_method = root_method
+        self.root_methods = root_methods or set([])
 
         self.modules = Instrument.Default.MODULES.union(set(modules))
 
         self.classes = Instrument.Default.CLASSES.union(set(classes))
 
         self.methods = Instrument.Default.METHODS
         self.methods.update(methods)
@@ -325,15 +344,15 @@
         self, query: Query, func: Callable, method_name: str, cls: type,
         obj: object
     ):
         """
         Instrument a method to capture its inputs/outputs/errors.
         """
 
-        assert self.root_method is not None, "Cannot instrument method without a `root_method`."
+        assert self.root_methods is not None, "Cannot instrument method without `root_methods`."
 
         if hasattr(func, Instrument.INSTRUMENT):
             logger.debug(f"\t\t\t{query}: {func} is already instrumented")
 
             # Already instrumented. Note that this may happen under expected
             # operation when the same chain is used multiple times as part of a
             # larger chain.
@@ -344,34 +363,123 @@
             # than once in the wrapped chain or are called more than once.
             # func = getattr(func, Instrument.INSTRUMENT)
 
         logger.debug(f"\t\t\t{query}: instrumenting {method_name}={func}")
 
         sig = signature(func)
 
-        def wrapper(*args, **kwargs):
-            # If not within TruChain._call, call the wrapped function without
+        async def awrapper(*args, **kwargs):
+            # TODO: figure out how to have less repetition between the async and
+            # sync versions of this method.
+
+            # If not within a root method, call the wrapped function without
             # any recording. This check is not perfect in threaded situations so
             # the next call stack-based lookup handles the rarer cases.
 
-            # NOTE(piotrm): Disabling this for now as it is not thread safe.
-            #if not self.recording:
-            #    return func(*args, **kwargs)
+            logger.debug(f"{query}: calling instrumented async method {func}")
+
+            def find_root_methods(f):
+                # TODO: generalize
+                return id(f) in set([id(rm.__code__) for rm in self.root_methods])
+
+            # Look up whether the root instrumented method was called earlier in
+            # the stack and "record" variable was defined there. Will use that
+            # for recording the wrapped call.
+            record = get_local_in_call_stack(
+                key="record", func=find_root_methods
+            )
+
+            if record is None:
+                logger.debug(f"{query}: no record found, not recording.")
+                return await func(*args, **kwargs)
+
+            # Otherwise keep track of inputs and outputs (or exception).
+
+            error = None
+            rets = None
+
+            def find_instrumented(f):
+                return id(f) == id(awrapper.__code__)
+
+            # If a wrapped method was called in this call stack, get the prior
+            # calls from this variable. Otherwise create a new chain stack.
+            stack = get_local_in_call_stack(
+                key="stack", func=find_instrumented, offset=1
+            ) or ()
+            frame_ident = RecordAppCallMethod(
+                path=query, method=Method.of_method(func, obj=obj, cls=cls)
+            )
+            stack = stack + (frame_ident,)
+
+            start_time = None
+            end_time = None
+
+            bindings = dict()
+
+            try:
+                # Using sig bind here so we can produce a list of key-value
+                # pairs even if positional arguments were provided.
+                bindings: BoundArguments = sig.bind(*args, **kwargs)
+                start_time = datetime.now()
+
+                rets = await func(*bindings.args, **bindings.kwargs)
+
+                end_time = datetime.now()
+
+            except BaseException as e:
+                end_time = datetime.now()
+                error = e
+                error_str = str(e)
+
+                logger.error(f"Error calling wrapped function {func.__name__}.")
+                logger.error(traceback.format_exc())
+
+            # Don't include self in the recorded arguments.
+            nonself = {
+                k: jsonify(v)
+                for k, v in bindings.arguments.items()
+                if k != "self"
+            }
+
+            row_args = dict(
+                args=nonself,
+                perf=Perf(start_time=start_time, end_time=end_time),
+                pid=os.getpid(),
+                tid=th.get_native_id(),
+                stack=stack,
+                rets=rets,
+                error=error_str if error is not None else None
+            )
+            row = RecordAppCall(**row_args)
+            record.append(row)
+
+            if error is not None:
+                raise error
+
+            return rets
+
+        def wrapper(*args, **kwargs):
+            # TODO: figure out how to have less repetition between the async and
+            # sync versions of this method.
+
+            # If not within a root method, call the wrapped function without
+            # any recording. This check is not perfect in threaded situations so
+            # the next call stack-based lookup handles the rarer cases.
 
             logger.debug(f"{query}: calling instrumented method {func}")
 
-            def find_root_method(f):
+            def find_root_methods(f):
                 # TODO: generalize
-                return id(f) == id(self.root_method.__code__)
+                return id(f) in set([id(rm.__code__) for rm in self.root_methods])
 
             # Look up whether the root instrumented method was called earlier in
             # the stack and "record" variable was defined there. Will use that
             # for recording the wrapped call.
             record = get_local_in_call_stack(
-                key="record", func=find_root_method
+                key="record", func=find_root_methods
             )
 
             if record is None:
                 logger.debug(f"{query}: no record found, not recording.")
                 return func(*args, **kwargs)
 
             # Otherwise keep track of inputs and outputs (or exception).
@@ -391,27 +499,32 @@
                 path=query, method=Method.of_method(func, obj=obj, cls=cls)
             )
             stack = stack + (frame_ident,)
 
             start_time = None
             end_time = None
 
+            bindings = dict()
+
             try:
                 # Using sig bind here so we can produce a list of key-value
                 # pairs even if positional arguments were provided.
                 bindings: BoundArguments = sig.bind(*args, **kwargs)
                 start_time = datetime.now()
                 rets = func(*bindings.args, **bindings.kwargs)
                 end_time = datetime.now()
 
             except BaseException as e:
                 end_time = datetime.now()
                 error = e
                 error_str = str(e)
 
+                logger.error(f"Error calling wrapped function {func.__name__}.")
+                logger.error(traceback.format_exc())
+
             # Don't include self in the recorded arguments.
             nonself = {
                 k: jsonify(v)
                 for k, v in bindings.arguments.items()
                 if k != "self"
             }
 
@@ -427,26 +540,35 @@
             row = RecordAppCall(**row_args)
             record.append(row)
 
             if error is not None:
                 raise error
 
             return rets
+        
+        w = wrapper
+        if inspect.iscoroutinefunction(func):
+            w = awrapper
 
         # Indicate that the wrapper is an instrumented method so that we dont
         # further instrument it in another layer accidentally.
-        setattr(wrapper, Instrument.INSTRUMENT, func)
+        setattr(w, Instrument.INSTRUMENT, func)
 
         # Put the address of the instrumented chain in the wrapper so that we
         # don't pollute its list of fields. Note that this address may be
         # deceptive if the same subchain appears multiple times in the wrapped
         # chain.
-        setattr(wrapper, Instrument.PATH, query)
+        setattr(w, Instrument.PATH, query)
+
+        # NOTE(piotrm): This is important; langchain checks signatures to adjust
+        # behaviour and we need to match. Without this, wrapper signatures will
+        # show up only as *args, **kwargs .
+        w.__signature__ = inspect.signature(func)
 
-        return wrapper
+        return w
 
     def instrument_object(self, obj, query: Query, done: Set[int] = None):
 
         done = done or set([])
 
         cls = type(obj)
```

## trulens_eval/provider_apis.py

```diff
@@ -1,32 +1,34 @@
 import inspect
 import json
 import logging
 from pprint import PrettyPrinter
 from queue import Queue
 from threading import Thread
 from time import sleep
+from types import AsyncGeneratorType
 from types import ModuleType
-from typing import (
-    Any, Callable, Dict, Optional, Sequence, Tuple, Type, TypeVar
-)
+from typing import (Any, Awaitable, Callable, Dict, List, Optional, Sequence,
+                    Tuple, Type, TypeVar)
 
 from langchain.callbacks.openai_info import OpenAICallbackHandler
+from langchain.schema import Generation
 from langchain.schema import LLMResult
 import pydantic
 import requests
 
 from trulens_eval.keys import _check_key
 from trulens_eval.keys import get_huggingface_headers
 from trulens_eval.schema import Cost
 from trulens_eval.util import get_local_in_call_stack
 from trulens_eval.util import JSON
 from trulens_eval.util import SerialModel
 from trulens_eval.util import SingletonPerName
 from trulens_eval.util import WithClassInfo
+from trulens_eval.utils.python import Thunk
 from trulens_eval.utils.text import UNICODE_CHECK
 
 logger = logging.getLogger(__name__)
 pp = PrettyPrinter()
 
 T = TypeVar("T")
 
@@ -41,17 +43,23 @@
     """
 
     cost: Cost = pydantic.Field(default_factory=Cost)
 
     def handle(self, response: Any) -> None:
         self.cost.n_requests += 1
 
+    def handle_chunk(self, response: Any) -> None:
+        self.cost.n_stream_chunks += 1
+
     def handle_generation(self, response: Any) -> None:
         self.handle(response)
 
+    def handle_generation_chunk(self, response: Any) -> None:
+        self.handle_chunk(response)
+
     def handle_classification(self, response: Any) -> None:
         self.handle(response)
 
 
 class HuggingfaceCallback(EndpointCallback):
 
     def handle_classification(self, response: requests.Response) -> None:
@@ -77,25 +85,55 @@
 
     # For openai cost tracking, we use the logic from langchain mostly
     # implemented in the OpenAICallbackHandler class:
     langchain_handler: OpenAICallbackHandler = pydantic.Field(
         default_factory=OpenAICallbackHandler, exclude=True
     )
 
+    chunks: List[Generation] = pydantic.Field(
+        default_factory=list, exclude=True
+    )
+
     def handle_classification(self, response: Dict) -> None:
         # OpenAI's moderation API is not text generation and does not return
         # usage information. Will count those as a classification.
 
         super().handle_classification(response)
 
         if "categories" in response:
             self.cost.n_successful_requests += 1
             self.cost.n_classes += len(response['categories'])
 
+    def handle_generation_chunk(self, response: Any) -> None:
+        """
+        Called on every streaming chunk from an openai text generation process.
+        """
+
+        # self.langchain_handler.on_llm_new_token() # does nothing
+
+        super().handle_generation_chunk(response=response)
+
+        self.chunks.append(response)
+
+        if response.generation_info['choices'][0]['finish_reason'] == 'stop':
+            llm_result = LLMResult(
+                llm_output=dict(
+                    token_usage=dict(),
+                    model_name=response.generation_info['model']
+                ),
+                generations=[self.chunks]
+            )
+            self.chunks = []
+            self.handle_generation(response=llm_result)
+
     def handle_generation(self, response: LLMResult) -> None:
+        """
+        Called upon a non-streaming text generation or at the completion of a
+        streamed generation.
+        """
 
         super().handle_generation(response)
 
         self.langchain_handler.on_llm_end(response)
 
         # Copy over the langchain handler fields we also have.
         for cost_field, langchain_field in [
@@ -226,15 +264,15 @@
 
         assert isinstance(
             j, Sequence
         ) and len(j) > 0, f"Post did not return a sequence: {j}"
 
         return j[0]
 
-    def run_me(self, thunk: Callable[[], T]) -> T:
+    def run_me(self, thunk: Thunk[T]) -> T:
         """
         Run the given thunk, returning itse output, on pace with the api.
         Retries request multiple times if self.retries > 0.
         """
 
         retries = self.retries + 1
         retry_delay = 2.0
@@ -282,15 +320,15 @@
 
         for m in dir(mod):
             obj = getattr(mod, m)
             self._instrument_class(obj, method_name=method_name)
 
     @staticmethod
     def track_all_costs(
-        thunk: Callable[[], T],
+        thunk: Thunk[T],
         with_openai: bool = True,
         with_hugs: bool = True
     ) -> Tuple[T, Sequence[EndpointCallback]]:
         """
         Track costs of all of the apis we can currently track, over the
         execution of thunk.
         """
@@ -316,32 +354,83 @@
                     "Huggingface API keys are not set. "
                     "Will not track usage."
                 )
 
         return Endpoint._track_costs(thunk, with_endpoints=endpoints)
 
     @staticmethod
+    async def atrack_all_costs(
+        thunk: Thunk[Awaitable],
+        with_openai: bool = True,
+        with_hugs: bool = True
+    ) -> Tuple[T, Sequence[EndpointCallback]]:
+        """
+        Track costs of all of the apis we can currently track, over the
+        execution of thunk.
+        """
+
+        endpoints = []
+
+        if with_openai:
+            try:
+                e = OpenAIEndpoint()
+                endpoints.append(e)
+            except:
+                logger.warning(
+                    "OpenAI API keys are not set. "
+                    "Will not track usage."
+                )
+
+        if with_hugs:
+            try:
+                e = HuggingfaceEndpoint()
+                endpoints.append(e)
+            except:
+                logger.warning(
+                    "Huggingface API keys are not set. "
+                    "Will not track usage."
+                )
+
+        return await Endpoint._atrack_costs(thunk, with_endpoints=endpoints)
+
+    @staticmethod
     def track_all_costs_tally(
-        thunk: Callable[[], T],
+        thunk: Thunk[T],
         with_openai: bool = True,
         with_hugs: bool = True
     ) -> Tuple[T, Cost]:
         """
         Track costs of all of the apis we can currently track, over the
         execution of thunk.
         """
 
         result, cbs = Endpoint.track_all_costs(
             thunk, with_openai=with_openai, with_hugs=with_hugs
         )
         return result, sum(cb.cost for cb in cbs)
 
     @staticmethod
+    async def atrack_all_costs_tally(
+        thunk: Thunk[Awaitable],
+        with_openai: bool = True,
+        with_hugs: bool = True
+    ) -> Tuple[T, Cost]:
+        """
+        Track costs of all of the apis we can currently track, over the
+        execution of thunk.
+        """
+
+        result, cbs = await Endpoint.atrack_all_costs(
+            thunk, with_openai=with_openai, with_hugs=with_hugs
+        )
+        return result, sum(cb.cost for cb in cbs)
+
+    @staticmethod
     def _track_costs(
-        thunk: Callable[[], T],
+        thunk: Thunk[T],
         with_endpoints: Sequence['Endpoint'] = None,
     ) -> Tuple[T, Sequence[EndpointCallback]]:
         """
         Root of all cost tracking methods. Runs the given `thunk`, tracking
         costs using each of the provided endpoints' callbacks.
         """
 
@@ -395,28 +484,95 @@
         # Call the thunk.
         result: T = thunk()
 
         # Return result and only the callbacks created here. Outer thunks might
         # return others.
         return result, callbacks
 
-    def track_cost(self, thunk: Callable[[], T]) -> Tuple[T, EndpointCallback]:
+    @staticmethod
+    async def _atrack_costs(
+        thunk: Thunk[Awaitable],
+        with_endpoints: Sequence['Endpoint'] = None,
+    ) -> Tuple[T, Sequence[EndpointCallback]]:
+        """
+        Root of all cost tracking methods. Runs the given `thunk`, tracking
+        costs using each of the provided endpoints' callbacks.
+        """
+
+        # Check to see if this call is within another _track_costs call:
+        endpoints: Dict[Type[EndpointCallback], Sequence[Tuple[Endpoint, EndpointCallback]]] = \
+            get_local_in_call_stack(
+                key="endpoints",
+                func=Endpoint.__find_tracker,
+                offset=1
+            )
+
+        if endpoints is None:
+            # If not, lets start a new collection of endpoints here along with
+            # the callbacks for each. See type above.
+
+            endpoints = dict()
+
+        else:
+            # We copy the dict here so that the outer call to _track_costs will
+            # have their own version unaffacted by our additions below. Once
+            # this frame returns, the outer frame will have its own endpoints
+            # again and any wrapped method will get that smaller set of
+            # endpoints.
+
+            # TODO: check if deep copy is needed given we are storing lists in
+            # the values and don't want to affect the existing ones here.
+            endpoints = endpoints.copy()
+
+        # Collect any new endpoints requested of us.
+        with_endpoints = with_endpoints or []
+
+        # Keep track of the new callback objects we create here for returning
+        # later.
+        callbacks = []
+
+        # Create the callbacks for the new requested endpoints only. Existing
+        # endpoints from other frames will keep their callbacks.
+        for endpoint in with_endpoints:
+            callback_class = endpoint.callback_class
+            callback = callback_class()
+
+            if callback_class not in endpoints:
+                endpoints[callback_class] = []
+
+            # And add them to the endpoints dict. This will be retrieved from
+            # locals of this frame later in the wrapped methods.
+            endpoints[callback_class].append((endpoint, callback))
+
+            callbacks.append(callback)
+
+        # Call the thunk.
+        result: T = await thunk()
+
+        # Return result and only the callbacks created here. Outer thunks might
+        # return others.
+        return result, callbacks
+
+    def track_cost(self, thunk: Thunk[T]) -> Tuple[T, EndpointCallback]:
         """
         Tally only the usage performed within the execution of the given thunk.
         Returns the thunk's result alongside the EndpointCallback object that
         includes the usage information.
         """
 
         result, callbacks = Endpoint._track_costs(thunk, with_endpoints=[self])
 
         return result, callbacks[0]
 
     @staticmethod
     def __find_tracker(f):
-        return id(f) == id(Endpoint._track_costs.__code__)
+        return id(f) in [
+            id(m.__code__)
+            for m in [Endpoint._track_costs, Endpoint._atrack_costs]
+        ]
 
     def handle_wrapped_call(
         self, bindings: inspect.BoundArguments, response: Any,
         callback: Optional[EndpointCallback]
     ) -> None:
         """
         This gets called with the results of every instrumented method. This
@@ -465,14 +621,143 @@
                 registered_callback_classes += [self.callback_class]
                 setattr(func, INSTRUMENT, registered_callback_classes)
 
                 return func
 
         # If INSTRUMENT is not set, create a wrapper method and return it.
 
+        async def _agenwrapper_completion(
+            responses: AsyncGeneratorType, *args, **kwargs
+        ):
+
+            bindings = inspect.signature(func).bind(*args, **kwargs)
+
+            # Get all of the callback classes suitable for handling this call.
+            # Note that we stored this in the INSTRUMENT attribute of the
+            # wrapper method.
+            registered_callback_classes = getattr(
+                _agenwrapper_completion, INSTRUMENT
+            )
+
+            # Look up the endpoints that are expecting to be notified and the
+            # callback tracking the tally. See Endpoint._track_costs for
+            # definition.
+            endpoints: Dict[Type[EndpointCallback], Sequence[Tuple[Endpoint, EndpointCallback]]] = \
+                get_local_in_call_stack(
+                    key="endpoints",
+                    func=self.__find_tracker,
+                    offset=0
+                )
+
+            # If wrapped method was not called from within _track_costs, we will
+            # get None here and do nothing but return wrapped function's
+            # response.
+
+            if endpoints is None:
+                logger.debug("No endpoints found.")
+                # Still need to yield the responses below.
+
+            async for response in responses:
+                yield response
+
+                if endpoints is None:
+                    continue
+
+                for callback_class in registered_callback_classes:
+                    logger.debug(f"Handling callback_class: {callback_class}.")
+                    if callback_class not in endpoints:
+                        logger.warning(
+                            f"Callback class {callback_class.__name__} is registered for handling {func.__name__}"
+                            " but there are no endpoints waiting to receive the result."
+                        )
+                        continue
+
+                    for endpoint, callback in endpoints[callback_class]:
+                        logger.debug(f"Handling endpoint {endpoint}.")
+                        endpoint.handle_wrapped_call(
+                            func=func,
+                            bindings=bindings,
+                            response=response,
+                            callback=callback
+                        )
+
+        async def agenwrapper(*args, **kwargs):
+            logger.debug(
+                f"Calling async generator wrapped {func.__name__} for {self.name}."
+            )
+
+            # Get the result of the wrapped function:
+            responses: AsyncGeneratorType = await func(*args, **kwargs)
+
+            return _agenwrapper_completion(responses, *args, **kwargs)
+
+        # TODO: async/sync code duplication
+        async def awrapper(*args, **kwargs):
+            logger.debug(
+                f"Calling async wrapped {func.__name__} for {self.name}."
+            )
+
+            # Get the result of the wrapped function:
+            response_or_generator = await func(*args, **kwargs)
+
+            # Check that it is an async generator first. Sometimes we cannot
+            # tell statically (via inspect) that a function will produce a
+            # generator.
+            if inspect.isasyncgen(response_or_generator):
+                return _agenwrapper_completion(
+                    response_or_generator, *args, **kwargs
+                )
+
+            # Otherwise this is not an async generator.
+            response = response_or_generator
+
+            bindings = inspect.signature(func).bind(*args, **kwargs)
+
+            # Get all of the callback classes suitable for handling this call.
+            # Note that we stored this in the INSTRUMENT attribute of the
+            # wrapper method.
+            registered_callback_classes = getattr(awrapper, INSTRUMENT)
+
+            # Look up the endpoints that are expecting to be notified and the
+            # callback tracking the tally. See Endpoint._track_costs for
+            # definition.
+            endpoints: Dict[Type[EndpointCallback], Sequence[Tuple[Endpoint, EndpointCallback]]] = \
+                get_local_in_call_stack(
+                    key="endpoints",
+                    func=self.__find_tracker,
+                    offset=0
+                )
+
+            # If wrapped method was not called from within _track_costs, we will
+            # get None here and do nothing but return wrapped function's
+            # response.
+            if endpoints is None:
+                logger.debug("No endpoints found.")
+                return response
+
+            for callback_class in registered_callback_classes:
+                logger.debug(f"Handling callback_class: {callback_class}.")
+                if callback_class not in endpoints:
+                    logger.warning(
+                        f"Callback class {callback_class.__name__} is registered for handling {func.__name__}"
+                        " but there are no endpoints waiting to receive the result."
+                    )
+                    continue
+
+                for endpoint, callback in endpoints[callback_class]:
+                    logger.debug(f"Handling endpoint {endpoint}.")
+                    endpoint.handle_wrapped_call(
+                        func=func,
+                        bindings=bindings,
+                        response=response,
+                        callback=callback
+                    )
+
+            return response
+
         def wrapper(*args, **kwargs):
             logger.debug(f"Calling wrapped {func.__name__} for {self.name}.")
 
             # Get the result of the wrapped function:
             response: Any = func(*args, **kwargs)
 
             bindings = inspect.signature(func).bind(*args, **kwargs)
@@ -513,21 +798,43 @@
                         bindings=bindings,
                         response=response,
                         callback=callback
                     )
 
             return response
 
-        setattr(wrapper, INSTRUMENT, [self.callback_class])
-        wrapper.__name__ = func.__name__
-        wrapper.__signature__ = inspect.signature(func)
+        # Determine which of the wrapper variants to return and to annotate.
+
+        if inspect.isasyncgenfunction(func):
+            # This is not always accurate hence.
+            w = agenwrapper
+            w2 = _agenwrapper_completion
+
+        elif inspect.iscoroutinefunction(func):
+            # An async coroutine can actually be an async generator so we
+            # annotate both the async and async generator wrappers.
+            w = awrapper
+            w2 = _agenwrapper_completion
+
+        else:
+            w = wrapper
+            w2 = None
+
+        setattr(w, INSTRUMENT, [self.callback_class])
+        w.__name__ = func.__name__
+        w.__signature__ = inspect.signature(func)
+
+        if w2 is not None:
+            # This attribute is internally by _agenwrapper_completion hence we
+            # need this.
+            setattr(w2, INSTRUMENT, [self.callback_class])
 
         logger.debug(f"Instrumenting {func.__name__} for {self.name} .")
 
-        return wrapper
+        return w
 
 
 class OpenAIEndpoint(Endpoint, WithClassInfo):
     """
     OpenAI endpoint. Instruments "create" methods in openai.* classes.
     """
 
@@ -560,14 +867,26 @@
             )
 
             self.global_callback.handle_generation(response=llm_res)
 
             if callback is not None:
                 callback.handle_generation(response=llm_res)
 
+        if 'choices' in response and 'delta' in response['choices'][0]:
+            # Streaming data.
+
+            content = response['choices'][0]['delta'].get('content')
+
+            gen = Generation(text=content or '', generation_info=response)
+            self.global_callback.handle_generation_chunk(gen)
+            if callback is not None:
+                callback.handle_generation_chunk(gen)
+
+            counted_something = True
+
         if results is not None:
             for res in results:
                 if "categories" in res:
                     counted_something = True
                     self.global_callback.handle_classification(response=res)
 
                     if callback is not None:
@@ -631,14 +950,15 @@
 
         # for WithClassInfo:
         kwargs['obj'] = self
 
         super().__init__(*args, **kwargs)
 
         self._instrument_module_members(openai, "create")
+        self._instrument_module_members(openai, "acreate")
 
 
 class HuggingfaceEndpoint(Endpoint, WithClassInfo):
     """
     Huggingface. Instruments the requests.post method for requests to
     "https://api-inference.huggingface.co".
     """
```

## trulens_eval/schema.py

```diff
@@ -31,14 +31,15 @@
 import pydantic
 
 from trulens_eval.util import Class
 from trulens_eval.util import Function
 from trulens_eval.util import FunctionOrMethod
 from trulens_eval.util import GetItemOrAttribute
 from trulens_eval.util import JSON
+from trulens_eval.util import jsonify
 from trulens_eval.util import JSONPath
 from trulens_eval.util import Method
 from trulens_eval.util import obj_id_of_obj
 from trulens_eval.util import SerialModel
 from trulens_eval.util import WithClassInfo
 
 T = TypeVar("T")
@@ -74,14 +75,17 @@
 
     # Number of class scores retrieved.
     n_classes: int = 0
 
     # Total tokens processed.
     n_tokens: int = 0
 
+    # In streaming mode, number of chunks produced.
+    n_stream_chunks: int = 0
+
     # Number of prompt tokens supplied.
     n_prompt_tokens: int = 0
 
     # Number of completion tokens generated.
     n_completion_tokens: int = 0
 
     # Cost in USD.
@@ -165,15 +169,15 @@
     # via `layout_calls_as_app`.
     calls: Sequence[RecordAppCall] = []
 
     def __init__(self, record_id: Optional[RecordID] = None, **kwargs):
         super().__init__(record_id="temporary", **kwargs)
 
         if record_id is None:
-            record_id = obj_id_of_obj(self.dict(), prefix="record")
+            record_id = obj_id_of_obj(jsonify(self), prefix="record")
 
         self.record_id = record_id
 
     def layout_calls_as_app(self) -> JSON:
         """
         Layout the calls in this record into the structure that follows that of
         the app that created this record. This uses the paths stored in each
```

## trulens_eval/tru.py

```diff
@@ -64,25 +64,25 @@
         Create a llama_index engine with database managed by self.
         """
 
         from trulens_eval.tru_llama import TruLlama
 
         return TruLlama(tru=self, app=engine, **kwargs)
 
-    def __init__(self):
+    def __init__(self, database_file: Optional[str] = None):
         """
         TruLens instrumentation, logging, and feedback functions for apps.
         Creates a local database 'default.sqlite' in current working directory.
         """
 
         if hasattr(self, "db"):
             # Already initialized by SingletonByName mechanism.
             return
 
-        self.db = LocalSQLite(filename=Path(Tru.DEFAULT_DATABASE_FILE))
+        self.db = LocalSQLite(filename=Path(database_file or Tru.DEFAULT_DATABASE_FILE))
 
     def reset_database(self):
         """
         Reset the database. Clears all tables.
         """
 
         self.db.reset_database()
@@ -397,15 +397,16 @@
 
         env_opts = {}
         if _dev is not None:
             env_opts['env'] = os.environ
             env_opts['env']['PYTHONPATH'] = str(_dev)
 
         proc = subprocess.Popen(
-            ["streamlit", "run", "--server.headless=True", leaderboard_path],
+            ["streamlit", "run", "--server.headless=True", leaderboard_path,
+             "--", "--database-file", self.db.filename],
             stdout=subprocess.PIPE,
             stderr=subprocess.PIPE,
             text=True,
             **env_opts
         )
 
         started = threading.Event()
```

## trulens_eval/tru_basic_app.py

```diff
@@ -29,15 +29,15 @@
         CLASSES = lambda: {TruWrapperApp}
 
         # Instrument only methods with these names and of these classes.
         METHODS = {"_call": lambda o: isinstance(o, TruWrapperApp)}
 
     def __init__(self):
         super().__init__(
-            root_method=TruBasicApp.call_with_record,
+            root_methods=set([TruBasicApp.call_with_record]),
             classes=TruBasicCallableInstrument.Default.CLASSES(),
             methods=TruBasicCallableInstrument.Default.METHODS
         )
 
 
 class TruWrapperApp(object):
     # the class level call (Should be immutable from the __init__)
```

## trulens_eval/tru_chain.py

```diff
@@ -1,64 +1,71 @@
 """
 # Langchain instrumentation and monitoring.
 """
 
 from datetime import datetime
 import logging
 from pprint import PrettyPrinter
-from typing import Any, ClassVar, Dict, List, Sequence, Union
+import traceback
+from typing import Any, ClassVar, Dict, List, Sequence, Tuple, Union
 
+# import nest_asyncio # NOTE(piotrm): disabling for now, need more investigation
 from pydantic import Field
 
 from trulens_eval.app import App
 from trulens_eval.instruments import Instrument
 from trulens_eval.provider_apis import Endpoint
 from trulens_eval.schema import Cost
+from trulens_eval.schema import Record
 from trulens_eval.schema import RecordAppCall
 from trulens_eval.util import Class
 from trulens_eval.util import FunctionOrMethod
 from trulens_eval.util import jsonify
 from trulens_eval.util import noserio
 from trulens_eval.util import OptionalImports
 from trulens_eval.util import REQUIREMENT_LANGCHAIN
 
 logger = logging.getLogger(__name__)
 
 pp = PrettyPrinter()
 
 with OptionalImports(message=REQUIREMENT_LANGCHAIN):
     import langchain
-    from langchain.callbacks import get_openai_callback
     from langchain.chains.base import Chain
 
 
 class LangChainInstrument(Instrument):
 
     class Default:
         MODULES = {"langchain."}
 
         # Thunk because langchain is optional.
         CLASSES = lambda: {
-            langchain.chains.base.Chain, langchain.vectorstores.base.
-            BaseRetriever, langchain.schema.BaseRetriever, langchain.llms.base.
-            BaseLLM, langchain.prompts.base.BasePromptTemplate, langchain.schema
-            .BaseMemory, langchain.schema.BaseChatMessageHistory
+            langchain.chains.base.Chain,
+            langchain.vectorstores.base.BaseRetriever,
+            langchain.schema.BaseRetriever,
+            langchain.llms.base.BaseLLM,
+            langchain.prompts.base.BasePromptTemplate,
+            langchain.schema.BaseMemory,
+            langchain.schema.BaseChatMessageHistory
         }
 
         # Instrument only methods with these names and of these classes.
         METHODS = {
             "_call": lambda o: isinstance(o, langchain.chains.base.Chain),
-            # "get_relevant_documents": lambda o: True,  # VectorStoreRetriever
+            "__call__": lambda o: isinstance(o, langchain.chains.base.Chain),
+            "_acall": lambda o: isinstance(o, langchain.chains.base.Chain),
+            "acall": lambda o: isinstance(o, langchain.chains.base.Chain),
             "_get_relevant_documents":
                 lambda o: True,  # VectorStoreRetriever, langchain >= 0.230
         }
 
     def __init__(self):
         super().__init__(
-            root_method=TruChain.call_with_record,
+            root_methods=set([TruChain.call_with_record, TruChain.acall_with_record]),
             modules=LangChainInstrument.Default.MODULES,
             classes=LangChainInstrument.Default.CLASSES(),
             methods=LangChainInstrument.Default.METHODS
         )
 
     def _instrument_dict(self, cls, obj: Any, with_class_info: bool = False):
         """
@@ -99,14 +106,15 @@
 class TruChain(App):
     """
     Wrap a langchain Chain to capture its configuration and evaluation steps. 
     """
 
     app: Chain
 
+    # TODO: what if _acall is being used instead?
     root_callable: ClassVar[FunctionOrMethod] = Field(
         default_factory=lambda: FunctionOrMethod.of_callable(TruChain._call),
         const=True
     )
 
     # Normally pydantic does not like positional args but chain here is
     # important enough to make an exception.
@@ -149,22 +157,49 @@
         # A message for cases where a user calls something that the wrapped
         # chain has but we do not wrap yet.
 
         if hasattr(self.app, __name):
             return RuntimeError(
                 f"TruChain has no attribute {__name} but the wrapped app ({type(self.app)}) does. ",
                 f"If you are calling a {type(self.app)} method, retrieve it from that app instead of from `TruChain`. "
-                f"TruChain only wraps the the Chain.__call__ and Chain._call methods presently."
+                f"TruChain presently only wraps Chain.__call__, Chain._call, and Chain._acall ."
             )
         else:
             raise RuntimeError(f"TruChain has no attribute named {__name}.")
 
-    # NOTE: Input signature compatible with langchain.chains.base.Chain.__call__
-    def call_with_record(self, inputs: Union[Dict[str, Any], Any], **kwargs):
-        """ Run the chain and also return a record metadata object.
+    """
+    # NOTE: Disabling this method for now as it may have compatibility issues
+    with various packages. Need some way to reduce code duplication between the
+    async and sync versions of various methods.
+
+    def _eval_sync_root_method(self, func, inputs, **kwargs) -> Any:
+        async def func_async(inputs, **kwargs):
+            return func(inputs, **kwargs)
+       
+        try:
+            # Required for reusing async methods inside sync methods if running
+            # inside some outer async loop. Note that jupyter notebook cells are
+            # run within such a loop.
+            
+            nest_asyncio.apply() evl = asyncio.get_event_loop()
+            
+            # Will fail if not inside an async loop, in that case, we are free #
+            to create one below.
+
+        except:
+            evl = asyncio.new_event_loop()
+
+        # requires nested asyncio
+        return evl.run_until_complete(self._eval_async_root_method(func_async, inputs, **kwargs))
+    """
+
+    # NOTE: Input signature compatible with langchain.chains.base.Chain.acall
+    async def acall_with_record(self, inputs: Union[Dict[str, Any], Any], **kwargs) -> Tuple[Any, Record]:
+        """
+        Run the chain and also return a record metadata object.
 
         Returns:
             Any: chain output
             dict: record metadata
         """
 
         # Wrapped calls will look this up by traversing the call stack. This
@@ -175,34 +210,95 @@
         error = None
 
         cost: Cost = Cost()
 
         start_time = None
         end_time = None
 
+        # langchain.__call__ specific:
+        inputs = self.app.prep_inputs(inputs)
+
         try:
-            # TODO: do this only if there is an openai model inside the chain:
-            with get_openai_callback() as cb:
-                start_time = datetime.now()
-                ret, cost = Endpoint.track_all_costs_tally(
-                    lambda: self.app.__call__(inputs=inputs, **kwargs)
-                )
-                end_time = datetime.now()
+            start_time = datetime.now()
+            ret, cost = await Endpoint.atrack_all_costs_tally(
+                lambda: self.app.acall(inputs=inputs, **kwargs)
+            )
+            end_time = datetime.now()
 
         except BaseException as e:
             end_time = datetime.now()
             error = e
             logger.error(f"App raised an exception: {e}")
-
+            logger.error(traceback.format_exc())
+            
         assert len(record) > 0, "No information recorded in call."
 
         ret_record_args = dict()
 
+        # Figure out the content of the "inputs" arg that __call__ constructs
+        # for _call so we can lookup main input and output.
+        input_key = self.input_keys[0]
+        output_key = self.output_keys[0]
+
+        ret_record_args['main_input'] = jsonify(inputs[input_key])
+
+        if ret is not None:
+            ret_record_args['main_output'] = jsonify(ret[output_key])
+
+        if error is not None:
+            ret_record_args['main_error'] = jsonify(error)
+
+        ret_record = self._post_record(
+            ret_record_args, error, cost, start_time, end_time, record
+        )
+
+        return ret, ret_record
+
+    # NOTE: Input signature compatible with langchain.chains.base.Chain.__call__
+    def call_with_record(self, inputs: Union[Dict[str, Any], Any], **kwargs) -> Tuple[Any, Record]:
+        """
+        Run the chain and also return a record metadata object.
+
+        Returns:
+            Any: chain output
+            dict: record metadata
+        """
+
+        # Wrapped calls will look this up by traversing the call stack. This
+        # should work with threads.
+        record: Sequence[RecordAppCall] = []
+
+        ret = None
+        error = None
+
+        cost: Cost = Cost()
+
+        start_time = None
+        end_time = None
+
+        # langchain.__call__ specific:
         inputs = self.app.prep_inputs(inputs)
 
+        try:
+            start_time = datetime.now()
+            ret, cost = Endpoint.track_all_costs_tally(
+                lambda: self.app.__call__(inputs=inputs, **kwargs)
+            )
+            end_time = datetime.now()
+
+        except BaseException as e:
+            end_time = datetime.now()
+            error = e
+            logger.error(f"App raised an exception: {e}")
+            logger.error(traceback.format_exc())
+
+        assert len(record) > 0, "No information recorded in call."
+
+        ret_record_args = dict()
+
         # Figure out the content of the "inputs" arg that __call__ constructs
         # for _call so we can lookup main input and output.
         input_key = self.input_keys[0]
         output_key = self.output_keys[0]
 
         ret_record_args['main_input'] = jsonify(inputs[input_key])
 
@@ -214,28 +310,27 @@
 
         ret_record = self._post_record(
             ret_record_args, error, cost, start_time, end_time, record
         )
 
         return ret, ret_record
 
-    # langchain.chains.base.py:Chain requirement:
+    
     def __call__(self, *args, **kwargs) -> Dict[str, Any]:
         """
-        Wrapped call to self.app.__call__ with instrumentation. If you need to
+        Wrapped call to self.app._call with instrumentation. If you need to
         get the record, use `call_with_record` instead. 
         """
 
+        return self._call(*args, **kwargs)
+    
+    # langchain.chains.base.py:Chain requirement:
+    def _call(self, *args, **kwargs) -> Any:
         ret, _ = self.call_with_record(*args, **kwargs)
 
         return ret
 
-    # langchain.chains.base.py:Chain requirement:
-    def _call(self, *args, **kwargs) -> Any:
-        # TODO(piotrm): figure out whether the combination of _call and __call__ is
-        # working right.
+    # optional langchain.chains.base.py:Chain requirement:
+    async def _acall(self, *args, **kwargs) -> Any:
+        ret, _ = await self.acall_with_record(*args, **kwargs)
 
-        # TODO(piotrm): potentially remove this. We don't want to be
-        # wrapping/passing through all of the methods that a langchain Chain
-        # supports.
-
-        return self.app._call(*args, **kwargs)
+        return ret
```

## trulens_eval/tru_llama.py

```diff
@@ -1,15 +1,16 @@
 """
 # Llama_index instrumentation and monitoring. 
 """
 
 from datetime import datetime
 import logging
 from pprint import PrettyPrinter
-from typing import ClassVar, Sequence, Tuple
+import traceback
+from typing import ClassVar, Sequence, Tuple, Union
 
 from pydantic import Field
 
 from trulens_eval.app import App
 from trulens_eval.instruments import Instrument
 from trulens_eval.provider_apis import Endpoint
 from trulens_eval.provider_apis import OpenAIEndpoint
@@ -27,15 +28,18 @@
 logger = logging.getLogger(__name__)
 
 pp = PrettyPrinter()
 
 with OptionalImports(message=REQUIREMENT_LLAMA):
     import llama_index
     from llama_index.indices.query.base import BaseQueryEngine
-    from llama_index.response.schema import Response
+    from llama_index.chat_engine.types import BaseChatEngine
+    from llama_index.chat_engine.types import AgentChatResponse, StreamingAgentChatResponse
+    from llama_index.response.schema import Response, StreamingResponse, RESPONSE_TYPE
+    from llama_index.indices.query.schema import QueryBundle, QueryType
 
 from trulens_eval.tru_chain import LangChainInstrument
 
 
 class LlamaInstrument(Instrument):
 
     class Default:
@@ -76,14 +80,30 @@
                     lambda o: isinstance(
                         o, llama_index.llm_predictor.base.BaseLLMPredictor
                     ),
                 "query":
                     lambda o: isinstance(
                         o, llama_index.indices.query.base.BaseQueryEngine
                     ),
+                "aquery":
+                    lambda o: isinstance(
+                        o, llama_index.indices.query.base.BaseQueryEngine
+                    ),
+                "chat":
+                    lambda o:
+                    isinstance(o, llama_index.chat_engine.types.BaseChatEngine),
+                "achat":
+                    lambda o:
+                    isinstance(o, llama_index.chat_engine.types.BaseChatEngine),
+                "stream_chat":
+                    lambda o:
+                    isinstance(o, llama_index.chat_engine.types.BaseChatEngine),
+                "astream_achat":
+                    lambda o:
+                    isinstance(o, llama_index.chat_engine.types.BaseChatEngine),
                 "retrieve":
                     lambda o: isinstance(
                         o, (
                             llama_index.indices.query.base.BaseQueryEngine,
                             llama_index.indices.base_retriever.BaseRetriever
                         )
                     ),
@@ -92,36 +112,41 @@
                         o, llama_index.indices.query.base.BaseQueryEngine
                     ),
             }, LangChainInstrument.Default.METHODS
         )
 
     def __init__(self):
         super().__init__(
-            root_method=TruLlama.query_with_record,
+            root_methods=set(
+                [
+                    TruLlama.query_with_record, TruLlama.aquery_with_record,
+                    TruLlama.chat_with_record, TruLlama.achat_with_record
+                ]
+            ),
             modules=LlamaInstrument.Default.MODULES,
             classes=LlamaInstrument.Default.CLASSES(),  # was thunk
             methods=LlamaInstrument.Default.METHODS
         )
 
 
 class TruLlama(App):
     """
     Wrap a llama index engine for monitoring.
 
     Arguments:
-    - app: RetrieverQueryEngine -- the engine to wrap.
+    - app: BaseQueryEngine | BaseChatEngine -- the engine to wrap.
     - More args in App
     - More args in AppDefinition
     - More args in WithClassInfo
     """
 
     class Config:
         arbitrary_types_allowed = True
 
-    app: BaseQueryEngine
+    app: Union[BaseQueryEngine, BaseChatEngine]
 
     root_callable: ClassVar[FunctionOrMethod] = Field(
         default_factory=lambda: FunctionOrMethod.of_callable(TruLlama.query),
         const=True
     )
 
     def __init__(self, app: BaseQueryEngine, **kwargs):
@@ -131,59 +156,435 @@
         # TruLlama specific:
         kwargs['app'] = app
         kwargs['root_class'] = Class.of_object(app)  # TODO: make class property
         kwargs['instrument'] = LlamaInstrument()
 
         super().__init__(**kwargs)
 
-    def query(self, *args, **kwargs) -> Response:
-        res, _ = self.query_with_record(*args, **kwargs)
-        return res
-
     @classmethod
     def select_source_nodes(cls) -> JSONPath:
         """
         Get the path to the source nodes in the query output.
         """
         return cls.select_outputs().source_nodes[:]
 
-    def query_with_record(self, str_or_query_bundle) -> Tuple[Response, Record]:
+    # llama_index.chat_engine.types.BaseChatEngine
+    def chat(self, *args,
+             **kwargs) -> AgentChatResponse:
+        assert isinstance(
+            self.app, llama_index.chat_engine.types.BaseChatEngine
+        )
+
+        res, _ = self.chat_with_record(*args, **kwargs)
+        return res
+
+    # llama_index.chat_engine.types.BaseChatEngine
+    async def achat(self, *args, **kwargs) -> AgentChatResponse:
+        assert isinstance(
+            self.app, llama_index.chat_engine.types.BaseChatEngine
+        )
+
+        res, _ = await self.achat_with_record(*args, **kwargs)
+        return res
+
+    # llama_index.chat_engine.types.BaseChatEngine
+    def stream_chat(self, *args,
+             **kwargs) -> StreamingAgentChatResponse:
+        assert isinstance(
+            self.app, llama_index.chat_engine.types.BaseChatEngine
+        )
+
+        res, _ = self.stream_chat_with_record(*args, **kwargs)
+        return res
+
+    # llama_index.chat_engine.types.BaseChatEngine
+    async def astream_chat(self, *args, **kwargs) -> StreamingAgentChatResponse:
+        assert isinstance(
+            self.app, llama_index.chat_engine.types.BaseChatEngine
+        )
+
+        res, _ = await self.astream_chat_with_record(*args, **kwargs)
+        return res
+
+    # llama_index.indices.query.base.BaseQueryEngine
+    def query(self, *args, **kwargs) -> RESPONSE_TYPE:
+        assert isinstance(
+            self.app, llama_index.indices.query.base.BaseQueryEngine
+        )
+
+        res, _ = self.query_with_record(*args, **kwargs)
+        return res
+
+    # llama_index.indices.query.base.BaseQueryEngine
+    async def aquery(self, *args, **kwargs) -> RESPONSE_TYPE:
+        assert isinstance(
+            self.app, llama_index.indices.query.base.BaseQueryEngine
+        )
+
+        res, _ = await self.aquery_with_record(*args, **kwargs)
+        return res
+
+    # Mirrors llama_index.indices.query.base.BaseQueryEngine.query .
+    def query_with_record(
+        self, str_or_query_bundle: QueryType
+    ) -> Tuple[RESPONSE_TYPE, Record]:
+        assert isinstance(
+            self.app, llama_index.indices.query.base.BaseQueryEngine
+        )
+
         # Wrapped calls will look this up by traversing the call stack. This
         # should work with threads.
         record: Sequence[RecordAppCall] = []
 
         ret = None
         error = None
 
         start_time = None
         end_time = None
 
         cost = Cost()
 
         try:
             start_time = datetime.now()
+
             ret, cost = Endpoint.track_all_costs_tally(
                 lambda: self.app.query(str_or_query_bundle)
             )
 
             end_time = datetime.now()
 
         except BaseException as e:
             end_time = datetime.now()
             error = e
             logger.error(f"Engine raised an exception: {e}")
+            logger.error(traceback.format_exc())
 
         assert len(record) > 0, "No information recorded in call."
 
         ret_record_args = dict()
 
         # TODO: generalize
         ret_record_args['main_input'] = str_or_query_bundle
         if ret is not None:
-            # TODO: generalize and error check
+
+            if isinstance(ret, Response):
+                ret_record_args['main_output'] = ret.response
+
+                ret_record = self._post_record(
+                    ret_record_args, error, cost, start_time, end_time, record
+                )
+
+                return ret, ret_record
+
+            elif isinstance(ret, StreamingResponse):
+                # Need to arrange the rest of this method to be called after the
+                # stream is complete. For now lets create a record of things as
+                # they are when the stream is created, but not completed.
+
+                logger.warn(
+                    "App produced a streaming response. "
+                    "Tracking content of streams in llama_index is not yet supported."
+                )
+
+                ret_record_args['main_output'] = None
+
+                ret_record = self._post_record(
+                    ret_record_args, error, cost, start_time, end_time, record
+                )
+
+                return ret, ret_record
+
+    # Mirrors llama_index.indices.query.base.BaseQueryEngine.aquery .
+    async def aquery_with_record(
+        self, str_or_query_bundle: QueryType
+    ) -> Tuple[RESPONSE_TYPE, Record]:
+        assert isinstance(
+            self.app, llama_index.indices.query.base.BaseQueryEngine
+        )
+
+        # Wrapped calls will look this up by traversing the call stack. This
+        # should work with threads.
+        record: Sequence[RecordAppCall] = []
+
+        ret = None
+        error = None
+
+        start_time = None
+        end_time = None
+
+        cost = Cost()
+
+        try:
+            start_time = datetime.now()
+
+            ret, cost = await Endpoint.atrack_all_costs_tally(
+                lambda: self.app.aquery(str_or_query_bundle)
+            )
+
+            end_time = datetime.now()
+
+        except BaseException as e:
+            end_time = datetime.now()
+            error = e
+            logger.error(f"Engine raised an exception: {e}")
+            logger.error(traceback.format_exc())
+
+        assert len(record) > 0, "No information recorded in call."
+
+        ret_record_args = dict()
+
+        # TODO: generalize
+        ret_record_args['main_input'] = str_or_query_bundle
+
+        if isinstance(ret, Response):
             ret_record_args['main_output'] = ret.response
 
+            ret_record = self._post_record(
+                ret_record_args, error, cost, start_time, end_time, record
+            )
+
+            return ret, ret_record
+
+        elif isinstance(ret, StreamingResponse):
+            # Need to arrange the rest of this method to be called after the
+            # stream is complete. For now lets create a record of things as
+            # they are when the stream is created, but not completed.
+
+            logger.warn(
+                "App produced a streaming response. "
+                "Tracking content of streams in llama_index is not yet supported."
+            )
+
+            ret_record_args['main_output'] = None
+
+            ret_record = self._post_record(
+                ret_record_args, error, cost, start_time, end_time, record
+            )
+
+            return ret, ret_record
+
+    # Compatible with llama_index.chat_engine.types.BaseChatEngine.chat .
+    def chat_with_record(self, message: str,
+                         **kwargs) -> Tuple[AgentChatResponse, Record]:
+        assert isinstance(
+            self.app, llama_index.chat_engine.types.BaseChatEngine
+        )
+
+        # Wrapped calls will look this up by traversing the call stack. This
+        # should work with threads.
+        record: Sequence[RecordAppCall] = []
+
+        ret = None
+        error = None
+
+        start_time = None
+        end_time = None
+
+        cost = Cost()
+
+        try:
+            start_time = datetime.now()
+
+            ret, cost = Endpoint.track_all_costs_tally(
+                lambda: self.app.chat(message, **kwargs)
+            )
+
+            end_time = datetime.now()
+
+        except BaseException as e:
+            end_time = datetime.now()
+            error = e
+            logger.error(f"Engine raised an exception: {e}")
+            logger.error(traceback.format_exc())
+
+        assert len(record) > 0, "No information recorded in call."
+
+        ret_record_args = dict()
+
+        # TODO: generalize
+        ret_record_args['main_input'] = message
+
+        assert isinstance(ret, AgentChatResponse)
+
+        ret_record_args['main_output'] = ret.response
+
+        ret_record = self._post_record(
+            ret_record_args, error, cost, start_time, end_time, record
+        )
+
+        return ret, ret_record
+
+
+    # Compatible with llama_index.chat_engine.types.BaseChatEngine.achat .
+    async def achat_with_record(
+        self, message: str, **kwargs
+    ) -> Tuple[AgentChatResponse, Record]:
+        assert isinstance(
+            self.app, llama_index.chat_engine.types.BaseChatEngine
+        )
+
+        # Wrapped calls will look this up by traversing the call stack. This
+        # should work with threads.
+        record: Sequence[RecordAppCall] = []
+
+        ret = None
+        error = None
+
+        start_time = None
+        end_time = None
+
+        cost = Cost()
+
+        try:
+            start_time = datetime.now()
+
+            ret, cost = await Endpoint.atrack_all_costs_tally(
+                lambda: self.app.achat(message, **kwargs)
+            )
+
+            end_time = datetime.now()
+
+        except BaseException as e:
+            end_time = datetime.now()
+            error = e
+            logger.error(f"Engine raised an exception: {e}")
+            logger.error(traceback.format_exc())
+
+        assert len(record) > 0, "No information recorded in call."
+
+        ret_record_args = dict()
+
+        # TODO: generalize
+        ret_record_args['main_input'] = message
+
+        assert isinstance(ret, AgentChatResponse)
+
+        ret_record_args['main_output'] = ret.response
+
+        ret_record = self._post_record(
+            ret_record_args, error, cost, start_time, end_time, record
+        )
+
+        return ret, ret_record
+
+    # Compatible with llama_index.chat_engine.types.BaseChatEngine.stream_chat .
+    def stream_chat_with_record(
+        self, message: str, **kwargs
+    ) -> Tuple[StreamingAgentChatResponse, Record]:
+        assert isinstance(
+            self.app, llama_index.chat_engine.types.BaseChatEngine
+        )
+
+        # Wrapped calls will look this up by traversing the call stack. This
+        # should work with threads.
+        record: Sequence[RecordAppCall] = []
+
+        ret = None
+        error = None
+
+        start_time = None
+        end_time = None
+
+        cost = Cost()
+
+        try:
+            start_time = datetime.now()
+
+            ret, cost = Endpoint.track_all_costs_tally(
+                lambda: self.app.stream_chat(message, **kwargs)
+            )
+
+            end_time = datetime.now()
+
+        except BaseException as e:
+            end_time = datetime.now()
+            error = e
+            logger.error(f"Engine raised an exception: {e}")
+            logger.error(traceback.format_exc())
+
+        assert len(record) > 0, "No information recorded in call."
+
+        ret_record_args = dict()
+
+        # TODO: generalize
+        ret_record_args['main_input'] = message
+
+        assert isinstance(ret, StreamingAgentChatResponse)
+        # Need to arrange the rest of this method to be called after the
+        # stream is complete. For now lets create a record of things as
+        # they are when the stream is created, but not completed.
+
+        logger.warn(
+            "App produced a streaming response. "
+            "Tracking content of streams in llama_index is not yet supported."
+        )
+
+        ret_record_args['main_output'] = None
+
+        ret_record = self._post_record(
+            ret_record_args, error, cost, start_time, end_time, record
+        )
+
+        return ret, ret_record
+
+
+
+    # Compatible with llama_index.chat_engine.types.BaseChatEngine.astream_chat .
+    async def astream_chat_with_record(
+        self, message: str, **kwargs
+    ) -> Tuple[StreamingAgentChatResponse, Record]:
+        assert isinstance(
+            self.app, llama_index.chat_engine.types.BaseChatEngine
+        )
+
+        # Wrapped calls will look this up by traversing the call stack. This
+        # should work with threads.
+        record: Sequence[RecordAppCall] = []
+
+        ret = None
+        error = None
+
+        start_time = None
+        end_time = None
+
+        cost = Cost()
+
+        try:
+            start_time = datetime.now()
+
+            ret, cost = await Endpoint.atrack_all_costs_tally(
+                lambda: self.app.astream_chat(message, **kwargs)
+            )
+
+            end_time = datetime.now()
+
+        except BaseException as e:
+            end_time = datetime.now()
+            error = e
+            logger.error(f"Engine raised an exception: {e}")
+            logger.error(traceback.format_exc())
+
+        assert len(record) > 0, "No information recorded in call."
+
+        ret_record_args = dict()
+
+        # TODO: generalize
+        ret_record_args['main_input'] = message
+
+        assert isinstance(ret, StreamingAgentChatResponse)
+        # Need to arrange the rest of this method to be called after the
+        # stream is complete. For now lets create a record of things as
+        # they are when the stream is created, but not completed.
+
+        logger.warn(
+            "App produced a streaming response. "
+            "Tracking content of streams in llama_index is not yet supported."
+        )
+
+        ret_record_args['main_output'] = None
+
         ret_record = self._post_record(
             ret_record_args, error, cost, start_time, end_time, record
         )
 
         return ret, ret_record
+
```

## trulens_eval/util.py

```diff
@@ -50,28 +50,30 @@
 
 from merkle_json import MerkleJson
 from munch import Munch as Bunch
 import pandas as pd
 import pydantic
 
 from trulens_eval.keys import redact_value
+from trulens_eval.utils.python import stack_with_tasks
 
 logger = logging.getLogger(__name__)
 pp = PrettyPrinter()
 
 T = TypeVar("T")
 
 # Optional requirements.
 
-langchain_version = "0.0.230"
-
+llama_version = "0.7.16"
 REQUIREMENT_LLAMA = (
-    "llama_index 0.6.24 or above is required for instrumenting llama_index apps. "
-    "Please install it before use: `pip install llama_index>=0.7.0`."
+    f"llama_index {llama_version} or above is required for instrumenting llama_index apps. "
+    f"Please install it before use: `pip install llama_index>={llama_version}`."
 )
+
+langchain_version = "0.0.230"
 REQUIREMENT_LANGCHAIN = (
     f"langchain {langchain_version} or above is required for instrumenting langchain apps. "
     f"Please install it before use: `pip install langchain>={langchain_version}`."
 )
 
 
 class Dummy(object):
@@ -919,15 +921,15 @@
 
         for i in self.items:
             obj[i] = val
 
         return obj
 
     def __repr__(self):
-        return f"[{','.join(self.indices)}]"
+        return f"[{','.join(self.items)}]"
 
 
 class JSONPath(SerialModel):
     """
     Utilitiy class for building JSONPaths.
 
     Usage:
@@ -1197,40 +1199,40 @@
     locals.
 
     This method works across threads as long as they are started using the TP
     class above.
 
     """
 
-    frames = stack()[offset + 1:]  # + 1 to skip this method itself
+    frames = stack_with_tasks()[offset + 1:]  # + 1 to skip this method itself
 
     # Using queue for frames as additional frames may be added due to handling threads.
     q = Queue()
     for f in frames:
         q.put(f)
 
     while not q.empty():
-        fi = q.get()
+        f = q.get()
 
-        logger.debug(f"{fi.frame.f_code}")
+        logger.debug(f"{f.f_code}")
 
-        if id(fi.frame.f_code) == id(_future_target_wrapper.__code__):
+        if id(f.f_code) == id(_future_target_wrapper.__code__):
             logger.debug(
                 "Found thread starter frame. "
                 "Will walk over frames prior to thread start."
             )
-            locs = fi.frame.f_locals
+            locs = f.f_locals
             assert "pre_start_stack" in locs, "Pre thread start stack expected but not found."
-            for f in locs['pre_start_stack']:
-                q.put(f)
+            for fi in locs['pre_start_stack']:
+                q.put(fi.frame)
             continue
 
-        if func(fi.frame.f_code):
-            logger.debug(f"looking via {func.__name__}; found {fi}")
-            locs = fi.frame.f_locals
+        if func(f.f_code):
+            logger.debug(f"looking via {func.__name__}; found {f}")
+            locs = f.f_locals
             if key in locs:
                 return locs[key]
             else:
                 raise RuntimeError(f"No local named {key} found.")
 
     return None
```

## trulens_eval/utils/python.py

```diff
@@ -1,9 +1,107 @@
 """
 Utilities related to core python functionalities.
 """
 
-from inspect import stack
+import inspect
+import asyncio
+from typing import Callable, Sequence, TypeVar
 
+T = TypeVar("T")
+Thunk = Callable[[], T]
 
-def caller_frame(offset=0):
-    return stack()[offset + 1].frame
+
+def caller_frame(offset=0) -> 'frame':
+    """
+    Get the caller's (of this function) frame. See
+    https://docs.python.org/3/reference/datamodel.html#frame-objects .
+    """
+
+    return inspect.stack()[offset + 1].frame
+
+
+STACK = "__tru_stack"
+
+
+def task_factory_with_stack(loop, coro, *args, **kwargs) -> Sequence['frame']:
+    """
+    A task factory that annotates created tasks with stacks of their parents.
+    """
+
+    parent_task = asyncio.current_task(loop=loop)
+    task = asyncio.tasks.Task(coro=coro, loop=loop, *args, **kwargs)
+
+    stack = [fi.frame for fi in inspect.stack()[2:]]
+
+    if parent_task is not None:
+        stack = merge_stacks(stack, parent_task.get_stack()[::-1])
+        # skipping create_task and task_factory
+
+    setattr(task, STACK, stack)
+
+    return task
+
+# Instrument new_event_loop to set the above task_factory upon creation:
+original_new_event_loop = asyncio.events.new_event_loop
+
+def _new_event_loop():
+    loop = original_new_event_loop()
+    loop.set_task_factory(task_factory_with_stack)
+    return loop
+
+asyncio.events.new_event_loop = _new_event_loop
+
+def get_task_stack(task: asyncio.Task) -> Sequence['frame']:
+    """
+    Get the annotated stack (if available) on the given task.
+    """
+    if hasattr(task, STACK):
+        return getattr(task, STACK)
+    else:
+        # get_stack order is reverse of inspect.stack:
+        return task.get_stack()[::-1]
+
+
+def merge_stacks(s1: Sequence['frame'],
+                 s2: Sequence['frame']) -> Sequence['frame']:
+    """
+    Assuming `s1` is a subset of `s2`, combine the two stacks in presumed call
+    order.
+    """
+
+    ret = []
+
+    while len(s1) > 1:
+        f = s1[0]
+        s1 = s1[1:]
+
+        ret.append(f)
+        try:
+            s2i = s2.index(f)
+            for _ in range(s2i):
+                ret.append(s2[0])
+                s2 = s2[1:]
+
+        except:
+            pass
+
+    return ret
+
+
+def stack_with_tasks() -> Sequence['frame']:
+    """
+    Get the current stack (not including this function) with frames reaching
+    across Tasks.
+    """
+
+    ret = [fi.frame for fi in inspect.stack()[1:]] # skip stack_with_task_stack
+
+    try:
+        task_stack = get_task_stack(asyncio.current_task())
+
+        return merge_stacks(
+            ret,  
+            task_stack
+        )
+    
+    except:
+        return ret
```

## Comparing `trulens_eval-0.7.0a0.dist-info/METADATA` & `trulens_eval-0.8.0a0.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: trulens-eval
-Version: 0.7.0a0
+Version: 0.8.0a0
 Summary: Library with langchain instrumentation to evaluate LLM based applications.
 Home-page: https://www.trulens.org
 Author: Truera Inc
 Author-email: all@truera.com
 License: MIT
 Classifier: Programming Language :: Python :: 3
 Classifier: Operating System :: OS Independent
@@ -13,15 +13,15 @@
 Requires-Python: >=3.8
 Description-Content-Type: text/markdown
 Requires-Dist: cohere (>=4.4.1)
 Requires-Dist: datasets (>=2.12.0)
 Requires-Dist: python-dotenv (>=1.0.0)
 Requires-Dist: kaggle (>=1.5.13)
 Requires-Dist: langchain (>=0.0.230)
-Requires-Dist: llama-index (>=0.7.0)
+Requires-Dist: llama-index (>=0.7.16)
 Requires-Dist: merkle-json (>=1.0.0)
 Requires-Dist: millify (>=0.1.1)
 Requires-Dist: openai (>=0.27.6)
 Requires-Dist: pinecone-client (>=2.2.1)
 Requires-Dist: pydantic (>=1.10.7)
 Requires-Dist: requests (>=2.30.0)
 Requires-Dist: slack-bolt (>=1.18.0)
@@ -50,32 +50,32 @@
 
 ## Quick Usage
 
 To quickly play around with the TruLens Eval library:
 
 Langchain:
 
-[langchain_quickstart.ipynb](https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.7.0/trulens_eval/examples/quickstart.ipynb).
-[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/releases/rc-trulens-eval-0.7.0/trulens_eval/examples/colab/quickstarts/langchain_quickstart_colab.ipynb)
+[langchain_quickstart.ipynb](https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.8.0/trulens_eval/examples/quickstart.ipynb).
+[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/releases/rc-trulens-eval-0.8.0/trulens_eval/examples/colab/quickstarts/langchain_quickstart_colab.ipynb)
 
-[langchain_quickstart.py](https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.7.0/trulens_eval/examples/quickstart.py).
+[langchain_quickstart.py](https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.8.0/trulens_eval/examples/quickstart.py).
 
 Llama Index: 
 
-[llama_index_quickstart.ipynb](https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.7.0/trulens_eval/examples/frameworks/llama_index/llama_index_quickstart.ipynb).
-[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/releases/rc-trulens-eval-0.7.0/trulens_eval/examples/colab/quickstarts/llama_index_quickstart_colab.ipynb)
+[llama_index_quickstart.ipynb](https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.8.0/trulens_eval/examples/frameworks/llama_index/llama_index_quickstart.ipynb).
+[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/releases/rc-trulens-eval-0.8.0/trulens_eval/examples/colab/quickstarts/llama_index_quickstart_colab.ipynb)
 
-[llama_index_quickstart.py](https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.7.0/trulens_eval/examples/llama_index_quickstart.py)
+[llama_index_quickstart.py](https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.8.0/trulens_eval/examples/llama_index_quickstart.py)
 
 No Framework: 
 
-[no_framework_quickstart.ipynb](https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.7.0/trulens_eval/examples/no_framework_quickstart.ipynb).
-[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/releases/rc-trulens-eval-0.7.0/trulens_eval/examples/colab/quickstarts/no_framework_quickstart_colab.ipynb)
+[no_framework_quickstart.ipynb](https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.8.0/trulens_eval/examples/no_framework_quickstart.ipynb).
+[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/releases/rc-trulens-eval-0.8.0/trulens_eval/examples/colab/quickstarts/no_framework_quickstart_colab.ipynb)
 
-[no_framework_quickstart.py](https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.7.0/trulens_eval/examples/no_framework_quickstart.py)
+[no_framework_quickstart.py](https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.8.0/trulens_eval/examples/no_framework_quickstart.py)
 
 ### 💡 Contributing
 
 Interested in contributing? See our [contribution guide](https://github.com/truera/trulens/tree/main/trulens_eval/CONTRIBUTING.md) for more details.
 
 ## Installation and Setup
 
@@ -103,14 +103,16 @@
 ```
 
 
 # Quickstart
 
 In this quickstart you will create a simple LLM Chain and learn how to log it and get feedback on an LLM response.
 
+[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/trulens_eval/examples/quickstart.ipynb)
+
 ## Setup
 ### Add API keys
 For this quickstart you will need Open AI and Huggingface keys
 
 
 ```python
 import os
```

## Comparing `trulens_eval-0.7.0a0.dist-info/RECORD` & `trulens_eval-0.8.0a0.dist-info/RECORD`

 * *Files 10% similar despite different names*

```diff
@@ -1,42 +1,42 @@
 trulens_eval/Example_TruBot.py,sha256=Sp3s8hNLkijLaML0xKZt0IKHUbShq-iki8sx34NvNfQ,5185
-trulens_eval/Leaderboard.py,sha256=51fLj50WFg9yodAvj41VivpKUYFlL4BJHyHHtGKpmQM,3475
-trulens_eval/__init__.py,sha256=7sDceKZS-7nuvXUQkXh5Ei2pIGtRCxbWNNQVp6vuamg,1424
-trulens_eval/app.py,sha256=1AxTgoOmf_EG2hCggyI-ZF8CvcJ87pobzG-tjhgVEnM,12040
-trulens_eval/db.py,sha256=K-hMAqkwZcZeUdkSyt-5Yh7RuffxGKaUN-TWsK5jk4A,21224
+trulens_eval/Leaderboard.py,sha256=xstL0NzkzBUcA6frXBWYLGhfgBwJeLk7b-tvLSlur6k,3933
+trulens_eval/__init__.py,sha256=n9vGRSDWEHhihztonzN7NqapGkiFY8VBBG-LUYvJDb4,1424
+trulens_eval/app.py,sha256=BHX0FJqq3P8jTXmSCWYe8HcTzwq4BadyfZpg-rfUeOA,12654
+trulens_eval/db.py,sha256=W3zywuyE2P5nejJPiZoS_L59vwknAtT3CULjAj5HbWc,21226
 trulens_eval/db_migration.py,sha256=nDWmwAq2H0JmZ_I9TdK4xa2U2TA3IrwrHDCeE00cFvM,14061
-trulens_eval/feedback.py,sha256=bofpgnl6WZZ0DFs2IODElaQLLDHeOijufojiR9LG_e8,69314
+trulens_eval/feedback.py,sha256=9sWm5-5c8f_HWY25bTiRYzsmq-z7hNr1gNKyzlWbycg,69337
 trulens_eval/feedback_prompts.py,sha256=UfBZ1bDxT7xbW81qw3otFaTmkcy7x_PAKkDB-CJCH1U,7034
-trulens_eval/instruments.py,sha256=SwHs25qeNXOU87xCSxeaPKn9JXthaO4RYc-DrlwmjWI,20145
+trulens_eval/instruments.py,sha256=WwH0QBrAaj-TK6IcEp-ctdLc6dGYYDSpXWk5GRP9XfE,25142
 trulens_eval/keys.py,sha256=yYrmWk88jJhAGOfR0eN8OGGpq17BvG8ITyh7QZfr4ao,13338
-trulens_eval/provider_apis.py,sha256=rJf-N4_3DIkZ9LJ9BMsrVj91phXesHt1zJBVkNizuFU,23228
-trulens_eval/schema.py,sha256=DwPsbuYztQP1taQvG90KSVMY54HlmjQBOaPE1jiIMVI,14134
-trulens_eval/tru.py,sha256=rPU2TCHdVXl1DxESlVOBr19gqFb1pIF3DQn2e5aBsnA,16238
+trulens_eval/provider_apis.py,sha256=uwmOmCn8bXXbmPqvZVRNucEKjVVMLqMWT8fFB8Ldyfc,35158
+trulens_eval/schema.py,sha256=usa9f06SP2jRyuHFDyPI4_G8D4m6at3PqTfqy7V1YLc,14256
+trulens_eval/tru.py,sha256=ZF0_kIhIMOsnWD29tgfCObJE8npGLuCsNFBQ_fRbxhI,16348
 trulens_eval/tru_app.py,sha256=QhY0tbhif5MfkzeMzv_uwi2Dqr_POVljg-9woBq2Ep8,293
-trulens_eval/tru_basic_app.py,sha256=01sBriY4MmZ0CrogkNa6WEVaSrHj38GtpocMiCSeFCE,3545
-trulens_eval/tru_chain.py,sha256=E-96_SbJoAmpW5MDuqNBLxol0zQ1B_NOk1FOdSyBUcs,7819
+trulens_eval/tru_basic_app.py,sha256=VD1DmDCVJgji3eSeMM_kZHqnjZE7RnQRu7mw6Txnih0,3553
+trulens_eval/tru_chain.py,sha256=BKPu2O7GAeYYasraQbuAvU83vrNrCRCSN5noU5ulRuo,10920
 trulens_eval/tru_db.py,sha256=_S9gtV_bizaQBaEDbayBVB6ns2j11Fi-hl_Bvy6_SXg,288
 trulens_eval/tru_feedback.py,sha256=Shc6KX33QfVZjqEF2iXg6wFmXD9ANd-l6BwhMkVR8gM,318
-trulens_eval/tru_llama.py,sha256=p_yMMVbUMii3OnFIfXOj7pTFDZBwbRxtboae_i0uzas,6321
-trulens_eval/util.py,sha256=ZTQtjLRP4S8QEi6ExmkF7KmfTiSRvtMofFnRrY0qEl8,47761
+trulens_eval/tru_llama.py,sha256=WCmYF_BQ4OYPQOpqByYrTJkSAo_-DHUXZKi9hwB3X-8,18967
+trulens_eval/util.py,sha256=lKiNmDI71OmZzTsB20kg9pmNiOnpMM-GmnbEvIKorLo,47842
 trulens_eval/pages/Evaluations.py,sha256=jYMfs72B57N5tf6uIqvYt6QdVtQ1pGWLnrfsS1_-Tb4,13110
 trulens_eval/pages/Progress.py,sha256=uuR_ZkEf-yK-WxAljqHR5Q70mZOhsrklCe2VKVZ_YR4,1123
 trulens_eval/react_components/record_viewer/__init__.py,sha256=HC-OeLHjf2ULICAnIVrWP1iO4xfgZDqJKrvzIKUyV7Y,3294
 trulens_eval/react_components/record_viewer/dist/index.html,sha256=j-oZ5ZwkXy3hgY_KlgFPD3JoQBMs6J7U1m6v4984Cz4,348
 trulens_eval/react_components/record_viewer/dist/assets/index-4e44137e.js,sha256=6we3ccFIORuU0VGws4TPrGwOSDpKDxyvzraJ_Fkni3Q,500498
 trulens_eval/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 trulens_eval/utils/command_line.py,sha256=K2vRIMp03IJhnevkxDnlXbn5Rux4TMXzIDgkqcePLTU,83
 trulens_eval/utils/langchain.py,sha256=y3by0IT0dYEIEqZVt3Px4deknj2YA9-jKapRd95ywMg,5892
 trulens_eval/utils/llama.py,sha256=TcFE8iID-B4-FL3WX3bXlCzYyH5Nd0ypJggKpAF9Dks,4831
 trulens_eval/utils/notebook_utils.py,sha256=QTB2tedjSNF5d25sfrJEg20aqcK3Kx3MfcteeWcRzxQ,1001
-trulens_eval/utils/python.py,sha256=CJq1mYK5Rxxoa6_el1igsAte00AtpHuGabGdwveQWpQ,151
+trulens_eval/utils/python.py,sha256=_R-EFL0QovPtrSmFNOYy3wD_mHUdTyOgT0LIUxgjK-4,2512
 trulens_eval/utils/text.py,sha256=ExH7jnTFNsbR5gUtv98vHy-QVGoaQ4BxYtB7IWpGAmo,166
 trulens_eval/utils/trulens.py,sha256=9NQOctB0_qRONaKModZq3bPWOORMLvysAEjwt4BA1l8,927
 trulens_eval/ux/add_logo.py,sha256=lIpLNwqSGbfSP2Td6VEE6s0A1-9x4vFfXpJUxY7BFdQ,1212
 trulens_eval/ux/components.py,sha256=Laxhj4a2ZT9qwm7CCBfcVHY3XVEExK79iTFkk2KbVnA,6421
 trulens_eval/ux/styles.py,sha256=9w4J6DQNDoLuxrNQYbnJmBXC0hgLJx6def2-LHssBbk,2307
 trulens_eval/ux/trulens_logo.svg,sha256=92RLTgG0YDPEtZcQWWI7aXTYZAW4wAOAkIIgKUbTiW8,29567
-trulens_eval-0.7.0a0.dist-info/METADATA,sha256=wcD_qwB7BAlopElkGgaB9VDar9Eeh8bfWkh04euyIDA,18744
-trulens_eval-0.7.0a0.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-trulens_eval-0.7.0a0.dist-info/entry_points.txt,sha256=EpSmkbk1fF0UH-djUia4lE2hzg1oMt1QvaVxA7SfZmo,70
-trulens_eval-0.7.0a0.dist-info/top_level.txt,sha256=AKIBExe5S-v-TbsBrhe1ctF06PubKoYmWNS9rJ1Rb_o,13
-trulens_eval-0.7.0a0.dist-info/RECORD,,
+trulens_eval-0.8.0a0.dist-info/METADATA,sha256=ENd15H5oAuy1c22Z9KX2cH1iSHxpIMrHuAkIiqarJEs,18929
+trulens_eval-0.8.0a0.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
+trulens_eval-0.8.0a0.dist-info/entry_points.txt,sha256=EpSmkbk1fF0UH-djUia4lE2hzg1oMt1QvaVxA7SfZmo,70
+trulens_eval-0.8.0a0.dist-info/top_level.txt,sha256=AKIBExe5S-v-TbsBrhe1ctF06PubKoYmWNS9rJ1Rb_o,13
+trulens_eval-0.8.0a0.dist-info/RECORD,,
```

