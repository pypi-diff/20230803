# Comparing `tmp/bitfount-0.5.86.tar.gz` & `tmp/bitfount-0.5.9.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "bitfount-0.5.86.tar", last modified: Thu Aug  3 12:52:49 2023, max compression
+gzip compressed data, was "bitfount-0.5.9.tar", last modified: Fri Feb  4 10:43:34 2022, max compression
```

## Comparing `bitfount-0.5.86.tar` & `bitfount-0.5.9.tar`

### file list

```diff
@@ -1,451 +1,335 @@
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.486227 bitfount-0.5.86/
--rw-r--r--   0 runner    (1001) docker     (123)    10932 2023-08-03 12:52:36.000000 bitfount-0.5.86/LICENSE
--rw-r--r--   0 runner    (1001) docker     (123)      839 2023-08-03 12:52:36.000000 bitfount-0.5.86/MANIFEST.in
--rw-r--r--   0 runner    (1001) docker     (123)    10330 2023-08-03 12:52:49.486227 bitfount-0.5.86/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)     8496 2023-08-03 12:52:36.000000 bitfount-0.5.86/README.md
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.406226 bitfount-0.5.86/bitfount/
--rw-r--r--   0 runner    (1001) docker     (123)     7790 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      526 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/__version__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.406226 bitfount-0.5.86/bitfount/backends/
--rw-r--r--   0 runner    (1001) docker     (123)      313 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/backends/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.406226 bitfount-0.5.86/bitfount/backends/lightgbm/
--rw-r--r--   0 runner    (1001) docker     (123)      398 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/backends/lightgbm/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.406226 bitfount-0.5.86/bitfount/backends/lightgbm/models/
--rw-r--r--   0 runner    (1001) docker     (123)       45 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/backends/lightgbm/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    10460 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/backends/lightgbm/models/base_models.py
--rw-r--r--   0 runner    (1001) docker     (123)     1424 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/backends/lightgbm/models/models.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.410225 bitfount-0.5.86/bitfount/backends/pytorch/
--rw-r--r--   0 runner    (1001) docker     (123)     1336 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/backends/pytorch/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2746 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/backends/pytorch/_torch_shims.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.410225 bitfount-0.5.86/bitfount/backends/pytorch/data/
--rw-r--r--   0 runner    (1001) docker     (123)       47 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/backends/pytorch/data/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3377 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/backends/pytorch/data/datafactory.py
--rw-r--r--   0 runner    (1001) docker     (123)     7020 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/backends/pytorch/data/dataloaders.py
--rw-r--r--   0 runner    (1001) docker     (123)      747 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/backends/pytorch/data/datasets.py
--rw-r--r--   0 runner    (1001) docker     (123)     2100 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/backends/pytorch/data/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.410225 bitfount-0.5.86/bitfount/backends/pytorch/federated/
--rw-r--r--   0 runner    (1001) docker     (123)       52 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/backends/pytorch/federated/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     9263 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/backends/pytorch/federated/mixins.py
--rw-r--r--   0 runner    (1001) docker     (123)     3399 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/backends/pytorch/federated/shim.py
--rw-r--r--   0 runner    (1001) docker     (123)     5236 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/backends/pytorch/loss.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.410225 bitfount-0.5.86/bitfount/backends/pytorch/models/
--rw-r--r--   0 runner    (1001) docker     (123)       44 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/backends/pytorch/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    68957 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/backends/pytorch/models/base_models.py
--rw-r--r--   0 runner    (1001) docker     (123)    25098 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/backends/pytorch/models/bitfount_model.py
--rw-r--r--   0 runner    (1001) docker     (123)    26374 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/backends/pytorch/models/models.py
--rw-r--r--   0 runner    (1001) docker     (123)    13945 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/backends/pytorch/models/nn.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.414226 bitfount-0.5.86/bitfount/backends/pytorch/models/torch_functions/
--rw-r--r--   0 runner    (1001) docker     (123)       83 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/backends/pytorch/models/torch_functions/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1440 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/backends/pytorch/models/torch_functions/mish.py
--rw-r--r--   0 runner    (1001) docker     (123)     1940 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/backends/pytorch/types.py
--rw-r--r--   0 runner    (1001) docker     (123)     5988 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/backends/pytorch/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     2277 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/backends/pytorch/weight_clipper.py
--rw-r--r--   0 runner    (1001) docker     (123)     9944 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/config.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.414226 bitfount-0.5.86/bitfount/data/
--rw-r--r--   0 runner    (1001) docker     (123)     6040 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/data/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6499 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/data/databunch.py
--rw-r--r--   0 runner    (1001) docker     (123)     5432 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/data/datafactory.py
--rw-r--r--   0 runner    (1001) docker     (123)     3029 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/data/dataloaders.py
--rw-r--r--   0 runner    (1001) docker     (123)    23079 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/data/datasets.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.418226 bitfount-0.5.86/bitfount/data/datasources/
--rw-r--r--   0 runner    (1001) docker     (123)     1623 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/data/datasources/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    25133 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/data/datasources/base_source.py
--rw-r--r--   0 runner    (1001) docker     (123)     3356 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/data/datasources/csv_source.py
--rw-r--r--   0 runner    (1001) docker     (123)    15066 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/data/datasources/database_source.py
--rw-r--r--   0 runner    (1001) docker     (123)     2489 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/data/datasources/dataframe_source.py
--rw-r--r--   0 runner    (1001) docker     (123)    11504 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/data/datasources/dicom_source.py
--rw-r--r--   0 runner    (1001) docker     (123)    10141 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/data/datasources/excel_source.py
--rw-r--r--   0 runner    (1001) docker     (123)     1486 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/data/datasources/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    22959 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/data/datasources/views.py
--rw-r--r--   0 runner    (1001) docker     (123)    16480 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/data/datasplitters.py
--rw-r--r--   0 runner    (1001) docker     (123)    32585 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/data/datastructure.py
--rw-r--r--   0 runner    (1001) docker     (123)     1819 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/data/exceptions.py
--rw-r--r--   0 runner    (1001) docker     (123)      581 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/data/helper.py
--rw-r--r--   0 runner    (1001) docker     (123)    44532 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/data/schema.py
--rw-r--r--   0 runner    (1001) docker     (123)    19981 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/data/types.py
--rw-r--r--   0 runner    (1001) docker     (123)    14618 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/data/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)      480 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/exceptions.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.422226 bitfount-0.5.86/bitfount/federated/
--rw-r--r--   0 runner    (1001) docker     (123)     3203 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.422226 bitfount-0.5.86/bitfount/federated/aggregators/
--rw-r--r--   0 runner    (1001) docker     (123)      641 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/aggregators/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    14130 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/aggregators/aggregator.py
--rw-r--r--   0 runner    (1001) docker     (123)     7390 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/aggregators/base.py
--rw-r--r--   0 runner    (1001) docker     (123)    12391 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/aggregators/secure.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.426226 bitfount-0.5.86/bitfount/federated/algorithms/
--rw-r--r--   0 runner    (1001) docker     (123)     4063 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/algorithms/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4718 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/algorithms/base.py
--rw-r--r--   0 runner    (1001) docker     (123)     3086 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/algorithms/column_avg.py
--rw-r--r--   0 runner    (1001) docker     (123)    15353 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/algorithms/compute_intersection_rsa.py
--rw-r--r--   0 runner    (1001) docker     (123)    12481 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/algorithms/csv_report_algorithm.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.426226 bitfount-0.5.86/bitfount/federated/algorithms/model_algorithms/
--rw-r--r--   0 runner    (1001) docker     (123)       62 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/algorithms/model_algorithms/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8520 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/algorithms/model_algorithms/base.py
--rw-r--r--   0 runner    (1001) docker     (123)     2344 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/algorithms/model_algorithms/evaluate.py
--rw-r--r--   0 runner    (1001) docker     (123)     7529 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/algorithms/model_algorithms/federated_training.py
--rw-r--r--   0 runner    (1001) docker     (123)     4972 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/algorithms/model_algorithms/inference.py
--rw-r--r--   0 runner    (1001) docker     (123)     2543 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/algorithms/model_algorithms/train_and_evaluate.py
--rw-r--r--   0 runner    (1001) docker     (123)    25079 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/algorithms/private_sql_query.py
--rw-r--r--   0 runner    (1001) docker     (123)     6608 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/algorithms/sql_query.py
--rw-r--r--   0 runner    (1001) docker     (123)     6802 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/algorithms/transformer_perplexity.py
--rw-r--r--   0 runner    (1001) docker     (123)     5780 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/algorithms/transformer_text_generation.py
--rw-r--r--   0 runner    (1001) docker     (123)    31501 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/authorisation_checkers.py
--rw-r--r--   0 runner    (1001) docker     (123)     2421 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/early_stopping.py
--rw-r--r--   0 runner    (1001) docker     (123)    16842 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/encryption.py
--rw-r--r--   0 runner    (1001) docker     (123)     4032 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/exceptions.py
--rw-r--r--   0 runner    (1001) docker     (123)     7528 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/helper.py
--rw-r--r--   0 runner    (1001) docker     (123)     5966 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/keys_setup.py
--rw-r--r--   0 runner    (1001) docker     (123)     9099 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/logging.py
--rw-r--r--   0 runner    (1001) docker     (123)    35921 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/mixins.py
--rw-r--r--   0 runner    (1001) docker     (123)    11555 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/model_reference.py
--rw-r--r--   0 runner    (1001) docker     (123)    25773 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/modeller.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.426226 bitfount-0.5.86/bitfount/federated/monitoring/
--rw-r--r--   0 runner    (1001) docker     (123)     1586 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/monitoring/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      805 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/monitoring/decorators.py
--rw-r--r--   0 runner    (1001) docker     (123)      515 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/monitoring/exceptions.py
--rw-r--r--   0 runner    (1001) docker     (123)     9891 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/monitoring/monitor.py
--rw-r--r--   0 runner    (1001) docker     (123)     1214 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/monitoring/types.py
--rw-r--r--   0 runner    (1001) docker     (123)    61537 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/pod.py
--rw-r--r--   0 runner    (1001) docker     (123)    21250 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/pod_db_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     1302 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/pod_response_message.py
--rw-r--r--   0 runner    (1001) docker     (123)     8967 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/pod_vitals.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.426226 bitfount-0.5.86/bitfount/federated/privacy/
--rw-r--r--   0 runner    (1001) docker     (123)       48 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/privacy/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    10170 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/privacy/differential.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.430226 bitfount-0.5.86/bitfount/federated/protocols/
--rw-r--r--   0 runner    (1001) docker     (123)     4050 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/protocols/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    26544 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/protocols/base.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.430226 bitfount-0.5.86/bitfount/federated/protocols/model_protocols/
--rw-r--r--   0 runner    (1001) docker     (123)       61 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/protocols/model_protocols/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    24440 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/protocols/model_protocols/federated_averaging.py
--rw-r--r--   0 runner    (1001) docker     (123)     7984 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/protocols/model_protocols/inference_csv_report.py
--rw-r--r--   0 runner    (1001) docker     (123)    12597 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/protocols/psi.py
--rw-r--r--   0 runner    (1001) docker     (123)    13772 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/protocols/results_only.py
--rw-r--r--   0 runner    (1001) docker     (123)     2130 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/roles.py
--rw-r--r--   0 runner    (1001) docker     (123)    13070 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/secure.py
--rw-r--r--   0 runner    (1001) docker     (123)     3619 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/shim.py
--rw-r--r--   0 runner    (1001) docker     (123)     2726 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/task_requests.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.430226 bitfount-0.5.86/bitfount/federated/transport/
--rw-r--r--   0 runner    (1001) docker     (123)      348 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/transport/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    50307 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/transport/base_transport.py
--rw-r--r--   0 runner    (1001) docker     (123)     3579 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/transport/config.py
--rw-r--r--   0 runner    (1001) docker     (123)      329 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/transport/exceptions.py
--rw-r--r--   0 runner    (1001) docker     (123)    23176 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/transport/handlers.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.434226 bitfount-0.5.86/bitfount/federated/transport/identity_verification/
--rw-r--r--   0 runner    (1001) docker     (123)      282 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/transport/identity_verification/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    20375 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/transport/identity_verification/oidc.py
--rw-r--r--   0 runner    (1001) docker     (123)    11715 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/transport/identity_verification/saml.py
--rw-r--r--   0 runner    (1001) docker     (123)     1279 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/transport/identity_verification/types.py
--rw-r--r--   0 runner    (1001) docker     (123)    47078 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/transport/message_service.py
--rw-r--r--   0 runner    (1001) docker     (123)    38127 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/transport/modeller_transport.py
--rw-r--r--   0 runner    (1001) docker     (123)     2770 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/transport/pod_transport.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.434226 bitfount-0.5.86/bitfount/federated/transport/protos/
--rw-r--r--   0 runner    (1001) docker     (123)      141 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/transport/protos/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8682 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/transport/protos/messages_pb2.py
--rw-r--r--   0 runner    (1001) docker     (123)    16440 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/transport/protos/messages_pb2_grpc.py
--rw-r--r--   0 runner    (1001) docker     (123)     2578 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/transport/types.py
--rw-r--r--   0 runner    (1001) docker     (123)     6421 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/transport/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    29656 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/transport/worker_transport.py
--rw-r--r--   0 runner    (1001) docker     (123)     4017 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/types.py
--rw-r--r--   0 runner    (1001) docker     (123)     4610 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    24803 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/federated/worker.py
--rw-r--r--   0 runner    (1001) docker     (123)    18813 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/hooks.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.434226 bitfount-0.5.86/bitfount/hub/
--rw-r--r--   0 runner    (1001) docker     (123)     1188 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/hub/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    50042 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/hub/api.py
--rw-r--r--   0 runner    (1001) docker     (123)     7282 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/hub/authentication_flow.py
--rw-r--r--   0 runner    (1001) docker     (123)    25948 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/hub/authentication_handlers.py
--rw-r--r--   0 runner    (1001) docker     (123)      772 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/hub/exceptions.py
--rw-r--r--   0 runner    (1001) docker     (123)    13467 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/hub/helper.py
--rw-r--r--   0 runner    (1001) docker     (123)    10466 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/hub/types.py
--rw-r--r--   0 runner    (1001) docker     (123)      873 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/hub/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    21005 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/metrics.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.438226 bitfount-0.5.86/bitfount/models/
--rw-r--r--   0 runner    (1001) docker     (123)     1017 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    32381 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/models/base_models.py
--rw-r--r--   0 runner    (1001) docker     (123)     3341 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/models/bitfount_model.py
--rw-r--r--   0 runner    (1001) docker     (123)    25909 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/models/models.py
--rw-r--r--   0 runner    (1001) docker     (123)      585 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/models/nn.py
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/py.typed
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.438226 bitfount-0.5.86/bitfount/runners/
--rw-r--r--   0 runner    (1001) docker     (123)      377 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/runners/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    33941 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/runners/config_schemas.py
--rw-r--r--   0 runner    (1001) docker     (123)      338 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/runners/exceptions.py
--rw-r--r--   0 runner    (1001) docker     (123)    19490 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/runners/modeller_runner.py
--rw-r--r--   0 runner    (1001) docker     (123)     8831 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/runners/pod_runner.py
--rw-r--r--   0 runner    (1001) docker     (123)    19021 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/runners/upload_task_templates.py
--rw-r--r--   0 runner    (1001) docker     (123)      132 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/runners/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.438226 bitfount-0.5.86/bitfount/schemas/
--rw-r--r--   0 runner    (1001) docker     (123)      221 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/schemas/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      189 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/schemas/exceptions.py
--rw-r--r--   0 runner    (1001) docker     (123)    13485 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/schemas/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    12561 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/storage.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.442226 bitfount-0.5.86/bitfount/transformations/
--rw-r--r--   0 runner    (1001) docker     (123)     2459 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/transformations/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4452 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/transformations/base_transformation.py
--rw-r--r--   0 runner    (1001) docker     (123)     6294 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/transformations/batch_operations.py
--rw-r--r--   0 runner    (1001) docker     (123)     2500 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/transformations/binary_operations.py
--rw-r--r--   0 runner    (1001) docker     (123)     4904 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/transformations/dataset_operations.py
--rw-r--r--   0 runner    (1001) docker     (123)     2387 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/transformations/exceptions.py
--rw-r--r--   0 runner    (1001) docker     (123)    10582 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/transformations/parser.py
--rw-r--r--   0 runner    (1001) docker     (123)    22144 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/transformations/processor.py
--rw-r--r--   0 runner    (1001) docker     (123)     3673 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/transformations/references.py
--rw-r--r--   0 runner    (1001) docker     (123)     5429 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/transformations/torchio_batch_operations.py
--rw-r--r--   0 runner    (1001) docker     (123)     8586 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/transformations/unary_operations.py
--rw-r--r--   0 runner    (1001) docker     (123)      685 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/transformations/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     8282 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/types.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.442226 bitfount-0.5.86/bitfount/utils/
--rw-r--r--   0 runner    (1001) docker     (123)    32464 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    12915 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/utils/concurrency_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     8324 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/utils/logging_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     9044 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/utils/ssl_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    20213 2023-08-03 12:52:36.000000 bitfount-0.5.86/bitfount/utils/web_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.406226 bitfount-0.5.86/bitfount.egg-info/
--rw-r--r--   0 runner    (1001) docker     (123)    10330 2023-08-03 12:52:49.000000 bitfount-0.5.86/bitfount.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)    15913 2023-08-03 12:52:49.000000 bitfount-0.5.86/bitfount.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (123)        1 2023-08-03 12:52:49.000000 bitfount-0.5.86/bitfount.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (123)       56 2023-08-03 12:52:49.000000 bitfount-0.5.86/bitfount.egg-info/entry_points.txt
--rw-r--r--   0 runner    (1001) docker     (123)     7422 2023-08-03 12:52:49.000000 bitfount-0.5.86/bitfount.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (123)       23 2023-08-03 12:52:49.000000 bitfount-0.5.86/bitfount.egg-info/top_level.txt
--rw-r--r--   0 runner    (1001) docker     (123)     2639 2023-08-03 12:52:36.000000 bitfount-0.5.86/pyproject.toml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.442226 bitfount-0.5.86/requirements/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.442226 bitfount-0.5.86/requirements/310/
--rw-r--r--   0 runner    (1001) docker     (123)     1920 2023-08-03 12:52:36.000000 bitfount-0.5.86/requirements/310/requirements-dev.in
--rw-r--r--   0 runner    (1001) docker     (123)     8612 2023-08-03 12:52:36.000000 bitfount-0.5.86/requirements/310/requirements-dev.txt
--rw-r--r--   0 runner    (1001) docker     (123)      279 2023-08-03 12:52:36.000000 bitfount-0.5.86/requirements/310/requirements-test.in
--rw-r--r--   0 runner    (1001) docker     (123)    16198 2023-08-03 12:52:36.000000 bitfount-0.5.86/requirements/310/requirements-test.txt
--rw-r--r--   0 runner    (1001) docker     (123)      747 2023-08-03 12:52:36.000000 bitfount-0.5.86/requirements/310/requirements-tutorial.in
--rw-r--r--   0 runner    (1001) docker     (123)     7829 2023-08-03 12:52:36.000000 bitfount-0.5.86/requirements/310/requirements-tutorial.txt
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.446226 bitfount-0.5.86/requirements/38/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.446226 bitfount-0.5.86/requirements/38/differential_privacy/
--rw-r--r--   0 runner    (1001) docker     (123)      487 2023-08-03 12:52:36.000000 bitfount-0.5.86/requirements/38/differential_privacy/constraints-dp.txt
--rw-r--r--   0 runner    (1001) docker     (123)      447 2023-08-03 12:52:36.000000 bitfount-0.5.86/requirements/38/differential_privacy/requirements-dp-test.in
--rw-r--r--   0 runner    (1001) docker     (123)      922 2023-08-03 12:52:36.000000 bitfount-0.5.86/requirements/38/differential_privacy/requirements-dp.in
--rw-r--r--   0 runner    (1001) docker     (123)     1893 2023-08-03 12:52:36.000000 bitfount-0.5.86/requirements/38/requirements-dev.in
--rw-r--r--   0 runner    (1001) docker     (123)     8850 2023-08-03 12:52:36.000000 bitfount-0.5.86/requirements/38/requirements-dev.txt
--rw-r--r--   0 runner    (1001) docker     (123)      279 2023-08-03 12:52:36.000000 bitfount-0.5.86/requirements/38/requirements-test.in
--rw-r--r--   0 runner    (1001) docker     (123)    16588 2023-08-03 12:52:36.000000 bitfount-0.5.86/requirements/38/requirements-test.txt
--rw-r--r--   0 runner    (1001) docker     (123)      747 2023-08-03 12:52:36.000000 bitfount-0.5.86/requirements/38/requirements-tutorial.in
--rw-r--r--   0 runner    (1001) docker     (123)     8277 2023-08-03 12:52:36.000000 bitfount-0.5.86/requirements/38/requirements-tutorial.txt
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.446226 bitfount-0.5.86/requirements/39/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.446226 bitfount-0.5.86/requirements/39/differential_privacy/
--rw-r--r--   0 runner    (1001) docker     (123)      487 2023-08-03 12:52:36.000000 bitfount-0.5.86/requirements/39/differential_privacy/constraints-dp.txt
--rw-r--r--   0 runner    (1001) docker     (123)      447 2023-08-03 12:52:36.000000 bitfount-0.5.86/requirements/39/differential_privacy/requirements-dp-test.in
--rw-r--r--   0 runner    (1001) docker     (123)      922 2023-08-03 12:52:36.000000 bitfount-0.5.86/requirements/39/differential_privacy/requirements-dp.in
--rw-r--r--   0 runner    (1001) docker     (123)     1893 2023-08-03 12:52:36.000000 bitfount-0.5.86/requirements/39/requirements-dev.in
--rw-r--r--   0 runner    (1001) docker     (123)     8591 2023-08-03 12:52:36.000000 bitfount-0.5.86/requirements/39/requirements-dev.txt
--rw-r--r--   0 runner    (1001) docker     (123)      279 2023-08-03 12:52:36.000000 bitfount-0.5.86/requirements/39/requirements-test.in
--rw-r--r--   0 runner    (1001) docker     (123)    16379 2023-08-03 12:52:36.000000 bitfount-0.5.86/requirements/39/requirements-test.txt
--rw-r--r--   0 runner    (1001) docker     (123)      747 2023-08-03 12:52:36.000000 bitfount-0.5.86/requirements/39/requirements-tutorial.in
--rw-r--r--   0 runner    (1001) docker     (123)     8144 2023-08-03 12:52:36.000000 bitfount-0.5.86/requirements/39/requirements-tutorial.txt
--rw-r--r--   0 runner    (1001) docker     (123)      781 2023-08-03 12:52:36.000000 bitfount-0.5.86/requirements/constraints-compatibility.txt
--rw-r--r--   0 runner    (1001) docker     (123)      503 2023-08-03 12:52:36.000000 bitfount-0.5.86/requirements/constraints-direct.txt
--rw-r--r--   0 runner    (1001) docker     (123)      745 2023-08-03 12:52:36.000000 bitfount-0.5.86/requirements/constraints-security.txt
--rw-r--r--   0 runner    (1001) docker     (123)     3941 2023-08-03 12:52:36.000000 bitfount-0.5.86/requirements/requirements.in
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.450226 bitfount-0.5.86/scripts/
--rw-r--r--   0 runner    (1001) docker     (123)       50 2023-08-03 12:52:36.000000 bitfount-0.5.86/scripts/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1098 2023-08-03 12:52:36.000000 bitfount-0.5.86/scripts/generate_schema.py
--rw-r--r--   0 runner    (1001) docker     (123)     9157 2023-08-03 12:52:36.000000 bitfount-0.5.86/scripts/generate_yaml_specs.py
--rw-r--r--   0 runner    (1001) docker     (123)     1253 2023-08-03 12:52:36.000000 bitfount-0.5.86/scripts/run_modeller.py
--rwxr-xr-x   0 runner    (1001) docker     (123)      720 2023-08-03 12:52:36.000000 bitfount-0.5.86/scripts/run_pod.py
--rw-r--r--   0 runner    (1001) docker     (123)     2784 2023-08-03 12:52:36.000000 bitfount-0.5.86/scripts/run_testing.py
--rw-r--r--   0 runner    (1001) docker     (123)      630 2023-08-03 12:52:36.000000 bitfount-0.5.86/scripts/script_runner.py
--rw-r--r--   0 runner    (1001) docker     (123)       38 2023-08-03 12:52:49.486227 bitfount-0.5.86/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (123)     9162 2023-08-03 12:52:36.000000 bitfount-0.5.86/setup.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.450226 bitfount-0.5.86/tests/
--rw-r--r--   0 runner    (1001) docker     (123)       98 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.450226 bitfount-0.5.86/tests/bitfount/
--rw-r--r--   0 runner    (1001) docker     (123)      177 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.450226 bitfount-0.5.86/tests/bitfount/backends/
--rw-r--r--   0 runner    (1001) docker     (123)       43 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/backends/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.450226 bitfount-0.5.86/tests/bitfount/backends/lightgbm/
--rw-r--r--   0 runner    (1001) docker     (123)       44 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/backends/lightgbm/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.450226 bitfount-0.5.86/tests/bitfount/backends/lightgbm/federated/
--rw-r--r--   0 runner    (1001) docker     (123)       63 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/backends/lightgbm/federated/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.450226 bitfount-0.5.86/tests/bitfount/backends/lightgbm/federated/transport/
--rw-r--r--   0 runner    (1001) docker     (123)       42 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/backends/lightgbm/federated/transport/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1880 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/backends/lightgbm/federated/transport/test_message_service_usage.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.450226 bitfount-0.5.86/tests/bitfount/backends/lightgbm/models/
--rw-r--r--   0 runner    (1001) docker     (123)       45 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/backends/lightgbm/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8594 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/backends/lightgbm/models/test_models.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.454226 bitfount-0.5.86/tests/bitfount/backends/pytorch/
--rw-r--r--   0 runner    (1001) docker     (123)       46 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/backends/pytorch/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    16273 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/backends/pytorch/conftest.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.454226 bitfount-0.5.86/tests/bitfount/backends/pytorch/data/
--rw-r--r--   0 runner    (1001) docker     (123)       46 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/backends/pytorch/data/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6004 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/backends/pytorch/data/conftest.py
--rw-r--r--   0 runner    (1001) docker     (123)      721 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/backends/pytorch/data/test_datafactory.py
--rw-r--r--   0 runner    (1001) docker     (123)     8725 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/backends/pytorch/data/test_dataloaders.py
--rw-r--r--   0 runner    (1001) docker     (123)    12942 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/backends/pytorch/data/test_datasets.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.454226 bitfount-0.5.86/tests/bitfount/backends/pytorch/federated/
--rw-r--r--   0 runner    (1001) docker     (123)       44 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/backends/pytorch/federated/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5892 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/backends/pytorch/federated/test_distributed_model.py
--rw-r--r--   0 runner    (1001) docker     (123)    16032 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/backends/pytorch/federated/test_protocol.py
--rw-r--r--   0 runner    (1001) docker     (123)    21765 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/backends/pytorch/federated/test_secure.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.454226 bitfount-0.5.86/tests/bitfount/backends/pytorch/federated/transport/
--rw-r--r--   0 runner    (1001) docker     (123)       49 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/backends/pytorch/federated/transport/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4982 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/backends/pytorch/federated/transport/test_message_service_usage.py
--rw-r--r--   0 runner    (1001) docker     (123)      724 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/backends/pytorch/helper.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.458226 bitfount-0.5.86/tests/bitfount/backends/pytorch/models/
--rw-r--r--   0 runner    (1001) docker     (123)       50 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/backends/pytorch/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    33110 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/backends/pytorch/models/test_bitfount_model.py
--rw-r--r--   0 runner    (1001) docker     (123)     5742 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/backends/pytorch/models/test_gradients.py
--rw-r--r--   0 runner    (1001) docker     (123)   110134 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/backends/pytorch/models/test_models.py
--rw-r--r--   0 runner    (1001) docker     (123)    10991 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/backends/pytorch/models/test_nn.py
--rw-r--r--   0 runner    (1001) docker     (123)     2946 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/backends/pytorch/test__torch_shims.py
--rw-r--r--   0 runner    (1001) docker     (123)     2677 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/backends/pytorch/test_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)     1318 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/backends/pytorch/test_types.py
--rw-r--r--   0 runner    (1001) docker     (123)    16420 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/backends/pytorch/test_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.458226 bitfount-0.5.86/tests/bitfount/data/
--rw-r--r--   0 runner    (1001) docker     (123)       46 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/data/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    10354 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/data/conftest.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.462226 bitfount-0.5.86/tests/bitfount/data/datasources/
--rw-r--r--   0 runner    (1001) docker     (123)       67 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/data/datasources/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    56054 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/data/datasources/test_datasource.py
--rw-r--r--   0 runner    (1001) docker     (123)    40008 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/data/datasources/test_views.py
--rw-r--r--   0 runner    (1001) docker     (123)     7857 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/data/test_databunch.py
--rw-r--r--   0 runner    (1001) docker     (123)     2366 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/data/test_datafactory.py
--rw-r--r--   0 runner    (1001) docker     (123)     9351 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/data/test_dataloaders.py
--rw-r--r--   0 runner    (1001) docker     (123)    38659 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/data/test_datasets.py
--rw-r--r--   0 runner    (1001) docker     (123)    21395 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/data/test_datasplitters.py
--rw-r--r--   0 runner    (1001) docker     (123)    24787 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/data/test_datastructure.py
--rw-r--r--   0 runner    (1001) docker     (123)    53798 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/data/test_schema.py
--rw-r--r--   0 runner    (1001) docker     (123)     4673 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/data/test_types.py
--rw-r--r--   0 runner    (1001) docker     (123)    21320 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/data/test_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.462226 bitfount-0.5.86/tests/bitfount/federated/
--rw-r--r--   0 runner    (1001) docker     (123)       62 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.466226 bitfount-0.5.86/tests/bitfount/federated/aggregators/
--rw-r--r--   0 runner    (1001) docker     (123)       29 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/aggregators/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    16541 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/aggregators/test_aggregator.py
--rw-r--r--   0 runner    (1001) docker     (123)     4630 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/aggregators/test_base.py
--rw-r--r--   0 runner    (1001) docker     (123)     9774 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/aggregators/test_secure.py
--rw-r--r--   0 runner    (1001) docker     (123)      816 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/aggregators/util.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.466226 bitfount-0.5.86/tests/bitfount/federated/algorithms/
--rw-r--r--   0 runner    (1001) docker     (123)       54 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/algorithms/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.470226 bitfount-0.5.86/tests/bitfount/federated/algorithms/model_algorithms/
--rw-r--r--   0 runner    (1001) docker     (123)       34 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/algorithms/model_algorithms/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      760 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/algorithms/model_algorithms/conftest.py
--rw-r--r--   0 runner    (1001) docker     (123)     1345 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/algorithms/model_algorithms/test_base.py
--rw-r--r--   0 runner    (1001) docker     (123)     5592 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/algorithms/model_algorithms/test_evaluate.py
--rw-r--r--   0 runner    (1001) docker     (123)     9681 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/algorithms/model_algorithms/test_inference.py
--rw-r--r--   0 runner    (1001) docker     (123)     4686 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/algorithms/model_algorithms/test_train_and_evaluate.py
--rw-r--r--   0 runner    (1001) docker     (123)     7919 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/algorithms/model_algorithms/test_training.py
--rw-r--r--   0 runner    (1001) docker     (123)       39 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/algorithms/test_base.py
--rw-r--r--   0 runner    (1001) docker     (123)     4388 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/algorithms/test_column_avg.py
--rw-r--r--   0 runner    (1001) docker     (123)    37121 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/algorithms/test_compute_intersection_rsa.py
--rw-r--r--   0 runner    (1001) docker     (123)    10047 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/algorithms/test_csv_report_algorithm.py
--rw-r--r--   0 runner    (1001) docker     (123)    45818 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/algorithms/test_private_sql_query.py
--rw-r--r--   0 runner    (1001) docker     (123)    10672 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/algorithms/test_sql_query.py
--rw-r--r--   0 runner    (1001) docker     (123)     7010 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/algorithms/test_transformer_perplexity.py
--rw-r--r--   0 runner    (1001) docker     (123)     7214 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/algorithms/test_transformer_text_generation.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.470226 bitfount-0.5.86/tests/bitfount/federated/monitoring/
--rw-r--r--   0 runner    (1001) docker     (123)       39 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/monitoring/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    23049 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/monitoring/test_monitoring.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.470226 bitfount-0.5.86/tests/bitfount/federated/privacy/
--rw-r--r--   0 runner    (1001) docker     (123)       42 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/privacy/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6586 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/privacy/test_differential.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.470226 bitfount-0.5.86/tests/bitfount/federated/protocols/
--rw-r--r--   0 runner    (1001) docker     (123)       53 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/protocols/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1089 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/protocols/conftest.py
--rw-r--r--   0 runner    (1001) docker     (123)     3226 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/protocols/test_base.py
--rw-r--r--   0 runner    (1001) docker     (123)    28718 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/protocols/test_fed_avg.py
--rw-r--r--   0 runner    (1001) docker     (123)     8653 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/protocols/test_inference_csv_protocol.py
--rw-r--r--   0 runner    (1001) docker     (123)    15895 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/protocols/test_psi.py
--rw-r--r--   0 runner    (1001) docker     (123)    32249 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/protocols/test_results_only.py
--rw-r--r--   0 runner    (1001) docker     (123)    60108 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/test_authorisation_checkers.py
--rw-r--r--   0 runner    (1001) docker     (123)     6188 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/test_early_stopping.py
--rw-r--r--   0 runner    (1001) docker     (123)    18965 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/test_encryption.py
--rw-r--r--   0 runner    (1001) docker     (123)     8984 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/test_helper.py
--rw-r--r--   0 runner    (1001) docker     (123)     9418 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/test_keys_setup.py
--rw-r--r--   0 runner    (1001) docker     (123)     6557 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/test_logging.py
--rw-r--r--   0 runner    (1001) docker     (123)    13474 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/test_model_reference.py
--rw-r--r--   0 runner    (1001) docker     (123)    45867 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/test_modeller.py
--rw-r--r--   0 runner    (1001) docker     (123)   119316 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/test_pod.py
--rw-r--r--   0 runner    (1001) docker     (123)    19597 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/test_pod_db_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    14627 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/test_pod_vitals.py
--rw-r--r--   0 runner    (1001) docker     (123)     1447 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/test_roles.py
--rw-r--r--   0 runner    (1001) docker     (123)     1036 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/test_secure.py
--rw-r--r--   0 runner    (1001) docker     (123)    68934 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/test_worker.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.474227 bitfount-0.5.86/tests/bitfount/federated/transport/
--rw-r--r--   0 runner    (1001) docker     (123)       45 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/transport/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.474227 bitfount-0.5.86/tests/bitfount/federated/transport/identity_verification/
--rw-r--r--   0 runner    (1001) docker     (123)       79 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/transport/identity_verification/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    46287 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/transport/identity_verification/test_oidc.py
--rw-r--r--   0 runner    (1001) docker     (123)    17923 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/transport/identity_verification/test_saml.py
--rw-r--r--   0 runner    (1001) docker     (123)     4091 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/transport/test_config.py
--rw-r--r--   0 runner    (1001) docker     (123)    88847 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/transport/test_message_service.py
--rw-r--r--   0 runner    (1001) docker     (123)     2049 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/transport/test_message_service_usage.py
--rw-r--r--   0 runner    (1001) docker     (123)    70011 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/transport/test_modeller_transport.py
--rw-r--r--   0 runner    (1001) docker     (123)     2687 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/transport/test_pod_transport.py
--rw-r--r--   0 runner    (1001) docker     (123)      688 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/transport/test_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    48365 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/federated/transport/test_worker_transport.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.474227 bitfount-0.5.86/tests/bitfount/hub/
--rw-r--r--   0 runner    (1001) docker     (123)       29 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/hub/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)   111229 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/hub/test_api.py
--rw-r--r--   0 runner    (1001) docker     (123)     8365 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/hub/test_authentication_flow.py
--rw-r--r--   0 runner    (1001) docker     (123)    43442 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/hub/test_authentication_handlers.py
--rw-r--r--   0 runner    (1001) docker     (123)    18664 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/hub/test_helper.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.478227 bitfount-0.5.86/tests/bitfount/models/
--rw-r--r--   0 runner    (1001) docker     (123)       56 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6350 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/models/test_base_models.py
--rw-r--r--   0 runner    (1001) docker     (123)     1571 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/models/test_bitfount_model.py
--rw-r--r--   0 runner    (1001) docker     (123)    23680 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/models/test_models.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.478227 bitfount-0.5.86/tests/bitfount/schemas/
--rw-r--r--   0 runner    (1001) docker     (123)       61 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/schemas/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4291 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/schemas/test_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     7773 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/test_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     9969 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/test_hooks.py
--rw-r--r--   0 runner    (1001) docker     (123)    16713 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/test_metrics.py
--rw-r--r--   0 runner    (1001) docker     (123)    36042 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/test_plugins.py
--rw-r--r--   0 runner    (1001) docker     (123)    36350 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/test_storage.py
--rw-r--r--   0 runner    (1001) docker     (123)      801 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/test_types.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.478227 bitfount-0.5.86/tests/bitfount/transformations/
--rw-r--r--   0 runner    (1001) docker     (123)       53 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/transformations/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      775 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/transformations/conftest.py
--rw-r--r--   0 runner    (1001) docker     (123)     4902 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/transformations/test_base_transformation.py
--rw-r--r--   0 runner    (1001) docker     (123)     5985 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/transformations/test_binary_operations.py
--rw-r--r--   0 runner    (1001) docker     (123)     3391 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/transformations/test_dataset_operations.py
--rw-r--r--   0 runner    (1001) docker     (123)    18686 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/transformations/test_parser.py
--rw-r--r--   0 runner    (1001) docker     (123)    33067 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/transformations/test_processor.py
--rw-r--r--   0 runner    (1001) docker     (123)     5614 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/transformations/test_references.py
--rw-r--r--   0 runner    (1001) docker     (123)    19365 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/transformations/test_unary_operations.py
--rw-r--r--   0 runner    (1001) docker     (123)     1007 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/transformations/transformation_test_helpers.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.478227 bitfount-0.5.86/tests/bitfount/utils/
--rw-r--r--   0 runner    (1001) docker     (123)       32 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    19485 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/utils/test_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    22367 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/bitfount/utils/test_web_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     9024 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/conftest.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.482226 bitfount-0.5.86/tests/integration/
--rw-r--r--   0 runner    (1001) docker     (123)      264 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/integration/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8191 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/integration/bitfount_web_interactions.py
--rw-r--r--   0 runner    (1001) docker     (123)     2648 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/integration/conftest.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.482226 bitfount-0.5.86/tests/integration/end_to_end/
--rw-r--r--   0 runner    (1001) docker     (123)       24 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/integration/end_to_end/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    18404 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/integration/end_to_end/test_end_to_end.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.482226 bitfount-0.5.86/tests/integration/end_to_end_mock/
--rw-r--r--   0 runner    (1001) docker     (123)       60 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/integration/end_to_end_mock/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6006 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/integration/end_to_end_mock/conftest.py
--rw-r--r--   0 runner    (1001) docker     (123)    28636 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/integration/end_to_end_mock/test_end_to_end_mock_bitfount_hub.py
--rw-r--r--   0 runner    (1001) docker     (123)     4488 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/integration/test_pod_creation.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.482226 bitfount-0.5.86/tests/integration/tutorials/
--rw-r--r--   0 runner    (1001) docker     (123)       22 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/integration/tutorials/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      163 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/integration/tutorials/conftest.py
--rw-r--r--   0 runner    (1001) docker     (123)     6473 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/integration/tutorials/notebook_logging.py
--rw-r--r--   0 runner    (1001) docker     (123)    55590 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/integration/tutorials/test_tutorials.py
--rw-r--r--   0 runner    (1001) docker     (123)     9882 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/integration/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.482226 bitfount-0.5.86/tests/utils/
--rw-r--r--   0 runner    (1001) docker     (123)      324 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6941 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/utils/concurrency_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 12:52:49.486227 bitfount-0.5.86/tests/utils/fixtures/
--rw-r--r--   0 runner    (1001) docker     (123)      571 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/utils/fixtures/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2449 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/utils/fixtures/authorisation_checker_fixtures.py
--rw-r--r--   0 runner    (1001) docker     (123)     3587 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/utils/fixtures/encryption_fixtures.py
--rw-r--r--   0 runner    (1001) docker     (123)     1228 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/utils/fixtures/env_fixtures.py
--rw-r--r--   0 runner    (1001) docker     (123)     2067 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/utils/fixtures/helper_fixtures.py
--rw-r--r--   0 runner    (1001) docker     (123)    14383 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/utils/fixtures/hub_and_am_fixtures.py
--rw-r--r--   0 runner    (1001) docker     (123)      782 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/utils/fixtures/schema_fixtures.py
--rw-r--r--   0 runner    (1001) docker     (123)     1260 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/utils/fixtures/storage_fixtures.py
--rw-r--r--   0 runner    (1001) docker     (123)     7299 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/utils/fixtures/transport_layer_fixtures.py
--rw-r--r--   0 runner    (1001) docker     (123)     2258 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/utils/fixtures/web_utils_fixtures.py
--rw-r--r--   0 runner    (1001) docker     (123)    35369 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/utils/helper.py
--rw-r--r--   0 runner    (1001) docker     (123)    26732 2023-08-03 12:52:36.000000 bitfount-0.5.86/tests/utils/mocks.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.682870 bitfount-0.5.9/
+-rw-r--r--   0 runner    (1001) docker     (121)    16929 2022-02-04 10:43:24.000000 bitfount-0.5.9/LICENSE.md
+-rw-r--r--   0 runner    (1001) docker     (121)      267 2022-02-04 10:43:24.000000 bitfount-0.5.9/MANIFEST.in
+-rw-r--r--   0 runner    (1001) docker     (121)     8985 2022-02-04 10:43:34.682870 bitfount-0.5.9/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (121)     7315 2022-02-04 10:43:24.000000 bitfount-0.5.9/README.md
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.654869 bitfount-0.5.9/bitfount/
+-rw-r--r--   0 runner    (1001) docker     (121)     3311 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (121)      525 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/__version__.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.658869 bitfount-0.5.9/bitfount/backends/
+-rw-r--r--   0 runner    (1001) docker     (121)      313 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/backends/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.658869 bitfount-0.5.9/bitfount/backends/lightgbm/
+-rw-r--r--   0 runner    (1001) docker     (121)      398 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/backends/lightgbm/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.658869 bitfount-0.5.9/bitfount/backends/lightgbm/models/
+-rw-r--r--   0 runner    (1001) docker     (121)       45 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/backends/lightgbm/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (121)     8239 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/backends/lightgbm/models/base_models.py
+-rw-r--r--   0 runner    (1001) docker     (121)     2219 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/backends/lightgbm/models/models.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.658869 bitfount-0.5.9/bitfount/backends/pytorch/
+-rw-r--r--   0 runner    (1001) docker     (121)      931 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/backends/pytorch/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.658869 bitfount-0.5.9/bitfount/backends/pytorch/data/
+-rw-r--r--   0 runner    (1001) docker     (121)       47 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/backends/pytorch/data/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (121)     1953 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/backends/pytorch/data/datafactory.py
+-rw-r--r--   0 runner    (1001) docker     (121)     1396 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/backends/pytorch/data/dataloaders.py
+-rw-r--r--   0 runner    (1001) docker     (121)     1098 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/backends/pytorch/data/datasets.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.658869 bitfount-0.5.9/bitfount/backends/pytorch/federated/
+-rw-r--r--   0 runner    (1001) docker     (121)       52 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/backends/pytorch/federated/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (121)     8049 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/backends/pytorch/federated/models.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.658869 bitfount-0.5.9/bitfount/backends/pytorch/models/
+-rw-r--r--   0 runner    (1001) docker     (121)       44 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/backends/pytorch/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (121)    49002 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/backends/pytorch/models/base_models.py
+-rw-r--r--   0 runner    (1001) docker     (121)    20039 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/backends/pytorch/models/bitfount_model.py
+-rw-r--r--   0 runner    (1001) docker     (121)    15546 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/backends/pytorch/models/models.py
+-rw-r--r--   0 runner    (1001) docker     (121)     8134 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/backends/pytorch/models/nn.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.658869 bitfount-0.5.9/bitfount/backends/pytorch/models/torch_functions/
+-rw-r--r--   0 runner    (1001) docker     (121)       83 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/backends/pytorch/models/torch_functions/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (121)     1440 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/backends/pytorch/models/torch_functions/mish.py
+-rw-r--r--   0 runner    (1001) docker     (121)     1928 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/backends/pytorch/types.py
+-rw-r--r--   0 runner    (1001) docker     (121)     1539 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/backends/pytorch/utils.py
+-rw-r--r--   0 runner    (1001) docker     (121)     4965 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/config.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.658869 bitfount-0.5.9/bitfount/data/
+-rw-r--r--   0 runner    (1001) docker     (121)     1054 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/data/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (121)     7662 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/data/databunch.py
+-rw-r--r--   0 runner    (1001) docker     (121)     4169 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/data/datafactory.py
+-rw-r--r--   0 runner    (1001) docker     (121)     4053 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/data/dataloader.py
+-rw-r--r--   0 runner    (1001) docker     (121)    10061 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/data/datasets.py
+-rw-r--r--   0 runner    (1001) docker     (121)     6612 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/data/datasource.py
+-rw-r--r--   0 runner    (1001) docker     (121)     9129 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/data/datasplitters.py
+-rw-r--r--   0 runner    (1001) docker     (121)    13881 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/data/datastructure.py
+-rw-r--r--   0 runner    (1001) docker     (121)      582 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/data/helper.py
+-rw-r--r--   0 runner    (1001) docker     (121)    28761 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/data/schema.py
+-rw-r--r--   0 runner    (1001) docker     (121)    14988 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/data/types.py
+-rw-r--r--   0 runner    (1001) docker     (121)     1717 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/data/utils.py
+-rw-r--r--   0 runner    (1001) docker     (121)      382 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/exceptions.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.662869 bitfount-0.5.9/bitfount/federated/
+-rw-r--r--   0 runner    (1001) docker     (121)     2710 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.662869 bitfount-0.5.9/bitfount/federated/aggregators/
+-rw-r--r--   0 runner    (1001) docker     (121)      641 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/aggregators/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (121)     4123 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/aggregators/aggregator.py
+-rw-r--r--   0 runner    (1001) docker     (121)     2826 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/aggregators/base.py
+-rw-r--r--   0 runner    (1001) docker     (121)     6198 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/aggregators/secure.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.662869 bitfount-0.5.9/bitfount/federated/algorithms/
+-rw-r--r--   0 runner    (1001) docker     (121)      629 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/algorithms/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (121)     2867 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/algorithms/base.py
+-rw-r--r--   0 runner    (1001) docker     (121)     2898 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/algorithms/column_avg.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.662869 bitfount-0.5.9/bitfount/federated/algorithms/model_algorithms/
+-rw-r--r--   0 runner    (1001) docker     (121)       62 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/algorithms/model_algorithms/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (121)     6237 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/algorithms/model_algorithms/base.py
+-rw-r--r--   0 runner    (1001) docker     (121)     2769 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/algorithms/model_algorithms/evaluate.py
+-rw-r--r--   0 runner    (1001) docker     (121)     6934 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/algorithms/model_algorithms/federated_training.py
+-rw-r--r--   0 runner    (1001) docker     (121)     3027 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/algorithms/model_algorithms/train_and_evaluate.py
+-rw-r--r--   0 runner    (1001) docker     (121)    38031 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/authorisation_checkers.py
+-rw-r--r--   0 runner    (1001) docker     (121)     2421 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/early_stopping.py
+-rw-r--r--   0 runner    (1001) docker     (121)     8865 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/encryption.py
+-rw-r--r--   0 runner    (1001) docker     (121)      901 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/exceptions.py
+-rw-r--r--   0 runner    (1001) docker     (121)     4961 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/helper.py
+-rw-r--r--   0 runner    (1001) docker     (121)     6644 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/logging.py
+-rw-r--r--   0 runner    (1001) docker     (121)     9541 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/mixins.py
+-rw-r--r--   0 runner    (1001) docker     (121)     9966 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/model_reference.py
+-rw-r--r--   0 runner    (1001) docker     (121)    14948 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/modeller.py
+-rw-r--r--   0 runner    (1001) docker     (121)    25940 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/pod.py
+-rw-r--r--   0 runner    (1001) docker     (121)     3167 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/pod_keys_setup.py
+-rw-r--r--   0 runner    (1001) docker     (121)     1302 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/pod_response_message.py
+-rw-r--r--   0 runner    (1001) docker     (121)     4023 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/pod_vitals.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.662869 bitfount-0.5.9/bitfount/federated/privacy/
+-rw-r--r--   0 runner    (1001) docker     (121)       48 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/privacy/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (121)     6538 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/privacy/differential.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.662869 bitfount-0.5.9/bitfount/federated/protocols/
+-rw-r--r--   0 runner    (1001) docker     (121)      623 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/protocols/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (121)     9783 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/protocols/base.py
+-rw-r--r--   0 runner    (1001) docker     (121)    29780 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/protocols/fed_avg.py
+-rw-r--r--   0 runner    (1001) docker     (121)     9886 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/protocols/results_only.py
+-rw-r--r--   0 runner    (1001) docker     (121)     2130 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/roles.py
+-rw-r--r--   0 runner    (1001) docker     (121)    11571 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/secure.py
+-rw-r--r--   0 runner    (1001) docker     (121)     1883 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/shim.py
+-rw-r--r--   0 runner    (1001) docker     (121)     3036 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/task_requests.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.666869 bitfount-0.5.9/bitfount/federated/transport/
+-rw-r--r--   0 runner    (1001) docker     (121)      348 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/transport/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (121)    17878 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/transport/base_transport.py
+-rw-r--r--   0 runner    (1001) docker     (121)     2721 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/transport/config.py
+-rw-r--r--   0 runner    (1001) docker     (121)    10145 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/transport/handlers.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.666869 bitfount-0.5.9/bitfount/federated/transport/identity_verification/
+-rw-r--r--   0 runner    (1001) docker     (121)      256 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/transport/identity_verification/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (121)    17955 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/transport/identity_verification/oidc.py
+-rw-r--r--   0 runner    (1001) docker     (121)     9945 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/transport/identity_verification/saml.py
+-rw-r--r--   0 runner    (1001) docker     (121)    25836 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/transport/message_service.py
+-rw-r--r--   0 runner    (1001) docker     (121)    27451 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/transport/modeller_transport.py
+-rw-r--r--   0 runner    (1001) docker     (121)     2614 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/transport/pod_transport.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.666869 bitfount-0.5.9/bitfount/federated/transport/protos/
+-rw-r--r--   0 runner    (1001) docker     (121)      141 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/transport/protos/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (121)    40604 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/transport/protos/messages_pb2.py
+-rw-r--r--   0 runner    (1001) docker     (121)    14198 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/transport/protos/messages_pb2_grpc.py
+-rw-r--r--   0 runner    (1001) docker     (121)     2108 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/transport/types.py
+-rw-r--r--   0 runner    (1001) docker     (121)      711 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/transport/utils.py
+-rw-r--r--   0 runner    (1001) docker     (121)    18146 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/transport/worker_transport.py
+-rw-r--r--   0 runner    (1001) docker     (121)     4260 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/types.py
+-rw-r--r--   0 runner    (1001) docker     (121)     5177 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/utils.py
+-rw-r--r--   0 runner    (1001) docker     (121)     6452 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/federated/worker.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.666869 bitfount-0.5.9/bitfount/hub/
+-rw-r--r--   0 runner    (1001) docker     (121)      656 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/hub/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (121)    39849 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/hub/api.py
+-rw-r--r--   0 runner    (1001) docker     (121)    19889 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/hub/authentication_flow.py
+-rw-r--r--   0 runner    (1001) docker     (121)    10685 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/hub/helper.py
+-rw-r--r--   0 runner    (1001) docker     (121)     8227 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/hub/types.py
+-rw-r--r--   0 runner    (1001) docker     (121)    15983 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/metrics.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.666869 bitfount-0.5.9/bitfount/models/
+-rw-r--r--   0 runner    (1001) docker     (121)     1055 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (121)    30143 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/models/base_models.py
+-rw-r--r--   0 runner    (1001) docker     (121)     1340 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/models/bitfount_model.py
+-rw-r--r--   0 runner    (1001) docker     (121)    24745 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/models/models.py
+-rw-r--r--   0 runner    (1001) docker     (121)      585 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/models/nn.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.666869 bitfount-0.5.9/bitfount/runners/
+-rw-r--r--   0 runner    (1001) docker     (121)      312 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/runners/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (121)    17465 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/runners/config_schemas.py
+-rw-r--r--   0 runner    (1001) docker     (121)    12515 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/runners/modeller_runner.py
+-rw-r--r--   0 runner    (1001) docker     (121)     2214 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/runners/pod_runner.py
+-rw-r--r--   0 runner    (1001) docker     (121)     2750 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/runners/utils.py
+-rw-r--r--   0 runner    (1001) docker     (121)     7508 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/storage.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.666869 bitfount-0.5.9/bitfount/transformations/
+-rw-r--r--   0 runner    (1001) docker     (121)     2115 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/transformations/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (121)     4497 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/transformations/base_transformation.py
+-rw-r--r--   0 runner    (1001) docker     (121)     4749 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/transformations/batch_operations.py
+-rw-r--r--   0 runner    (1001) docker     (121)     2359 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/transformations/binary_operations.py
+-rw-r--r--   0 runner    (1001) docker     (121)     1837 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/transformations/dataset_operations.py
+-rw-r--r--   0 runner    (1001) docker     (121)     2387 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/transformations/exceptions.py
+-rw-r--r--   0 runner    (1001) docker     (121)    10650 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/transformations/parser.py
+-rw-r--r--   0 runner    (1001) docker     (121)    19458 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/transformations/processor.py
+-rw-r--r--   0 runner    (1001) docker     (121)     3625 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/transformations/references.py
+-rw-r--r--   0 runner    (1001) docker     (121)     8513 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/transformations/unary_operations.py
+-rw-r--r--   0 runner    (1001) docker     (121)      702 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/transformations/utils.py
+-rw-r--r--   0 runner    (1001) docker     (121)     7593 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/types.py
+-rw-r--r--   0 runner    (1001) docker     (121)     8049 2022-02-04 10:43:24.000000 bitfount-0.5.9/bitfount/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.658869 bitfount-0.5.9/bitfount.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (121)     8985 2022-02-04 10:43:34.000000 bitfount-0.5.9/bitfount.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (121)    11302 2022-02-04 10:43:34.000000 bitfount-0.5.9/bitfount.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (121)        1 2022-02-04 10:43:34.000000 bitfount-0.5.9/bitfount.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (121)       57 2022-02-04 10:43:34.000000 bitfount-0.5.9/bitfount.egg-info/entry_points.txt
+-rw-r--r--   0 runner    (1001) docker     (121)     6675 2022-02-04 10:43:34.000000 bitfount-0.5.9/bitfount.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (121)       23 2022-02-04 10:43:34.000000 bitfount-0.5.9/bitfount.egg-info/top_level.txt
+-rw-r--r--   0 runner    (1001) docker     (121)     2487 2022-02-04 10:43:24.000000 bitfount-0.5.9/pyproject.toml
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.666869 bitfount-0.5.9/requirements/
+-rw-r--r--   0 runner    (1001) docker     (121)      839 2022-02-04 10:43:24.000000 bitfount-0.5.9/requirements/constraints-security.txt
+-rw-r--r--   0 runner    (1001) docker     (121)     7487 2022-02-04 10:43:24.000000 bitfount-0.5.9/requirements/requirements-dev.txt
+-rw-r--r--   0 runner    (1001) docker     (121)    12676 2022-02-04 10:43:24.000000 bitfount-0.5.9/requirements/requirements-test.txt
+-rw-r--r--   0 runner    (1001) docker     (121)     5778 2022-02-04 10:43:24.000000 bitfount-0.5.9/requirements/requirements-tutorial.txt
+-rw-r--r--   0 runner    (1001) docker     (121)     2531 2022-02-04 10:43:24.000000 bitfount-0.5.9/requirements/requirements.in
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.670869 bitfount-0.5.9/scripts/
+-rw-r--r--   0 runner    (1001) docker     (121)       50 2022-02-04 10:43:24.000000 bitfount-0.5.9/scripts/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (121)     1026 2022-02-04 10:43:24.000000 bitfount-0.5.9/scripts/generate_schema.py
+-rw-r--r--   0 runner    (1001) docker     (121)     5677 2022-02-04 10:43:24.000000 bitfount-0.5.9/scripts/pod_ui_utils.py
+-rw-r--r--   0 runner    (1001) docker     (121)      822 2022-02-04 10:43:24.000000 bitfount-0.5.9/scripts/run_modeller.py
+-rwxr-xr-x   0 runner    (1001) docker     (121)     2307 2022-02-04 10:43:24.000000 bitfount-0.5.9/scripts/run_pod.py
+-rw-r--r--   0 runner    (1001) docker     (121)     2381 2022-02-04 10:43:24.000000 bitfount-0.5.9/scripts/run_testing.py
+-rw-r--r--   0 runner    (1001) docker     (121)      521 2022-02-04 10:43:24.000000 bitfount-0.5.9/scripts/script_runner.py
+-rw-r--r--   0 runner    (1001) docker     (121)       38 2022-02-04 10:43:34.682870 bitfount-0.5.9/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (121)     3507 2022-02-04 10:43:24.000000 bitfount-0.5.9/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.670869 bitfount-0.5.9/tests/
+-rw-r--r--   0 runner    (1001) docker     (121)       39 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.670869 bitfount-0.5.9/tests/bitfount/
+-rw-r--r--   0 runner    (1001) docker     (121)      177 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.670869 bitfount-0.5.9/tests/bitfount/backends/
+-rw-r--r--   0 runner    (1001) docker     (121)       43 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/backends/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.670869 bitfount-0.5.9/tests/bitfount/backends/lightgbm/
+-rw-r--r--   0 runner    (1001) docker     (121)       44 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/backends/lightgbm/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.670869 bitfount-0.5.9/tests/bitfount/backends/lightgbm/federated/
+-rw-r--r--   0 runner    (1001) docker     (121)       63 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/backends/lightgbm/federated/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.670869 bitfount-0.5.9/tests/bitfount/backends/lightgbm/federated/transport/
+-rw-r--r--   0 runner    (1001) docker     (121)       42 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/backends/lightgbm/federated/transport/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (121)     1849 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/backends/lightgbm/federated/transport/test_message_service_usage.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.670869 bitfount-0.5.9/tests/bitfount/backends/lightgbm/models/
+-rw-r--r--   0 runner    (1001) docker     (121)       45 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/backends/lightgbm/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (121)     5290 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/backends/lightgbm/models/test_models.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.670869 bitfount-0.5.9/tests/bitfount/backends/pytorch/
+-rw-r--r--   0 runner    (1001) docker     (121)       46 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/backends/pytorch/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (121)     8246 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/backends/pytorch/conftest.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.670869 bitfount-0.5.9/tests/bitfount/backends/pytorch/data/
+-rw-r--r--   0 runner    (1001) docker     (121)       46 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/backends/pytorch/data/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (121)    10658 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/backends/pytorch/data/test_datasets.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.670869 bitfount-0.5.9/tests/bitfount/backends/pytorch/federated/
+-rw-r--r--   0 runner    (1001) docker     (121)       44 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/backends/pytorch/federated/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (121)     5474 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/backends/pytorch/federated/test_distributed_model.py
+-rw-r--r--   0 runner    (1001) docker     (121)    14915 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/backends/pytorch/federated/test_protocol.py
+-rw-r--r--   0 runner    (1001) docker     (121)    19728 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/backends/pytorch/federated/test_secure.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.670869 bitfount-0.5.9/tests/bitfount/backends/pytorch/federated/transport/
+-rw-r--r--   0 runner    (1001) docker     (121)       49 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/backends/pytorch/federated/transport/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (121)     4840 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/backends/pytorch/federated/transport/test_message_service_usage.py
+-rw-r--r--   0 runner    (1001) docker     (121)      724 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/backends/pytorch/helper.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.670869 bitfount-0.5.9/tests/bitfount/backends/pytorch/models/
+-rw-r--r--   0 runner    (1001) docker     (121)       50 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/backends/pytorch/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (121)    11191 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/backends/pytorch/models/test_bitfount_model.py
+-rw-r--r--   0 runner    (1001) docker     (121)    53708 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/backends/pytorch/models/test_models.py
+-rw-r--r--   0 runner    (1001) docker     (121)     2426 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/backends/pytorch/models/test_nn.py
+-rw-r--r--   0 runner    (1001) docker     (121)     3288 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/backends/pytorch/test_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.674869 bitfount-0.5.9/tests/bitfount/data/
+-rw-r--r--   0 runner    (1001) docker     (121)       46 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/data/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (121)     4257 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/data/conftest.py
+-rw-r--r--   0 runner    (1001) docker     (121)     6998 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/data/test_databunch.py
+-rw-r--r--   0 runner    (1001) docker     (121)     1897 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/data/test_datafactory.py
+-rw-r--r--   0 runner    (1001) docker     (121)     9996 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/data/test_dataloader.py
+-rw-r--r--   0 runner    (1001) docker     (121)    11003 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/data/test_datasets.py
+-rw-r--r--   0 runner    (1001) docker     (121)    11443 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/data/test_datasource.py
+-rw-r--r--   0 runner    (1001) docker     (121)    12595 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/data/test_datasplitters.py
+-rw-r--r--   0 runner    (1001) docker     (121)     5542 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/data/test_datastructure.py
+-rw-r--r--   0 runner    (1001) docker     (121)    27635 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/data/test_schema.py
+-rw-r--r--   0 runner    (1001) docker     (121)     2259 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/data/test_types.py
+-rw-r--r--   0 runner    (1001) docker     (121)     4535 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/data/test_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.674869 bitfount-0.5.9/tests/bitfount/federated/
+-rw-r--r--   0 runner    (1001) docker     (121)       62 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.674869 bitfount-0.5.9/tests/bitfount/federated/aggregators/
+-rw-r--r--   0 runner    (1001) docker     (121)       29 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/aggregators/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (121)     3014 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/aggregators/test_aggregator.py
+-rw-r--r--   0 runner    (1001) docker     (121)     5117 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/aggregators/test_secure.py
+-rw-r--r--   0 runner    (1001) docker     (121)      609 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/aggregators/util.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.674869 bitfount-0.5.9/tests/bitfount/federated/algorithms/
+-rw-r--r--   0 runner    (1001) docker     (121)       54 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/algorithms/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.674869 bitfount-0.5.9/tests/bitfount/federated/algorithms/model_algorithms/
+-rw-r--r--   0 runner    (1001) docker     (121)       34 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/algorithms/model_algorithms/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (121)      335 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/algorithms/model_algorithms/conftest.py
+-rw-r--r--   0 runner    (1001) docker     (121)       46 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/algorithms/model_algorithms/test_base.py
+-rw-r--r--   0 runner    (1001) docker     (121)     3738 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/algorithms/model_algorithms/test_evaluate.py
+-rw-r--r--   0 runner    (1001) docker     (121)     2559 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/algorithms/model_algorithms/test_train_and_evaluate.py
+-rw-r--r--   0 runner    (1001) docker     (121)     3352 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/algorithms/model_algorithms/test_training.py
+-rw-r--r--   0 runner    (1001) docker     (121)       39 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/algorithms/test_base.py
+-rw-r--r--   0 runner    (1001) docker     (121)     1657 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/algorithms/test_column_avg.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.674869 bitfount-0.5.9/tests/bitfount/federated/privacy/
+-rw-r--r--   0 runner    (1001) docker     (121)       42 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/privacy/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (121)     6383 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/privacy/test_differential.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.678870 bitfount-0.5.9/tests/bitfount/federated/protocols/
+-rw-r--r--   0 runner    (1001) docker     (121)       53 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/protocols/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (121)     1089 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/protocols/conftest.py
+-rw-r--r--   0 runner    (1001) docker     (121)     2023 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/protocols/test_base.py
+-rw-r--r--   0 runner    (1001) docker     (121)    19492 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/protocols/test_fed_avg.py
+-rw-r--r--   0 runner    (1001) docker     (121)     6379 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/protocols/test_results_only.py
+-rw-r--r--   0 runner    (1001) docker     (121)    70660 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/test_authorisation_checkers.py
+-rw-r--r--   0 runner    (1001) docker     (121)     6215 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/test_early_stopping.py
+-rw-r--r--   0 runner    (1001) docker     (121)    10554 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/test_encryption.py
+-rw-r--r--   0 runner    (1001) docker     (121)     8237 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/test_helper.py
+-rw-r--r--   0 runner    (1001) docker     (121)     5208 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/test_logging.py
+-rw-r--r--   0 runner    (1001) docker     (121)     9671 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/test_model_reference.py
+-rw-r--r--   0 runner    (1001) docker     (121)    28048 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/test_modeller.py
+-rw-r--r--   0 runner    (1001) docker     (121)    42933 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/test_pod.py
+-rw-r--r--   0 runner    (1001) docker     (121)     7459 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/test_pod_vitals.py
+-rw-r--r--   0 runner    (1001) docker     (121)     1447 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/test_roles.py
+-rw-r--r--   0 runner    (1001) docker     (121)      942 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/test_utils.py
+-rw-r--r--   0 runner    (1001) docker     (121)    12938 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/test_worker.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.678870 bitfount-0.5.9/tests/bitfount/federated/transport/
+-rw-r--r--   0 runner    (1001) docker     (121)       45 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/transport/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.678870 bitfount-0.5.9/tests/bitfount/federated/transport/identity_verification/
+-rw-r--r--   0 runner    (1001) docker     (121)       79 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/transport/identity_verification/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (121)    39867 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/transport/identity_verification/test_oidc.py
+-rw-r--r--   0 runner    (1001) docker     (121)    11522 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/transport/identity_verification/test_saml.py
+-rw-r--r--   0 runner    (1001) docker     (121)     2749 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/transport/test_config.py
+-rw-r--r--   0 runner    (1001) docker     (121)    52357 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/transport/test_message_service.py
+-rw-r--r--   0 runner    (1001) docker     (121)     2103 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/transport/test_message_service_usage.py
+-rw-r--r--   0 runner    (1001) docker     (121)    53016 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/transport/test_modeller_transport.py
+-rw-r--r--   0 runner    (1001) docker     (121)     2610 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/transport/test_pod_transport.py
+-rw-r--r--   0 runner    (1001) docker     (121)      688 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/transport/test_utils.py
+-rw-r--r--   0 runner    (1001) docker     (121)    25829 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/federated/transport/test_worker_transport.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.678870 bitfount-0.5.9/tests/bitfount/hub/
+-rw-r--r--   0 runner    (1001) docker     (121)       29 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/hub/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (121)    78759 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/hub/test_api.py
+-rw-r--r--   0 runner    (1001) docker     (121)    37419 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/hub/test_authentication_flow.py
+-rw-r--r--   0 runner    (1001) docker     (121)    12922 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/hub/test_helper.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.678870 bitfount-0.5.9/tests/bitfount/models/
+-rw-r--r--   0 runner    (1001) docker     (121)       56 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (121)     6994 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/models/test_base_models.py
+-rw-r--r--   0 runner    (1001) docker     (121)    23730 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/models/test_models.py
+-rw-r--r--   0 runner    (1001) docker     (121)     7484 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/test_config.py
+-rw-r--r--   0 runner    (1001) docker     (121)     9559 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/test_metrics.py
+-rw-r--r--   0 runner    (1001) docker     (121)    21903 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/test_storage.py
+-rw-r--r--   0 runner    (1001) docker     (121)     7711 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/test_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.678870 bitfount-0.5.9/tests/bitfount/transformations/
+-rw-r--r--   0 runner    (1001) docker     (121)       53 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/transformations/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (121)      806 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/transformations/conftest.py
+-rw-r--r--   0 runner    (1001) docker     (121)     4925 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/transformations/test_base_transformation.py
+-rw-r--r--   0 runner    (1001) docker     (121)     5985 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/transformations/test_binary_operations.py
+-rw-r--r--   0 runner    (1001) docker     (121)     3391 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/transformations/test_dataset_operations.py
+-rw-r--r--   0 runner    (1001) docker     (121)    19444 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/transformations/test_parser.py
+-rw-r--r--   0 runner    (1001) docker     (121)    25630 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/transformations/test_processor.py
+-rw-r--r--   0 runner    (1001) docker     (121)     5614 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/transformations/test_references.py
+-rw-r--r--   0 runner    (1001) docker     (121)    19458 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/transformations/test_unary_operations.py
+-rw-r--r--   0 runner    (1001) docker     (121)     1007 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/bitfount/transformations/transformation_test_helpers.py
+-rw-r--r--   0 runner    (1001) docker     (121)     4675 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/conftest.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.678870 bitfount-0.5.9/tests/integration/
+-rw-r--r--   0 runner    (1001) docker     (121)      386 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/integration/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (121)     7778 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/integration/bitfount_web_interactions.py
+-rw-r--r--   0 runner    (1001) docker     (121)     2402 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/integration/conftest.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.678870 bitfount-0.5.9/tests/integration/deployments/
+-rw-r--r--   0 runner    (1001) docker     (121)       44 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/integration/deployments/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (121)     1974 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/integration/deployments/test_deployment_configs.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.678870 bitfount-0.5.9/tests/integration/end_to_end/
+-rw-r--r--   0 runner    (1001) docker     (121)       24 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/integration/end_to_end/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (121)     8426 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/integration/end_to_end/test_end_to_end.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.682870 bitfount-0.5.9/tests/integration/end_to_end_mock/
+-rw-r--r--   0 runner    (1001) docker     (121)       60 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/integration/end_to_end_mock/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (121)     5753 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/integration/end_to_end_mock/conftest.py
+-rw-r--r--   0 runner    (1001) docker     (121)    25053 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/integration/end_to_end_mock/test_end_to_end_mock_bitfount_hub.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.682870 bitfount-0.5.9/tests/integration/tutorials/
+-rw-r--r--   0 runner    (1001) docker     (121)       22 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/integration/tutorials/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (121)      163 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/integration/tutorials/conftest.py
+-rw-r--r--   0 runner    (1001) docker     (121)     5663 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/integration/tutorials/notebook_logging.py
+-rw-r--r--   0 runner    (1001) docker     (121)    31850 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/integration/tutorials/test_tutorials.py
+-rw-r--r--   0 runner    (1001) docker     (121)     4992 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/integration/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.682870 bitfount-0.5.9/tests/utils/
+-rw-r--r--   0 runner    (1001) docker     (121)      667 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/utils/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-02-04 10:43:34.682870 bitfount-0.5.9/tests/utils/fixtures/
+-rw-r--r--   0 runner    (1001) docker     (121)      517 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/utils/fixtures/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (121)     2449 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/utils/fixtures/authorisation_checker_fixtures.py
+-rw-r--r--   0 runner    (1001) docker     (121)     3587 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/utils/fixtures/encryption_fixtures.py
+-rw-r--r--   0 runner    (1001) docker     (121)     2092 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/utils/fixtures/env_fixtures.py
+-rw-r--r--   0 runner    (1001) docker     (121)     1994 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/utils/fixtures/helper_fixtures.py
+-rw-r--r--   0 runner    (1001) docker     (121)    12967 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/utils/fixtures/hub_and_am_fixtures.py
+-rw-r--r--   0 runner    (1001) docker     (121)      782 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/utils/fixtures/schema_fixtures.py
+-rw-r--r--   0 runner    (1001) docker     (121)      976 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/utils/fixtures/storage_fixtures.py
+-rw-r--r--   0 runner    (1001) docker     (121)     3942 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/utils/fixtures/transport_layer_fixtures.py
+-rw-r--r--   0 runner    (1001) docker     (121)    26264 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/utils/helper.py
+-rw-r--r--   0 runner    (1001) docker     (121)    16512 2022-02-04 10:43:24.000000 bitfount-0.5.9/tests/utils/mocks.py
```

### Comparing `bitfount-0.5.86/README.md` & `bitfount-0.5.9/README.md`

 * *Files 24% similar despite different names*

```diff
@@ -1,42 +1,36 @@
-<div align="center">
-<img src="https://bitfount-web-resources.s3.eu-west-2.amazonaws.com/bitfount_logo_horizontal.png" width="600px">
+# Bitfount
 
-**Federated learning and data analytics that just works**
-
----
-
-</br>
 <!-- Github workflow badges are case sensitive - the name must match the name of the workflow exactly -->
 
-![Python versions](https://img.shields.io/pypi/pyversions/bitfount)
-[![PyPI Latest Release](https://img.shields.io/pypi/v/bitfount.svg)](https://pypi.org/project/bitfount/)
-[![PyPI Downloads](https://pepy.tech/badge/bitfount)](https://pepy.tech/project/bitfount)
-![](https://github.com/bitfount/bitfount/workflows/CI/badge.svg?branch=develop)
-![](https://github.com/bitfount/bitfount/workflows/tutorials/badge.svg?branch=develop)
+![Python 3.8](https://img.shields.io/badge/python-3.8-blue.svg)
+![CI](https://github.com/bitfount/bitfount/workflows/CI/badge.svg?branch=develop)
+![tutorials](https://github.com/bitfount/bitfount/workflows/tutorials/badge.svg?branch=develop)
 [![codecov](https://codecov.io/gh/bitfount/bitfount/branch/develop/graph/badge.svg?token=r1hulrgehK)](https://codecov.io/gh/bitfount/bitfount)
 [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
 [![security: bandit](https://img.shields.io/badge/security-bandit-yellow.svg)](https://github.com/PyCQA/bandit)
-[![mypy type checked](https://img.shields.io/badge/mypy-checked-blue)](https://github.com/python/mypy)
+[![mypy type checked](https://img.shields.io/badge/mypy-checked-blue)](#mypy)
 [![flake8](https://img.shields.io/badge/linter-flake8-success)](https://github.com/PyCQA/flake8)
-[![license](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/bitfount/bitfount/blob/develop/LICENSE)
 
 <!-- ![docs-coverage](interrogate.svg) -->
 
-</div>
+This repository enables quick and easy experimentation with machine learning and federated learning models.
 
 ## Table of Contents
 
-- [Using the Docker images](#using-the-docker-images)
-- [Running the Python code](#running-the-python-code)
-  - [Installation](#installation)
-  - [Getting started (Tutorials)](#getting-started-tutorials)
-  - [Federated training scripts](#federated-training-scripts)
-  - [Basic Local Usage](#basic-local-usage)
-- [License](#license)
+- [Bitfount](#bitfount)
+  - [Table of Contents](#table-of-contents)
+  - [Using the Docker images](#using-the-docker-images)
+  - [Running the Python code](#running-the-python-code)
+    - [Installation](#installation)
+    - [Environment variables](#environment-variables)
+    - [Getting started (Tutorials)](#getting-started-tutorials)
+    - [Federated training scripts](#federated-training-scripts)
+    - [Basic Local Usage](#basic-local-usage)
+  - [License](#license)
 
 ## Using the Docker images
 
 There are two docker images, one for running a Pod (`ghcr.io/bitfount/pod:stable`),
 and another for running a modelling task (`ghcr.io/bitfount/modeller:stable`).
 
 Both of the images require a `config.yaml` file to be provided to them,
@@ -63,69 +57,50 @@
 #### Where to get it
 
 Binary installers for the latest released version are available at the [Python
 Package Index (PyPI)](https://pypi.org/project/bitfount).
 
 `pip install bitfount`
 
-If you want to use differential privacy (DP), you will need to install the DP extras:
-
-`pip install 'bitfount[dp]'`
-
-Ensure you are using python 3.8 or 3.9. The DP extra is not supported on 3.10.
-
-If you are planning on using the `bitfount` package with Jupyter Notebooks, we recommend you install the splinter package `bitfount[tutorials]` which will make sure you are running compatible jupyter dependencies.
-
-`pip install 'bitfount[tutorials]'`
-
 #### Installation from sources
 
-To install `bitfount` from source you need to create a python virtual environment.
+To install `bitfount` from source you need to create a python 3.8 virtual environment.
 
-In the `bitfount` directory (same one where you found this file after cloning the git repo), execute:
+In the `bitfount` directory (same one where you found this file after
+cloning the git repo), execute:
 
 `pip install -r requirements/requirements.in`
 
 These requirements are set to permissive ranges but are not guaranteed to work for all releases, especially the latest versions. For a pinned version of these requirements which are guaranteed to work, run the following command instead:
 
-```bash
-#!/bin/bash
-PYTHON_VERSION=$(python -c "import platform; print(''.join(platform.python_version_tuple()[:2]))")
-pip install -r requirements/${PYTHON_VERSION}/requirements.txt
-```
-
-To be able to use differential privacy (DP), you will need to additionally install the DP requirements. Please note that this is only compatible with Python version 3.8 and 3.9. Also, it is restricted to non-arm architectures:
-
-```bash
-#!/bin/bash
-PYTHON_VERSION=$(python -c "import platform; print(''.join(platform.python_version_tuple()[:2]))")
-PLATFORM_PROCESSOR=$(python -c "import platform; print(platform.processor())")
-
-if [[ ${PYTHON_VERSION} == "38" || ${PYTHON_VERSION} == "39" ]] && [[ ${PLATFORM_PROCESSOR} != "arm" ]]; then
-    pip install -r requirements/${PYTHON_VERSION}/differential_privacy/requirements-dp.txt
-fi
-```
+`pip install -r requirements/requirements.txt`
 
 For MacOS you also need to install `libomp`:
 
 `brew install libomp`
 
+### Environment variables
+
+The following environment variables can optionally be set:
+
+- `BITFOUNT_ENGINE`: determines the backend used. Current accepted values are "basic" or "pytorch". If pytorch is installed, this will automatically be selected
+- `BITFOUNT_LOG_TO_FILE`: determines whether bitfount logs to file as well as console. Accepted values are "true" or "false". Defaults to "true"
+- `BITFOUNT_LOGS_DIR`: determines where logfiles are stored. If empty, logs will be stored in a subdirectory called `bitfount_logs` in the directory where the script is run from
+- `BITFOUNT_ENVIRONMENT`: accepted values are "production" or "staging". Defaults to "production". Should only be used for development purposes.
+- `BITFOUNT_POD_VITALS_PORT`: determines the TCP port number to serve the pod vitals health check over. You can check the state of a running pod's health by accessing `http://localhost:{{ BITFOUNT_POD_VITALS_PORT }}/health`. A random open port will be selected if `BITFOUNT_POD_VITALS_PORT` is not set.
+
 ### Getting started (Tutorials)
 
 In order to run the tutorials, you also need to install the tutorial requirements:
 
-```bash
-#!/bin/bash
-PYTHON_VERSION=$(python -c "import platform; print(''.join(platform.python_version_tuple()[:2]))")
-pip install -r requirements/${PYTHON_VERSION}/requirements-tutorial.txt
-```
+`pip install -r requirements/requirements-tutorial.txt`
 
-To get started using the Bitfount package in a federated setting, we recommend
-that you start with our tutorials. Run `jupyter notebook`and open up the first
-tutorial in the "Connecting Data & Creating Pods folder: `running_a_pod.ipynb`
+To get started using the Bitfount package in a federated setting,
+we recommend that you start with our tutorials. Run `jupyter notebook`
+and open up the first tutorial at: `tutorials/FL - Part 1 - Training a model.ipynb`
 
 ### Federated training scripts
 
 Some simple scripts have been provided to run a Pod or Modelling job from a config file.
 
 > ⚠️ If you are running from a source install (such as from `git clone`) you will
 > need to use <span style="white-space: nowrap">`python -m scripts.<script_name>`</span>
@@ -145,94 +120,75 @@
 
 **1\. Import bitfount**
 
 ```python
 import bitfount as bf
 ```
 
-**2\. Create DataSource and load data**
+**2\. Create DataSource**
 
 ```python
-census_income = bf.CSVSource(
-    path="https://bitfount-hosted-downloads.s3.eu-west-2.amazonaws.com/bitfount-tutorials/census_income.csv",
-    ignore_cols=["fnlwgt"],
+prosper = bf.DataSource(
+    data="https://bitfount-hosted-downloads.s3.eu-west-2.amazonaws.com/prosper.csv",
+    ignore_cols=['NPV', 'IRR', 'PROFIT'],
 )
-census_income.load_data()
 ```
 
 **3\. Create Schema**
 
 ```python
-schema = bf.BitfountSchema(
-    census_income,
-    table_name="census_income",
-    force_stypes={
-        "census_income": {
-            "categorical":[
-                "TARGET",
-                "workclass",
-                "marital-status",
-                "occupation",
-                "relationship",
-                "race",
-                "native-country",
-                "gender",
-                "education"
-            ]
-        }
-    }
-)
+schema = bf.BitfountSchema(prosper, force_stype={'categorical':["TARGET", "CreditGrade"]})
 ```
 
 **4\. Transform Data**
 
 ```python
 clean_data = bf.CleanDataTransformation()
-processor = bf.TransformationProcessor([clean_data], schema.get_table_schema("census_income"))
-census_income.data = processor.transform(census_income.data)
-schema.add_datasource_tables(census_income, table_name="census_income")
+processor = bf.TransformationProcessor([clean_data], schema)
+prosper.data = processor.transform(prosper.data)
+schema.add_datasource_features(prosper)
 ```
 
 **5\. Create DataStructure**
 
 ```python
-census_income_data_structure=bf.DataStructure(
-  table="census_income",
-  target="TARGET",
+prosper_data_structure=bf.DataStructure(
+  target='TARGET'
 )
 ```
 
 **6\. Create and Train Model**
 
 ```python
 nn = bf.PyTorchTabularClassifier(
-    datastructure=census_income_data_structure,
+    datastructure=prosper_data_structure,
     schema=schema,
     epochs=2,
     batch_size=256,
-    optimizer=bf.Optimizer("RAdam", {"lr": 0.001}),
+    optimizer=bf.Optimizer("RAdam", {"lr": 0.001})
 )
-nn.fit(census_income)
+nn.fit(prosper)
 nn.serialize("demo_task_model.pt")
 ```
 
 **7\. Evaluate**
 
 ```python
-preds, target = nn.evaluate()
+preds, targs = nn.evaluate()
 metrics = bf.MetricCollection.create_from_model(nn)
-results = metrics.compute(target, preds)
+results = metrics.compute(targs, preds)
 print(results)
 ```
 
 **8\. Assert results**
 
 ```python
 import numpy as np
-assert nn._validation_results[-1]["validation_loss"] is not np.nan
+assert nn._train_loss is not np.nan
+assert nn._validation_loss is not np.nan
 assert results["AUC"] > 0.7
 ```
 
 ## License
 
-The license for this software is available in the `LICENSE` file.
+The license for this software is available in the `LICENSE.md` file.
 This can be found in the Github Repository, as well as inside the Docker image.
```

### Comparing `bitfount-0.5.86/bitfount/__version__.py` & `bitfount-0.5.9/bitfount/__version__.py`

 * *Files 21% similar despite different names*

```diff
@@ -7,8 +7,8 @@
 
 __author__ = "Bitfount"
 __author_email__ = "info@bitfount.com"
 __copyright__ = "Copyright 2021 Bitfount Ltd"
 __description__ = "Machine Learning and Federated Learning Library."
 __title__ = "bitfount"
 __url__ = "https://github.com/bitfount/bitfount"
-__version__ = "0.5.86"
+__version__ = "0.5.9"
```

### Comparing `bitfount-0.5.86/bitfount/backends/lightgbm/models/base_models.py` & `bitfount-0.5.9/bitfount/backends/lightgbm/models/base_models.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,48 +1,27 @@
 """Base forest-related models using LightGBM as the gradient booster."""
 from abc import abstractmethod
-from io import BytesIO
 import os
-from typing import Any, ClassVar, Dict, List, Optional, Sequence, Tuple, Union, cast
+from typing import Any, Dict, Optional, Sequence, Tuple, Union
 
 import lightgbm as lgb
 from marshmallow import fields
 import numpy as np
 
-from bitfount.data.databunch import BitfountDataBunch
-from bitfount.data.dataloaders import BitfountDataLoader
-from bitfount.data.datasources.base_source import BaseSource
-from bitfount.models.base_models import ClassifierMixIn, _BaseModel
-from bitfount.types import T_FIELDS_DICT
+from bitfount.data.dataloader import _BitfountDataLoader
+from bitfount.data.datasource import DataSource
+from bitfount.models.base_models import _BaseModel
+from bitfount.types import _DataFrameType, _JSONDict
 
 # TODO: [BIT-1041] Introduce non-implementation base model
-from bitfount.utils import delegates
 
 
-@delegates()
 class BaseLGBMRandomForest(_BaseModel):
     """Implements an (optionally Gradient Boosted) Random Forest from LightGBM."""
 
-    fields_dict: ClassVar[T_FIELDS_DICT] = {
-        "gradient_boosting": fields.Boolean(),
-        "num_leaves": fields.Integer(allow_none=True),
-        "max_depth": fields.Integer(allow_none=True),
-        "subsample_for_bin": fields.Integer(allow_none=True),
-        "num_iterations": fields.Integer(allow_none=True),
-        "learning_rate": fields.Float(allow_none=True),
-        "reg_alpha": fields.Float(allow_none=True),
-        "reg_lambda": fields.Float(allow_none=True),
-        "bagging_freq": fields.Integer(allow_none=True),
-        "bagging_fraction": fields.Float(allow_none=True),
-        "feature_fraction": fields.Float(allow_none=True),
-        "early_stopping_rounds": fields.Integer(allow_none=True),
-        "verbose": fields.Integer(allow_none=True),
-        "min_split_gain": fields.Float(allow_none=True),
-    }
-
     def __init__(
         self,
         gradient_boosting: bool = False,
         num_leaves: Optional[int] = None,
         max_depth: Optional[int] = None,
         subsample_for_bin: Optional[int] = None,
         num_iterations: Optional[int] = None,
@@ -77,55 +56,64 @@
         )
         self.verbose = 100 if verbose is None else verbose
         self.min_split_gain = 0.025 if min_split_gain is None else min_split_gain
         self.test_set = None
         self.objective: Optional[str] = None
         self.training_metrics: Optional[Sequence[str]] = None
 
+    class _Schema(_BaseModel._Schema):
+        gradient_boosting = fields.Boolean()
+        num_leaves = fields.Integer(allow_none=True)
+        max_depth = fields.Integer(allow_none=True)
+        subsample_for_bin = fields.Integer(allow_none=True)
+        num_iterations = fields.Integer(allow_none=True)
+        learning_rate = fields.Float(allow_none=True)
+        reg_alpha = fields.Float(allow_none=True)
+        reg_lambda = fields.Float(allow_none=True)
+        bagging_freq = fields.Integer(allow_none=True)
+        bagging_fraction = fields.Float(allow_none=True)
+        feature_fraction = fields.Float(allow_none=True)
+        early_stopping_rounds = fields.Integer(allow_none=True)
+        verbose = fields.Integer(allow_none=True)
+        min_split_gain = fields.Float(allow_none=True)
+
+        @abstractmethod
+        def recreate_model(self, data: _JSONDict, **kwargs: Any) -> _BaseModel:
+            """Recreates model using Schema."""
+            raise NotImplementedError
+
     @abstractmethod
     def _set_objective(self) -> None:
         """Set training objective."""
         raise NotImplementedError
 
     @abstractmethod
     def _set_training_metrics(self) -> None:
         """Set training metrics."""
         raise NotImplementedError
 
-    def predict(self, *args: Any, **kwargs: Any) -> List[np.ndarray]:
-        """Returns model predictions. Not implemented yet."""
-        # TODO: [BIT-2406] Implement this method
-        raise NotImplementedError
-
     def serialize(self, filename: Union[str, os.PathLike]) -> None:
         """Serialize model."""
         self._model.save_model(filename, num_iteration=self._model.best_iteration)
 
-    def deserialize(self, content: Union[str, os.PathLike, bytes]) -> None:
+    def deserialize(self, filename: Union[str, os.PathLike]) -> None:
         """Deserialize model."""
-        load_contents = BytesIO(content) if isinstance(content, bytes) else content
-        self._model = lgb.Booster(model_file=load_contents)
+        self._model = lgb.Booster(model_file=filename)
 
     def evaluate(
-        self, test_dl: Optional[BitfountDataLoader] = None, *args: Any, **kwargs: Any
+        self, test_dl: Optional[_BitfountDataLoader] = None, **kwargs: Any
     ) -> Tuple[np.ndarray, np.ndarray]:
         """Perform inference on test set and save dictionary of metrics."""
         if test_dl is None:
-            if isinstance(self.test_dl, BitfountDataLoader):
+            if isinstance(self.test_dl, _BitfountDataLoader):
                 test_dl = self.test_dl
             else:
                 raise ValueError("There is no test data to evaluate the model on.")
 
-        test_df = test_dl.get_x_dataframe()
-        if isinstance(test_df, tuple):
-            raise ValueError(
-                "Multiple dataframes retrieved unexpectedly; "
-                "this model does not support combination tabular and image data."
-            )
-
+        test_df: _DataFrameType = test_dl.get_x_dataframe()
         test_preds = np.zeros(test_df.shape[0])
         best_iteration = self._model.best_iteration
         test_preds += self._model.predict(test_df, num_iteration=best_iteration)
         test_target = test_dl.get_y_dataframe().to_numpy()
 
         return test_preds, test_target
 
@@ -149,39 +137,27 @@
             "verbose": self.verbose,
         }
 
         return params
 
     def _create_dataset(self) -> Tuple[lgb.Dataset, Optional[lgb.Dataset]]:
         """Create LightGBM Dataset object for better memory management."""
-        x_train = self.train_dl.get_x_dataframe()
-        if isinstance(x_train, tuple):
-            raise ValueError(
-                "Multiple dataframes retrieved unexpectedly; "
-                "this model does not support combination tabular and image data."
-            )
-
+        x_train: _DataFrameType = self.train_dl.get_x_dataframe()
         y_train = self.train_dl.get_y_dataframe().astype("int64")
         train_df = lgb.Dataset(
             data=x_train,
             label=y_train,
             free_raw_data=True,
         )
 
         weights_col = self.datastructure.loss_weights_col
         if weights_col is not None:
             train_df = train_df.set_weight(weight=list(x_train[weights_col]))
-        if isinstance(self.validation_dl, BitfountDataLoader):
-            x_valid = self.validation_dl.get_x_dataframe()
-            if isinstance(x_valid, tuple):
-                raise ValueError(
-                    "Multiple dataframes retrieved unexpectedly; "
-                    "this model does not support combination tabular and image data."
-                )
-
+        if isinstance(self.validation_dl, _BitfountDataLoader):
+            x_valid: _DataFrameType = self.validation_dl.get_x_dataframe()
             y_valid = self.validation_dl.get_y_dataframe().astype("int64")
             # For LightGBM DataFrame.dtypes for label must be int, float or bool
 
             validation_df = lgb.Dataset(
                 data=x_valid,
                 label=y_valid,
                 free_raw_data=True,
@@ -192,60 +168,35 @@
                     weight=list(x_valid[weights_col])
                 )
         else:
             validation_df = None
 
         return train_df, validation_df
 
-    def fit(self, data: Optional[BaseSource] = None, *args: Any, **kwargs: Any) -> None:
-        """Trains a model using the training set provided by the BaseSource object."""
+    def fit(self, data: Optional[DataSource] = None, *args: Any, **kwargs: Any) -> None:
+        """Trains a model using the training set provided by the DataSource object."""
         if data:
-            if self.datastructure.query:
-                table_schema = self.datastructure._override_schema()
-                self.databunch = BitfountDataBunch(
-                    data_structure=self.datastructure,
-                    schema=table_schema,
-                    datasource=data,
-                )
-            elif self.datastructure.table:
-                table_name = self.datastructure.get_table_name()
-                table_schema = self.schema.get_table_schema(table_name)
-                self._add_datasource_to_schema(data)
-
-            if self.objective == "binary":
-                # The casts here are to assuage mypy because it (incorrectly) asserts
-                # that a subclass of both ClassifierMixIn and LightGBMModel can't exist.
-                # We utilise a subclass of both in the tests to assure ourselves.
-                if isinstance(cast(ClassifierMixIn, self), ClassifierMixIn):
-                    cast(ClassifierMixIn, self).set_number_of_classes(table_schema)
-                else:
-                    raise TypeError(
-                        "Training objective is classification but this model does not "
-                        "inherit from ClassifierMixIn"
-                    )
+            self._add_datasource_to_schema(data)
             self._set_dataloaders()
-
         else:
             raise ValueError(
                 "No data provided. This model can only be trained on local data."
             )
 
         params = self.get_params()
         lgb_train, lgb_valid = self._create_dataset()
-        if lgb_valid:
-            valid_sets = [lgb_valid, lgb_train]
-            valid_names = ["valid", "train"]
-        else:
-            valid_sets = [lgb_train]
-            valid_names = ["train"]
-
         model = lgb.train(
             params=params,
             num_boost_round=self.num_iterations,
             train_set=lgb_train,
-            valid_sets=valid_sets,
-            valid_names=valid_names,
+            valid_sets=[lgb_valid, lgb_train],
+            valid_names=["valid", "train"],
             verbose_eval=self.verbose,
             early_stopping_rounds=self.early_stopping_rounds,
             init_model=self._model,
         )
-        self._model: lgb.Booster = model
+        self._model: lgb = model
+
+    @property
+    def training_needed(self) -> bool:
+        """Dictates whether the model needs training."""
+        return self.num_iterations > 0
```

### Comparing `bitfount-0.5.86/bitfount/backends/lightgbm/models/models.py` & `bitfount-0.5.9/bitfount/backends/lightgbm/models/models.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,47 +1,70 @@
 """Models using LightGBM as the backend."""
 from __future__ import annotations
 
-from typing import Any
+from typing import Any, Type
+
+from marshmallow import post_load
 
 from bitfount.backends.lightgbm.models.base_models import BaseLGBMRandomForest
 from bitfount.models.base_models import ClassifierMixIn, RegressorMixIn
-from bitfount.utils import delegates
+from bitfount.types import _JSONDict
 
 
-@delegates()
 class LGBMRandomForestClassifier(ClassifierMixIn, BaseLGBMRandomForest):
     """LGBM Classifier for Random Forests and GBMs.
 
     Currently only supports binary classification.
     """
 
     def __init__(self, **kwargs: Any):
         super().__init__(**kwargs)
         self._set_objective()
         self._set_training_metrics()
+        self.set_number_of_classes()
 
     def _set_objective(self) -> None:
-        """Set training objective."""
         self.objective: str = "binary"
 
     def _set_training_metrics(self) -> None:
-        """Set training metrics."""
         self.training_metrics = ["binary_logloss", "auc"]
 
+    class _Schema(BaseLGBMRandomForest._Schema, ClassifierMixIn._Schema):
+        @post_load
+        def recreate_model(
+            self, data: _JSONDict, **kwargs: Any
+        ) -> LGBMRandomForestClassifier:
+            """Recreate LGBM Classifier."""
+            return LGBMRandomForestClassifier(**data)
+
+    @classmethod
+    def get_schema(cls) -> Type[LGBMRandomForestClassifier._Schema]:
+        """Get the model schema."""
+        return cls._Schema
+
 
-@delegates()
 class LGBMRandomForestRegressor(RegressorMixIn, BaseLGBMRandomForest):
     """LGBM Regressor for Random Forests and GBMs."""
 
     def __init__(self, **kwargs: Any):
         super().__init__(**kwargs)
         self._set_objective()
         self._set_training_metrics()
 
     def _set_objective(self) -> None:
-        """Set training objective."""
         self.objective = "regression"
 
     def _set_training_metrics(self) -> None:
-        """Set training metrics."""
         self.training_metrics = ["mae", "mse", "rmse"]
+
+    class _Schema(BaseLGBMRandomForest._Schema):
+        @post_load
+        def recreate_model(
+            self, data: _JSONDict, **kwargs: Any
+        ) -> LGBMRandomForestRegressor:
+            """Recreate LGBM Regressor."""
+            return LGBMRandomForestRegressor(**data)
+
+    @classmethod
+    def get_schema(cls) -> Type[LGBMRandomForestRegressor._Schema]:
+        """Get the model schema."""
+        return cls._Schema
```

### Comparing `bitfount-0.5.86/bitfount/backends/pytorch/__init__.py` & `bitfount-0.5.9/bitfount/backends/pytorch/__init__.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,42 +1,30 @@
 """PyTorch implementations for the Bitfount framework."""
 
 from typing import List
 
-from bitfount.backends.pytorch.data.dataloaders import (
-    DEFAULT_BUFFER_SIZE,
-    PyTorchBitfountDataLoader,
-    PyTorchIterableBitfountDataLoader,
-)
-from bitfount.backends.pytorch.federated.shim import PyTorchBackendTensorShim
-from bitfount.backends.pytorch.loss import SoftDiceLoss, soft_dice_loss
+from bitfount.backends.pytorch.federated.models import PyTorchBackendTensorShim
 from bitfount.backends.pytorch.models.base_models import PyTorchClassifierMixIn
 from bitfount.backends.pytorch.models.bitfount_model import PyTorchBitfountModel
 from bitfount.backends.pytorch.models.models import (
     PyTorchImageClassifier,
     PyTorchTabularClassifier,
     TabNetClassifier,
 )
 from bitfount.backends.pytorch.models.torch_functions.mish import Mish
-from bitfount.backends.pytorch.utils import LoggerType, autodetect_gpu
+from bitfount.backends.pytorch.utils import LoggerType
 
 __all__: List[str] = [
-    "autodetect_gpu",
-    "DEFAULT_BUFFER_SIZE",
     "LoggerType",
     "Mish",
     "PyTorchBackendTensorShim",
-    "PyTorchBitfountDataLoader",
     "PyTorchBitfountModel",
     "PyTorchClassifierMixIn",
     "PyTorchImageClassifier",
-    "PyTorchIterableBitfountDataLoader",
     "PyTorchTabularClassifier",
-    "soft_dice_loss",
-    "SoftDiceLoss",
     "TabNetClassifier",
 ]
 
 # See top level `__init__.py` for an explanation
 __pdoc__ = {}
 for _obj in __all__:
     __pdoc__[_obj] = False
```

### Comparing `bitfount-0.5.86/bitfount/backends/pytorch/data/datafactory.py` & `bitfount-0.5.9/bitfount/backends/pytorch/data/datafactory.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,87 +1,48 @@
 """PyTorch implementations of the datafactory module contents."""
-from __future__ import annotations
+from typing import Any, List, Literal, Mapping, Optional, Union, cast
 
-from typing import TYPE_CHECKING, Any, List, Mapping, Optional, Union, cast
+import pandas as pd
+from torch.utils.data import Dataset
 
-from bitfount.backends.pytorch.data.dataloaders import (
-    PyTorchBitfountDataLoader,
-    PyTorchIterableBitfountDataLoader,
-    _BasePyTorchBitfountDataLoader,
-)
-from bitfount.backends.pytorch.data.datasets import (
-    _PyTorchDataset,
-    _PyTorchIterableDataset,
-)
-from bitfount.data.datafactory import _DataFactory
-from bitfount.data.datasources.base_source import BaseSource, IterableSource
-from bitfount.data.datasources.views import DataView
-
-if TYPE_CHECKING:
-    from bitfount.data.datasets import _BaseBitfountDataset
-    from bitfount.data.datasplitters import DatasetSplitter
-    from bitfount.data.schema import TableSchema
-    from bitfount.data.types import DataSplit, _SemanticTypeValue
-    from bitfount.transformations.batch_operations import BatchTimeOperation
+from bitfount.backends.pytorch.data.dataloaders import _PyTorchBitfountDataLoader
+from bitfount.backends.pytorch.data.datasets import _PyTorchBaseDataset, _PyTorchDataset
+from bitfount.data.datafactory import _BaseDataset, _DataFactory
+from bitfount.data.types import _SemanticTypeValue
+from bitfount.transformations.batch_operations import BatchTimeOperation
 
 
 class _PyTorchDataFactory(_DataFactory):
     """A PyTorch-specific implementation of the DataFactory provider."""
 
     def create_dataloader(
-        self,
-        dataset: _BaseBitfountDataset,
-        batch_size: Optional[int] = None,
-    ) -> _BasePyTorchBitfountDataLoader:
+        self, data: _BaseDataset, batch_size: Optional[int] = None
+    ) -> _PyTorchBitfountDataLoader:
         """See base class."""
-        if isinstance(dataset, _PyTorchIterableDataset):
-            return PyTorchIterableBitfountDataLoader(
-                dataset=dataset, batch_size=batch_size
+        if not isinstance(data, Dataset):
+            raise TypeError(
+                "The PyTorchBitfountDataLoader class only supports "
+                "subclasses of PyTorch Dataset."
             )
-        elif isinstance(dataset, _PyTorchDataset):
-            return PyTorchBitfountDataLoader(dataset, batch_size=batch_size)
 
-        raise TypeError(
-            "The _PyTorchDataFactory class only supports "
-            "subclasses of PyTorch Dataset for creating a DataLoader."
-        )
+        # If it's an instance of Dataset and of BitfountDataset, we can try to use it
+        data = cast(_PyTorchBaseDataset, data)  # stop type-checker from complaining
+        return _PyTorchBitfountDataLoader(data, batch_size)
 
     def create_dataset(
         self,
-        datasource: Union[BaseSource, DataView],
-        data_splitter: Optional[DatasetSplitter],
-        data_split: DataSplit,
-        schema: TableSchema,
-        selected_cols: List[str],
-        selected_cols_semantic_types: Mapping[_SemanticTypeValue, List[str]],
+        data: pd.DataFrame,
+        selected_cols: Mapping[_SemanticTypeValue, List[str]],
         target: Optional[Union[str, List[str]]] = None,
         batch_transforms: Optional[List[BatchTimeOperation]] = None,
-        auto_convert_grayscale_images: bool = True,
+        batch_transformation_step: Optional[Literal["train", "validation"]] = None,
         **kwargs: Any,
-    ) -> Union[_PyTorchDataset, _PyTorchIterableDataset]:
+    ) -> _PyTorchDataset:
         """See base class."""
-        if datasource.iterable:
-            return _PyTorchIterableDataset(
-                schema=schema,
-                selected_cols_semantic_types=selected_cols_semantic_types,
-                data_splitter=data_splitter,
-                datasource=cast(IterableSource, datasource),
-                target=target,
-                selected_cols=selected_cols,
-                batch_transforms=batch_transforms,
-                data_split=data_split,
-                auto_convert_grayscale_images=auto_convert_grayscale_images,
-                **kwargs,
-            )
-
         return _PyTorchDataset(
-            schema=schema,
-            selected_cols_semantic_types=selected_cols_semantic_types,
-            data_splitter=data_splitter,
-            datasource=datasource,
+            data=data,
             target=target,
             selected_cols=selected_cols,
             batch_transforms=batch_transforms,
-            data_split=data_split,
-            auto_convert_grayscale_images=auto_convert_grayscale_images,
+            batch_transformation_step=batch_transformation_step,
             **kwargs,
         )
```

### Comparing `bitfount-0.5.86/bitfount/backends/pytorch/models/base_models.py` & `bitfount-0.5.9/bitfount/backends/pytorch/models/base_models.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,232 +1,157 @@
 """Base models and helper classes using PyTorch as the backend."""
 from __future__ import annotations
 
 from abc import abstractmethod
 import contextlib
 from functools import partial
 import inspect
-from io import BytesIO
 from itertools import chain
 import logging
 import os
-import re
 from typing import (
     Any,
     Callable,
-    ClassVar,
     Dict,
     Generic,
-    Iterable,
     Iterator,
     List,
     Literal,
     MutableMapping,
     Optional,
-    OrderedDict,
     Tuple,
     Type,
     TypeVar,
     Union,
     cast,
 )
 
-import desert
 from marshmallow import fields
 import numpy as np
+from opacus import PrivacyEngine
+from opacus.dp_model_inspector import DPModelInspector, IncompatibleModuleException
+from opacus.utils import module_modification
 import pandas as pd
 import pytorch_lightning as pl
-from pytorch_lightning.callbacks import (
-    Callback,
-    ModelCheckpoint,
-    StochasticWeightAveraging,
-)
 from pytorch_lightning.callbacks.early_stopping import EarlyStopping
-from pytorch_lightning.callbacks.progress import TQDMProgressBar
-from pytorch_lightning.loggers import TensorBoardLogger
-from sklearn.base import BaseEstimator
+from pytorch_lightning.loggers import LightningLoggerBase, TensorBoardLogger
+import pytorch_tabnet as tabnet
 import torch
-from torch import Tensor, nn as nn
+from torch import Tensor
+from torch import nn as nn
 
 # Convention is to import functional as F
 # noinspection PyPep8Naming
 from torch.nn import functional as F
 import torch.optim as optimizers
-from torch.optim.lr_scheduler import _LRScheduler
 import torch.optim.lr_scheduler as schedulers
 from torch.utils.data import DataLoader
 import torch_optimizer as torch_optimizers
 
-from bitfount.backends.pytorch._torch_shims import LightningLoggerBase
-from bitfount.backends.pytorch.data.dataloaders import (
-    PyTorchBitfountDataLoader,
-    PyTorchIterableBitfountDataLoader,
-    _BasePyTorchBitfountDataLoader,
-)
-from bitfount.backends.pytorch.federated.mixins import _PyTorchDistributedModelMixIn
+from bitfount.backends.pytorch.data.datafactory import _PyTorchDataFactory
+from bitfount.backends.pytorch.federated.models import _PyTorchDistributedModelMixIn
 from bitfount.backends.pytorch.types import (
     ImgAndTabBatch,
     ImgAndTabDataSplit,
     ImgDataReturnType,
     ImgFwdTypes,
     ImgXorTabDataSplit,
     TabDataReturnType,
     TabFwdTypes,
     TabxorImgBatch,
     _AdaptorForPyTorchTensor,
 )
-from bitfount.backends.pytorch.utils import (
-    LoggerType,
-    autodetect_gpu,
-    enhanced_torch_load,
-)
-from bitfount.backends.pytorch.weight_clipper import _PytorchParamConstraint
-from bitfount.config import BITFOUNT_LOGS_DIR, BITFOUNT_OUTPUT_DIR, DP_AVAILABLE
-from bitfount.data.databunch import BitfountDataBunch
-from bitfount.data.dataloaders import BitfountDataLoader
-from bitfount.data.datasets import _BitfountDataset
-from bitfount.data.datasources.base_source import BaseSource
-from bitfount.data.datasources.dataframe_source import DataFrameSource
-from bitfount.data.types import DataSplit, _SingleOrMulti
-from bitfount.federated.helper import TaskContext
+from bitfount.backends.pytorch.utils import LoggerType, _autodetect_gpu
+from bitfount.config import BITFOUNT_LOGS_DIR
+from bitfount.data.dataloader import _BitfountDataLoader
+from bitfount.data.datasets import _Dataset
+from bitfount.data.datasource import DataSource
+from bitfount.data.types import _SingleOrMulti
 from bitfount.federated.privacy.differential import (
     DPModellerConfig,
     _DifferentiallyPrivate,
 )
 from bitfount.metrics import Metric, MetricCollection
 from bitfount.models.base_models import (
     ClassifierMixIn,
     LoggerConfig,
+    ModelContext,
     NeuralNetworkMixIn,
     NeuralNetworkPredefinedModel,
     Optimizer,
     Scheduler,
     _BaseModel,
 )
-from bitfount.types import T_DTYPE, T_FIELDS_DICT, _StrAnyDict, _TensorLike
-from bitfount.utils import _merge_list_of_dicts, delegates, seed_all
-from bitfount.utils.logging_utils import filter_stderr
+from bitfount.types import T_DTYPE, _JSONDict, _TensorLike
+from bitfount.utils import _merge_list_of_dicts
 
 logger = logging.getLogger(__name__)
 
-
-if DP_AVAILABLE:
-    from opacus import GradSampleModule, PrivacyEngine
-    from opacus.accountants import RDPAccountant
-    from opacus.validators import ModuleValidator, register_module_fixer
-    from opacus.validators.errors import UnsupportedModuleError
-
-
 _OptimizerType = Union[torch_optimizers.Optimizer, optimizers.Optimizer]
 
 _OPTIMIZERS: Dict[str, Type[_OptimizerType]] = {
     name: class_
     # If there are name clashes, preference is given to the original torch version
     for name, class_ in dict(
-        cast(
-            Iterator[Tuple[str, Any]],
-            chain.from_iterable(
-                d.items() for d in (vars(torch_optimizers), vars(optimizers))
-            ),
+        chain.from_iterable(
+            d.items() for d in (vars(torch_optimizers), vars(optimizers))
         )
     ).items()
     if inspect.isclass(class_)
     and issubclass(class_, (optimizers.Optimizer, torch_optimizers.Optimizer))
     and not inspect.isabstract(class_)
     and name != "Optimizer"
 }
 
-_SCHEDULERS: Dict[str, Type[_LRScheduler]] = {
+_SCHEDULERS: Dict[str, Type[schedulers._LRScheduler]] = {
     name: class_
     for name, class_ in vars(schedulers).items()
     if inspect.isclass(class_)
-    and issubclass(class_, _LRScheduler)
+    and issubclass(class_, schedulers._LRScheduler)
     and not inspect.isabstract(class_)
     and name != "_LRScheduler"
 }
 
 _TORCH_DTYPES: Dict[int, torch.dtype] = {
     16: torch.float16,
     32: torch.float32,
     64: torch.float64,
 }
-_STEP_OUTPUT = Union[torch.Tensor, _StrAnyDict]  # From pl.LightningModule
+_STEP_OUTPUT = Union[torch.Tensor, Dict[str, Any]]  # From pl.LightningModule
 
 
 def _calculate_embedding_sizes(
-    categorical_feature_sizes: Iterable[int],
+    categorical_feature_sizes: List[int],
 ) -> List[Tuple[int, int]]:
     """Calculate embedding sizes.
 
     The formula for determining the size of the embeddings is determined by
     empirical evidence alone and borrowed from the fast.ai library.
-
-    Args:
-        categorical_feature_sizes: An iterable of the number of categories in each
-            categorical feature.
-
-    Returns:
-        A list of (number of categories, embedding vector length) pairs for each
-        categorical feature.
     """
     embedding_sizes = [
         (n_categories, min(600, round(1.6 * n_categories**0.56)))
         for n_categories in categorical_feature_sizes
     ]
     return embedding_sizes
 
 
-if DP_AVAILABLE:
-    # Module fixer implementation that is more friendly with our expected layer sizes,
-    # etc. We register this as the preferred module replacer for BatchNorm layers in
-    # Opacus.
-    @register_module_fixer(  # type: ignore[misc] # Reason: untyped decorator (not ours)
-        [nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d, nn.SyncBatchNorm]
-    )
-    def _fix_batch_norm(
-        module: Union[nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d, nn.SyncBatchNorm]
-    ) -> nn.GroupNorm:
-        # A default value of 32 is chosen for the number of groups based on the paper
-        # *Group Normalization* https://arxiv.org/abs/1803.08494.
-        # If this is not appropriate for the num_features in the layer we then try to
-        # find an alternative.
-        num_groups = min(32, module.num_features)
-        while num_groups > 1:
-            # If num_features is divisible by num_groups we can use it
-            if module.num_features % num_groups == 0:
-                break
-            # Otherwise, try the next number down
-            num_groups -= 1
-        else:
-            # If we reach the end of the while-loop we couldn't find a compatible
-            # number of groups.
-            raise UnsupportedModuleError(
-                "Unable to find compatible number of groups for GroupNorm replacement "
-                "of BatchNorm. Tried [2,32]."
-            )
-
-        return nn.GroupNorm(num_groups, module.num_features, affine=module.affine)
-
-
-@delegates()
 class PyTorchClassifierMixIn(ClassifierMixIn):
     """MixIn for PyTorch classification problems.
 
     PyTorch classification models must have this class in their inheritance hierarchy.
     """
 
     def _do_output_activation(self, output: torch.Tensor) -> torch.Tensor:
         """Perform final activation function on output."""
         if self.multilabel:
             return torch.sigmoid(output)
         else:
             return F.softmax(output, dim=1)
 
 
-@delegates()
 class _PyTorchNeuralNetworkMixIn(NeuralNetworkMixIn):
     """All Pytorch Neural Networks must inherit from this abstract class.
 
     Specifies model structure and hyperparameters.
     """
 
     @staticmethod
@@ -237,27 +162,27 @@
 
         raise ValueError(
             "Optimizer not supported.",
             "Please provide one supported by 'torch' or 'torch_optimizer'.",
         )
 
     @staticmethod
-    def _get_scheduler(scheduler: Scheduler) -> Callable[..., _LRScheduler]:
+    def _get_scheduler(scheduler: Scheduler) -> Callable[..., schedulers._LRScheduler]:
         """Returns appropriate scheduler class."""
         if scheduler.name in _SCHEDULERS:
             return partial(_SCHEDULERS[scheduler.name], **scheduler.params)
 
         raise ValueError(
             "Scheduler not supported.",
             "Please provide one supported by 'torch'.",
         )
 
     @staticmethod
     def _get_early_stopping_callback(
-        early_stopping_params: Optional[_StrAnyDict] = None,
+        early_stopping_params: Optional[Dict[str, Any]] = None,
     ) -> EarlyStopping:
         """Returns EarlyStopping Callback."""
         if early_stopping_params is None:
             early_stopping_params = dict(
                 monitor="validation_loss",
                 min_delta=0.00,
                 patience=2,
@@ -267,22 +192,21 @@
 
         return EarlyStopping(**early_stopping_params)
 
 
 T_PYTORCH = TypeVar("T_PYTORCH", bound="BasePyTorchModel")
 
 
-@delegates()
 class BasePyTorchModel(
-    _PyTorchDistributedModelMixIn[T_DTYPE],
+    _PyTorchDistributedModelMixIn,
     _PyTorchNeuralNetworkMixIn,
+    pl.LightningModule,
     _DifferentiallyPrivate,
     _BaseModel,
-    Generic[T_DTYPE, T_PYTORCH],
-    pl.LightningModule,
+    Generic[T_PYTORCH],
 ):
     """Implements a Neural Network in PyTorch Lightning.
 
     Args:
         model_name: Used for tensorboard logging. Model name will be left blank and
             default to "default" if none provided. Optional.
         model_version: Used for tensorboard logging.If version is not specified the
@@ -293,46 +217,28 @@
         stochastic_weight_avg: Whether to use [Stochastic Weight Averaging (SWA)](
             https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-\
 weight-averaging).
         logger_config: Logger configuration. Optional. Will default to Tensorboard if
             not provided.
 
     Attributes:
-        epochs: Number of epochs to train for when calling fit.
         model_name: Name of the model.
         model_version: Version of the model.
-        steps: Number of steps to train for when calling fit.
         stochastic_weight_avg: Whether to use Stochastic Weight Averaging (SWA).
     """
 
-    # Validate attributes
-    _validation_results: List[Dict[str, str]]
+    # Train/validate attributes
+    _train_loss: float
+    _validation_loss: float
+    _metrics_results: Dict[str, str]
 
     # Test attributes
     _test_preds: Optional[List[np.ndarray]]
     _test_targets: Optional[List[np.ndarray]]
 
-    train_dl: _BasePyTorchBitfountDataLoader
-    # The below fields are left as nested due the
-    # `load_only`/`dump_only` methods they have
-    fields_dict: ClassVar[T_FIELDS_DICT] = {
-        "_dp_config": fields.Nested(
-            desert.schema(DPModellerConfig),
-            allow_none=True,
-            data_key="dp_config",
-            dump_only=True,
-        ),
-        "dp_config": fields.Nested(
-            desert.schema(DPModellerConfig),
-            allow_none=True,
-            data_key="dp_config",
-            load_only=True,
-        ),
-    }
-
     def __init__(
         self,
         model_name: Optional[str] = None,
         model_version: Optional[Union[int, str]] = None,
         stochastic_weight_avg: bool = False,
         logger_config: Optional[LoggerConfig] = None,
         **kwargs: Any,
@@ -344,21 +250,17 @@
         self.stochastic_weight_avg = stochastic_weight_avg
         # Set training attributes
         self._model: Optional[nn.Module] = None  # actual underlying model
         self._pl_logger = self._get_logger_from_config(logger_config)
         self._pl_trainer: pl.Trainer = self.trainer_init()  # cannot be "self.trainer"
         self._trained_on_previous_batch: bool = False
         self._total_num_batches_trained: int = 0
-        self._model_weights_rolled_back: bool = False
-        self._datasource: Optional[BaseSource] = None
 
-        # Store initial values for steps/epochs as these may get overwritten in
-        # a set_model_training_iterations() call.
-        self._total_epochs = self.epochs
-        self._total_steps = self.steps
+        # Set placeholder for DP
+        self._dp_engine: Optional[PrivacyEngine] = None
 
     @abstractmethod
     def _create_model(self) -> nn.Module:
         """Creates and returns model."""
         raise NotImplementedError
 
     @abstractmethod
@@ -376,260 +278,195 @@
     def _do_output_activation(self, output: torch.Tensor) -> torch.Tensor:
         """Perform final activation function on output.
 
         Should be provided by MixIn class.
         """
         raise NotImplementedError
 
+    class _Schema(
+        _PyTorchNeuralNetworkMixIn._Schema,
+        _DifferentiallyPrivate._Schema,
+        _BaseModel._Schema,
+    ):
+        """No extra parameters to serialize here."""
+
+        @abstractmethod
+        def recreate_model(self, data: _JSONDict, **kwargs: Any) -> T_PYTORCH:
+            """Recreates model."""
+            raise NotImplementedError
+
     def tensor_precision(self) -> T_DTYPE:
         """Returns tensor dtype used by Pytorch Lightning Trainer.
 
+        Returns:
+            Pytorch tensor dtype.
+
         :::note
 
         Currently only 32-bit training is supported.
 
         :::
-
-        Returns:
-            Pytorch tensor dtype.
         """
         # TODO: [BIT-727] support non-32 bit training
-        return cast(T_DTYPE, _TORCH_DTYPES[int(self._pl_trainer.precision)])
+        return cast(T_DTYPE, _TORCH_DTYPES[cast(int, self._pl_trainer.precision)])
 
     def initialise_model(
         self,
-        data: Optional[BaseSource] = None,
-        context: Optional[TaskContext] = None,
+        data: Optional[DataSource] = None,
+        context: Optional[ModelContext] = None,
     ) -> None:
         """Any initialisation of models/dataloaders to be done here.
 
         Initialises the dataloaders and sets `self._model` to be the output from
         `self.create_model`. Any initialisation ahead of federated or local training,
         serialization or deserialization should be done here.
 
         Args:
             data: The datasource for model training. Defaults to None.
             context: Indicates if the model is running as a modeller or worker.
                 If None, there is no difference between modeller and worker.
         """
         self._initialised = True
-        seed_all(self.seed)
         # Set initialisation context
         self._context = context
-        if self._context == TaskContext.MODELLER:
-            # In a distributed setting, the Modeller needs to first initialise its
-            # own model before it can be used. The pod identifier needs to be set
-            # before the model is initialised so the the relevant details can be
-            # retrieved from the schema. For this we just use the first pod
-            # identifier specified in the datastructure as it is assumed the the
-            # schemas for all the Pods are the same.
-            pod_identifiers = self.datastructure.get_pod_identifiers()
-            if pod_identifiers:
-                self.set_datastructure_identifier(pod_identifiers[0])
-
         if data is not None:
-            if self.datastructure.query:
-                table_schema = self.datastructure._override_schema(
-                    data_identifier=self._datastructure_identifier
-                )
-                self.databunch = BitfountDataBunch(
-                    data_structure=self.datastructure,
-                    schema=table_schema,
-                    datasource=data,
-                )
-            elif self.datastructure.table:
-                if context:
-                    table_schema = self.schema.get_table_schema(
-                        self.datastructure.get_table_name(
-                            self._datastructure_identifier
-                        )
-                    )
-                    self.databunch = BitfountDataBunch(
-                        data_structure=self.datastructure,
-                        schema=table_schema,
-                        datasource=data,
-                    )
-                else:
-                    # For local training, we can add the datasource to the schema.
-                    data.load_data(table_name=self.datastructure.table)
-                    self._add_datasource_to_schema(data)
-
+            self._add_datasource_to_schema(
+                datasource=data, datafactory=_PyTorchDataFactory()
+            )
             # Initialise model details
-            if self._context != TaskContext.MODELLER:
+            if self._context != ModelContext.MODELLER:
                 self._set_dataloaders(self.batch_size)
         self._opt_func: Callable[..., _OptimizerType] = self._get_optimizer(
             self.optimizer
         )
-        self._scheduler_func: Optional[Callable[..., _LRScheduler]] = None
+        self._scheduler_func: Optional[Callable[..., schedulers._LRScheduler]] = None
         if isinstance(self.scheduler, Scheduler):
             self._scheduler_func = self._get_scheduler(self.scheduler)
 
         self._model = self._create_model()
 
         # Initialise DP if requested
         self._initialise_differential_privacy()
 
     def _get_logger_from_config(
         self, logger_config: Optional[LoggerConfig]
     ) -> LightningLoggerBase:
         """Initialises the logger for the model."""
         if logger_config is None:
-            ml_logger = TensorBoardLogger(
+            logger = TensorBoardLogger(
                 name=self.model_name,
                 version=self.model_version,
                 save_dir=str(BITFOUNT_LOGS_DIR),
             )
-        elif logger_config.name == "Neptune":
-            # Neptune logger doesn't have the concept of a save_dir so no need to set
-            ml_logger = LoggerType[logger_config.name].value(**logger_config.params)
         else:
-            ml_logger = LoggerType[logger_config.name].value(
-                save_dir=str(logger_config.save_dir or BITFOUNT_LOGS_DIR),
-                **logger_config.params,
-            )
-        return ml_logger
+            if logger_config.name == "Neptune":
+                logger = LoggerType[logger_config.name].value(**logger_config.params)
+            else:
+                if logger_config.save_dir is None:
+                    logger_config.save_dir = BITFOUNT_LOGS_DIR
+                logger = LoggerType[logger_config.name].value(
+                    save_dir=logger_config.save_dir, **logger_config.params
+                )
+        return logger
 
     def _initialise_differential_privacy(self) -> None:
         """Initialises the Differential Privacy Engine if requested."""
-        if self._dp_config:
-            conf = self._dp_config
+        conf = self._dp_config
+        if conf:
             # Check DP can be used with model
-            if not self._model:
-                raise ValueError("Model uninitialized")
-            validation_errors = ModuleValidator.validate(self._model)
-            if validation_errors:
+            dp_inspector = DPModelInspector(should_throw=True)
+            try:
+                dp_inspector.validate(self)
+            except IncompatibleModuleException as e:
                 # If autofix, try to do so
                 if conf.auto_fix:
-                    errors_str = "\n\t".join(repr(err) for err in validation_errors)
-                    # Logging the opacus errors for debugging purposes in our logs.
-                    logger.debug(
-                        f"Incompatible modules detected in model: \n\t{errors_str}"
-                    )
-                    # Still raise a warning.
-                    logger.warning(
-                        "Some of the modules used in the model are incompatible "
-                        "with `opacus`, attempting to autofix."
-                    )
-                    self._model = ModuleValidator.fix_and_validate(self._model)
+                    logger.warning(f"Incompatible modules detected in model: {e}")
+                    logger.warning("Attempting to autofix.")
+
+                    # Convert batchnorm to groupnorm
+                    module_modification.convert_batchnorm_modules(self)
+
+                    # Check again
+                    dp_inspector.validate(self)
                 else:
-                    raise UnsupportedModuleError(validation_errors)
+                    raise e
 
-            # If not on modeller:
-            # - Create Engine
-            # - Change Dataloaders
-            # - Create Noise Multiplier
-            if self._context != TaskContext.MODELLER:
-                self._dp_engine = PrivacyEngine(secure_mode=True)
-
-                # Recreate train dataloader with shuffled data, which is required for
-                # privacy guarantees. It will be replaced with a CSPRNG shuffler later
-                # by Opacus itself.
-                # We use the private method to do this to ensure the created dataloader
-                # uses the expected dataset and batch size.
-                logger.info("Creating shuffled training dataloader")
-                self.train_dl.shuffle = True
-                if isinstance(self.train_dl, PyTorchIterableBitfountDataLoader):
-                    self.train_dl.secure_rng = True
-                elif isinstance(self.train_dl, PyTorchBitfountDataLoader):
-                    self.train_dl.dataloader = self.train_dl.get_pytorch_dataloader()
-
-                self._noise_multiplier = conf.noise_multiplier
-
-            # In order for the model to be compatible on both modeller and worker
-            # we need to manually make it privacy compatible by wrapping it in a
-            # GradSampleModule.
-            # This is the same approach taken in PrivacyEngine.make_private() and
-            # doing it now, ahead of time, is not incompatible with that call as it's
-            # able to detect that the model is already wrapped.
-            # We use batch_first=True as that's the default value in
-            # PrivacyEngine.make_private() and we don't expose the ability to
-            # specify a different value. make_private() will also raise an error
-            # if the values of batch_first or loss_reduction differ which makes
-            # this forward-compatible with any changes to the default values in
-            # make_private().
-            if not self._model:
-                raise ValueError("Model uninitialized")
-            self._model = GradSampleModule(
-                self._model,
-                batch_first=True,
-                loss_reduction=conf.loss_reduction,
-            )
+            # Create engine if not on modeller
+            if self._context != ModelContext.MODELLER:
+                self._dp_engine = PrivacyEngine(
+                    module=self,  # use self rather than self._model
+                    batch_size=self.batch_size,
+                    sample_size=len(self._databunch.train_ds),
+                    max_grad_norm=conf.max_grad_norm,
+                    noise_multiplier=conf.noise_multiplier,
+                    alphas=conf.alphas,
+                    secure_rng=False,  # TODO: [BIT-511] check need for True
+                    target_delta=conf.target_delta,
+                    loss_reduction=conf.loss_reduction,
+                )
+
+    def _privacy_guarantees_exceeded(self) -> bool:
+        """Checks whether the privacy spent exceeds the allowed max.
+
+        Returns: True if maximum privacy budget exceeded, False if not or if DP not
+            being checked.
+        """
+        # If monitoring DP, perform checks
+        if self._dp_engine:
+            epsilon, _ = self._dp_engine.get_privacy_spent()
+
+            # Check privacy constraints
+            if epsilon >= cast(DPModellerConfig, self._dp_config).max_epsilon:
+                return True
+
+        # If not exceeded or not being checked
+        return False
 
     def trainer_init(self) -> pl.Trainer:
         """Initialises the Lightning Trainer for this model.
 
         Documentation for pytorch-lightning trainer can be found here:
         https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html
 
         Returns:
             The pytorch lightning trainer.
         """
-        callbacks: List[Callback] = [TQDMProgressBar(refresh_rate=1)]
-
-        if self.stochastic_weight_avg:
-            # If SWA is requested, add it with a default value
-            # https://pytorch-lightning.readthedocs.io/en/stable/advanced/training_tricks.html#stochastic-weight-averaging  # noqa: B950
-            callbacks.append(StochasticWeightAveraging(swa_lrs=1e-2))
+        callbacks = []
 
         if self.early_stopping:
             callbacks.append(
                 self._get_early_stopping_callback(self.early_stopping.params)
             )
 
-        if self._dp_config:
-            # This combination of arguments saves just the most recent copy of the
-            # weights in the checkpoint each batch and makes a copy of it called
-            # 'last.ckpt' so the name is always known
-            logger.debug("Model checkpointing callback initialised")
-            checkpoint_callback = ModelCheckpoint(
-                save_last=True,
-                save_weights_only=True,
-                save_top_k=1,
-                every_n_train_steps=1,
-            )
-            callbacks.append(checkpoint_callback)
-
-        # torch emits warnings to stderr that are not relevant for us, so we need
-        # to filter them out
-        with filter_stderr(
-            re.escape(
-                "[W Context.cpp:70] Warning:"
-                " torch.use_deterministic_algorithms is in beta"
-            )
-        ):
-            # TODO: [BIT-1412] allow the user to provide these arguments in the
-            # constructor
-            gpu_kwargs = autodetect_gpu()
-            trainer = pl.Trainer(
-                enable_checkpointing=True if self._dp_config else False,
-                max_epochs=self.epochs or -1,
-                max_steps=self.steps or -1,
-                deterministic=True,
-                auto_lr_find=True,
-                logger=self._pl_logger,
-                callbacks=callbacks,
-                check_val_every_n_epoch=1,
-                default_root_dir=str(BITFOUNT_OUTPUT_DIR),
-                **gpu_kwargs,
-            )
-            return trainer
+        trainer = pl.Trainer(
+            max_epochs=self.epochs,
+            max_steps=self.steps,
+            progress_bar_refresh_rate=1,
+            deterministic=True,
+            stochastic_weight_avg=self.stochastic_weight_avg,
+            auto_lr_find=True,
+            logger=self._pl_logger,
+            callbacks=callbacks,
+            gpus=_autodetect_gpu(),  # will be 1 (first GPU) or 0 (CPU)
+        )
+        return trainer
 
-    def train_dataloader(self) -> Optional[BitfountDataLoader]:  # type: ignore[override] # Reason: see below  # noqa: B950
+    def train_dataloader(self) -> Optional[_BitfountDataLoader]:  # type: ignore[override] # Reason: see below  # noqa: B950
         """Returns training dataloader."""
         # We override the dataloader return annotation as the LightningModule
         # expects a pytorch DataLoader, and we return PyTorchBitfountDataLoader
         return self.train_dl
 
-    def val_dataloader(self) -> Optional[BitfountDataLoader]:  # type: ignore[override] # Reason: see below  # noqa: B950
+    def val_dataloader(self) -> Optional[_BitfountDataLoader]:  # type: ignore[override] # Reason: see below  # noqa: B950
         """Returns validation dataloader."""
         # We override the dataloader return annotation as the LightningModule
         # expects a pytorch DataLoader, and we returnPyTorchBitfountDataLoader
         return self.validation_dl
 
-    def test_dataloader(self) -> Optional[BitfountDataLoader]:  # type: ignore[override] # Reason: see below  # noqa: B950
+    def test_dataloader(self) -> Optional[_BitfountDataLoader]:  # type: ignore[override] # Reason: see below  # noqa: B950
         """Returns test dataloader."""
         # We override the dataloader return annotation as the LightningModule
         # expects a pytorch DataLoader, and we return PyTorchBitfountDataLoader
         return self.test_dl
 
     def serialize(self, filename: Union[str, os.PathLike]) -> None:
         """Serialize model to file with provided `filename`.
@@ -637,46 +474,36 @@
         Args:
             filename: Path to file to save serialized model.
         """
         if not self._initialised:
             logger.info("Model not yet initialised. Auto-initialising model.")
             self.initialise_model()
         # Model has been initialised, assuring mypy of this
-        assert self._model is not None  # nosec assert_used
+        assert self._model is not None  # nosec
         torch.save(self._model.state_dict(), filename)
 
-    def deserialize(
-        self,
-        content: Union[str, os.PathLike, bytes],
-        weights_only: bool = True,
-        **kwargs: Any,
-    ) -> None:
+    def deserialize(self, filename: Union[str, os.PathLike]) -> None:
         """Deserialize model.
 
+        Args:
+            filename: Path to file containing serialized model.
+
         :::danger
 
-        If `weights_only` is set to False, this should not be used on a model file that
-        has been received across a trust boundary due to underlying use of `pickle` by
-        `torch`.
+        This should not be used on a model file that has been received across a
+        trust boundary due to underlying use of `pickle` by `torch`.
 
         :::
-
-        Args:
-            content: Path to file containing serialized model.
-            weights_only: Whether to load only the weights or the entire model.
-            **kwargs: Additional keyword arguments to pass to `torch.load`.
         """
-        kwargs.update({"weights_only": weights_only})
         if not self._initialised:
             logger.info("Model not yet initialised. Auto-initialising model.")
             self.initialise_model()
         # Model has been initialised, assuring mypy of this
-        assert self._model is not None  # nosec assert_used
-        load_contents = BytesIO(content) if isinstance(content, bytes) else content
-        self._model.load_state_dict(enhanced_torch_load(load_contents, **kwargs))
+        assert self._model is not None  # nosec
+        self._model.load_state_dict(torch.load(filename))
 
     @staticmethod
     def _get_layer_sizes(
         layers: List[int], input_size: int
     ) -> Tuple[Tuple[int, int], ...]:
         """Converts layer sizes to in a tuple of # in and out parameters."""
         layer_sizes = []
@@ -690,47 +517,33 @@
 
     def _get_loss(
         self,
         output: torch.Tensor,
         target: torch.Tensor,
         loss_modifiers: Tuple[torch.Tensor, ...],
     ) -> Tensor:
-        """Computes the appropriate (weighted) loss.
-
-        Args:
-            output: The model output.
-            target: The expected output.
-            loss_modifiers: A tuple of tensors representing additional loss modifiers:
-                - 0: weightings for each sample in this batch
-                - 1: categories
-
-        Returns:
-            A scalar tensor representing the loss.
-        """
+        """Computes the appropriate (weighted) loss."""
         sample_weight: torch.Tensor = loss_modifiers[0]
         categories: torch.Tensor = loss_modifiers[1]
         criterion = cast(Callable, self.loss_func)(reduction="none")
         # Handle categorical classification
         output = self._handle_categories_or_tuple(output, categories)
 
         if self.loss_func == nn.CrossEntropyLoss:
-            loss: torch.Tensor = criterion(output, target.long())
+            loss = criterion(output, target.long())
         elif self.loss_func == nn.BCEWithLogitsLoss:
             loss = criterion(output, target.type_as(output))
             loss = loss.mean(dim=1)
         else:
             raise ValueError(
                 f"Loss function {type(self.loss_func).__name__} is unsupported."
             )
 
-        # Calculate weighted mean loss
         loss = loss * sample_weight.squeeze() / sample_weight.squeeze().sum()
-        loss = loss.sum()
-
-        return loss
+        return cast(torch.Tensor, loss.sum())
 
     @staticmethod
     def _handle_categories_or_tuple(
         output: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]],
         categories: Optional[torch.Tensor] = None,
     ) -> torch.Tensor:
         """Handles conversion of output if categorical or tensor."""
@@ -763,84 +576,49 @@
         results_with_str_values: Dict[str, str] = {}
         for k, v in results.items():
             results_with_str_values[k] = str(v)
         return results_with_str_values
 
     def configure_optimizers(
         self,
-    ) -> Union[_OptimizerType, Tuple[List[_OptimizerType], List[_LRScheduler]]]:
+    ) -> Union[
+        _OptimizerType, Tuple[List[_OptimizerType], List[schedulers._LRScheduler]]
+    ]:
         """Configures the optimizer(s) and scheduler(s) for backpropagation.
 
         Returns:
             Either the optimizer of your choice or a tuple of optimizers and learning
             rate schedulers.
         """
         parameters = filter(lambda p: p.requires_grad, self.parameters())
         optimizer: _OptimizerType = self._opt_func(parameters)
 
         # Attach PrivacyEngine if present
         if self._dp_engine:
-            if not self._dp_config:
-                raise ValueError(
-                    "DP Engine created but no configuration could be found."
-                )
-
-            if isinstance(self.train_dl, PyTorchBitfountDataLoader):
-                dataloader = self.train_dl.dataloader
-            elif isinstance(self.train_dl, PyTorchIterableBitfountDataLoader):
-                # TODO: [BIT-1685] We are using a custom dataloader which is not what
-                # opacus expects so we need to have a workaround for this `make_private`
-                # method call. We likely do not even need to replace the dataloader
-                # anyway so if possible we should make the model and optimizer private
-                # individually.
-                dataloader = self.train_dl  # type: ignore[assignment] # Reason: See above # noqa: B950
-
-            # Create DP-sensitive (model, optimizer, dataloader)
-            if not self._model:
-                raise ValueError("Model uninitialized")
-            dp_model, dp_optimizer, dp_dataloader = self._dp_engine.make_private(
-                module=self._model,
-                optimizer=optimizer,
-                data_loader=dataloader,
-                # If self._dp_engine exists then self._noise_multiplier will
-                # also have been set, and this method is only used on the worker.
-                noise_multiplier=cast(float, self._noise_multiplier),
-                max_grad_norm=self._dp_config.max_grad_norm,
-                loss_reduction=self._dp_config.loss_reduction,
-                # TODO: [BIT-1474] Identify what changes are needed to enable
-                #       poisson_sampling=True.
-                poisson_sampling=False,
-            )
-
-            logger.info("Replacing model with DP-sensitive version.")
-            self._model = dp_model
+            self._dp_engine.attach(optimizer)
 
-            # TODO: [BIT-1685] Confirm that we don't need to replace the dataloader if
-            # it is iterable
-            if isinstance(self.train_dl, PyTorchBitfountDataLoader):
-                logger.info("Replacing dataloader with DP-sensitive version.")
-                self.train_dl.dataloader = dp_dataloader
-
-            optimizer = dp_optimizer
         if self._scheduler_func:
             scheduler = self._scheduler_func(optimizer)
             return [optimizer], [scheduler]
 
         return optimizer
 
-    def forward(self, x: Union[ImgFwdTypes, TabFwdTypes]) -> Any:
+    def forward(self, x: Union[ImgFwdTypes, TabFwdTypes]) -> Any:  # type: ignore[override] # reason: see below # noqa: B950
         """Performs a forward pass of the underlying model.
 
         Args:
             x: Input to the model.
 
         Returns:
             Output of the model.
 
         """
+        # Signature incompatible with pl.LightningModule:
+        # Superclass: def forward(self, *args: Any, **kwargs: Any) -> Any
+        # Subclass: def forward(self, x: Union[ImgFwdTypes, TabFwdTypes]) -> Any
         return self._model(x)  # type: ignore[misc] # reason: the model is set by this stage # noqa: B950
 
     def _skip_training_batch(self, batch_idx: int) -> bool:
         """Checks if the current batch from the training set should be skipped.
 
         This is a workaround for the fact that PyTorch Lightning starts the Dataloader
         iteration from the beginning every time `fit` is called. This means that if we
@@ -856,24 +634,26 @@
         """
         # TODO: [BIT-1237] remove this code block and find a better way to do this that
         # doesn't involve loading every batch into memory until we get to the right one
         if self.steps:
             # If we have trained on the previous batch, we can avoid the checks because
             # it means we have already reached the target start batch.
             if not self._trained_on_previous_batch:
-                if (self.steps != self._pl_trainer.max_steps) and (
+                # mypy cannot see that the trainer has this attribute due to how it is set in pytorch lightning # noqa: B950
+                if (self.steps != self._pl_trainer.max_steps) and (  # type: ignore[attr-defined] # Reason: see above # noqa: B950
                     batch_idx < (self._total_num_batches_trained % len(self.train_dl))
                 ):
                     return True
                 else:
                     self._trained_on_previous_batch = True
 
             # `_total_num_batches_trained` hasn't been incremented yet so we need to add
             # 1 here to get the correct batch number.
-            if self._total_num_batches_trained + 1 == self._pl_trainer.max_steps:
+            # mypy cannot see that the trainer has this attribute due to how it is set in pytorch lightning # noqa: B950
+            if self._total_num_batches_trained + 1 == self._pl_trainer.max_steps:  # type: ignore[attr-defined] # Reason: see above # noqa: B950
                 self._trained_on_previous_batch = False
 
         if not self._pl_trainer.sanity_checking:
             self._total_num_batches_trained += 1
 
         return False
 
@@ -890,174 +670,88 @@
         """
         # Override the pl.lightning method, as it requires *args
         # and **kwargs as arguments whereas we will always have
         # the batch and the batch_idx as args.
         # `data` contains the X batch and often some additional information.
         # This can be split into relevant chunks by the `split_dataloader_output()`
         # method, which details how to perform that split.
-        if self.param_clipping:
-            # Make sure that the parameters stay within required bounds
-            # for SecureAggregation
-            clipper = _PytorchParamConstraint(**cast(dict, self.param_clipping))
-            # If we are in this check, self.param_clipping is
-            # a dictionary so it is safe to cast
-
-            # Check for BatchNorm modules
-            modules_to_clip = [
-                mod_name
-                for mod_name, modules in self._model._modules.items()  # type: ignore[union-attr] # Reason: _model is initialized before training  # noqa: B950
-                if "BatchNorm" in str(modules)
-            ]
-            # Apply the parameter contraint to all batch norm modules
-            for mod in modules_to_clip:
-                self._model._modules[mod].apply(clipper)  # type: ignore[union-attr] # Reason: _model is initialized before training  # noqa: B950
 
         if self._skip_training_batch(batch_idx):
             # Returning `None` to skip this batch
             return None
 
         data, y = batch
         x, *loss_modifiers = self._split_dataloader_output(data)
         # Perform forward pass and get loss
         y_hat = self(x)
         loss = self._get_loss(
             output=y_hat,
             target=y,
             loss_modifiers=cast(Tuple[torch.Tensor, ...], loss_modifiers),
         )
+
         self.log(
             "train_loss", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True
         )
-        if self._dp_engine and self._dp_config:
-            # Calculate current epsilon level
-            epsilon = self._dp_engine.get_epsilon(self._dp_config.delta)
-
-            # Log current epsilon level
-            self.log("epsilon", epsilon, prog_bar=True, logger=True)
-
         return loss
 
-    def training_epoch_end(self, kwargs: Any) -> None:
-        """Extract gradient from weights after training."""
-        if self.param_clipping:
-            # Make sure that the parameters stay within required bounds
-            # for SecureAggregation
-            clipper = _PytorchParamConstraint(**cast(dict, self.param_clipping))
-            # If we are in this check, self.param_clipping
-            # is a dictionary so it is safe to cast
-
-            # Check for BatchNorm modules
-            modules_to_clip = [
-                mod_name
-                for mod_name, modules in self._model._modules.items()  # type: ignore[union-attr] # Reason: _model is initialized before training  # noqa: B950
-                if "BatchNorm" in str(modules)
-            ]
-            # Apply the parameter contraint to all batch norm modules
-            for mod in modules_to_clip:
-                self._model._modules[mod].apply(  # type: ignore[union-attr] # Reason: _model is initialized before training  # noqa: B950
-                    clipper
-                )
-
-        try:
-            # For now, this will fail for the resnet model since it is defined with
-            # no layers, so wrapping in a try...except.
-            self._training_epoch_end_gradients = [
-                layer.weight.grad.detach().clone()
-                for layer in self._model.layers  # type: ignore[union-attr] # Reason: _model is initialized before training  # noqa: B950
-            ]
-        except AttributeError:
-            pass
-
-    def _roll_back_model_weights(self, state_dict: OrderedDict[str, Tensor]) -> None:
-        """Rolls back model weights to the most recent weights available."""
-        if not self._model_weights_rolled_back and self._model is not None:
-            logger.warning(
-                "Rolling back model weights to before privacy guarantee was exceeded."
-            )
-            # We don't use the `load_from_checkpoint` method because we want to only
-            # load the state dictionary from the checkpoint
-            self.load_state_dict(state_dict)
-            self._model_weights_rolled_back = True
-        elif self._model_weights_rolled_back:
-            logger.debug("Ignoring state dictionary. Model weights already rolled back")
-        else:
-            logger.warning("Uninitialised model. Unable to roll back weights.")
-
-    def on_save_checkpoint(self, checkpoint: _StrAnyDict) -> None:
-        """Hook that runs prior to saving a model checkpoint.
-
-        This is used to roll back the model weights in the event the privacy guarantee
-        has been exceeded prior to overwriting the checkpoint.
-        """
-        if self._is_privacy_guarantee_exceeded():
-            try:
-                # `log_dir` attribute is present on the pytorch lightning logger and
-                # documented but mypy is unaware for some reason.
-                previous_state_dict = enhanced_torch_load(
-                    f"{self._pl_trainer.logger.log_dir}/checkpoints/last.ckpt"  # type: ignore[union-attr] # Reason: see above # noqa: B950
-                )["state_dict"]
-            except FileNotFoundError:
-                # This will likely be the case when the limit is exceeded on
-                # the first batch
-                logger.debug(
-                    f"Could not find file {self._pl_trainer.logger.log_dir}/checkpoints/last.ckpt"  # type: ignore[union-attr] # Reason: see above # noqa: B950
-                )
-                logger.warning(
-                    "No checkpoint available to roll back model weights. "
-                    "Re-initialising model."
-                )
-                self._pl_trainer.should_stop = True
-                self.initialise_model(data=self._datasource, context=TaskContext.WORKER)
-                checkpoint["state_dict"] = self.state_dict()
-                self._model_weights_rolled_back = True
-            else:
-                checkpoint["state_dict"] = previous_state_dict
-                self._roll_back_model_weights(previous_state_dict)
-                self._pl_trainer.should_stop = True
-
-    def on_train_batch_start(
+    def on_train_batch_start(  # type: ignore[override] # reason: incompatible with supertype return type # noqa: B950
         self,
         batch: _SingleOrMulti[_SingleOrMulti[np.ndarray]],
         batch_idx: int,
+        dataloader_idx: int,
     ) -> Optional[Literal[-1]]:
         """Checks if any privacy guarantees have been exceeded and stops training if so.
 
         Args:
             batch: The batch to be trained on.
             batch_idx: The index of the batch to be trained on from the train
                 dataloader.
+            dataloader_idx: The index of the dataloader from which the batch was
+                taken. This is useful for multi-dataloader training.
 
         Returns:
             -1 if the entire epoch should be skipped, otherwise None.
         """
         # Check privacy constraints and set stopping criterion as needed
-        if self._is_privacy_guarantee_exceeded():
-            # This skips training for the rest of the current epoch
-            # and only works in the `on_train_batch_start` hook
-            return -1
+        if self._privacy_guarantees_exceeded():
+            self._pl_trainer.should_stop = True  # stops later epochs from running
+            return -1  # this skips training for the rest of the current epoch
+        else:
+            return None
 
-        return None
+    def training_epoch_end(self, outputs: List[Dict[str, torch.Tensor]]) -> None:  # type: ignore[override] # Reason: see below # noqa: B950
+        """Called at the end of the training epoch with all training step outputs.
 
-    def validation_step(
-        self,
-        batch: Tuple[Union[TabxorImgBatch, ImgAndTabBatch], torch.Tensor],
-        batch_idx: int,
-    ) -> Dict[str, Tensor]:
+        Ensures that the losses from a training epoch are aggregated and stored.
+
+        Args:
+            outputs: List of outputs from each training step.
+        """
+        # Override the pl.lightning method, as it requires a different type for outputs.
+        merged_outputs: Dict[str, List[torch.Tensor]] = _merge_list_of_dicts(outputs)
+        avg_train_loss = torch.mean(torch.stack(merged_outputs["loss"]))
+        self._train_loss = avg_train_loss.item()
+
+    def validation_step(self, batch: Tuple[Union[TabxorImgBatch, ImgAndTabBatch], torch.Tensor], batch_idx: int) -> Dict[str, Tensor]:  # type: ignore[override] # Reason: see below # noqa: B950
         """Validation step.
 
         Args:
             batch: The batch to be evaluated.
             batch_idx: The index of the batch to be evaluated from the validation
                 dataloader.
 
         Returns:
             A dictionary of strings and values that should be averaged at the end of
             every epoch and logged e.g. `{"validation_loss": loss}`. These will be
             passed to the `validation_epoch_end` method.
         """
+        # Override the pl.lightning method, as it requires
+        # *args and **kwargs as arguments whereas we will
+        # always have the batch and the batch_idx as args
         # Extract X, y and other data from batch
         data, y = batch
         x, *loss_modifiers = self._split_dataloader_output(data)
         # Get validation output and loss
         y_hat = self(x)
         loss = self._get_loss(
             output=y_hat,
@@ -1084,49 +778,42 @@
 
         Args:
             outputs: List of outputs from each validation step.
         """
         # Override the pl.lightning method, as it requires a different type for outputs.
         # Merge outputs into singular lists rather than a list of dicts
         merged_outputs: Dict[str, List[torch.Tensor]] = _merge_list_of_dicts(outputs)
+        # Calculate val loss average
+        avg_val_loss = torch.mean(torch.stack(merged_outputs["validation_loss"]))
+        self._validation_loss = avg_val_loss.item()
 
         # Calculate metrics from outputs. This will also log them out to self.log().
-        if not self._pl_trainer.sanity_checking:
-            validation_metrics = self._compute_training_metrics(
-                outputs=np.asarray(
-                    [i.cpu().numpy() for i in merged_outputs["outputs"]]
-                ),
-                targets=np.asarray(
-                    [i.cpu().numpy() for i in merged_outputs["targets"]]
-                ),
-                training_metrics=self.metrics,
-            )
-            # Calculate average validation loss and add to validation metrics dictionary
-            avg_val_loss = torch.mean(torch.stack(merged_outputs["validation_loss"]))
-            validation_metrics["validation_loss"] = f"{avg_val_loss.item():.4f}"
-            self._validation_results.append(validation_metrics)
+        self._metrics_results: Dict[str, str] = self._compute_training_metrics(
+            outputs=np.asarray([i.cpu().numpy() for i in merged_outputs["outputs"]]),
+            targets=np.asarray([i.cpu().numpy() for i in merged_outputs["targets"]]),
+            training_metrics=self.metrics,
+        )
 
-    def test_step(
-        self,
-        batch: Tuple[Union[TabxorImgBatch, ImgAndTabBatch], torch.Tensor],
-        batch_idx: int,
-    ) -> _StrAnyDict:
-        """Make sure to set self.preds and self.target before returning in this method.
+    def test_step(self, batch: Tuple[Union[TabxorImgBatch, ImgAndTabBatch], torch.Tensor], batch_idx: int) -> Dict[str, Any]:  # type: ignore[override] # Reason: see below # noqa: B950
+        """Make sure to set self.preds and self.targs before returning in this method.
 
         They will be returned by the `evaluate` method.
 
         Args:
             batch: The batch to be evaluated.
             batch_idx: The index of the batch to be evaluated from the test
                 dataloader.
 
         Returns:
             A dictionary of predictions and targets. These will be passed to the
             `test_epoch_end` method.
         """
+        # Override the pl.lightning method, as it requires
+        # *args and **kwargs as arguments whereas we will
+        # always have the batch and the batch_idx as args
         # Extract X, y and other data from batch
         data, y = batch
         x, *loss_modifiers = self._split_dataloader_output(data)
 
         # Get validation output and loss
         y_hat = self(x)
 
@@ -1147,15 +834,15 @@
         # Override the pl.lightning method, as it requires a different type for outputs.
         merged_outputs: Dict[str, List[torch.Tensor]] = _merge_list_of_dicts(outputs)
         self._test_preds = [i.cpu().numpy() for i in merged_outputs["predictions"]]
         self._test_targets = [i.cpu().numpy() for i in merged_outputs["targets"]]
 
     def _fit_local(
         self,
-        data: BaseSource,
+        data: DataSource,
         metrics: Optional[MutableMapping[str, Metric]] = None,
         **kwargs: Any,
     ) -> Dict[str, str]:
         """Fits the model.
 
         Args:
             data: The data source to be used for training the model.
@@ -1170,152 +857,113 @@
             logger.debug("Model already initialised, will not reinitialise.")
         else:
             logger.info("Model not yet initialised. Auto-initialising model.")
             self.initialise_model(data)
 
         # TODO: [BIT-499] Check DP privacy guarantees before we even start; return NULL
         #       state if exceeded.
-        self._validation_results = []
-        with self._use_metrics(metrics), self._use_datasource(data):
+
+        self._reset_train_val_attrs()
+        with self._use_metrics(metrics):
             # will perform self.epochs or self.steps of local training
             self._pl_trainer.fit(self)
+        # Set validation loss in metric dict
+        self._metrics_results["validation_loss"] = f"{self._validation_loss:.4f}"
 
-            # If training has been specified in terms of epochs, we are guaranteed to
-            # have run validation after training. However if training is specified in
-            # terms of steps, we only would have run validation after training if the
-            # the number of steps is the same as that in an epoch. Therefore we must
-            # manually run validation to ensure we have the latest validation metrics
-            # that reflect the state of the model at the end of training.
-            if self.steps and self.steps % len(self.train_dl) != 0:
-                self._pl_trainer.validate(self)
-
-        try:
-            # Getting the most recent set of validation metrics
-            validation_results = self._validation_results[-1]
-        except IndexError:
-            # If we haven't had a full epoch of training, then we don't have any
-            # validation results in the dictionary so we must create an empty one.
-            # This is required if training is run with steps or epochs set to 0.
-            validation_results = {}
-
-        # Add DP metrics to the final epoch validation results
+        # Add DP metrics
         if self._dp_engine:
-            if not self._dp_config:
-                raise ValueError(
-                    "DP Engine created but no configuration could be found."
-                )
-
-            epsilon, alpha = cast(
-                RDPAccountant, self._dp_engine.accountant
-            ).get_privacy_spent(
-                delta=self._dp_config.delta, alphas=self._dp_config.alphas
-            )
-            validation_results["alpha"] = str(alpha)
-            validation_results["epsilon"] = str(epsilon)
-
-        # Return the validation metrics for the final epoch
-        return validation_results
-
-    @contextlib.contextmanager
-    def _use_datasource(self, datasource: BaseSource) -> Iterator[None]:
-        """Context manager which temporarily sets `datasource` to `self._datasource`.
-
-        Args:
-            data: Datasource provided to fit method.
-
-        Returns:
-            Empty Iterator.
-        """
-        self._datasource = datasource
-        try:
-            yield None
-        finally:
-            self._datasource = None
+            epsilon, alpha = self._dp_engine.get_privacy_spent()
+            self._metrics_results["alpha"] = str(alpha)
+            self._metrics_results["epsilon"] = str(epsilon)
+
+        return self._metrics_results
+
+    def _reset_train_val_attrs(self) -> None:
+        """Resets train/val attributes to None."""
+        self._train_loss = None  # type: ignore[assignment] # Reason: see below
+        self._validation_loss = None  # type: ignore[assignment] # Reason: see below
+        self._metrics_results = None  # type: ignore[assignment] # Reason: see below
+        # The whole point of this function is to reset this variables
+        # to None, so ignore the assignment here
 
     @contextlib.contextmanager
     def _use_metrics(
         self, metrics: Optional[MutableMapping[str, Metric]] = None
     ) -> Iterator[None]:
         """Context manager which temporarily sets `metrics` to `self.metrics`.
 
         Args:
             metrics: Validation metrics to print during training. Defaults to None and
                 will instead use metrics appropriate to the task. Optional.
 
         Returns:
             Empty Iterator.
         """
-        metrics_store: Optional[MutableMapping[str, Metric]] = None
         # Stash the old metrics if new ones provided
         if metrics:
             metrics_store = self.metrics
             self.metrics = metrics
         try:
             yield None
         finally:
             # Restore the old metrics if new ones were provided
             if metrics:
                 self.metrics = metrics_store
 
-    def _evaluate_local(
-        self, test_dl: Optional[BitfountDataLoader] = None, **kwargs: Any
+    def evaluate(
+        self, test_dl: Optional[_BitfountDataLoader] = None, **kwargs: Any
     ) -> Tuple[np.ndarray, np.ndarray]:
         """This method runs inference on the test dataloader.
 
         This is done by calling `self.test_step` under the hood.
         Args:
             test_dl: Optional dataloader to run inference on which takes precedence over
                 the dataloader returned by `self.test_dataloader`.
 
         Returns:
             A tuple of predictions and targets as numpy arrays.
         """
         # Reset metrics
         self._reset_test_attrs()
 
-        self._pl_trainer.test(model=self, dataloaders=cast(DataLoader, test_dl))
-        return np.asarray(self._test_preds), np.asarray(self._test_targets)
+        self._pl_trainer.test(model=self, test_dataloaders=cast(DataLoader, test_dl))
+        return cast(np.ndarray, self._test_preds), cast(np.ndarray, self._test_targets)
 
-    def _predict_local(self, data: BaseSource, **kwargs: Any) -> List[np.ndarray]:
+    def predict(self, data: DataSource, **kwargs: Any) -> np.ndarray:
         """This method runs inference on the test data, returns predictions.
 
         This is done by calling `test_step` under the hood. Customise this method as you
         please but it must return a list of predictions and a list of targets. Note that
         as this is the prediction function, only the predictions are returned.
 
         Returns:
             A numpy array containing the prediction values.
+
+        Raises:
+            ValueError: If no data is provided to test with.
         """
-        if data is not None:
-            data.load_data()
-            if not hasattr(self, "databunch"):
-                self._add_datasource_to_schema(data)  # Also sets `self.databunch
-            if not self.databunch:
-                self._add_datasource_to_schema(data)  # Also sets `self.databunch
-            test_dl = self.databunch.get_test_dataloader(self.batch_size)
-            if isinstance(test_dl, BitfountDataLoader):
-                logger.info(
-                    f"Using test portion of dataset for inference - this has "
-                    f"{len(test_dl.dataset)} record(s)."
-                )
-            else:
-                raise ValueError("No test data to infer in the provided datasource.")
+        self._add_datasource_to_schema(data, datafactory=_PyTorchDataFactory())
+        test_dl = self._databunch.get_test_dataloader(self.batch_size)
+        if isinstance(test_dl, _BitfountDataLoader):
+            logger.info(
+                f"Using test portion of dataset for inference - this has"
+                f"{len(test_dl)} records."
+            )
+        else:
+            raise ValueError("No data provided to run inference for the model on.")
 
-        self._pl_trainer.test(model=self, dataloaders=cast(DataLoader, test_dl))
-        # Reassuring mypy that `self._test_preds` cannot be None at this point
-        assert self._test_preds is not None  # nosec assert_used
-        return self._test_preds
+        self._pl_trainer.test(model=self, test_dataloaders=cast(DataLoader, test_dl))
+        return cast(np.ndarray, self._test_preds)
 
     def _reset_test_attrs(self) -> None:
         """Resets test attributes to None."""
         self._test_preds = None
         self._test_targets = None
 
 
-@delegates()
 class BaseTabNetModel(
     _PyTorchDistributedModelMixIn, _PyTorchNeuralNetworkMixIn, _BaseModel
 ):
     """TabNet Model as described in https://arxiv.org/abs/1908.07442.
 
     This is a wrapper around the implementation from DreamQuark. Documentation can be
     found here: https://dreamquark-ai.github.io/tabnet.
@@ -1331,49 +979,36 @@
         inverse_class_weights: Inverse class weights (only for classification problems).
             Defaults to True.
         mask_type: Mask type. Defaults to "sparsemax".
         decision_prediction_layer_size: Final feedforward layer size. Defaults to 8.
         attention_embedding_layer_size: Attention embedding layer size. Defaults to 8.
         num_steps: Number of steps. Defaults to 3.
 
-
     Raises:
         ValueError: If virtual batch size > batch size.
         ValueError: If the `model_structure` does not match the TabNet model.
         ValueError: If training is specified in `steps` rather than `epochs`. Training
             steps are not supported.
     """
 
-    train_dl: _BasePyTorchBitfountDataLoader
-    fields_dict: ClassVar[T_FIELDS_DICT] = {
-        "virtual_batch_size": fields.Integer(),
-        "patience": fields.Integer(),
-        "embedding_sizes": fields.Integer(allow_none=True),
-        "num_workers": fields.Integer(),
-        "inverse_class_weights": fields.Bool(),
-        "mask_type": fields.String(),
-        "decision_prediction_layer_size": fields.Integer(),
-        "attention_embedding_layer_size": fields.Integer(),
-        "num_steps": fields.Integer(),
-    }
-
     def __init__(
         self,
         virtual_batch_size: int = 64,
         patience: int = 2,
         embedding_sizes: Optional[Union[int, List[int]]] = None,
         num_workers: int = 0,
         inverse_class_weights: bool = True,
         mask_type: str = "sparsemax",
         decision_prediction_layer_size: int = 8,
         attention_embedding_layer_size: int = 8,
         num_steps: int = 3,
         **kwargs: Any,
     ):
         super().__init__(**kwargs)
+
         if self.steps:
             raise ValueError(
                 "TabNet does not support steps. Training must be specified in epochs."
             )
 
         if virtual_batch_size > self.batch_size:
             raise ValueError("Virtual batch size must be smaller than batch size.")
@@ -1395,160 +1030,117 @@
         self.num_workers = num_workers
         self.inverse_class_weights = inverse_class_weights
         self.mask_type = mask_type
         self.decision_prediction_layer_size = decision_prediction_layer_size
         self.attention_embedding_layer_size = attention_embedding_layer_size
         self.num_steps = num_steps
 
-        # This is unused by the TabNet model but is required for compatibility with
-        # DistributedModelProtocol
-        self._total_num_batches_trained: int = 0
-
         # TODO: [BIT-1152] support FederatedAveraging by fixing initialisation
         logger.warning("TabNet model currently does not support FederatedAveraging.")
 
+    class _Schema(_PyTorchNeuralNetworkMixIn._Schema, _BaseModel._Schema):
+        """Marshmallow Schema."""
+
+        virtual_batch_size = fields.Integer()
+        patience = fields.Integer()
+        embedding_sizes = fields.Integer(allow_none=True)
+        num_workers = fields.Integer()
+        inverse_class_weights = fields.Bool()
+        mask_type = fields.String()
+        decision_prediction_layer_size = fields.Integer()
+        attention_embedding_layer_size = fields.Integer()
+        num_steps = fields.Integer()
+
+        @abstractmethod
+        def recreate_model(self, data: _JSONDict, **kwargs: Any) -> BaseTabNetModel:
+            """Recreates model."""
+            raise NotImplementedError
+
     @abstractmethod
-    def _create_model(self) -> BaseEstimator:
+    def _create_model(self) -> None:
         """Returns appropriate model."""
         raise NotImplementedError
 
     def get_param_states(self) -> Dict[str, _TensorLike]:
         """See base class."""
         aux = dict(self._model.network.state_dict())  # type: ignore[union-attr] # Reason: _model is initialised in subclass # noqa: B950
 
         return {key: _AdaptorForPyTorchTensor(value) for key, value in aux.items()}
 
     def initialise_model(
         self,
-        data: Optional[BaseSource] = None,
-        context: Optional[TaskContext] = None,
+        data: Optional[DataSource] = None,
+        context: Optional[ModelContext] = None,
     ) -> None:
         """Any initialisation of models/dataloaders to be done here.
 
         Initialises the dataloaders and sets `self._model` to be the output from
         `self.create_model`. Any initialisation ahead of federated or local training,
         serialization or deserialization should be done here.
 
         Args:
             data: The datasource for model training. Defaults to None.
             context: Indicates if the model is running as a modeller or worker.
                 If None, there is no difference between modeller and worker.
         """
         self._context = context
         self._initialised = True
-        if self._context == TaskContext.MODELLER:
-            # In a distributed setting, the Modeller needs to first initialise its
-            # own model before it can be used. The pod identifier needs to be set
-            # before the model is initialised so the the relevant details can be
-            # retrieved from the schema. For this we just use the first pod
-            # identifier specified in the datastructure as it is assumed the the
-            # schemas for all the Pods are the same.
-            pod_identifiers = self.datastructure.get_pod_identifiers()
-            if pod_identifiers:
-                self.set_datastructure_identifier(pod_identifiers[0])
 
         if data is not None:
-            if self.datastructure.query:
-                table_schema = self.datastructure._override_schema(
-                    data_identifier=self._datastructure_identifier
-                )
-                self.databunch = BitfountDataBunch(
-                    data_structure=self.datastructure,
-                    schema=table_schema,
-                    datasource=data,
-                )
-            elif self.datastructure.table:
-                if context:
-                    table_schema = self.schema.get_table_schema(
-                        self.datastructure.get_table_name(
-                            self._datastructure_identifier
-                        )
-                    )
-                    self.databunch = BitfountDataBunch(
-                        data_structure=self.datastructure,
-                        schema=table_schema,
-                        datasource=data,
-                    )
-                else:
-                    # For local training, we can add the datasource to the schema.
-                    data.load_data()
-                    self._add_datasource_to_schema(data)
+            self._add_datasource_to_schema(data)
 
         # TODO: [BIT-1152] initialise model without setting dataloaders
-        if self._context != TaskContext.MODELLER:
+        if self._context != ModelContext.MODELLER:
             self._set_dataloaders(batch_size=self.batch_size)
-
         self._model = self._create_model()
         # Make sure we only use the tabular part of the data.
         train_x_df_aux = self.train_dl.get_x_dataframe()
         if isinstance(train_x_df_aux, tuple):
             train_x_df, _ = train_x_df_aux
             train_x_df = train_x_df
         else:
             train_x_df = train_x_df_aux
-
-        # Check that a target exists as needed for dataframe indexing below
-        # This needs to be _fairly_ lazily checked as databunch may not be set
-        # earlier in this method.
-        target = self.databunch.target
-        if target is None:
-            raise ValueError(
-                f"No `target` specified in databunch, needed for model evaluation"
-                f" and DataFrame indexing in {self.__class__.__name__}"
-            )
-
         # 'Train' with 0 epochs just to set model parameters
         self._model.fit(  # type: ignore[attr-defined] # reason: the model should be set by the time we fit it # noqa: B950
             train_x_df.values,
-            self.train_dl.get_y_dataframe()[target].values,
+            self.train_dl.get_y_dataframe()[self.datastructure.target].values,
             max_epochs=0,
             virtual_batch_size=self.virtual_batch_size,
             num_workers=self.num_workers,
             weights=int(self.inverse_class_weights),
             drop_last=False,
         )
 
-    def _evaluate_local(
-        self, test_dl: Optional[BitfountDataLoader] = None, **kwargs: Any
+    def evaluate(
+        self, test_dl: Optional[_BitfountDataLoader] = None, **kwargs: Any
     ) -> Tuple[np.ndarray, np.ndarray]:
         """This method runs inference on the test dataloader.
 
         This is done by calling `self.test_step` under the hood.
         Args:
             test_dl: Optional dataloader to run inference on which takes precedence over
                 the dataloader returned by `self.test_dataloader`.
 
         Returns:
             A tuple of predictions and targets as numpy arrays.
         """
-        # Check that a target exists as needed for dataframe indexing below
-        target = self.databunch.target
-        if target is None:
-            raise ValueError(
-                f"No `target` specified in databunch, needed for model evaluation"
-                f" and DataFrame indexing in {self.__class__.__name__}"
-            )
-
         if test_dl is None:
-            if isinstance(self.test_dl, BitfountDataLoader):
+            if isinstance(self.test_dl, _BitfountDataLoader):
                 test_dl = self.test_dl
             else:
                 raise ValueError("There is no test data to evaluate the model on.")
         X_aux = test_dl.get_x_dataframe()
-        X: pd.DataFrame
         if isinstance(X_aux, tuple):
             X, _ = X_aux
         elif isinstance(X_aux, pd.DataFrame):
             X = X_aux
-
-        # Cast as .values can return custom array types
-        Y: np.ndarray = cast(np.ndarray, test_dl.get_y_dataframe()[target].values)
-
-        preds: np.ndarray = self._model.predict_proba(  # type:ignore[union-attr] # reason: the model should be set by the time we evaluate this function # noqa: B950
-            X.values
+        X = X.values
+        Y = test_dl.get_y_dataframe()[self._databunch.target].values
+        preds = self._model.predict_proba(  # type:ignore[union-attr] # reason: the model should be set by the time we evaluate this function # noqa: B950
+            X
         )
 
         return preds, Y
 
     def serialize(self, filename: Union[str, os.PathLike]) -> None:
         """Serialize model to file with provided `filename`.
 
@@ -1560,120 +1152,91 @@
             logger.error(
                 "Can't serialize uninitialized model. "
                 "Model can't be initialised without data."
             )
             return
         torch.save(self._model.network.state_dict(), filename)  # type: ignore[union-attr] # reason: model is initialised # noqa: B950
 
-    def deserialize(
-        self,
-        content: Union[str, os.PathLike, bytes],
-        weights_only: bool = True,
-        **kwargs: Any,
-    ) -> None:
+    def deserialize(self, filename: Union[str, os.PathLike]) -> None:
         """Deserialize model.
 
+        Args:
+            filename: Path to file containing serialized model.
+
         :::danger
 
-        If `weights_only` is set to False, this should not be used on a model file that
-        has been received across a trust boundary due to underlying use of `pickle` by
-        `torch`.
-        :::
+        This should not be used on a model file that has been received across a
+        trust boundary due to underlying use of `pickle` by `torch`.
 
-        Args:
-            content: Bytestream or path to file containing serialized model.
-            weights_only: If True, only the weights of the model will be loaded.
-            **kwargs: Keyword arguments provided to `torch.load` under the hood.
+        :::
         """
-        kwargs.update({"weights_only": weights_only})
         if not self._initialised:
             # TODO: [BIT-1152] initialise model here instead of logging an error
             logger.error(
                 "Can't deserialize uninitialized model. "
                 "Model can't be initialised without data."
             )
             return
-        load_contents = BytesIO(content) if isinstance(content, bytes) else content
-        self._model.network.load_state_dict(enhanced_torch_load(load_contents, **kwargs))  # type: ignore[union-attr] # reason: model is initialised # noqa: B950
+
+        self._model.network.load_state_dict(torch.load(filename))  # type: ignore[union-attr] # reason: model is initialised # noqa: B950
 
     def _fit_local(
         self,
-        data: BaseSource,
-        metrics: Optional[Union[str, List[str], MutableMapping[str, Metric]]] = None,
+        data: DataSource,
+        metrics: Optional[tabnet.metrics.Metric] = None,
         **kwargs: Any,
     ) -> Dict[str, str]:
         """Fit model and return results.
 
         Args:
             data: The data used for model training.
-            metrics: list of metrics
+            metrics (List[Union[str, pytorch_tabnet.metrics.Metric]]): list of metrics
                 to use on validation set every epoch. Last metric in this list is the
                 one to use for early stopping
         """
         if self._initialised:
             logger.debug("Model already initialised, will not reinitialise.")
         else:
             logger.info("Model not yet initialised. Auto-initialising model.")
             self.initialise_model(data=data)
-
-        # Check that a target exists as needed for dataframe indexing below
-        target = self.databunch.target
-        if target is None:
-            raise ValueError(
-                f"No `target` specified in databunch, needed for model fit"
-                f" and DataFrame indexing in {self.__class__.__name__}"
-            )
-
         if metrics is None:
             metrics = ["logloss"]
-        elif isinstance(metrics, str):
-            metrics = [metrics]
-        elif isinstance(metrics, MutableMapping):
-            logger.warning(
-                "TabNet models are incompatible with bitfount Metric objects;"
-                " will derive TabNet Metric objects using supplied names."
-            )
-            metrics = [k for k in metrics.keys()]
-
-        if "logloss" not in metrics:
+        elif "logloss" not in metrics:
             metrics.insert(0, "logloss")
 
         # Make sure we only use the tabular part of the data.
         train_x_df_aux = self.train_dl.get_x_dataframe()
         if isinstance(train_x_df_aux, tuple):
             train_x_df, _ = train_x_df_aux
             train_x_df = train_x_df
         else:
             train_x_df = train_x_df_aux
 
-        if not isinstance(self.validation_dl, BitfountDataLoader):
+        if not isinstance(self.validation_dl, _BitfountDataLoader):
             empty_df = pd.DataFrame(columns=train_x_df.columns)
-            self.validation_dl = BitfountDataLoader(
-                _BitfountDataset(
-                    datasource=DataFrameSource(empty_df),
-                    target=target,
-                    selected_cols_semantic_types=self.datastructure.selected_cols_w_types,  # noqa: B950
-                    selected_cols=self.datastructure.selected_cols,
-                    data_split=DataSplit.VALIDATION,
-                    schema=self.databunch.schema,
+            self.validation_dl = _BitfountDataLoader(
+                _Dataset(
+                    empty_df,
+                    target=self._databunch.target,
+                    selected_cols=self.datastructure.selected_cols_w_types,
                 )
             )
         val_x_df_aux = self.validation_dl.get_x_dataframe()
         if isinstance(val_x_df_aux, tuple):
             val_x_df, _ = val_x_df_aux
         else:
             val_x_df = val_x_df_aux
 
         self._model.fit(  # type: ignore[union-attr] # reason: the model should be set by the time we fit it # noqa: B950
             train_x_df.values,
-            self.train_dl.get_y_dataframe()[target].values,
+            self.train_dl.get_y_dataframe()[self._databunch.target].values,
             eval_set=[
                 (
                     val_x_df.values,
-                    self.validation_dl.get_y_dataframe()[target].values,
+                    self.validation_dl.get_y_dataframe()[self._databunch.target].values,
                 )
             ],
             eval_metric=metrics,
             max_epochs=self.epochs,
             patience=self.patience,
             batch_size=self.batch_size,
             virtual_batch_size=self.virtual_batch_size,
```

### Comparing `bitfount-0.5.86/bitfount/backends/pytorch/models/bitfount_model.py` & `bitfount-0.5.9/bitfount/backends/pytorch/models/bitfount_model.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,65 +1,42 @@
 """Contains PyTorch implementations of the BitfountModel paradigm."""
 from abc import abstractmethod
 from collections import defaultdict
-from io import BytesIO
 import logging
 import os
-import re
-from typing import (
-    Any,
-    Dict,
-    Generic,
-    List,
-    MutableMapping,
-    Optional,
-    Tuple,
-    Union,
-    cast,
-)
+from typing import Any, Dict, List, MutableMapping, Optional, Tuple, Union, cast
 
 import numpy as np
 import pytorch_lightning as pl
-from pytorch_lightning.callbacks import Callback
-from pytorch_lightning.callbacks.progress import TQDMProgressBar
-from pytorch_lightning.loggers import TensorBoardLogger
 import torch
 from torch import nn as nn
 from torch.optim.lr_scheduler import _LRScheduler
 from torch.utils.data import DataLoader as PyTorchDataLoader
 
-from bitfount.backends.pytorch.data.dataloaders import _BasePyTorchBitfountDataLoader
-from bitfount.backends.pytorch.federated.mixins import _PyTorchDistributedModelMixIn
+from bitfount.backends.pytorch.data.datafactory import _PyTorchDataFactory
+from bitfount.backends.pytorch.data.dataloaders import _PyTorchBitfountDataLoader
+from bitfount.backends.pytorch.federated.models import _PyTorchDistributedModelMixIn
 from bitfount.backends.pytorch.models.base_models import (
     _STEP_OUTPUT,
     _TORCH_DTYPES,
     _OptimizerType,
 )
-from bitfount.backends.pytorch.utils import autodetect_gpu, enhanced_torch_load
-from bitfount.config import BITFOUNT_LOGS_DIR, BITFOUNT_OUTPUT_DIR
-from bitfount.data.databunch import BitfountDataBunch
-from bitfount.data.dataloaders import BitfountDataLoader
-from bitfount.data.datasources.base_source import BaseSource
-from bitfount.federated.helper import TaskContext
+from bitfount.backends.pytorch.utils import _autodetect_gpu
+from bitfount.data.dataloader import _BitfountDataLoader
+from bitfount.data.datasource import DataSource
 from bitfount.metrics import Metric
-from bitfount.models.base_models import ClassifierMixIn
+from bitfount.models.base_models import ClassifierMixIn, ModelContext
 from bitfount.models.bitfount_model import BitfountModel
-from bitfount.types import T_DTYPE, _StrAnyDict
-from bitfount.utils import _merge_list_of_dicts, delegates
-from bitfount.utils.logging_utils import filter_stderr
+from bitfount.types import T_DTYPE
 
 logger = logging.getLogger(__name__)
 
 
-@delegates()
 class PyTorchBitfountModel(
-    _PyTorchDistributedModelMixIn[T_DTYPE],
-    BitfountModel,
-    pl.LightningModule,
-    Generic[T_DTYPE],
+    _PyTorchDistributedModelMixIn, pl.LightningModule, BitfountModel
 ):
     """Blueprint for a pytorch custom model in the lightning format.
 
     This class must be subclassed in its own module. A `Path` to the module containing
     the subclass can then be passed to `BitfountModelReference` and on to your
     `Algorithm` of choice which will send the model to Bitfount Hub.
 
@@ -67,45 +44,32 @@
     users feel free to override or overwrite any variables/methods in your subclass.
 
     Take a look at the pytorch-lightning documentation on how to properly create a
     `LightningModule`:
 
     https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html
 
-    :::caution
-
-    Ensure you set `self.metrics` in the `__init__` method of your subclass to ensure
-    they pertain appropriately to your model. If not, Bitfount will attempt to set
-    these appropriately for you but there is no guarantee it will get it right.
-
-    :::
-
     Args:
         batch_size: The batch size to use for training. Defaults to 32.
         epochs: The number of epochs to train for.
         steps: The number of steps to train for.
         **kwargs: Any additional arguments to pass to parent constructors.
 
     Attributes:
         batch_size: The batch size to use for training.
         epochs: The number of epochs to train for.
         steps: The number of steps to train for.
         preds: The predictions from the most recent test run.
-        target: The targets from the most recent test run.
+        targs: The targets from the most recent test run.
         val_stats: Metrics from the validation set during training.
 
     Raises:
         ValueError: If both `epochs` and `steps` are specified.
     """
 
-    train_dl: _BasePyTorchBitfountDataLoader
-    # Test attributes
-    _test_preds: Optional[List[np.ndarray]]
-    _test_targets: Optional[List[np.ndarray]]
-
     def __init__(
         self,
         batch_size: int = 32,
         epochs: Optional[int] = None,
         steps: Optional[int] = None,
         **kwargs: Any,
     ) -> None:
@@ -122,55 +86,39 @@
         self.steps: Optional[int] = steps
 
         # Set training attributes
         # Override self._model with your model
         self._model: Optional[nn.Module] = None
         self._pl_trainer: pl.Trainer = self.trainer_init()  # cannot be "self.trainer"
         self.preds: List[float] = []
-        self.target: List[float] = []
+        self.targs: List[float] = []
         self.val_stats: List[Dict[str, float]] = []
         self._trained_on_previous_batch: bool = False
         self._total_num_batches_trained: int = 0
 
-    @staticmethod
-    def _get_import_statements() -> List[str]:
-        """Returns a list of import statements likely to be required for the model.
-
-        Returns:
-            A list of import statements.
-        """
-        return [
-            "import os",
-            "import torch",
-            "from torch import nn as nn",
-            "from torch.nn import functional as F",
-            "from bitfount import *",
-            "import bitfount",
-        ]
-
     @abstractmethod
-    def forward(self, x: Any) -> Any:
+    def forward(self, x: Any) -> Any:  # type:ignore[override] # Reason: see below
         """Forward method of the model - just like a regular `torch.nn.Module` class.
 
-        :::tip
+        Args:
+            x: Input to the model.
 
+        Returns:
+            Output of the model.
+
+        :::tip
         This will depend on your model but could be as simple as:
 
         ```python
         return self._model(x)
         ```
 
         :::
-
-        Args:
-            x: Input to the model.
-
-        Returns:
-            Output of the model.
         """
+        # pl.LightningModule incompatible with our forward function.
         raise NotImplementedError
 
     @abstractmethod
     def configure_optimizers(
         self,
     ) -> Union[_OptimizerType, Tuple[List[_OptimizerType], List[_LRScheduler]]]:
         """Configures the optimizer(s) and scheduler(s) for backpropagation.
@@ -187,17 +135,26 @@
 
         Returns:
             Underlying pytorch model. This is set to `self._model`.
         """
         raise NotImplementedError
 
     @abstractmethod
-    def training_step(self, batch: Any, batch_idx: int) -> _STEP_OUTPUT:
+    def training_step(self, batch: Any, batch_idx: int) -> _STEP_OUTPUT:  # type: ignore[override] # Reason: see below  # noqa: B950
         """Training step.
 
+        Args:
+            batch: The batch to be trained on.
+            batch_idx: The index of the batch to be trained on from the train
+                dataloader.
+
+        Returns:
+            The loss from this batch as a `torch.Tensor`. Or a dictionary which includes
+            the key `loss` and the loss as a `torch.Tensor`.
+
         :::caution
 
         If iterations have been specified in terms of steps, the default behaviour of
         pytorch lightning is to train on the first _n_ steps of the dataloader every
         time `fit` is called. This default behaviour is not desirable and has been dealt
         with in the built-in Bitfount models but, until this bug gets fixed by the
         pytorch lightning team, this needs to be implemented by the user for custom
@@ -212,256 +169,184 @@
 
         ```python
         if self.skip_training_batch(batch_idx):
             return None
         ```
 
         :::
-
-        Args:
-            batch: The batch to be trained on.
-            batch_idx: The index of the batch to be trained on from the train
-                dataloader.
-
-        Returns:
-            The loss from this batch as a `torch.Tensor`. Or a dictionary which includes
-            the key `loss` and the loss as a `torch.Tensor`.
         """
+        # Override the pl.lightning method, as it requires *args
+        # and **kwargs as arguments whereas we will always have
+        # the batch and the batch_idx as args
         raise NotImplementedError
 
     @abstractmethod
-    def validation_step(self, batch: Any, batch_idx: int) -> _StrAnyDict:
+    def validation_step(self, batch: Any, batch_idx: int) -> _STEP_OUTPUT:  # type: ignore[override] # Reason: see below # noqa: B950
         """Validation step.
 
         Args:
             batch: The batch to be evaluated.
             batch_idx: The index of the batch to be evaluated from the validation
                 dataloader.
 
         Returns:
             A dictionary of strings and values that should be averaged at the end of
             every epoch and logged e.g. `{"validation_loss": loss}`. These will be
             passed to the `validation_epoch_end` method.
         """
+        # Override the pl.lightning method, as it requires *args
+        # and **kwargs as arguments whereas we will always have
+        # the batch and the batch_idx as args
         raise NotImplementedError
 
     @abstractmethod
-    def test_step(self, batch: Any, batch_idx: int) -> _STEP_OUTPUT:
-        """Operates on a single batch of data from the test set.
+    def test_step(self, batch: Any, batch_idx: int) -> Optional[_STEP_OUTPUT]:  # type: ignore[override] # Reason: see below  # noqa: B950
+        """Performs test step and must set `self.preds` and `self.targs`.
+
+        They will be returned by the `evaluate` method.
 
         Args:
             batch: The batch to be evaluated.
             batch_idx: The index of the batch to be evaluated from the test
                 dataloader.
 
         Returns:
-            A dictionary of predictions and targets, with the dictionary
-            keys being "predictions" and "targets" for each of them, respectively.
-            These will be passed to the `test_epoch_end` method.
-        """
+            Any object or value of interest. These will be passed to the
+            `test_epoch_end` method. If returning None, testing will skip to the next
+            batch.
+        """
+        # Override the pl.lightning method, as it requires
+        # *args and **kwargs as arguments whereas we will
+        # always have the batch and the batch_idx as args
         raise NotImplementedError
 
     def tensor_precision(self) -> T_DTYPE:
         """Returns tensor dtype used by Pytorch Lightning Trainer.
 
+        Returns:
+            Pytorch tensor dtype.
+
         :::note
 
         Currently only 32-bit training is supported.
 
         :::
-
-        Returns:
-            Pytorch tensor dtype.
         """
         # TODO: [BIT-727] support non-32 bit training
-        return cast(T_DTYPE, _TORCH_DTYPES[int(self._pl_trainer.precision)])
+        return cast(T_DTYPE, _TORCH_DTYPES[cast(int, self._pl_trainer.precision)])
 
     def initialise_model(
-        self, data: Optional[BaseSource] = None, context: Optional[TaskContext] = None
+        self, data: Optional[DataSource] = None, context: Optional[ModelContext] = None
     ) -> None:
         """Any initialisation of models/dataloaders to be done here.
 
         Initialises the dataloaders and sets `self._model` to be the output from
         `self.create_model`. Any initialisation ahead of federated or local training,
         serialization or deserialization should be done here.
 
         Args:
             data: The datasource for model training. Defaults to None.
             context: Indicates if the model is running as a modeller or worker.
                 If None, there is no difference between modeller and worker.
         """
         self._context = context
         self._initialised = True
-        if self._context == TaskContext.MODELLER:
-            # In a distributed setting, the Modeller needs to first initialise its
-            # own model before it can be used. The pod identifier needs to be set
-            # before the model is initialised so the the relevant details can be
-            # retrieved from the schema. For this we just use the first pod
-            # identifier specified in the datastructure as it is assumed the the
-            # schemas for all the Pods are the same.
-            pod_identifiers = self.datastructure.get_pod_identifiers()
-            if pod_identifiers:
-                self.set_datastructure_identifier(pod_identifiers[0])
         if data is not None:
-            if self.datastructure.query:
-                table_schema = self.datastructure._override_schema(
-                    data_identifier=self._datastructure_identifier
-                )
-                self.databunch = BitfountDataBunch(
-                    data_structure=self.datastructure,
-                    schema=table_schema,
-                    datasource=data,
-                )
-            elif self.datastructure.table:
-                if context:
-                    table_schema = self.schema.get_table_schema(
-                        self.datastructure.get_table_name(
-                            self._datastructure_identifier
-                        )
-                    )
-                    self.databunch = BitfountDataBunch(
-                        data_structure=self.datastructure,
-                        schema=table_schema,
-                        datasource=data,
-                    )
-                else:
-                    # For local training, we can add the datasource to the schema.
-                    data.load_data(table_name=self.datastructure.table)
-                    self._add_datasource_to_schema(data)
-                    table_schema = self.schema.get_table_schema(
-                        self.datastructure.get_table_name(
-                            self._datastructure_identifier
-                        )
-                    )
-
-            if self._context != TaskContext.MODELLER:
+            self._add_datasource_to_schema(data, datafactory=_PyTorchDataFactory())
+            if self._context != ModelContext.MODELLER:
                 self._set_dataloaders(self.batch_size)
-        else:
-            if self.datastructure.query:
-                table_schema = self.datastructure._override_schema(
-                    data_identifier=self._datastructure_identifier
-                )
-            else:
-                table_schema = self.schema.get_table_schema(
-                    self.datastructure.get_table_name(self._datastructure_identifier)
-                )
-        self.datastructure.set_training_input_size(table_schema)
-
-        if hasattr(self, "_objective") and self._objective == "classification":
+        self.datastructure.set_training_input_size(self.schema)
+        if self._objective == "classification":
             # The casts here are to assuage mypy because it (incorrectly) asserts
             # that a subclass of both ClassifierMixIn and BitfountModel cannot exist.
             # We utilise a subclass of both in the tests to assure ourselves.
             if isinstance(cast(ClassifierMixIn, self), ClassifierMixIn):
-                cast(ClassifierMixIn, self).set_number_of_classes(table_schema)
+                cast(ClassifierMixIn, self).set_number_of_classes()
             else:
                 raise TypeError(
                     "Training objective is classification but this model does not "
                     "inherit from ClassifierMixIn"
                 )
         self._model = self.create_model()
 
     def trainer_init(self) -> pl.Trainer:
         """Initialises the Lightning Trainer for this model.
 
         Documentation for pytorch-lightning trainer can be found here:
         https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html
 
+        Returns:
+            The pytorch lightning trainer.
+
         :::tip
 
         Override this method to choose your own `Trainer` arguments.
 
         :::
-
-        Returns:
-            The pytorch lightning trainer.
         """
-        callbacks: List[Callback] = [TQDMProgressBar(refresh_rate=1)]
+        trainer = pl.Trainer(
+            max_epochs=self.epochs,
+            max_steps=self.steps,
+            progress_bar_refresh_rate=1,
+            deterministic=True,
+            auto_lr_find=True,
+            gpus=_autodetect_gpu(),  # will be 1 (first GPU) or 0 (CPU)
+        )
+        return trainer
 
-        # torch emits warnings to stderr that are not relevant for us, so we need
-        # to filter them out
-        with filter_stderr(
-            re.escape(
-                "[W Context.cpp:70] Warning:"
-                " torch.use_deterministic_algorithms is in beta"
-            )
-        ):
-            gpu_kwargs = autodetect_gpu()
-            trainer = pl.Trainer(
-                max_epochs=self.epochs or -1,
-                max_steps=self.steps or -1,
-                deterministic=True,
-                auto_lr_find=True,
-                callbacks=callbacks,
-                logger=TensorBoardLogger(save_dir=str(BITFOUNT_LOGS_DIR)),
-                default_root_dir=str(BITFOUNT_OUTPUT_DIR),
-                **gpu_kwargs,
-            )
-            return trainer
-
-    def train_dataloader(self) -> _BasePyTorchBitfountDataLoader:  # type: ignore[override] # Reason: see below # noqa: B950
+    def train_dataloader(self) -> _PyTorchBitfountDataLoader:  # type: ignore[override] # Reason: see below # noqa: B950
         """Returns training dataloader."""
         # We override the dataloader return annotation as the LightningModule
         # expects a pytorch DataLoader, and we return out PyTorchBitfountDataLoader
-        return self.train_dl
+        return cast(_PyTorchBitfountDataLoader, self.train_dl)
 
-    def val_dataloader(self) -> _BasePyTorchBitfountDataLoader:  # type: ignore[override] # Reason: see below # noqa: B950
+    def val_dataloader(self) -> _PyTorchBitfountDataLoader:  # type: ignore[override] # Reason: see below # noqa: B950
         """Returns validation dataloader."""
         # We override the dataloader return annotation as the LightningModule
         # expects a pytorch DataLoader, and we return out PyTorchBitfountDataLoader
-        return cast(_BasePyTorchBitfountDataLoader, self.validation_dl)
+        return cast(_PyTorchBitfountDataLoader, self.validation_dl)
 
-    def test_dataloader(self) -> _BasePyTorchBitfountDataLoader:  # type: ignore[override] # Reason: see below # noqa: B950
+    def test_dataloader(self) -> _PyTorchBitfountDataLoader:  # type: ignore[override] # Reason: see below # noqa: B950
         """Returns test dataloader."""
         # We override the dataloader return annotation as the LightningModule
         # expects a pytorch DataLoader, and we return out PyTorchBitfountDataLoader
-        return cast(_BasePyTorchBitfountDataLoader, self.test_dl)
+        return cast(_PyTorchBitfountDataLoader, self.test_dl)
 
     def serialize(self, filename: Union[str, os.PathLike]) -> None:
         """Serialize model to file with provided `filename`.
 
         Args:
             filename: Path to file to save serialized model.
         """
         if not self._initialised:
             logger.info("Model not yet initialised. Auto-initialising model.")
             self.initialise_model()
         # Model has been initialised, assuring mypy of this
-        assert self._model is not None  # nosec assert_used
+        assert self._model is not None  # nosec
         torch.save(self._model.state_dict(), filename)
 
-    def deserialize(
-        self,
-        content: Union[str, os.PathLike, bytes],
-        weights_only: bool = True,
-        **kwargs: Any,
-    ) -> None:
+    def deserialize(self, filename: Union[str, os.PathLike]) -> None:
         """Deserialize model.
 
+        Args:
+            filename: Path to file containing serialized model.
+
         :::danger
 
-        If `weights_only` is set to False, this should not be used on a model file that
-        has been received across a trust boundary due to underlying use of `pickle` by
-        `torch`.
+        This should not be used on a model file that has been received across a
+        trust boundary due to underlying use of `pickle` by `torch`.
 
         :::
-
-        Args:
-            content: Path to file containing serialized model.
-            weights_only: If True, only load the weights of the model. If False, load
-                the entire model. Defaults to True.
-            **kwargs: Keyword arguments provided to `torch.load` under the hood.
         """
-        kwargs.update({"weights_only": weights_only})
         if not self._initialised:
             logger.info("Model not yet initialised. Auto-initialising model.")
             self.initialise_model()
         # Model has been initialised, assuring mypy of this
-        assert self._model is not None  # nosec assert_used
-        load_contents = BytesIO(content) if isinstance(content, bytes) else content
-        self._model.load_state_dict(enhanced_torch_load(load_contents, **kwargs))
+        assert self._model is not None  # nosec
+        self._model.load_state_dict(torch.load(filename))
 
     def skip_training_batch(self, batch_idx: int) -> bool:
         """Checks if the current batch from the training set should be skipped.
 
         This is a workaround for the fact that PyTorch Lightning starts the Dataloader
         iteration from the beginning every time `fit` is called. This means that if we
         are training in steps, we are always training on the same batches. So this
@@ -476,160 +361,126 @@
         """
         # TODO: [BIT-1237] remove this code block and find a better way to do this that
         # doesn't involve loading every batch into memory until we get to the right one
         if self.steps:
             # If we have trained on the previous batch, we can avoid the checks because
             # it means we have already reached the target start batch.
             if not self._trained_on_previous_batch:
-                if (self.steps != self._pl_trainer.max_steps) and (
+                # mypy cannot see that the trainer has this attribute due to how it is set in pytorch lightning # noqa: B950
+                if (self.steps != self._pl_trainer.max_steps) and (  # type: ignore[attr-defined] # Reason: see above # noqa: B950
                     batch_idx < (self._total_num_batches_trained % len(self.train_dl))
                 ):
                     return True
                 else:
                     self._trained_on_previous_batch = True
 
             # `_total_num_batches_trained` hasn't been incremented yet so we need to add
             # 1 here to get the correct batch number.
-            if self._total_num_batches_trained + 1 == self._pl_trainer.max_steps:
+            # mypy cannot see that the trainer has this attribute due to how it is set in pytorch lightning # noqa: B950
+            if self._total_num_batches_trained + 1 == self._pl_trainer.max_steps:  # type: ignore[attr-defined] # Reason: see above # noqa: B950
                 self._trained_on_previous_batch = False
 
         if not self._pl_trainer.sanity_checking:
             self._total_num_batches_trained += 1
 
         return False
 
     def validation_epoch_end(  # type: ignore[override] # Reason: see below
-        self, outputs: List[_StrAnyDict]
+        self, outputs: List[Dict[str, Any]]
     ) -> None:
         """Called at the end of the validation epoch with all validation step outputs.
 
         Ensures that the average metrics from a validation epoch is stored. Logs results
         and also appends to `self.val_stats`.
 
         Args:
             outputs: List of outputs from each validation step.
         """
-        # Override the pl.lightning method, as its outputs can be
-        # List[Union[Tensor, _StrAnyDict]],
+        # Override the pl.lightning method, as it outputs can be
+        # List[Union[Tensor, Dict[str, Any]]],
         # whereas we force outputs to be a Dict
         avgs = self._compute_metric_averages(outputs)
         self.val_stats.append(avgs)
 
         # Also log out these averaged metrics
         for k, v in avgs.items():
             self.log(f"avg_{k}", v)
 
-    def test_epoch_end(self, outputs: List[Dict[str, torch.Tensor]]) -> None:  # type: ignore[override] # Reason: see below # noqa: B950
-        """Aggregates the predictions and targets from the test set.
-
-        :::caution
-
-        If you are overwriting this method, ensure you set `self._test_preds` to
-        maintain compatibility with `self._predict_local` unless you are overwriting
-        both of them.
-
-        :::
-
-        Args:
-            outputs: List of outputs from each test step.
-        """
-        # Override the pl.lightning method, as it requires a different type for outputs.
-        merged_outputs: Dict[str, List[torch.Tensor]] = _merge_list_of_dicts(outputs)
-        self._test_preds = [i.cpu().numpy() for i in merged_outputs["predictions"]]
-        self._test_targets = [i.cpu().numpy() for i in merged_outputs["targets"]]
-
     @staticmethod
-    def _compute_metric_averages(outputs: List[_StrAnyDict]) -> Dict[str, float]:
+    def _compute_metric_averages(outputs: List[Dict[str, Any]]) -> Dict[str, float]:
         """Compute the average metrics from a list of outputs."""
         # Stack up shared dict keys into lists of entries
         stacked_dict = defaultdict(list)
         for output in outputs:
             for k, v in output.items():
                 stacked_dict[k].append(v)
 
         # Calculate the average value of each key and convert to float
         avgs = {}
         for k, v_list in stacked_dict.items():
             avgs[k] = float(torch.stack(v_list).mean().item())
         return avgs
 
-    def _evaluate_local(
-        self, test_dl: Optional[BitfountDataLoader] = None, **kwargs: Any
+    def evaluate(
+        self, test_dl: Optional[_BitfountDataLoader] = None, **kwargs: Any
     ) -> Tuple[np.ndarray, np.ndarray]:
         """This method runs inference on the test dataloader.
 
         This is done by calling `self.test_step` under the hood. Customise this method
         as you please but it must return a list of predictions and a list of targets.
 
         Args:
             test_dl: Optional dataloader to run inference on which takes precedence over
                 the dataloader returned by `self.test_dataloader`.
 
         Returns:
             A tuple of predictions and targets as numpy arrays.
         """
-        # Reset test attributes to None
-        self._reset_test_attrs()
         if test_dl is None:
-            if isinstance(self.test_dl, _BasePyTorchBitfountDataLoader):
+            if isinstance(self.test_dl, _PyTorchBitfountDataLoader):
                 test_dl = self.test_dl
             else:
                 raise ValueError("No test data to evaluate the model on.")
-        self._pl_trainer.test(model=self, dataloaders=cast(PyTorchDataLoader, test_dl))
-        return np.asarray(self._test_preds), np.asarray(self._test_targets)
+        self._pl_trainer.test(test_dataloaders=cast(PyTorchDataLoader, test_dl))
+        return np.asarray(self.preds), np.asarray(self.targs)
 
-    def _reset_test_attrs(self) -> None:
-        """Resets test attributes to None."""
-        self._test_preds = None
-        self._test_targets = None
-
-    def _predict_local(self, data: BaseSource, **kwargs: Any) -> List[np.ndarray]:
+    def predict(self, data: DataSource, **kwargs: Any) -> np.ndarray:
         """This method runs inference on the test data, returns predictions.
 
         This is done by calling `test_step` under the hood. Customise this method as you
         please but it must return a list of predictions and a list of targets. Note that
         as this is the prediction function, only the predictions are returned.
 
-        :::tip
-
-        Feel free to overwrite this method just so long as you return a numpy array to
-        maintain compatability with the `ModelInference` algorithm - you are not limited
-        to just returning predictions.
-
-        :::
-
         Returns:
             A numpy array containing the prediction values.
-        """
-        if data is not None:
-            data.load_data()
-            if not hasattr(self, "databunch"):
-                self._add_datasource_to_schema(data)  # Also sets `self.databunch
-            if not self.databunch:
-                self._add_datasource_to_schema(data)  # Also sets `self.databunch
-            test_dl = self.databunch.get_test_dataloader(self.batch_size)
-            if isinstance(test_dl, BitfountDataLoader):
-                logger.info(
-                    f"Using test portion of dataset for inference - this has "
-                    f"{len(test_dl.dataset)} record(s)."
-                )
-            else:
-                raise ValueError("No test data to infer in the provided datasource.")
-
-        self._pl_trainer.test(model=self, dataloaders=cast(PyTorchDataLoader, test_dl))
 
-        if self._test_preds is not None:
-            return self._test_preds
+        Raises:
+            ValueError: If no data is provided to test with.
+        """
+        self._add_datasource_to_schema(
+            datasource=data, datafactory=_PyTorchDataFactory()
+        )
+        test_dl = self._databunch.get_test_dataloader(self.batch_size)
+        if isinstance(test_dl, _BitfountDataLoader):
+            logger.info(
+                f"Using test portion of dataset for inference - this has"
+                f"{len(test_dl)} records."
+            )
+        else:
+            raise ValueError("No data provided to run inference for the model on.")
 
-        raise ValueError("'self._test_preds' was not set by the model after inference.")
+        self._pl_trainer.test(
+            model=self, test_dataloaders=cast(PyTorchDataLoader, test_dl)
+        )
+        return cast(np.ndarray, self.preds)
 
     def _fit_local(
         self,
-        data: BaseSource,
-        metrics: Optional[Union[str, List[str], MutableMapping[str, Metric]]] = None,
+        data: DataSource,
+        metrics: Optional[MutableMapping[str, Metric]] = None,
         **kwargs: Any,
     ) -> Dict[str, str]:
         """Trains the model on local data.
 
         Returns:
             Validation metrics for the final epoch.
         """
@@ -637,7 +488,16 @@
             logger.info("Model not yet initialised. Auto-initialising model.")
             self.initialise_model(data)
 
         self._pl_trainer.fit(self)
 
         # Return the validation stats to be sent back
         return {k: ("%.4f" % v) for k, v in self.val_stats[-1].items()}
+
+    @property
+    def training_needed(self) -> bool:
+        """Dictates whether the model needs training.
+
+        Returns:
+            True if one of either `self.epochs` or `self.steps` are > 0.
+        """
+        return sum(filter(None, (self.epochs, self.steps))) > 0
```

### Comparing `bitfount-0.5.86/bitfount/backends/pytorch/models/models.py` & `bitfount-0.5.9/bitfount/models/base_models.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,677 +1,849 @@
-"""Model implementations using PyTorch."""
+"""Defines abstract models, mixins, and other common backend-agnostic classes.
+
+Implementations of these abstract models should be located in `bitfount.models.models`
+or in the `models` subpackage of a backend.
+"""
 from __future__ import annotations
 
+from abc import ABC, abstractmethod
+from dataclasses import dataclass, field
+from enum import Enum, auto
 import logging
-from typing import Any, ClassVar, List, Optional, Tuple, Union, cast
-
-from marshmallow import fields
-import pandas as pd
-from pytorch_tabnet.tab_model import TabNetClassifier as TabNetClassifier_
-import torch
-from torch import nn as nn
-from torch.nn import Module
-
-from bitfount.backends.pytorch.data.dataloaders import (
-    PyTorchBitfountDataLoader,
-    _BasePyTorchBitfountDataLoader,
-)
-from bitfount.backends.pytorch.models.base_models import (
-    _OPTIMIZERS,
-    _SCHEDULERS,
-    BasePyTorchModel,
-    BaseTabNetModel,
-    PyTorchClassifierMixIn,
-    _calculate_embedding_sizes,
-)
-from bitfount.backends.pytorch.models.nn import (
-    _get_torchvision_classification_model,
-    _PyTorchConvNeuralNet,
-    _PyTorchFeedForwardNeuralNet,
-    _PyTorchLogisticRegression,
-)
-from bitfount.backends.pytorch.types import (
-    ImgAndTabDataSplit,
-    ImgDataReturnType,
-    ImgFwdTypes,
-    ImgXorTabDataSplit,
-    TabDataReturnType,
+import os
+from pathlib import Path
+from typing import (
+    TYPE_CHECKING,
+    Any,
+    Callable,
+    Dict,
+    Generic,
+    List,
+    MutableMapping,
+    Optional,
+    Tuple,
+    Type,
+    TypeVar,
+    Union,
+    cast,
 )
+
+from cryptography.hazmat.primitives.asymmetric.rsa import RSAPrivateKey
+import desert
+from marshmallow import Schema as MarshmallowSchema
+from marshmallow import fields, post_load
+import numpy as np
+
+from bitfount.config import BITFOUNT_LOGS_DIR
+from bitfount.data.databunch import _BitfountDataBunch
+from bitfount.data.datafactory import _BasicDataFactory, _DataFactory
+from bitfount.data.dataloader import _BitfountDataLoader
+from bitfount.data.datasource import DataSource
 from bitfount.data.datastructure import DataStructure
-from bitfount.data.types import SemanticType
-from bitfount.models.base_models import (
-    CNNModelStructure,
-    FeedForwardModelStructure,
-    NeuralNetworkModelStructure,
-    NeuralNetworkPredefinedModel,
-    Scheduler,
-)
-from bitfount.types import T_FIELDS_DICT
-from bitfount.utils import DEFAULT_SEED, delegates
+from bitfount.data.schema import BitfountSchema
+from bitfount.types import _JSONDict
+from bitfount.utils import _add_this_to_list, seed_all
+
+if TYPE_CHECKING:
+    from bitfount.metrics import Metric
+
+# Valid pooling functions for Convolutional Neural Networks.
+POOLING_FUNCTIONS = ["max", "avg"]
+
 
 logger = logging.getLogger(__name__)
 
 
-@delegates()
-class PyTorchImageClassifier(PyTorchClassifierMixIn, BasePyTorchModel):
-    """A Pytorch model designed specifically for image classification problems.
+@dataclass
+class LoggerConfig:
+    """Configuration for the logger.
+
+    The configured logger will log training events, metrics, model checkpoints, etc. to
+    your chosen platform. If no logger configuration is provided, the default logger is
+    a Tensorboard logger.
 
-    The model can handle binary, multiclass and multilabel classification problems.
+    Args:
+        name: The name of the logger. Should be one of the loggers supported by the
+            chosen backend
+        save_dir: The directory to save the logs. Defaults to `BITFOUNT_LOGS_DIR`
+        params: A dictionary of keyword arguments to pass to the logger. Defaults to an
+            empty dictionary
+    """
 
-    :::info
+    # Desert bug causing incompatability with mypy regarding type assignment
+    # (https://github.com/python-desert/desert/issues/101)
 
-    Currently images are scaled to 224 x 224.
+    #: same as argument
+    name: str
+    #: same as argument
+    save_dir: Optional[Path] = desert.field(
+        fields.Function(
+            deserialize=lambda path: path if path is None else Path(path).expanduser()
+        ),
+        default=BITFOUNT_LOGS_DIR,
+    )  # type: ignore[assignment] # Reason: see above
+    #: same as argument
+    params: Optional[_JSONDict] = desert.field(
+        fields.Dict(keys=fields.Str), default_factory=dict
+    )  # type: ignore[assignment] # Reason: see above
 
-    :::
 
-    Raises:
-        ValueError: If the model structure is not an instance of `CNNModelStructure` or
-            `NeuralNetworkPredefinedModel`.
-        ValueError: If the loss function is not `nn.BCEWithLogitsLoss` or
-            `nn.CrossEntropyLoss`.
+class ModelContext(Enum):
+    """Describes the context (modeller or worker) in which the model is running.
+
+    This is used for models where the model differs depending on if it is on the
+    modeller-side or worker-side of the federated process.
     """
 
-    train_dl: _BasePyTorchBitfountDataLoader
-    datastructure: DataStructure
-    input_dim: int = 224  # default input images are 224 x 224
+    MODELLER = auto()
+    WORKER = auto()
 
-    def __init__(self, **kwargs: Any):
-        model_structure = kwargs.pop("model_structure", CNNModelStructure())
-        if not isinstance(
-            model_structure, (CNNModelStructure, NeuralNetworkPredefinedModel)
-        ):
-            raise ValueError("Model structure does not match model")
 
-        super().__init__(model_structure=model_structure, **kwargs)
-        if self.loss_func is None:
-            if self.multilabel:
-                self.loss_func = nn.BCEWithLogitsLoss
-            else:
-                self.loss_func = nn.CrossEntropyLoss
-        elif self.loss_func not in [nn.BCEWithLogitsLoss, nn.CrossEntropyLoss]:
-            raise ValueError("This loss function is not currently supported")
-
-    def forward(self, x: ImgFwdTypes) -> Any:  # type: ignore[override] # Reason: see below # noqa: B950
-        """Performs a forward pass of the model."""
-        # override as the forward function is incompatible with pl.LightningModule
-        if self.datastructure.number_of_images > 1:
-            aux = []
-            for i in range(len(x)):
-                aux.append(self._model(x[i]))  # type: ignore[misc] # reason: model should be initialised already # noqa: B950
-            return torch.cat([item[0] for item in aux], 1)
-        else:
-            return self._model(x)  # type: ignore[misc] # reason: model should be initialised already # noqa: B950
+ModelType = TypeVar("ModelType", bound="_BaseModel")
 
-    def _split_dataloader_output(
-        self,
-        data: Union[
-            ImgAndTabDataSplit,
-            ImgXorTabDataSplit,
-        ],
-    ) -> Union[ImgDataReturnType, TabDataReturnType]:
-        """Splits dataloader output into image tensor, weights and category."""
-        images, sup = cast(Tuple[torch.Tensor, torch.Tensor], data)
-        weights = sup[:, 0].float()
-        category: Optional[torch.Tensor]
-        if sup.shape[1] > 2:
-            category = sup[:, -1:].long()
-        else:
-            category = None
 
-        return images, weights, category
+class _BaseModel(ABC, Generic[ModelType]):
+    """Abstract Base Model from which all other models must inherit.
 
-    def _get_convolution_final_output_dimension(self) -> int:
-        """Calculates the output size of the final convolutional layer.
+    This class is designed to be at the very bottom of the inheritance hierarchy.
+    The only reason it has a `super().__init__()` call is to call the parent classes of
+    other classes defined in other libraries. It also takes kwargs so that we do not
+    throw an error if there are unexpected keyword arguments. These unexpected keyword
+    arguments will end up in this constructor where they will simply be ignored.
+    """
 
-        This will become the input size of the first feedforward layer
-        """
-        input_dim: float = self.input_dim
-        if isinstance(self.model_structure, CNNModelStructure):
-            # self.model_structure.layers is set in post_init, so we can just cast
-            for _ in cast(List[int], self.model_structure.layers):
-                output_dim = self._get_convolution_output_dimension(
-                    input_dim,
-                    self.model_structure.kernel_size,
-                    self.model_structure.padding,
-                    self.model_structure.stride,
-                )
-                output_dim = output_dim / 2  # due to pooling
-                input_dim = output_dim
+    def __init__(
+        self,
+        datastructure: DataStructure,
+        schema: BitfountSchema,
+        seed: Optional[int] = None,
+        **kwargs: Any,
+    ):
+        self.name = type(self).__name__
+        self._context: Optional[ModelContext] = None
+        self.metrics: Optional[MutableMapping[str, Metric]] = None
+        self._model: Optional[ModelType] = None
+        self._initialised: bool = False
+        self.seed = seed
+        seed_all(self.seed)
+        self.datastructure = datastructure
+        self._databunch: _BitfountDataBunch
+        self.schema = schema
+        self._objective: str
+
+        # Placeholders for dataloaders
+        self.train_dl: _BitfountDataLoader
+        self.validation_dl: Optional[_BitfountDataLoader] = None
+        self.test_dl: Optional[_BitfountDataLoader] = None
 
-            return int(
-                (output_dim**2) * cast(List[int], self.model_structure.layers)[-1]
-            )
-        else:
-            raise TypeError("This method only works with the cnn model structure.")
+        for unexpected_kwarg in kwargs:
+            logger.warning(f"Ignoring unexpected keyword argument {unexpected_kwarg}")
 
-    @staticmethod
-    def _get_convolution_output_dimension(
-        input_size: Union[int, float], kernel_size: int, padding: int, stride: int
-    ) -> float:
-        """Gets convolution output dimension."""
-        return ((input_size - kernel_size + (2 * padding)) / stride) + 1
-
-    def _create_model(self) -> nn.Module:
-        """Creates the model to use.
-
-        If `self.model_structure` is a `NeuralNetworkPredefinedModel`, then calls
-        `get_torchvision_classification_model` to adapt model head for the task
-        before returning the model
+        super().__init__()
 
-        Otherwise, creates model as defined by `CNNModelStructure`
+    def _set_dataloaders(
+        self,
+        batch_size: Optional[int] = None,
+    ) -> None:
+        """Sets train, validation and test dataloaders.
+
+        Args:
+            batch_size: The batch size to use for the dataloaders. Defaults to None.
         """
-        table_schema = self.datastructure.get_table_schema(
-            schema=self.schema, data_identifier=self._datastructure_identifier
+        if self._databunch is None:
+            raise ValueError(
+                "_set_dataloaders() requires the databunch to be set "
+                "before being called."
+            )
+        self.train_dl = self._databunch.get_train_dataloader(batch_size)
+        self.validation_dl = self._databunch.get_validation_dataloader(batch_size)
+        self.test_dl = self._databunch.get_test_dataloader(batch_size)
+
+    def _add_datasource_to_schema(
+        self,
+        datasource: DataSource,
+        datafactory: Optional[_DataFactory] = None,
+    ) -> None:
+        """Adds the datasource to the schema and sets the `_databunch` attribute."""
+        # self.datastructure.apply_transformations(datasource)
+        # TODO: [BIT-1167] Once we add the transformations we can force the target
+        #  to be categorical in the transformations as this can be specified directly
+        if datafactory is None:
+            datafactory = _BasicDataFactory()
+        if self.datastructure.image_cols:
+            for image in self.datastructure.image_cols:
+                if image not in datasource.data.columns:
+                    raise ValueError(f"Could not find {image} in dataset columns")
+        # Add the datasource features to the datastructure.
+        self.schema.add_datasource_features(
+            datasource,
+            ignore_cols=self.datastructure.ignore_cols,
+            force_stype=self.datastructure._force_stype,
+        )
+        datasource = self.datastructure.apply_dataset_transformations(datasource)
+
+        # Create the BitfountDataBunch that will be used in training
+        self._databunch = _BitfountDataBunch(
+            data_structure=self.datastructure,
+            schema=self.schema,
+            datasource=datasource,
+            data_factory=datafactory,
         )
 
-        self.set_number_of_classes(table_schema)
+    def initialise_model(
+        self,
+        data: Optional[DataSource] = None,
+        context: Optional[ModelContext] = None,
+    ) -> None:
+        """Can be implemented to initialise model if necessary.
 
-        if isinstance(self.model_structure, NeuralNetworkPredefinedModel):
-            kwargs = self.model_structure.kwargs or {}
-            model = _get_torchvision_classification_model(
-                self.model_structure.name,
-                self.model_structure.pretrained,
-                self.n_classes,
-                **kwargs,
-            )
-        elif isinstance(self.model_structure, CNNModelStructure):
-            # self.model_structure.layers is set in post_init, so we can just cast
-            layer_sizes = self._get_layer_sizes(
-                cast(List[int], self.model_structure.layers), 3
-            )
+        This is automatically called by the `fit()` method if necessary.
 
-            head_sizes = [
-                (self.model_structure.ff_layers[-1], self.n_classes)
-                for _ in range(self.model_structure.num_heads)
-            ]
-            ff_layer_sizes = self._get_layer_sizes(
-                self.model_structure.ff_layers,
-                self._get_convolution_final_output_dimension(),
-            )
-            logger.debug(f"Creating model with {self.model_structure.num_heads} heads")
-            # self.model_structure.dropout_probs is set in post_init,
-            # so we can just cast below
-            model = _PyTorchConvNeuralNet(
-                layer_sizes=layer_sizes,
-                dropout_probs=cast(List[float], self.model_structure.dropout_probs),
-                mish=self.model_structure.mish_activation_function,
-                head_sizes=head_sizes,
-                ff_layer_sizes=ff_layer_sizes,
-                ff_dropout_probs=self.model_structure.ff_dropout_probs,
-                kernel_size=self.model_structure.kernel_size,
-                padding=self.model_structure.padding,
-                stride=self.model_structure.stride,
-                pooling_function=self.model_structure.pooling_function,
+        Args:
+            data: The data used for model training.
+            context: Indicates if the model is running as a modeller or worker. If None,
+                there is no difference between modeller and worker. Defaults to None.
+        """
+        self._initialised = True
+        if data:
+            self._add_datasource_to_schema(
+                datasource=data,
             )
-        return model
 
+    @property
+    def initialised(self) -> bool:
+        """Returns True if the model has been initialised, otherwise False.
 
-@delegates()
-class PyTorchTabularClassifier(PyTorchClassifierMixIn, BasePyTorchModel):
-    """A Pytorch model designed specifically for tabular classification problems.
+        I.e. whether the `initialise_model` method has been called.
+        """
+        return self._initialised
 
-    The model can handle binary, multiclass and multilabel classification problems.
+    @abstractmethod
+    def serialize(self, filename: Union[str, os.PathLike]) -> None:
+        """Implement this method to serialise a model."""
+        raise NotImplementedError
+
+    @abstractmethod
+    def deserialize(self, filename: Union[str, os.PathLike]) -> None:
+        """Implement this method to deserialise a model."""
+        raise NotImplementedError
 
-    Raises:
-        ValueError: If the model structure is not an instance of
-            `FeedForwardModelStructure`.
-        ValueError: If the loss function is not `nn.BCEWithLogitsLoss` or
-            `nn.CrossEntropyLoss`.
-    """
+    @abstractmethod
+    def evaluate(
+        self,
+        test_dl: Optional[_BitfountDataLoader] = None,
+        **kwargs: Any,
+    ) -> Tuple[np.ndarray, np.ndarray]:
+        """Implement this method to perform inference on the test set.
 
-    train_dl: _BasePyTorchBitfountDataLoader
-
-    def __init__(self, **kwargs: Any):
-        model_structure = kwargs.pop("model_structure", FeedForwardModelStructure())
-        if (
-            isinstance(model_structure, NeuralNetworkPredefinedModel)
-            and model_structure.name == "TabNet"
-        ):
-            raise ValueError("Please create a TabNetClassifier directly.")
-        elif not isinstance(model_structure, FeedForwardModelStructure):
-            raise ValueError("Please provide a FeedForwardModelStructure")
-        super().__init__(
-            model_structure=cast(NeuralNetworkModelStructure, model_structure),
-            **kwargs,
-        )
+        Args:
+            test_dl: Optional `BitfountDataLoader` object containing test data. If this
+                is not provided, the test set from the `DataSource` used to train the
+                model should be used if present.
 
-        if self.loss_func is None:
-            if hasattr(self, "multilabel") and (self.multilabel is not False):
-                self.loss_func = nn.BCEWithLogitsLoss
-            else:
-                self.loss_func = nn.CrossEntropyLoss
-        elif self.loss_func not in [nn.BCEWithLogitsLoss, nn.CrossEntropyLoss]:
-            raise ValueError("This loss function is not currently supported")
+        Returns:
+            A tuple of numpy arrays containing the predicted and actual values.
+        """
+        raise NotImplementedError
 
-    def _create_model(self) -> _PyTorchFeedForwardNeuralNet:
-        """Creates model to use.
+    def predict(self, data: DataSource, **kwargs: Any) -> np.ndarray:
+        """This method runs inference on the test data, returns predictions.
 
-        Takes number of continuous features and number of heads. Creates and
-        returns model.
-        """
-        table_schema = self.datastructure.get_table_schema(
-            schema=self.schema, data_identifier=self._datastructure_identifier
-        )
-        ignore_cols_for_training = self.datastructure.get_columns_ignored_for_training(
-            table_schema
-        )
+        Args:
+            data: `DataSource` object containing the data to run prediction on.
+                Predictions will be generated for the test subset (as defined
+                by the `DataSetSplitter`).
 
-        self.set_number_of_classes(table_schema)
-        num_continuous = len(
-            [
-                col
-                for col in table_schema.get_feature_names(
-                    SemanticType.CONTINUOUS,
-                )
-                if col not in ignore_cols_for_training
-            ]
-        )
-        embedding_sizes = _calculate_embedding_sizes(
-            table_schema.get_categorical_feature_sizes(ignore_cols_for_training)
-        )
-        self.model_structure = cast(FeedForwardModelStructure, self.model_structure)
-        num_heads = self.model_structure.num_heads
-        num_categorical = sum(size for _, size in embedding_sizes)
-        # self.model_structure.layers is set in post_init, so we can just cast
-        layer_sizes = self._get_layer_sizes(
-            cast(
-                List[int],
-                self.model_structure.layers,
-            ),
-            num_continuous + num_categorical,
-        )
-        head_sizes = [
-            (
-                cast(
-                    List[int],
-                    self.model_structure.layers,
-                )[-1],
-                self.n_classes,
-            )
-            for _ in range(num_heads)
-        ]
-        # self.model_structure.dropout_probs is set in post_init,
-        # so we can just cast below
-        logger.debug(f"Creating model with {num_heads} heads")
-        model = _PyTorchFeedForwardNeuralNet(
-            embedding_sizes,
-            self.model_structure.embedding_dropout,
-            num_continuous,
-            layer_sizes,
-            cast(List[float], self.model_structure.dropout_probs),
-            self.model_structure.mish_activation_function,
-            head_sizes,
-        )
-        return model
+        Returns:
+            A numpy array containing the prediction values.
+        """
+        raise NotImplementedError
 
-    def _split_dataloader_output(
+    @abstractmethod
+    def fit(
         self,
-        data: Union[
-            ImgAndTabDataSplit,
-            ImgXorTabDataSplit,
-        ],
-    ) -> Union[ImgDataReturnType, TabDataReturnType]:
-        """Splits dataloader output.
+        data: Optional[DataSource] = None,
+        metrics: Optional[Dict[str, Metric]] = None,
+        pod_identifiers: Optional[List[str]] = None,
+        private_key_or_file: Optional[Union[RSAPrivateKey, Path]] = None,
+        **kwargs: Any,
+    ) -> Optional[Dict[str, str]]:
+        """Must be implemented to fit the model.
 
-        Splits it into pieces for categorical, continuous, weights and categories.
+        Args:
+            data: The data used for local model training.
+            metrics: A dictionary of metrics to use for validation.
+            pod_identifiers: A list of pod identifiers to use for federated training.
+            private_key_or_file: A private key or path to private key file if doing
+                federated training.
 
-        NB: `ignore_classes` is never returned
+        Must call `initialise_model()` within the method if the model needs to be
+        initialised.
         """
-        tab, sup = cast(ImgXorTabDataSplit, data)
+        raise NotImplementedError
 
-        table_schema = self.datastructure.get_table_schema(
-            schema=self.schema, data_identifier=self._datastructure_identifier
-        )
-        ignore_cols_for_training = self.datastructure.get_columns_ignored_for_training(
-            table_schema
-        )
+    @property
+    @abstractmethod
+    def training_needed(self) -> bool:
+        """Dictates whether the model needs training."""
+        raise NotImplementedError
+
+    class _Schema(MarshmallowSchema):
+        name = fields.Str()
+        datastructure = fields.Nested(DataStructure._Schema)
+        schema = fields.Nested(BitfountSchema._Schema, allow_none=True)
+        seed = fields.Integer(allow_none=True)
+
+        @abstractmethod
+        @post_load
+        def recreate_model(self, data: _JSONDict, **_kwargs: Any) -> _BaseModel:
+            """Recreates model using Schema."""
+            raise NotImplementedError
+
+    @classmethod
+    @abstractmethod
+    def get_schema(cls) -> Type[_BaseModel._Schema]:
+        """Get the model schema."""
+        raise NotImplementedError
 
-        n_cat = len(
-            _calculate_embedding_sizes(
-                table_schema.get_categorical_feature_sizes(ignore_cols_for_training)
-            )
-        )
-        n_cont = len(
-            [
-                col
-                for col in table_schema.get_feature_names(
-                    SemanticType.CONTINUOUS,
-                )
-                if col not in ignore_cols_for_training
-            ]
-        )
-        # Get items according to the order they are in the tensor
-        cat_pos = n_cat
-        cont_pos = cat_pos + n_cont
-        x_1 = tab[:, :cat_pos].long()  # categorical features
-        x_2 = tab[:, cat_pos:cont_pos].float()  # continuous features
-        weights = sup[:, 0].float()
-        # If category is present, return it, otherwise return None
-        category: Optional[torch.Tensor]
-        if sup.shape[1] > 2:
-            category = sup[:, -1:].long()
-        else:
-            category = None
-        return (x_1.t(), x_2), weights, category
 
+class ClassifierMixIn:
+    """MixIn for classification problems.
 
-@delegates()
-class PyTorchLogisticRegressionClassifier(PyTorchClassifierMixIn, BasePyTorchModel):
-    """A Logistic Regression classifier implemented in PyTorch.
-
-    Utilises softmax regression to allow extension to more than 2 classes.
-    The input and output dimensions are calculated from the data automatically.
-    A softmax regressor is used to ensure this can work on multi-class
-        problems.
+    Classification models must have this class in their inheritance hierarchy.
 
     Args:
-        bias: Whether the underlying linear layer should learn an additive
-            bias. Default True.
-        l1_regularization_weight: The weight of L1 regularization to apply, if any.
-        l2_regularization_weight: The weight of L2 regularization to apply, if any.
-        embed_categorical: Whether to use categorical embedding layers to handle
-            categorical variables, or to treat them as inherently label-encoded.
-            Default True.
-        embed_categorical_dropout: The dropout probability to apply to categorical
-            variables if using embed_categorical.
-        **kwargs: Other keyword arguments for the model.
-    """
-
-    train_dl: PyTorchBitfountDataLoader
-    fields_dict: ClassVar[T_FIELDS_DICT] = {
-        "bias": fields.Boolean(),
-        "l1_regularization_weight": fields.Float(allow_none=True),
-        "l2_regularization_weight": fields.Float(allow_none=True),
-        "embed_categorical": fields.Boolean(),
-        "embed_categorical_dropout": fields.Float(),
-    }
+        multilabel: Whether the problem is a multi-label problem. i.e. each datapoint
+            belongs to multiple classes
 
-    def __init__(
+    Attributes:
+        multilabel: Whether the problem is a multi-label problem
+        n_classes: Number of classes in the problem
+    """
+
+    #: set in _BaseModel
+    datastructure: DataStructure
+    #: set in _BaseModel
+    schema: BitfountSchema
+
+    def __init__(self, multilabel: bool = False, **kwargs: Any) -> None:
+        super().__init__(**kwargs)
+        self.multilabel = multilabel
+        self.n_classes: int
+        if "n_classes" in kwargs:
+            self.n_classes = int(kwargs["n_classes"])
+        self._objective = "classification"
+
+    def set_number_of_classes(self) -> None:
+        """Sets the target number of classes for the classifier.
+
+        If the data is a multi-label problem, the number of classes is set to the number
+        of target columns as specified in the `DataStructure`. Otherwise, the number of
+        classes is set to the number of unique values in the target column as specified
+        in the `BitfountSchema`. The value is stored in the `n_classes` attribute.
+        """
+        if self.datastructure.target is None and hasattr(self, "n_classes"):
+            logger.warning(
+                "No target specified in data. Using explicitly provided n_classes."
+                "Note that only inference results will be valid, not training"
+                "or evaluation for this model and dataset."
+            )
+        elif self.datastructure.target is not None:
+            self.n_classes = (
+                len(self.datastructure.target)
+                if self.multilabel
+                else self.schema.get_categorical_feature_size(self.datastructure.target)
+            )
+        else:
+            raise ValueError(
+                "No target specified in data, and number of classes not specified "
+                "explicitly. Not able to determine dimensions of head of model."
+            )
+
+    def _add_datasource_to_schema(
         self,
-        bias: bool = True,
-        l1_regularization_weight: Optional[float] = None,
-        l2_regularization_weight: Optional[float] = None,
-        embed_categorical: bool = True,
-        embed_categorical_dropout: float = 0.04,
-        **kwargs: Any,
+        datasource: DataSource,
+        datafactory: Optional[_DataFactory] = None,
     ) -> None:
-        """Create a logistic regression classifier using PyTorch."""
-        # Override model_structure; model_structure is required by the
-        # NeuralNetworkMixin, but is not used in this class, so we override it
-        # with an empty one and log out that this has happened
-        # Have to use empty lists as otherwise they get overwritten with defaults
-        empty_model_structure = FeedForwardModelStructure(
-            layers=[], dropout_probs=[], mish_activation_function=False
+        """Adds the datasource to the schema and sets the `_databunch` attribute."""
+        # Add the datasource features to the datastructure.
+        if datafactory is None:
+            datafactory = _BasicDataFactory()
+        force_categorical: List[str] = []
+        force_categorical = _add_this_to_list(
+            self.datastructure.target, force_categorical
+        )
+        force_categorical = _add_this_to_list(
+            self.datastructure.multihead_col, force_categorical
+        )
+        if self.datastructure.image_cols:
+            for image in self.datastructure.image_cols:
+                if image not in datasource.data.columns:
+                    raise ValueError(f"Could not find {image} in dataset columns")
+
+        if "categorical" not in self.datastructure._force_stype.keys():
+            self.datastructure._force_stype["categorical"] = force_categorical
+        else:
+            _add_this_to_list(
+                force_categorical, self.datastructure._force_stype["categorical"]
+            )
+
+        self.schema.add_datasource_features(
+            datasource,
+            ignore_cols=self.datastructure.ignore_cols,
+            force_stype=self.datastructure._force_stype,
         )
-        try:
-            if kwargs["model_structure"] != empty_model_structure:
-                logger.warning(
-                    f"Specified model structure is not compatible with "
-                    f"{self.__class__.__name__}; will be overridden."
-                )
-                kwargs["model_structure"] = empty_model_structure
-        except KeyError:
-            kwargs["model_structure"] = empty_model_structure
+        datasource = self.datastructure.apply_dataset_transformations(datasource)
 
-        super().__init__(**kwargs)
+        # Create the BitfountDataBunch that will be used in training
+        self._databunch = _BitfountDataBunch(
+            data_structure=self.datastructure,
+            schema=self.schema,
+            datasource=datasource,
+            data_factory=datafactory,
+        )
+
+    class _Schema(MarshmallowSchema):
+        multilabel = fields.Bool()
+
+
+class RegressorMixIn:
+    """MixIn for regression problems.
+
+    Currently, just used for tagging purposes.
+    """
+
+    pass
+
+
+@dataclass
+class NeuralNetworkModelStructure(ABC):
+    """Dataclass defining the structure of a neural network model.
+
+    Args:
+        layers: List of hidden layer sizes i.e. not including the input/output layers.
+            The layer type depends on the specific model structure. Defaults to 2 layers
+            with 1000 and 500 nodes respectively.
+        dropout_probs: List of dropout probabilities for each layer. Must be the same
+            length as `layers`. Defaults to 0.01 and 0.1 respectively.
+        mish_activation_function: Whether to use Mish activation function. If False,
+            ReLU is used instead. Defaults to True.
+
+    Attributes:
+        num_heads: Number of heads for multihead models. I.e. the number of output
+            layers.
 
-        # Multilabel LogReg is not supported
-        if self.multilabel:
+    Raises:
+        ValueError: If `layers` and `dropout_probs` are not the same length.
+    """
+
+    #: same as argument
+    layers: Optional[List[int]] = None
+    #: same as argument
+    dropout_probs: Optional[List[float]] = None
+    #: same as argument
+    mish_activation_function: bool = True
+
+    def __post_init__(self) -> None:
+        if self.layers is None:
+            self.layers = [1000, 500]
+
+        if self.dropout_probs is None:
+            self.dropout_probs = [0.01, 0.1]
+        if len(self.layers) != len(self.dropout_probs):
             raise ValueError(
-                f"{self.__class__.__name__} does not support multilabel "
-                f"classification problems."
+                "Number of neural layers must equal number of dropout layers."
             )
-        # Multihead LogReg is not supported
-        try:
-            model_structure = cast(NeuralNetworkModelStructure, self.model_structure)
-            if model_structure.num_heads != 1:
-                logger.warning("Multihead LogReg is not supported, setting to 1.")
-                model_structure.num_heads = 1
-        except AttributeError:
-            pass
-
-        # Set loss function. Only CrossEntropyLoss is currently supported.
-        if self.loss_func is None:
-            self.loss_func = nn.CrossEntropyLoss
-        elif self.loss_func is not nn.CrossEntropyLoss:
-            raise ValueError("This loss function is not currently supported")
-
-        self.bias = bias
-        self.l1_regularization_weight = l1_regularization_weight
-        self.l2_regularization_weight = l2_regularization_weight
-        self.embed_categorical = embed_categorical
-        self.embed_categorical_dropout = embed_categorical_dropout
-
-    def _create_model(self) -> nn.Module:
-        table_schema = self.datastructure.get_table_schema(
-            schema=self.schema, data_identifier=self._datastructure_identifier
-        )
-        # Set number of output classes from datastructure
-        self.set_number_of_classes(table_schema)
+        self.num_heads: int
+
+    def set_num_heads(self, datastructure: DataStructure) -> None:
+        """Sets the `num_heads` attribute.
+
+        This is taken from the `multihead_size` attribute of the `datastructure`
+        if present. Otherwise, it is set to 1.
 
-        # Get details on continuous and categorical features
-        ignore_cols_for_training = self.datastructure.get_columns_ignored_for_training(
-            table_schema
+        Args:
+            datastructure: The data structure the model has been designed for.
+
+        Raises:
+            ValueError: If `datastructure.multihead_size` is not a positive integer.
+
+        """
+        self.num_heads = (
+            datastructure.multihead_size
+            if datastructure.multihead_size is not None
+            else 1
         )
 
-        if self.embed_categorical:
-            # If categorical embedding requested, calculate embedding sizes
-            logger.info("Generating Categorical Embeddings for categorical features.")
-            embedding_sizes = _calculate_embedding_sizes(
-                table_schema.get_categorical_feature_sizes(ignore_cols_for_training)
-            )
-            num_continuous = table_schema.get_num_continuous(
-                ignore_cols=ignore_cols_for_training
-            )
-            model = _PyTorchLogisticRegression(
-                num_classes=self.n_classes,
-                num_continuous=num_continuous,
-                embedding_sizes=embedding_sizes,
-                embedding_dropout_frac=self.embed_categorical_dropout,
-                bias=self.bias,
-            )
-        else:
-            # Treat categorical values as label encoded
-            logger.info(
-                "No categorical embedding requested;"
-                " categorical data should be label-encoded."
-            )
-            num_categorical = table_schema.get_num_categorical(
-                ignore_cols=ignore_cols_for_training
-            )
-            num_continuous = table_schema.get_num_continuous(
-                ignore_cols=ignore_cols_for_training
+    class _Schema(MarshmallowSchema):
+        layers = fields.List(fields.Int())
+        dropout_probs = fields.List(fields.Float())
+        mish_activation_function = fields.Bool()
+
+
+@dataclass
+class FeedForwardModelStructure(NeuralNetworkModelStructure):
+    """Dataclass defining the structure of a feedforward neural network model.
+
+    This model structure only defines linear and dropout layers.
+
+    Args:
+        embedding_dropout: Dropout probability for embedding layer. Defaults to 0.04.
+    """
+
+    #: same as argument
+    embedding_dropout: float = 0.04
+
+    class _Schema(NeuralNetworkModelStructure._Schema):
+        embedding_dropout = fields.Float()
+
+        @post_load
+        def recreate_model(
+            self, data: _JSONDict, **_kwargs: Any
+        ) -> FeedForwardModelStructure:
+            """Recreates model using Schema."""
+            return FeedForwardModelStructure(**data)
+
+
+@dataclass
+class CNNModelStructure(NeuralNetworkModelStructure):
+    """Dataclass defining the structure of a convolutional neural network model.
+
+    This model structure has multiple convolutional layers followed by multiple
+    linear and dropout layers.
+
+    Args:
+        kernel_size: Kernel size for convolutional layers. Defaults to 5.
+        padding: Padding for convolutional layers. Defaults to 2.
+        stride: Stride for convolutional layers. Defaults to 1.
+        pooling_function: Pooling function for convolutional layers. Options are "max"
+            or "avg". Defaults to "max".
+        ff_layers: List of linear hidden layer sizes following the convolutional layers.
+            Defaults to [1000, 500].
+        ff_dropout_probs: List of dropout probabilities for each linear layer. Must be
+            the same length as `ff_layers`. Defaults to 0.01 and 0.1 respectively.
+
+    Raises:
+        ValueError: If `ff_layers` and `ff_dropout_probs` are not the same length.
+        ValueError: If `pooling_function` is not "max" or "avg".
+    """
+
+    #: same as argument
+    kernel_size: int = 5
+    #: same as argument
+    padding: int = 2
+    #: same as argument
+    stride: int = 1
+    #: same as argument
+    pooling_function: str = "max"
+    #: same as argument
+    ff_layers: List[int] = field(default_factory=list)
+    #: same as argument
+    ff_dropout_probs: List[float] = field(default_factory=list)
+
+    def __post_init__(self) -> None:
+        if not isinstance(self.layers, list):
+            self.layers = [16, 32]
+        super().__post_init__()
+        if self.pooling_function not in POOLING_FUNCTIONS:
+            raise ValueError(
+                f"Pooling function can only be one of: {POOLING_FUNCTIONS}"
             )
-            input_dim = num_continuous + num_categorical
-            model = _PyTorchLogisticRegression(
-                num_classes=self.n_classes,
-                input_dim=input_dim,
-                bias=self.bias,
+        self.ff_layers = [1000, 500] if len(self.ff_layers) == 0 else self.ff_layers
+        self.ff_dropout_probs = (
+            [0.01, 0.1] if len(self.ff_dropout_probs) == 0 else self.ff_dropout_probs
+        )
+        if len(self.ff_layers) != len(self.ff_dropout_probs):
+            raise ValueError(
+                "Number of neural layers must equal number of dropout layers."
             )
 
-        return model
+    class _Schema(NeuralNetworkModelStructure._Schema):
 
-    def _split_dataloader_output(
-        self,
-        data: Union[
-            ImgAndTabDataSplit,
-            ImgXorTabDataSplit,
-        ],
-    ) -> Union[ImgDataReturnType, TabDataReturnType]:
-        """Splits dataloader output.
+        kernel_size = fields.Int()
+        padding = fields.Int()
+        stride = fields.Int()
+        pooling_function = fields.Str()
+        ff_layers = fields.List(fields.Int())
+        ff_dropout_probs = fields.List(fields.Float())
+
+        @post_load
+        def recreate_model(self, data: _JSONDict, **_kwargs: Any) -> CNNModelStructure:
+            """Recreates model using Schema."""
+            return CNNModelStructure(**data)
+
+
+@dataclass
+class NeuralNetworkPredefinedModel:
+    """This class encompasses what is required to use a predefined model e.g. ResNet.
+
+    The currently supported models are:
+        - AlexNet
+        - DenseNet{121,161,169,201}
+        - ResNet{18,34,50,101,152}
+        - SqueezeNet{1_0,1_1}
+        - VGG{11,11_bn,13,13_bn,16,16_bn,19,19_bn}
+        - TabNet
 
-        Splits it into pieces for categorical, continuous, weights and
-        (optionally) categories.
+    Args:
+        name: name of the model
+        pretrained: flag to denote whether to download the pretrained parameters
+        **kwargs: additional arguments to pass to the model constructor
+    """
 
-        Depending on whether embed_categorical is True or not will affect the shape
-        of the returned categorical data tensor, either to ensure it can pass through
-        the embedding layers correctly or to treat it as inherently label-encoded.
+    #: same as argument
+    name: str
+    #: same as argument
+    pretrained: bool = True
+    #: same as argument
+    kwargs: Optional[Dict[str, bool]] = None
+
+    class _Schema(MarshmallowSchema):
+        name = fields.Str()
+        pretrained = fields.Bool()
+        kwargs = fields.Dict(keys=fields.Str(), values=fields.Bool(), allow_none=True)
+
+        @post_load
+        def recreate_model(
+            self, data: _JSONDict, **_kwargs: Any
+        ) -> NeuralNetworkPredefinedModel:
+            """Recreates model using Schema."""
+            return NeuralNetworkPredefinedModel(**data)
+
+
+@dataclass
+class Optimizer:
+    """Class for specifying the optimizer for a neural network.
 
-        NB: `ignore_classes` is never returned
-        """
-        # Split into the tabular data and supplementary data
-        tab, sup = cast(ImgXorTabDataSplit, data)
+    The options for the optimizer will depend on the backend being used.
 
-        # Gather schema-related information
-        table_schema = self.datastructure.get_table_schema(
-            schema=self.schema, data_identifier=self._datastructure_identifier
-        )
-        ignore_cols_for_training = self.datastructure.get_columns_ignored_for_training(
-            table_schema
-        )
+    Args:
+        name: name of the optimizer
+        params: dictionary of keyword arguments for the optimizer constructor
+    """
 
-        # Calculate the number of continuous and categorical columns
-        n_categorical = table_schema.get_num_categorical(
-            ignore_cols=ignore_cols_for_training
-        )
-        n_continuous = table_schema.get_num_continuous(
-            ignore_cols=ignore_cols_for_training
-        )
+    #: same as argument
+    name: str
+    #: same as argument
+    params: Dict[str, Any] = field(default_factory=dict)
+
+    class _Schema(MarshmallowSchema):
+        name = fields.String()
+        params = fields.Dict()
+
+        @post_load
+        def recreate(self, data: _JSONDict, **_kwargs: Any) -> Optimizer:
+            """Recreate Optimizer instance from schema."""
+            return Optimizer(**data)
+
+
+@dataclass
+class Scheduler:
+    """Class for specifying the scheduler for a neural network.
 
-        # Split tensor into categorical and continuous items
-        end_categorical_idx = n_categorical
-        x_categorical = tab[:, :end_categorical_idx].long()  # categorical features
-        end_continuous_idx = end_categorical_idx + n_continuous
-        x_continuous = tab[
-            :, end_categorical_idx:end_continuous_idx
-        ].float()  # continuous features
-
-        # Weights will be first entry in the supplementary tensor
-        weights = sup[:, 0].float()
-
-        # If category is present (the remaining entries in the supplementary tensor),
-        # return it, otherwise return None
-        category: Optional[torch.Tensor]
-        if sup.shape[1] > 2:
-            category = sup[:, -1:].long()
-        else:
-            category = None
+    The options for the scheduler will depend on the backend being used.
 
-        if self.embed_categorical:
-            # Return (transposed categorical tensor, continuous tensor), weights
-            # tensor and category tensor (or None).
-            # Categorical must be transposed to allow it to pass through the
-            # embedding layers correctly.
-            return (x_categorical.t(), x_continuous), weights, category
-        else:
-            # Otherwise, return categorical as is, expecting it to be label-encoded.
-            # Return (categorical tensor, continuous tensor), weights tensor and
-            # category tensor (or None).
-            return (x_categorical, x_continuous), weights, category
+    Args:
+        name: name of the scheduler
+        params: dictionary of keyword arguments for the scheduler constructor
+    """
 
-    def _get_loss(
-        self,
-        output: torch.Tensor,
-        target: torch.Tensor,
-        loss_modifiers: Tuple[torch.Tensor, ...],
-    ) -> torch.Tensor:
-        """Computes the appropriate (weighted) loss.
+    #: same as argument
+    name: str
+    #: same as argument
+    params: Dict[str, Any] = field(default_factory=dict)
+
+    class _Schema(MarshmallowSchema):
+        name = fields.String()
+        params = fields.Dict()
+
+        @post_load
+        def recreate(self, data: _JSONDict, **_kwargs: Any) -> Scheduler:
+            """Recreate Scheduler instance from schema."""
+            return Scheduler(**data)
+
+
+@dataclass
+class EarlyStopping:
+    """Class for specifying early stopping parameters.
 
-        Applies L1 and L2 regularization if requested.
+    Args:
+        params: dictionary of keyword arguments for the early stopping constructor. The
+            parameters will depend on the backend being used.
+    """
 
-        Args:
-            output: The model output.
-            target: The expected output.
-            loss_modifiers: A tuple of tensors representing additional loss modifiers:
-                - 0: weightings for each sample in this batch
-                - 1: categories
+    #: same as argument
+    params: Dict[str, Any] = field(default_factory=dict)
 
-        Returns:
-            A scalar tensor representing the loss.
-        """
-        loss = super()._get_loss(output, target, loss_modifiers)
+    class _Schema(MarshmallowSchema):
+        params = fields.Dict()
 
-        sample_weight: torch.Tensor = loss_modifiers[0]
-        weight_sum = sample_weight.squeeze().sum()
+        @post_load
+        def recreate(self, data: _JSONDict, **_kwargs: Any) -> EarlyStopping:
+            """Recreate EarlyStopping instance from schema."""
+            return EarlyStopping(**data)
 
-        # Apply L1 regularisation if requested
-        if (
-            self.l1_regularization_weight is not None
-            and self.l1_regularization_weight > 0
-        ):
-            model: Module = cast(Module, self._model)
-            linear_layer: Module = cast(Module, model.linear)
-            weight_tensor: torch.Tensor = cast(torch.Tensor, linear_layer.weight)
-
-            l1_reg_term = weight_tensor.abs().sum()
-            # As loss is already a (weighted) average we need to similarly divide the
-            # regularisation term by the total weight.
-            l1_reg_term /= weight_sum
-            loss += self.l1_regularization_weight * l1_reg_term
-
-        # Apply L2 regularisation if requested
-        if (
-            self.l2_regularization_weight is not None
-            and self.l2_regularization_weight > 0
-        ):
-            model = cast(Module, self._model)
-            linear_layer = cast(Module, model.linear)
-            weight_tensor = cast(torch.Tensor, linear_layer.weight)
 
-            l2_reg_term = weight_tensor.pow(2).sum()
-            # As loss is already a (weighted) average we need to similarly divide the
-            # regularisation term by the total weight.
-            l2_reg_term /= weight_sum
-            loss += self.l2_regularization_weight * l2_reg_term
+class NeuralNetworkMixIn:
+    """Specifies model structure and hyperparameters for neural network models.
+
+    All neural network models must inherit from this class.
+
+    Args:
+        model_structure: The structure of the model.
+        batch_size: The number of data points in each batch. Defaults to 512.
+        epochs: The number of epochs to train for. If `steps` is provided, `epochs`
+            cannot be provided. Defaults to None.
+        steps: The number of steps to train for. If `epochs` is provided, `steps`
+            cannot be provided. Defaults to None.
+        optimizer: The optimizer to use. Defaults to "AdamW" with a learning rate of
+            0.01.
+        scheduler: The scheduler to use. Defaults to None.
+        custom_loss_func: A custom loss function to use. Defaults to None.
+        early_stopping: Early stopping parameters. Defaults to None.
+
+    Attributes:
+        model_structure: The structure of the model.
+        batch_size: The number of data points in each batch.
+        epochs: The number of epochs to train for.
+        steps: The number of steps to train for.
+        optimizer: The optimizer to use.
+        scheduler: The scheduler to use.
+        loss_func: A custom loss function to use.
+        early_stopping: Early stopping parameters.
 
-        return loss
+    Raises:
+        ValueError: If both `epochs` and `steps` are provided.
 
+    :::caution
 
-@delegates()
-class TabNetClassifier(PyTorchClassifierMixIn, BaseTabNetModel):
-    """TabNet Classifier for binary and multiclass tabular classification problems.
+    `custom_loss_func` cannot be serialized and therefore cannot be used in Federated
+    Learning.
 
-    See base class for more information.
+    :::
     """
 
-    train_dl: PyTorchBitfountDataLoader
+    #: set in _BaseModel
+    datastructure: DataStructure
 
-    def __init__(self, **kwargs: Any) -> None:
+    def __init__(
+        self,
+        model_structure: Union[
+            NeuralNetworkModelStructure, NeuralNetworkPredefinedModel
+        ],
+        batch_size: int = 512,
+        epochs: Optional[int] = None,
+        steps: Optional[int] = None,
+        optimizer: Optional[Optimizer] = None,
+        scheduler: Optional[Scheduler] = None,
+        custom_loss_func: Optional[Callable] = None,
+        early_stopping: Optional[EarlyStopping] = None,
+        **kwargs: Any,
+    ):
         super().__init__(**kwargs)
 
-    def _create_model(self) -> TabNetClassifier_:
-        """Create model for Binary or Multiclass classification."""
-        table_schema = self.datastructure.get_table_schema(
-            schema=self.schema, data_identifier=self._datastructure_identifier
-        )
-        ignore_cols_for_training = self.datastructure.get_columns_ignored_for_training(
-            table_schema
-        )
+        if (steps is None and epochs is None) or (
+            isinstance(steps, int) and isinstance(epochs, int)
+        ):
+            raise ValueError("You must specify one (and only one) of steps or epochs.")
 
-        self.set_number_of_classes(table_schema)
+        if optimizer is None:
+            logger.info("No optimizer provided, using AdamW with learning rate 1e-2.")
+            self.optimizer = Optimizer("AdamW", {"lr": 0.01})
+        elif isinstance(optimizer, Optimizer):
+            self.optimizer = optimizer
+
+        self.model_structure = model_structure
+        self.epochs = epochs
+        self.steps = steps
+        self.batch_size = batch_size
+        self.scheduler = scheduler
+        self._opt_func: Optional[Callable] = None  # To be set by initialise_model()
+        self._scheduler_func: Optional[
+            Callable
+        ] = None  # To be set by initialise_model()
+        self.loss_func: Optional[Callable] = custom_loss_func  # Only works locally
+        self.early_stopping = early_stopping
 
-        # Only consider the tabular part
-        X: pd.DataFrame
-        x_dataframe = self.train_dl.get_x_dataframe()
-        if isinstance(x_dataframe, tuple):
-            X, _ = x_dataframe
-        elif isinstance(x_dataframe, pd.DataFrame):
-            X = x_dataframe
-        cat_idxs = [
-            i
-            for i, f in enumerate(X.columns)
-            if f
-            in table_schema.get_feature_names(
-                SemanticType.CATEGORICAL,
-            )
-            if f not in ignore_cols_for_training
-        ]
+        if isinstance(self.model_structure, NeuralNetworkModelStructure):
+            self.model_structure.set_num_heads(self.datastructure)
 
-        if self.embedding_sizes is None:
-            embedding_sizes = _calculate_embedding_sizes(
-                table_schema.get_categorical_feature_sizes(ignore_cols_for_training)
-            )
-            self.embedding_sizes = cast(List[int], [i[1] for i in embedding_sizes])
+    @staticmethod
+    @abstractmethod
+    def _get_optimizer(optimizer: Optimizer) -> Callable:
+        """Returns appropriate optimizer class."""
+        raise NotImplementedError
 
-        return TabNetClassifier_(
-            cat_idxs=cat_idxs,
-            cat_dims=table_schema.get_categorical_feature_sizes(
-                ignore_cols=ignore_cols_for_training
-            ),
-            cat_emb_dim=self.embedding_sizes,
-            optimizer_fn=_OPTIMIZERS[self.optimizer.name],
-            optimizer_params=self.optimizer.params,
-            scheduler_params=cast(Scheduler, self.scheduler).params,
-            scheduler_fn=_SCHEDULERS[cast(Scheduler, self.scheduler).name],
-            mask_type=self.mask_type,
-            seed=self.seed or DEFAULT_SEED,
-        )
+    @staticmethod
+    @abstractmethod
+    def _get_scheduler(scheduler: Scheduler) -> Callable:
+        """Returns appropriate scheduler class."""
+        raise NotImplementedError
+
+    @property
+    def training_needed(self) -> bool:
+        """Dictates whether the model needs training.
+
+        Returns:
+            bool: True if one of either epochs or steps are > 0.
+        """
+        return sum(filter(None, (self.epochs, self.steps))) > 0
+
+    class _Schema(MarshmallowSchema):
+        model_structure = fields.Method(
+            serialize="_nn_model_structure_serialize",
+            deserialize="_nn_model_structure_deserialize",
+        )
+        epochs = fields.Integer(allow_none=True)
+        steps = fields.Integer(allow_none=True)
+        batch_size = fields.Integer(allow_none=True)
+        optimizer = fields.Nested(Optimizer._Schema, allow_none=True)
+        scheduler = fields.Nested(Scheduler._Schema, allow_none=True)
+        early_stopping = fields.Nested(EarlyStopping._Schema, allow_none=True)
+        custom_loss_func = fields.Raw(allow_none=True)
+
+        def _nn_model_structure_serialize(
+            self, base_object: NeuralNetworkMixIn
+        ) -> _JSONDict:
+            """Custom serialization of Neural Network structure.
+
+            Required for the NeuralNetworkMixIn to know which Schema to use
+            for serialization.
+            """
+            class_to_schema = {
+                "NeuralNetworkPredefinedModel": NeuralNetworkPredefinedModel._Schema,
+                "CNNModelStructure": CNNModelStructure._Schema,
+                "FeedForwardModelStructure": FeedForwardModelStructure._Schema,
+            }
+            try:
+                schema: MarshmallowSchema = class_to_schema[
+                    base_object.model_structure.__class__.__name__
+                ]()
+                return cast(_JSONDict, schema.dump(base_object.model_structure))
+            except KeyError as e:
+                raise KeyError(f"{e}. Schema Object not recognised")
+
+        def _nn_model_structure_deserialize(
+            self, object_dict: _JSONDict
+        ) -> Union[
+            CNNModelStructure, FeedForwardModelStructure, NeuralNetworkPredefinedModel
+        ]:
+            """Custom deserialization of Neural Network structure.
+
+            Required for the NeuralNetworkMixIn to know which Schema to use
+            for deserialization.
+            """
+            if object_dict.get("kernel_size"):
+                cnn_model_structure: CNNModelStructure = (
+                    CNNModelStructure._Schema().load(object_dict)
+                )
+                return cnn_model_structure
+            elif object_dict.get("embedding_dropout"):
+                ff_model_structure: FeedForwardModelStructure = (
+                    FeedForwardModelStructure._Schema().load(object_dict)
+                )
+                return ff_model_structure
+            else:
+                predefined_model: NeuralNetworkPredefinedModel = (
+                    NeuralNetworkPredefinedModel._Schema().load(object_dict)
+                )
+                return predefined_model
```

### Comparing `bitfount-0.5.86/bitfount/backends/pytorch/models/torch_functions/mish.py` & `bitfount-0.5.9/bitfount/backends/pytorch/models/torch_functions/mish.py`

 * *Files identical despite different names*

### Comparing `bitfount-0.5.86/bitfount/backends/pytorch/types.py` & `bitfount-0.5.9/bitfount/backends/pytorch/types.py`

 * *Files 4% similar despite different names*

```diff
@@ -22,15 +22,15 @@
         return _AdaptorForPyTorchTensor(self.torchtensor * other)
 
     def __sub__(self, other: Any) -> _AdaptorForPyTorchTensor:
         return _AdaptorForPyTorchTensor(self.torchtensor - other)
 
     def squeeze(self, axis: Optional[Any] = None) -> _AdaptorForPyTorchTensor:
         """Returns a tensor with all the dimensions of input of size 1 removed."""
-        if axis is not None:
+        if axis:
             return _AdaptorForPyTorchTensor(torch.squeeze(self.torchtensor, dim=axis))
         else:
             return _AdaptorForPyTorchTensor(torch.squeeze(self.torchtensor))
 
 
 # Pytorch Types
```

### Comparing `bitfount-0.5.86/bitfount/data/databunch.py` & `bitfount-0.5.9/bitfount/data/databunch.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,159 +1,186 @@
 """Classes concerning databunches."""
 from __future__ import annotations
 
 import logging
-from typing import TYPE_CHECKING, Optional
+from typing import Any, Literal, Optional
 
-from bitfount.data.datafactory import _DataFactory, _get_default_data_factory
-from bitfount.data.types import DataSplit, SemanticType
+from bitfount.data.datafactory import _DataFactory, _load_default_data_factory
+from bitfount.data.dataloader import _BitfountDataLoader
+from bitfount.data.datasets import _BaseDataset, _Dataset
+from bitfount.data.datasource import DataSource
+from bitfount.data.datastructure import DataStructure
+from bitfount.data.schema import BitfountSchema
+from bitfount.data.types import SemanticType
+from bitfount.types import _DataFrameType
 from bitfount.utils import _add_this_to_list
 
-if TYPE_CHECKING:
-    from bitfount.data.dataloaders import BitfountDataLoader
-    from bitfount.data.datasets import _BaseBitfountDataset
-    from bitfount.data.datasources.base_source import BaseSource
-    from bitfount.data.datastructure import DataStructure
-    from bitfount.data.schema import TableSchema
-
 logger = logging.getLogger(__name__)
 
 
-class BitfountDataBunch:
-    """Wrapper for train, validation and test dataloaders.
+class _BitfountDataBunch:
+    """Wrapper for train, validation and test data.
 
-    Provides methods to access dataloaders for training and evaluation. This is strictly
-    a model concept and is not necessary for algorithms that do not have models.
+    Provides methods to access dataloaders for training.
 
     Args:
         data_structure: A `DataStructure` object.
-        schema: A `TableSchema` object.
-        datasource: A `BaseSource` object.
-        data_factory: A `_DataFactory` instance for creating datasets and dataloaders.
-            Defaults to None.
+        datasource: A `DataSource` object.
+        data_factory: A `_DataFactory` instance for creating
+            datasets and dataloaders. Defaults to None.
+         **kwargs: additional arguments to pass to the `_DataBunch`.
     """
 
     def __init__(
         self,
         data_structure: DataStructure,
-        schema: TableSchema,
-        datasource: BaseSource,
+        schema: BitfountSchema,
+        datasource: DataSource,
         data_factory: Optional[_DataFactory] = None,
+        **kwargs: Any,
     ):
         self.data_structure = data_structure
-        self.schema = schema
         self.datasource = datasource
-        self.data_factory = (
-            data_factory if data_factory is not None else _get_default_data_factory()
-        )
-
-        # Column attributes
-        self.target = self.data_structure.target
-        self.loss_weights_col = self.data_structure.loss_weights_col
-        self.multihead_col = self.data_structure.multihead_col
-        self.ignore_classes_col = self.data_structure.ignore_classes_col
+        self.schema = schema
+        self.target = data_structure.target
+        self.loss_weights_col = data_structure.loss_weights_col
+        self.multihead_col = data_structure.multihead_col
+        self.ignore_classes_col = data_structure.ignore_classes_col
 
         # Placeholders for generated datasets
-        self.train_ds: _BaseBitfountDataset  # training data is not optional
-        self.validation_ds: Optional[_BaseBitfountDataset] = None
-        self.test_ds: Optional[_BaseBitfountDataset] = None
-
-        self.data_structure.set_training_column_split_by_semantic_type(self.schema)
-        self._disallow_text_features()
-
-        # TODO: [BIT-1167] This probably needs to be removed once we have implemented
-        # dataset transformations. Currently, this call does nothing.
-        self.datasource = self.data_structure.apply_dataset_transformations(
-            self.datasource
-        )
-        self._load_data()
-        self._create_datasets()
-
-    def _load_data(self) -> None:
-        """Loads the data from the datasource and applies dataset transformations."""
-        kwargs = {}
-        if isinstance(self.data_structure.query, str):
-            kwargs["sql_query"] = self.data_structure.query
-
-        if isinstance(self.data_structure.table, str):
-            kwargs["table_name"] = self.data_structure.table
-
-        # In the federated setting, the data should already have been loaded by the
-        # Worker. So this call is effectively just for the local setting. The call is
-        # idempotent so it doesn't matter if the data is already loaded.
-        self.datasource.load_data(**kwargs)
+        self.train_ds: _BaseDataset
+        self.validation_ds: Optional[_BaseDataset] = None
+        self.test_ds: Optional[_BaseDataset] = None
+
+        # Make sure that the appropriate columns are selected from schema
+        if (
+            not self.data_structure.selected_cols
+            and not self.data_structure.ignore_cols
+        ):
+            self.data_structure.selected_cols = [
+                col
+                for col in self.schema.feature_names()
+                if col not in schema.feature_names(SemanticType.TEXT)
+            ]
+        elif not self.data_structure.selected_cols:
+            self.data_structure.selected_cols = [
+                col
+                for col in self.schema.feature_names()
+                if col not in self.data_structure.ignore_cols
+                and col not in schema.feature_names(SemanticType.TEXT)
+            ]
+        elif not self.data_structure.ignore_cols:
+            self.data_structure.ignore_cols = [
+                col
+                for col in self.schema.feature_names()
+                if col not in self.data_structure.selected_cols
+            ]
 
-    def _disallow_text_features(self) -> None:
-        """Removes columns with semantic type TEXT from the data structure."""
         disallowed_columns = []
         for col in self.data_structure.selected_cols:
-            if col in self.schema.get_feature_names(SemanticType.TEXT):
+            if col in schema.feature_names(SemanticType.TEXT):
                 disallowed_columns.append(col)
                 logger.warning(
                     f"DataStructure has selected the text column {col} "
                     f"which is not supported. Removing this from the selection."
                 )
         self.data_structure.ignore_cols = _add_this_to_list(
             disallowed_columns, self.data_structure.ignore_cols
         )
         self.data_structure.selected_cols = [
             i for i in self.data_structure.selected_cols if i not in disallowed_columns
         ]
+        # Make sure that the appropriate columns are selected from schema
+        self.data_structure.set_training_column_split_by_semantic_type(schema)
+        self.ignore_cols = self.data_structure.ignore_cols[:]
+        self.ignore_cols = _add_this_to_list(self.target, self.ignore_cols)
+        self.ignore_cols = _add_this_to_list(self.loss_weights_col, self.ignore_cols)
+        self.ignore_cols = _add_this_to_list(self.ignore_classes_col, self.ignore_cols)
+
+        # In future we may want to allow different choices of features from the schema
+        self.categorical_features = [
+            feature
+            for feature in self.schema.feature_names(SemanticType.CATEGORICAL)
+            if feature not in self.ignore_cols
+        ]
+        self.continuous_features = [
+            feature
+            for feature in self.schema.feature_names(SemanticType.CONTINUOUS)
+            if feature not in self.ignore_cols
+        ]
+        if data_factory is None:
+            data_factory = _load_default_data_factory()
+        self.data_factory = data_factory
+
+        # Datasource will always be provided with the new refactor.
+        self.create_datasets()
 
     def _data_to_dataset(
-        self,
-        data_split: DataSplit,
-    ) -> _BaseBitfountDataset:
+        self, data: _DataFrameType, step: Literal["train", "validation"]
+    ) -> _Dataset:
         """Converts pandas dataframe to relevant BitfountDataset."""
-        return self.data_factory.create_dataset(
-            datasource=self.datasource,
-            data_splitter=self.data_structure.data_splitter,
-            target=self.target,
-            schema=self.schema,
-            selected_cols_semantic_types=self.data_structure.selected_cols_w_types,
-            selected_cols=self.data_structure.selected_cols,
-            batch_transforms=self.data_structure.get_batch_transformations(),
-            data_split=data_split,
+        rel_cols = self.data_structure.selected_cols
+        # `rel_cols` needs to be passed to the `apply` method here to ensure that we
+        # don't end up removing the extra columns in our dataframe that are used during
+        # training (e.g. loss_weights_col, etc.) but aren't part of the schema
+        data = self.schema.apply(data, keep_cols=rel_cols)
+        # Applying the schema adds extra columns to the dataframe if they are missing.
+        # Therefore we need to subset the data columns here to ensure we are only using
+        # the columns specified for this task
+        data = data[rel_cols].reset_index(drop=True)
+        extra_cols = dict(
             weights_col=self.loss_weights_col,
             multihead_col=self.multihead_col,
             ignore_classes_col=self.ignore_classes_col,
-            auto_convert_grayscale_images=self.data_structure.auto_convert_grayscale_images,
         )
 
-    def _create_datasets(self) -> None:
+        transforms = self.data_structure.get_batch_transformations()
+
+        return self.data_factory.create_dataset(
+            data=data,
+            target=self.target,
+            selected_cols=self.data_structure.selected_cols_w_types,
+            batch_transforms=transforms,
+            batch_transformation_step=step,
+            **extra_cols,
+        )
+
+    def create_datasets(self) -> None:
         """Creates datasets for dataloaders.
 
-        Sets `self.train_ds`, `self.validation_ds` and `self.test_ds`.
+        Sets `self.train_ds`, `self.validation_ds` and `self.test_ds`
         """
-        self.train_ds = self._data_to_dataset(DataSplit.TRAIN)
-        self.validation_ds = self._data_to_dataset(DataSplit.VALIDATION)
-        self.test_ds = self._data_to_dataset(DataSplit.TEST)
+        self.train_ds = self._data_to_dataset(self.datasource.train_set, "train")
+        self.validation_ds = self._data_to_dataset(
+            self.datasource.validation_set, "validation"
+        )
+        self.test_ds = self._data_to_dataset(self.datasource.test_set, "validation")
 
     def get_train_dataloader(
         self, batch_size: Optional[int] = None
-    ) -> BitfountDataLoader:
+    ) -> _BitfountDataLoader:
         """Gets the relevant data loader for training data."""
         return self.data_factory.create_dataloader(self.train_ds, batch_size=batch_size)
 
     def get_validation_dataloader(
         self, batch_size: Optional[int] = None
-    ) -> Optional[BitfountDataLoader]:
+    ) -> Optional[_BitfountDataLoader]:
         """Gets the relevant data loader for validation data."""
         if not self.validation_ds:
             logging.warning(
                 "No validation data in the dataset. Validation DataLoader is 'None'."
             )
             return None
 
         return self.data_factory.create_dataloader(
             self.validation_ds, batch_size=batch_size
         )
 
     def get_test_dataloader(
         self, batch_size: Optional[int] = None
-    ) -> Optional[BitfountDataLoader]:
+    ) -> Optional[_BitfountDataLoader]:
         """Gets the relevant data loader for test data."""
         if not self.test_ds:
             logging.warning("No test data in the dataset. Test DataLoader is 'None'.")
             return None
 
         return self.data_factory.create_dataloader(self.test_ds, batch_size=batch_size)
```

### Comparing `bitfount-0.5.86/bitfount/data/dataloaders.py` & `bitfount-0.5.9/bitfount/data/dataloader.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,85 +1,106 @@
 """Classes concerning data loading and dataloaders."""
 from __future__ import annotations
 
 import math
-from typing import TYPE_CHECKING, Any, Iterator, List, Optional, Tuple, Union
+from typing import TYPE_CHECKING, Iterator, Optional, Tuple, Union
 
-import pandas as pd
+import numpy as np
 
-if TYPE_CHECKING:
-    from bitfount.data.datasets import _BaseBitfountDataset
-    from bitfount.data.types import _SingleOrMulti
+from bitfount.data.types import _SingleOrMulti
+from bitfount.types import _DataFrameType
+from bitfount.utils import _get_df_library
 
+if TYPE_CHECKING:
+    from bitfount.data.datasets import _BaseDataset
 
-class BitfountDataLoader:
-    """An backend-agnostic data loader.
 
-    Args:
-        dataset: The dataset for the dataloader.
-        batch_size: The batch size for the dataloader.
-            Defaults to None.
-    """
+class _BitfountDataLoader:
+    """An agnostic data loader for models that load data batch by batch."""
 
-    def __init__(self, dataset: _BaseBitfountDataset, batch_size: Optional[int] = None):
+    def __init__(self, dataset: _BaseDataset, batch_size: Optional[int] = None):
         self.dataset = dataset
         self.batch_size = batch_size
 
+    def __iter__(self) -> Iterator[_SingleOrMulti[_SingleOrMulti[np.ndarray]]]:
+        """Iterates through the dataset, returning (X,Y) pairs.
+
+        A naive batch-based iteration over numpy representations of the x- and
+        y- dataframes.
+
+        If no batch size is provided, uses a batch size of 1.
+        """
+        batch_size = self.batch_size if self.batch_size else 1
+        num_batches = math.ceil(len(self.dataset) / batch_size)
+
+        x_dataframe = self.get_x_dataframe()
+        x_data: Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]
+        if isinstance(x_dataframe, tuple):
+            tab, img = x_dataframe
+            x_data = (tab.to_numpy(), img.to_numpy())
+        else:
+            x_data = x_dataframe.to_numpy()
+        y_data: np.ndarray = self.get_y_dataframe().to_numpy()
+
+        for i in range(num_batches):
+            x_batch: Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]
+            if isinstance(x_data, tuple):
+                tab, img = x_data
+                x_batch = (
+                    tab[i * batch_size : (i + 1) * batch_size],
+                    img[i * batch_size : (i + 1) * batch_size],
+                )
+            else:
+                x_batch = x_data[i * batch_size : (i + 1) * batch_size]
+            y_batch: np.ndarray = y_data[i * batch_size : (i + 1) * batch_size]
+            yield x_batch, y_batch
+
     def __len__(self) -> int:
         """Number of batches or number of elements if batch size is None."""
         if not self.batch_size:
             return len(self.dataset)
         return math.ceil(len(self.dataset) / self.batch_size)
 
-    def __iter__(self) -> Iterator[List[_SingleOrMulti[Any]]]:
-        """This should be implemented to allow batch by batch loading.
-
-        Currently there are no backend-agnostic models that can operate on iterable
-        datasets so it has not been implemented.
-
-        Returns:
-            An iterator over batches of x and y numpy arrays.
-        """
-        raise NotImplementedError
-
     def get_x_dataframe(
         self,
-    ) -> Union[pd.DataFrame, Tuple[pd.DataFrame, pd.DataFrame]]:
+    ) -> Union[_DataFrameType, Tuple[_DataFrameType, _DataFrameType]]:
         """Gets the x-dataframe of the data i.e. features.
 
         For models incompatible with the __iter__ approach.
         """
         tabular, image, support_cols = self.dataset.x_var
+        bf = _get_df_library(self.dataset.data)
         if tabular.size and image.size:
             tab_cols = [
                 col
                 for col in self.dataset.x_columns
                 if col not in self.dataset.image_columns
             ]
-            tab_df = pd.DataFrame(data=tabular, columns=tab_cols)
+            tab_df = bf.DataFrame(data=tabular, columns=tab_cols)
             tab_df[self.dataset.embedded_col_names] = tab_df[
                 self.dataset.embedded_col_names
             ].astype("int64")
-            img_df = pd.DataFrame(data=image, columns=self.dataset.image_columns)
+            img_df = bf.DataFrame(data=image, columns=self.dataset.image_columns)
             return tab_df, img_df
         elif image.size:
-            img_df = pd.DataFrame(data=image, columns=self.dataset.image_columns)
+            img_df = bf.DataFrame(data=image, columns=self.dataset.image_columns)
             return img_df
         elif tabular.size:
             columns = self.dataset.x_columns
-            tab_df = pd.DataFrame(data=tabular, columns=columns)
+            tab_df = bf.DataFrame(data=tabular, columns=columns)
             tab_df[self.dataset.embedded_col_names] = tab_df[
                 self.dataset.embedded_col_names
             ].astype("int64")
             return tab_df
         else:
             raise ValueError("No tabular or image data to train with.")
 
-    def get_y_dataframe(self) -> pd.DataFrame:
+    def get_y_dataframe(self) -> _DataFrameType:
         """Gets the y-dataframe of the data i.e. target.
 
         For models incompatible with the __iter__ approach.
         """
         columns = self.dataset.y_columns
         data = self.dataset.y_var
-        dataframe = pd.DataFrame(data=data, columns=columns)
+        bf = _get_df_library(self.dataset.data)
+        dataframe = bf.DataFrame(data=data, columns=columns)
         return dataframe
```

### Comparing `bitfount-0.5.86/bitfount/data/datasources/base_source.py` & `bitfount-0.5.9/bitfount/models/models.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,710 +1,678 @@
-"""Module containing BaseSource class.
+"""Backend-agnostic models that have no extra requirements.
 
-BaseSource is the abstract data source class from which all concrete data sources
-must inherit.
+:::info
+
+Models defined here cannot be trained in a federated manner.
+
+:::
 """
+
 from __future__ import annotations
 
-from abc import ABC, abstractmethod
-import inspect
+from dataclasses import dataclass
+from enum import Enum, auto
 import logging
 import os
-from pathlib import Path
-import shutil
-from typing import (
-    Any,
-    Callable,
-    Dict,
-    Iterable,
-    Iterator,
-    List,
-    Optional,
-    Sequence,
-    Set,
-    TypeVar,
-    Union,
-    cast,
-)
 
-from mypy_extensions import Arg, DefaultNamedArg, KwArg, VarArg
+# TODO: [BIT-987] Review use of pickle
+import pickle  # nosec
+from typing import Any, List, Literal, Mapping, Optional, Tuple, Type, Union, cast
+
+from marshmallow import fields, post_load
 import numpy as np
 import pandas as pd
-from pandas.core.dtypes.common import is_datetime64_any_dtype as is_datetime
-
-from bitfount.config import BITFOUNT_CACHE_DIR
-from bitfount.data.datasources.utils import _modify_column, _modify_file_paths
-from bitfount.data.datasplitters import DatasetSplitter
-from bitfount.data.exceptions import DataNotLoadedError
-from bitfount.data.types import (
-    DataPathModifiers,
-    _Column,
-    _GetColumnCallable,
-    _GetDtypesCallable,
-    _SingleOrMulti,
-)
-from bitfount.data.utils import _generate_dtypes_hash, _hash_str
-from bitfount.types import _Dtypes
-from bitfount.utils import delegates, seed_all
+from pandas import DataFrame
+from sklearn.linear_model import LinearRegression as sklearnLinearRegression
+from sklearn.linear_model import LogisticRegression as sklearnLogisticRegression
+from sklearn.neighbors import KNeighborsClassifier
+import statsmodels.api as sm
+
+from bitfount.data.dataloader import _BitfountDataLoader
+from bitfount.data.datasource import DataSource
+from bitfount.metrics import Metric
+from bitfount.models.base_models import ClassifierMixIn, RegressorMixIn, _BaseModel
+from bitfount.types import _DataFrameType, _JSONDict
 
 logger = logging.getLogger(__name__)
 
 
-T = TypeVar("T", bound="BaseSource")
-BaseSourceInitSignature = Callable[
-    [
-        Arg(T, "self"),  # noqa: F821
-        VarArg(Any),
-        DefaultNamedArg(Optional[DatasetSplitter], "data_splitter"),  # noqa: F821
-        DefaultNamedArg(Optional[int], "seed"),  # noqa: F821
-        DefaultNamedArg(
-            Optional[Dict[str, DataPathModifiers]], "modifiers"  # noqa: F821
-        ),
-        DefaultNamedArg(Union[str, Sequence[str], None], "ignore_cols"),  # noqa: F821
-        KwArg(Any),
-    ],
-    None,
-]
+class LogisticRegressionClassifier(ClassifierMixIn, _BaseModel):
+    """Wrapper around `sklearn.linear_model.LogisticRegression` model.
 
-
-class BaseSource(ABC):
-    """Abstract Base Source from which all other data sources must inherit.
+    For more details on the parameters, go to the scikit-learn documentation.
 
     Args:
-        data_splitter: Approach used for splitting the data into training, test,
-            validation. Defaults to None.
-        seed: Random number seed. Used for setting random seed for all libraries.
-            Defaults to None.
-        modifiers: Dictionary used for modifying paths/ extensions in the dataframe.
-            Defaults to None.
-        ignore_cols: Column/list of columns to be ignored from the data.
-            Defaults to None.
+        inverse_regularisation: Inverse regularisation parameter. Defaults to 0.0001.
+        max_steps: Maximum number of steps to take. Defaults to 10000.
+        model_type: Type of solver to use. Defaults to "lbfgs".
+        penalty: Penalty to use. Defaults to "l2".
+        early_stopping_tolerance: Tolerance for early stopping. Defaults to 1e-05.
+        verbose: Verbosity level. Defaults to 0.
 
     Attributes:
-        data: A Dataframe-type object which contains the data.
-        data_splitter: Approach used for splitting the data into training, test,
-            validation.
-        seed: Random number seed. Used for setting random seed for all libraries.
+        inverse_regularisation: Inverse regularisation parameter.
+        max_steps: Maximum number of steps to take.
+        model_type: Type of solver to use.
+        penalty: Penalty to use.
+        early_stopping_tolerance: Tolerance for early stopping.
+        verbose: Verbosity level.
     """
 
     def __init__(
         self,
-        data_splitter: Optional[DatasetSplitter] = None,
-        seed: Optional[int] = None,
-        modifiers: Optional[Dict[str, DataPathModifiers]] = None,
-        ignore_cols: Optional[Union[str, Sequence[str]]] = None,
+        inverse_regularisation: Optional[float] = None,
+        max_steps: Optional[int] = None,
+        model_type: Optional[str] = None,
+        penalty: Optional[str] = None,
+        early_stopping_tolerance: Optional[float] = None,
+        verbose: Optional[int] = None,
         **kwargs: Any,
-    ) -> None:
-        self._base_source_init = True
-        self.data_splitter = data_splitter
-        self.seed = seed
-        self._modifiers = modifiers
-        self._data_is_split: bool = False
-        self._data_is_loaded: bool = False
-        seed_all(self.seed)
-
-        self._train_idxs: Optional[np.ndarray] = None
-        self._validation_idxs: Optional[np.ndarray] = None
-        self._test_idxs: Optional[np.ndarray] = None
-
-        self._data: pd.DataFrame
-        self._table_hashes: Set[str] = set()
-
-        self._ignore_cols: List[str] = []
-        if isinstance(ignore_cols, str):
-            self._ignore_cols = [ignore_cols]
-        elif ignore_cols is not None:
-            self._ignore_cols = list(ignore_cols)
-
-        for unexpected_kwarg in kwargs:
-            logger.warning(f"Ignoring unexpected keyword argument {unexpected_kwarg}")
-
-        super().__init__()
-
-    def __init_subclass__(cls, **kwargs: Any) -> None:
-        if not (inspect.isabstract(cls) or ABC in cls.__bases__):
-            cls.get_dtypes = cls._get_dtypes()  # type: ignore[method-assign] # reason: wrap subclass get_dtypes # noqa: B950
-            cls.get_column = cls._get_column()  # type: ignore[method-assign] # reason: wrap subclass get_column # noqa: B950
-
-    @classmethod
-    def _get_dtypes(cls) -> _GetDtypesCallable:
-        """Decorate subclass' get_dtypes implementation.
-
-        Decorate subclass' implementation of get_dtypes to handle ignored
-        columns and handle `_table_hashes`.
-        """
-        subclass_get_dtypes = cls.get_dtypes
-
-        def get_dtypes(self: BaseSource, *args: Any, **kwargs: Any) -> _Dtypes:
-            dtypes: _Dtypes = subclass_get_dtypes(self, *args, **kwargs)
-            if self._ignore_cols:
-                for col in self._ignore_cols:
-                    if col in dtypes:
-                        del dtypes[col]
-            self._table_hashes.add(_generate_dtypes_hash(dtypes))
-            return dtypes
-
-        return get_dtypes
+    ):
+        super().__init__(**kwargs)
 
-    @classmethod
-    def _get_column(
-        cls,
-    ) -> _GetColumnCallable:
-        """Decorate subclass' get_column implementation.
-
-        Decorate subclass' implementation of get_column to handle ignored
-        columns and modifiers.
-        """
-        subclass_get_column = cls.get_column
-
-        def get_column(
-            self: BaseSource, col_name: str, *args: Any, **kwargs: Any
-        ) -> _Column:
-            column = subclass_get_column(self, col_name, *args, **kwargs)
-            if self._modifiers:
-                if modifier_dict := self._modifiers.get(col_name):
-                    column = _modify_column(column, modifier_dict)
-            return column
+        self.inverse_regularisation = (
+            0.0001 if inverse_regularisation is None else inverse_regularisation
+        )
+        self.max_steps = 10000 if max_steps is None else max_steps
+        self.model_type = "lbfgs" if model_type is None else model_type
+        self.penalty = "l2" if penalty is None else penalty
+        self.early_stopping_tolerance = (
+            1e-05 if early_stopping_tolerance is None else early_stopping_tolerance
+        )
+        self.verbose = 0 if verbose is None else verbose
 
-        return get_column
+    def serialize(self, filename: Union[str, os.PathLike]) -> None:
+        """Serialize model to file with provided `filename`.
 
-    @property
-    def is_initialised(self) -> bool:
-        """Checks if `BaseSource` was initialised."""
-        if hasattr(self, "_base_source_init"):
-            return True
-        else:
-            return False
-
-    @property
-    def data(self) -> pd.DataFrame:
-        """A property containing the underlying dataframe if the data has been loaded.
-
-        Raises:
-            DataNotLoadedError: If the data has not been loaded yet.
+        Args:
+            filename: Path to file to save serialized model.
         """
-        if self._data_is_loaded:
-            return self._data
-        else:
-            raise DataNotLoadedError(
-                "Data is not loaded yet. Please call `load_data` first."
-            )
-
-    @data.setter
-    def data(self, _data: Optional[pd.DataFrame]) -> None:
-        """Data setter."""
-        if _data is not None:
-            if isinstance(_data, pd.DataFrame):
-                if self._ignore_cols:
-                    # If columns already ignored in data, ignore errors.
-                    _data = _data.drop(columns=self._ignore_cols, errors="ignore")
-                self._data = _data
+        with open(filename, "wb") as f:
+            pickle.dump(self._model, f, protocol=pickle.HIGHEST_PROTOCOL)
 
-                if self._modifiers:
-                    _modify_file_paths(self._data, self._modifiers)
+    def deserialize(self, filename: Union[str, os.PathLike]) -> None:
+        """Deserialize model.
 
-                self._data_is_loaded = True
-            else:
-                raise TypeError(
-                    "Invalid data attribute. "
-                    "Expected pandas dataframe for attribute 'data' "
-                    f"but received :{type(_data)}"
-                )
+        Args:
+            filename: Path to file containing serialized model.
 
-    @property
-    def hash(self) -> str:
-        """The hash associated with this BaseSource.
+        :::danger
 
-        This is the hash of the static information regarding the underlying DataFrame,
-        primarily column names and content types but NOT anything content-related
-        itself. It should be consistent across invocations, even if additional data
-        is added, as long as the DataFrame is still compatible in its format.
+        This should not be used on a model file that has been received across a
+        trust boundary due to underlying use of `pickle`.
 
-        Returns:
-            The hexdigest of the DataFrame hash.
+        :::
         """
-        if not self._table_hashes:
-            raise DataNotLoadedError(
-                "Data is not loaded yet. Please call `get_dtypes` first."
-            )
-        else:
-            return _hash_str(str(sorted(self._table_hashes)))
-
-    @staticmethod
-    def _get_data_dtypes(data: pd.DataFrame) -> _Dtypes:
-        """Returns the nullable column types of the dataframe.
+        with open(filename, "rb") as f:
+            # TODO: [BIT-987] Review use of pickle.load()
+            self._model = pickle.load(f)  # nosec
 
-        This is called by the `get_dtypes` method. This method also overrides datetime
-        column dtypes to be strings. This is not done for date columns which are of
-        type object.
-        """
-        data = data.convert_dtypes()
-        dtypes: _Dtypes = data.dtypes.to_dict()
-        for name in list(dtypes):
-            if is_datetime(data[name]):
-                dtypes[name] = pd.StringDtype()
+    def evaluate(
+        self, test_dl: Optional[_BitfountDataLoader] = None, **kwargs: Any
+    ) -> Tuple[np.ndarray, np.ndarray]:
+        """Perform inference on test set and return predictions and targets.
 
-        return dtypes
+        Args:
+            test_dl: Optional `BitfountDataLoader` object containing test data. If this
+                is not provided, the test set from the `DataSource` used to train the
+                model is used if present.
 
-    def load_data(self, **kwargs: Any) -> None:
-        """Load the data for the datasource.
+        Returns:
+            A tuple of numpy arrays containing the predicted and actual values.
 
         Raises:
-            TypeError: If data format is not supported.
+            ValueError: If there is no test data to evaluate the model on
         """
-        if not self._data_is_loaded and (data := self.get_data(**kwargs)) is not None:
-            self.data = data
+        logger.info("Evaluating logistic regression classifier.")
 
-    @abstractmethod
-    def get_values(
-        self, col_names: List[str], **kwargs: Any
-    ) -> Dict[str, Iterable[Any]]:
-        """Implement this method to get distinct values from list of columns."""
-        raise NotImplementedError
-
-    @abstractmethod
-    def get_column(self, col_name: str, **kwargs: Any) -> _Column:
-        """Implement this method to get single column from dataset.
-
-        Used in the `ColumnAverage` algorithm as well as to iterate over image columns
-        for the purposes of schema generation.
-        """
-        raise NotImplementedError
-
-    @abstractmethod
-    def get_data(self, **kwargs: Any) -> Optional[pd.DataFrame]:
-        """Implement this method to load and return dataset."""
-        raise NotImplementedError
-
-    @abstractmethod
-    def get_dtypes(self, **kwargs: Any) -> _Dtypes:
-        """Implement this method to get the columns and column types from dataset."""
-        raise NotImplementedError
-
-    @abstractmethod
-    def __len__(self) -> int:
-        """Implement this method to get the number of rows in the dataset."""
-        raise NotImplementedError
-
-    @property
-    def multi_table(self) -> bool:
-        """This returns False if the DataSource does not subclass `MultiTableSource`.
-
-        However, this property must be re-implemented in `MultiTableSource`, therefore
-        it is not necessarily True if the DataSource inherits from `MultiTableSource`.
-        """
-        return False
-
-    @property
-    def iterable(self) -> bool:
-        """This returns False if the DataSource does not subclass `IterableSource`.
+        if test_dl is None:
+            if isinstance(self.test_dl, _BitfountDataLoader):
+                test_dl = self.test_dl
+            else:
+                raise ValueError("There is no test data to evaluate the model on.")
+        test_df = test_dl.get_x_dataframe()
+        test_preds: np.ndarray = cast(
+            Type[sklearnLogisticRegression], self._model
+        ).predict_proba(test_df)
+        test_target = test_dl.get_y_dataframe().to_numpy()
 
-        However, this property must be re-implemented in `IterableSource`, therefore it
-        is not necessarily True if the DataSource inherits from `IterableSource`.
-        """
-        return False
+        return test_preds, test_target
 
+    def fit(
+        self,
+        data: Optional[DataSource] = None,
+        *args: Any,
+        **kwargs: Any,
+    ) -> None:
+        """Trains a model using the training set provided by `data`.
 
-@delegates()
-class MultiTableSource(BaseSource, ABC):
-    """Abstract base source that supports multiple tables.
+        Args:
+            data: `DataSource` object containing training data.
 
-    This class is used to define a data source that supports multiple tables. The
-    datasources do not necessarily have multiple tables though.
-    """
+        :::info
 
-    @property
-    @abstractmethod
-    def multi_table(self) -> bool:
-        """Implement this method to define whether the data source is multi-table.
+        The validation set in `data` is not used when training this model.
 
-        The datasource must inherit from `MultiTableSource` if this is True. However,
-        the inverse is not necessarily True.
+        :::
         """
-        raise NotImplementedError
+        logger.info("Fitting logistic regression classifier.")
 
-    @property
-    @abstractmethod
-    def table_names(self) -> List[str]:
-        """Implement this to return a list of table names.
-
-        If there is only one table, it should return a list with one element.
-        """
-        raise NotImplementedError
-
-    @abstractmethod
-    def _validate_table_name(self, table_name: str) -> None:
-        """Validate the table name exists in the multi-table datasource.
-
-        This method should raise a ValueError if the table_name is not valid.
-        """
-        raise NotImplementedError
-
-    @abstractmethod
-    def get_values(
-        self, col_names: List[str], table_name: Optional[str] = None, **kwargs: Any
-    ) -> Dict[str, Iterable[Any]]:
-        """Implement this method to get distinct values from list of columns."""
-        raise NotImplementedError
-
-    @abstractmethod
-    def get_column(
-        self, col_name: str, table_name: Optional[str] = None, **kwargs: Any
-    ) -> _Column:
-        """Implement this method to get single column from dataset."""
-        raise NotImplementedError
-
-    @abstractmethod
-    def get_data(
-        self, table_name: Optional[str] = None, **kwargs: Any
-    ) -> Optional[pd.DataFrame]:
-        """Implement this method to loads and return dataset."""
-        raise NotImplementedError
-
-    @abstractmethod
-    def get_dtypes(self, table_name: Optional[str] = None, **kwargs: Any) -> _Dtypes:
-        """Implement this method to get the columns and column types from dataset."""
-        raise NotImplementedError
-
-
-@delegates()
-class IterableSource(BaseSource, ABC):
-    """Abstract base source that supports iterating over the data.
-
-    This is used for streaming data in batches as opposed to loading the entire dataset
-    into memory.
-
-    Args:
-        partition_size: The size of each partition when iterating over the data.
-    """
+        if not self.initialised:
+            self.initialise_model(data=data)
+        self.set_number_of_classes()
+        self._set_dataloaders()
+        model = sklearnLogisticRegression(
+            C=self.inverse_regularisation,
+            random_state=self.seed,
+            max_iter=self.max_steps,
+            solver=self.model_type,
+            verbose=self.verbose,
+        )
 
-    def __init__(
-        self,
-        partition_size: int = 100,
-        **kwargs: Any,
-    ) -> None:
-        self.partition_size = partition_size
-        super().__init__(**kwargs)
+        model.fit(
+            self.train_dl.get_x_dataframe(),
+            self.train_dl.get_y_dataframe()[self._databunch.target].to_numpy(),
+        )
+        self._model = model
 
     @property
-    @abstractmethod
-    def iterable(self) -> bool:
-        """Implement this method to define whether the data source is iterable.
+    def training_needed(self) -> bool:
+        """Dictates whether the model needs training.
 
-        The datasource must inherit from `IterableSource` if this is True. However,
-        the inverse is not necessarily True.
+        Returns:
+            bool: True if `self.max_steps` is greater than 0.
         """
-        raise NotImplementedError
+        return self.max_steps > 0
 
-    @abstractmethod
-    def get_data(self, **kwargs: Any) -> Optional[pd.DataFrame]:
-        """This method must return None if the data source is iterable."""
-        raise NotImplementedError
+    class _Schema(_BaseModel._Schema, ClassifierMixIn._Schema):
+        inverse_regularisation = fields.Float(allow_none=True)
+        max_steps = fields.Integer(allow_none=True)
+        model_type = fields.String(allow_none=True)
+        penalty = fields.String(allow_none=True)
+        early_stopping_tolerance = fields.Float(allow_none=True)
+        verbose = fields.Integer(allow_none=True)
+
+        @post_load
+        def recreate_model(
+            self, data: _JSONDict, **kwargs: Any
+        ) -> LogisticRegressionClassifier:
+            """Recreate model from Schema."""
+            return LogisticRegressionClassifier(**data)
 
-    @abstractmethod
-    def yield_data(self, **kwargs: Any) -> Iterator[pd.DataFrame]:
-        """Implement this method to yield dataframes."""
-        raise NotImplementedError
+    @classmethod
+    def get_schema(cls) -> Type[LogisticRegressionClassifier._Schema]:
+        """Get the model schema."""
+        return cls._Schema
 
 
-@delegates()
-class FileSystemIterableSource(IterableSource, ABC):
-    """Abstract base source that supports iterating over file-based data.
+class RegBoostRegressor(RegressorMixIn, _BaseModel):
+    """Gradient Boosted Linear Regression Model.
 
-    This is used for Iterable data sources that whose data is stored as files on disk.
+    Implementation of "RegBoost: a gradient boosted multivariate regression algorithm"
+    by Li et al. (2020). For more details, see the paper:
+    https://www.emerald.com/insight/content/doi/10.1108/IJCS-10-2019-0029/full/html
 
     Args:
-        path: Path to the directory which contains the data files. Subdirectories
-            will be searched recursively.
-        output_path: The path where to save intermediary output files. Defaults to
-            'preprocessed/'.
-        iterable: Whether the data source is iterable. This is used to determine
-            whether the data source can be used in a streaming context during a task.
-            Defaults to False.
-        fast_load: Whether the data will be loaded in fast mode. This is used to
-            determine whether the data will be iterated over during set up for schema
-            generation and splitting (where necessary). Only relevant if `iterable` is
-            True, otherwise it is ignored. Defaults to False.
-        file_extension: File extension(s) of the data files. If None, all files
-            will be searched. Can either be a single file extension or a list of
-            file extensions.
+        learning_rate: Learning rate for gradient boosting. Defaults to 0.1.
+        max_depth: Maximum depth of tree (number of nodes between root and leaf).
+            A depth of 0 is equivalent to a single Linear Regression model. Defaults to
+                10.
+        min_data_points_per_node: Minimum number of data points required to split a
+            node. Defaults to 5.
+        stepwise_regression: Whether stepwise regression should go "forward" or
+            "backward". Defaults to "forward".
+        stepwise_regression_threshold: Threshold for stepwise regression. Defaults to
+            0.15.
+
+    Attributes:
+        learning_rate: Learning rate for gradient boosting.
+        max_depth: Maximum depth of tree (number of nodes between root and leaf).
+        min_data_points_per_node: Minimum number of data points required to split a
+            node.
+        stepwise_regression: Whether stepwise regression should go "forward" or
+            "backward".
+        stepwise_regression_threshold: Threshold for stepwise regression.
     """
 
     def __init__(
         self,
-        path: Union[os.PathLike, str],
-        output_path: Optional[Union[os.PathLike, str]] = None,
-        iterable: bool = False,
-        fast_load: bool = False,
-        file_extension: Optional[_SingleOrMulti[str]] = None,
+        learning_rate: float = 0.1,
+        max_depth: int = 10,
+        min_data_points_per_node: int = 5,
+        stepwise_regression: Literal["forward", "backward"] = "forward",
+        stepwise_regression_threshold: float = 0.15,
         **kwargs: Any,
-    ) -> None:
+    ):
         super().__init__(**kwargs)
-        self.path = Path(path)
-        self.out_path: Path
-        if output_path is None:
-            self.out_path = BITFOUNT_CACHE_DIR
-        else:
-            self.out_path = Path(output_path)
-        self.out_path.mkdir(exist_ok=True, parents=True)  # create if not exists
 
-        self._iterable = iterable
-        self.fast_load = fast_load
+        self.learning_rate = learning_rate
+        self.max_depth = max_depth
+        self.min_data_points_per_node = min_data_points_per_node
+        self.stepwise_regression = stepwise_regression
+        self.stepwise_regression_threshold = stepwise_regression_threshold
+
+    class _ModelTreeSide(Enum):
+        """Represents the side which a node represents in ModelTree."""
+
+        POSITIVE = auto()
+        NEGATIVE = auto()
+
+    @dataclass
+    class _ModelTree:
+        """Core RegBoost model. Binary decision Tree of Linear Regressors.
 
-        self.file_extension: Optional[List[str]]
-        if file_extension:
-            file_extension_: List[str] = (
-                [file_extension]
-                if isinstance(file_extension, str)
-                else list(file_extension)
+        Args:
+            data_indices: np array of data indices
+            depth: number of parent nodes above this node until the root node
+            side: -ve or +ve
+        """
+
+        data_indices: np.ndarray
+        depth: int
+        side: Optional[RegBoostRegressor._ModelTreeSide] = None
+
+        def __post_init__(self) -> None:
+            self.model = sklearnLinearRegression()
+            self.classifier = None  # only used at inference time
+            self.features: List[str] = []
+            self.negatives: Optional[RegBoostRegressor._ModelTree] = None
+            self.positives: Optional[RegBoostRegressor._ModelTree] = None
+
+        def __str__(self) -> str:
+            """Prints model structure depth first vertically."""
+            ret = (
+                "┃\t" * self.depth
+                + "┣"
+                + repr(
+                    f"Depth: {self.depth}, "
+                    + f"Features: {len(self.features)}, "
+                    + f"Data: {len(self.data_indices)}"
+                )
+                + "\n"
             )
-            self.file_extension = [
-                f".{fe}" if not fe.startswith(".") else fe for fe in file_extension_
-            ]
-        else:
-            self.file_extension = None
-        # This is used to select a subset of file names by the data splitter rather than
-        # every file returned by `file_names`.
-        self.selected_file_names_override: List[str] = []
-
-    @property
-    def selected_file_names(self) -> List[str]:
-        """Returns a list of selected file names."""
-        if self.selected_file_names_override:
-            return self.selected_file_names_override
-
-        try:
-            filenames = list(self.data["_original_filename"])
-        except DataNotLoadedError:
-            filenames = self.file_names
-
-        return filenames
-
-    @property
-    def file_names(self) -> List[str]:
-        """Returns a list of file names in the directory."""
-        files = [x for x in self.path.glob("**/*") if x.is_file()]
-        if self.file_extension:
-            files = [x for x in files if x.suffix in self.file_extension]
+            for child in [self.negatives, self.positives]:
+                if child is not None:
+                    ret += child.__str__()
+            return ret
+
+        def fit(self, *args: Any, **kwargs: Any) -> sklearnLinearRegression:
+            """Call self.model fit method."""
+            return self.model.fit(*args, **kwargs)
+
+        def predict(self, *args: Any, **kwargs: Any) -> np.ndarray:
+            """Call self.model predict method."""
+            return cast(np.ndarray, self.model.predict(*args, **kwargs))
+
+        @property
+        def is_leaf(self) -> bool:
+            """Returns boolean indicating whether this node is a leaf node."""
+            return self.positives is None and self.negatives is None
+
+        def add_child(
+            self,
+            side: RegBoostRegressor._ModelTreeSide,
+            data_indices: np.ndarray,
+        ) -> RegBoostRegressor._ModelTree:
+            """Add child node (either `negatives` or `positives`) and return it.
+
+            Args:
+                side: -ve or +ve
+                data_indices: np array of data indices
+
+            Returns:
+                RegBoostRegressor._ModelTree: the child node just created
+            """
+            model = RegBoostRegressor._ModelTree(data_indices, self.depth + 1, side)
+            if side == RegBoostRegressor._ModelTreeSide.NEGATIVE:
+                self.negatives = model
+            elif side == RegBoostRegressor._ModelTreeSide.POSITIVE:
+                self.positives = model
 
-        return [str(x.resolve()) for x in files]
+            return model
 
-    @property
-    def stale(self) -> bool:
-        """Whether the data source is stale.
+    def _fit_model_tree(
+        self,
+        model: _ModelTree,
+        parent_preds: Optional[np.ndarray] = None,
+        parent_features: Optional[List[str]] = None,
+    ) -> None:
+        """Fits `model` recursively.
 
-        This is defined by whether the data is loaded and the number of files matches
-        the number of rows in the dataframe.
+        Builds `model` tree recursively depth-first and fits the Linear Regression
+        model at each node.
         """
-        if self._data_is_loaded and len(self.data) == len(self.file_names):
-            return False
-
-        return True
+        data = self.train_dl.get_x_dataframe()
 
-    @property
-    def iterable(self) -> bool:
-        """Defines whether the data source is iterable.
-
-        This is defined by the user when instantiating the class.
-        """
-        return self._iterable
+        if isinstance(data, DataFrame):
+            X = data.loc[model.data_indices]
+        else:
+            X, _ = data
+            X = X.loc[model.data_indices]
+        y = (
+            self.train_dl.get_y_dataframe()[self.datastructure.target]
+            .to_numpy()[model.data_indices]
+            .astype(np.float32)
+        )
+
+        # Update target to be the residual from parent * learning rate
+        if parent_preds is not None:
+            y -= self.learning_rate * parent_preds
+
+        # Subset features to same as parent
+        if parent_features is not None:
+            X = X[parent_features]
+
+        # Perform stepwise regression feature selection if we have more than one feature
+        if len(X.columns) > 1:
+            model.features = self._perform_stepwise_regression(X, y)
+            X = X[model.features]
+        else:
+            model.features = parent_features or list(X.columns)
 
-    def _load_data_iteratively(self, **kwargs: Any) -> None:
-        """Load data iteratively using `yield_data`.
+        # Fit linear regression model
+        model.fit(X, y)
 
-        This is only applicable for FileSystemIterableSource.
-        """
-        dfs = []
-        for df in self.yield_data(**kwargs):
-            dfs.append(df)
-        self.data = pd.concat(dfs)
+        # If we haven't reached the maximum depth, create positive and negative children
+        if model.depth < self.max_depth:
+            preds = model.predict(X)
+
+            for side, indices in zip(
+                RegBoostRegressor._ModelTreeSide,
+                [preds <= y, preds > y],
+            ):
+                # Only create children if there are enough data points for both children
+                if (
+                    indices.sum() >= self.min_data_points_per_node
+                    and (len(preds) - indices.sum()) >= self.min_data_points_per_node
+                ):
+                    model_ = model.add_child(side, X.index[indices])
+
+                    # Add parent predictions to current predictions
+                    new_preds = preds[indices]
+                    if parent_preds is not None:
+                        new_preds += parent_preds[indices]
 
-    @abstractmethod
-    def _get_data(
-        self, file_names: Optional[List[str]] = None, **kwargs: Any
-    ) -> pd.DataFrame:
-        """Implement to return data corresponding to the provided file names.
+                    # Fit child model tree
+                    self._fit_model_tree(model_, new_preds, model.features)
 
-        This method is called under the hood by `get_data` and `yield_data`. This
-        method must return a dataframe with the columns `_original_filename` and
-        `_last_modified" containing the original file name of each row, and the
-        timestamp when the file was last modified in ISO 8601 format, respectively.
+    def _perform_stepwise_regression(self, X: pd.DataFrame, y: np.ndarray) -> List[str]:
+        """Performs either forward or backward stepwise regression.
 
         Args:
-            file_names: List of file names to load. If None, all files should be
-                loaded.
-
-        Returns:
-            A dataframe containing the data.
-        """
-        raise NotImplementedError
-
-    def get_values(
-        self, col_names: List[str], **kwargs: Any
-    ) -> Dict[str, Iterable[Any]]:
-        """Get distinct values from columns in the dataset.
+            X (pd.DataFrame): dataframe of features
+            y (np.ndarray): target array
 
-        Args:
-            col_names: The list of the columns whose distinct values should be
-                returned.
+        Raises:
+            ValueError: if stepwise regression direction not supported
 
         Returns:
-            The distinct values of the requested column as a mapping from col name to
-            a series of distinct values.
+            List[str]: list of new features
         """
-        dic: Dict[str, Iterable[Any]] = {}
-        if not self.stale:
-            pass
-        elif self.iterable:
-            self._load_data_iteratively(**kwargs)
+        if self.stepwise_regression == "forward":
+            features = self._forward_stepwise_regression(
+                X, y, self.stepwise_regression_threshold
+            )
+        elif self.stepwise_regression == "backward":
+            features = self._backward_stepwise_regression(
+                X, y, self.stepwise_regression_threshold
+            )
         else:
-            self.data = self._get_data()
+            raise ValueError(
+                "Stepwise regression only supports 'forward' and 'backward'"
+            )
+        return features
+
+    @staticmethod
+    def _forward_stepwise_regression(
+        X: pd.DataFrame, y: np.ndarray, p_threshold: float
+    ) -> List[str]:
+        """Perform forward stepwise regression.
+
+        Starts off with no features and keeps adding them until we reach p_threshold.
+        """
+        included: List[str] = []
+        while True:
+            if len(included) == len(X.columns):
+                break
+
+            p_values = {}
+            for feature in [i for i in X.columns if i not in included]:
+                model = sm.OLS(
+                    y, sm.add_constant(pd.DataFrame(X[included + [feature]]))
+                )
+                results = model.fit()
+                p_values[feature] = results.pvalues[feature]
+            getter: Any = p_values.get
+            best_feature = min(p_values, key=getter)
+            best_pval = p_values[best_feature]
 
-        for col in col_names:
-            try:
-                dic[col] = self.data[col].unique()
-            except TypeError:
-                logger.warning(f"Found unhashable value type, skipping column {col}.")
-        return dic
+            if best_pval > p_threshold:
+                break
 
-    def get_column(self, col_name: str, **kwargs: Any) -> Union[np.ndarray, pd.Series]:
-        """Loads and returns single column from the dataset.
+            included.append(best_feature)
 
-        Args:
-            col_name: The name of the column which should be loaded.
+        return included or [best_feature]
 
-        Returns:
-            The column request as a series.
+    @staticmethod
+    def _backward_stepwise_regression(
+        X: pd.DataFrame, y: np.ndarray, p_threshold: float
+    ) -> List[str]:
+        """Perform backward stepwise regression.
+
+        Starts off with all features and keeps removing them until we reach p_threshold.
         """
-        if not self.stale:
-            pass
-        elif self.iterable:
-            self._load_data_iteratively(**kwargs)
-        else:
-            self.data = self._get_data()
+        included = list(X.columns)
+        while True:
+            if len(included) == 1:
+                break
 
-        return self.data[col_name]
+            model = sm.OLS(y, sm.add_constant(X[included]))
+            results = model.fit()
 
-    def get_dtypes(self, **kwargs: Any) -> _Dtypes:
-        """Loads and returns the column names and types of the dataframe.
+            # use all coefs except intercept
+            p_values = dict(results.pvalues.iloc[1:])
+            getter: Any = p_values.get
+            worst_feature = max(p_values, key=getter)
+            worst_pval = p_values[worst_feature]
 
-        If `fast_load` is set to True, only the first file will be loaded to get the
-        column names and types. Otherwise, all files will be loaded.
+            if worst_pval <= p_threshold:
+                break
 
-        Returns:
-            A mapping from column names to column types.
-        """
-        if not self.stale:
-            pass
-        elif self.fast_load:
-            self.data = self._get_data(file_names=[self.file_names[0]], **kwargs)
-        elif self.iterable:
-            self._load_data_iteratively(**kwargs)
-        else:
-            self.data = self._get_data(**kwargs)
+            included.remove(worst_feature)
 
-        return self._get_data_dtypes(self.data)
+        return included
 
-    def yield_data(
-        self, file_names: Optional[List[str]] = None, **kwargs: Any
-    ) -> Iterator[pd.DataFrame]:
-        """Yields data in batches from files that match the given file names.
+    def fit(
+        self,
+        data: Optional[DataSource] = None,
+        metrics: Optional[Mapping[str, Metric]] = None,
+        *args: Any,
+        **kwargs: Any,
+    ) -> None:
+        """Trains a model using the training set provided by the DataSource object."""
+        logger.info("Fitting RegBoost regressor.")
+        self.initialise_model(data)
+        self._set_dataloaders()
+        model = self._ModelTree(np.array(range(len(self.train_dl))), 0)
+        self._fit_model_tree(model)
+        self._model: RegBoostRegressor._ModelTree = model
+
+    def evaluate(
+        self, test_dl: Optional[_BitfountDataLoader] = None, **kwargs: Any
+    ) -> Tuple[np.ndarray, np.ndarray]:
+        """Perform inference on test set and return predictions and targets.
 
         Args:
-            file_names: An optional list of file names to use for yielding data.
-                Otherwise, all files that have already been found will be used.
-                `file_names` is always provided when this method is called from the
-                Dataset as part of a task.
+            test_dl: Optional `BitfountDataLoader` object containing test data. If this
+                is not provided, the test set from the `DataSource` used to train the
+                model is used if present.
+
+        Returns:
+            A tuple of numpy arrays containing the predicted and actual values.
 
         Raises:
-            ValueError: If no file names provided and no files have been found.
+            ValueError: If there is no test data to evaluate the model on
         """
-        if not file_names and not self.file_names:
-            raise ValueError("No files found to yield data from.")
+        logger.info("Evaluating RegBoost regressor.")
 
-        file_names = file_names or self.file_names
+        k = cast(int, kwargs.get("k"))
+        if test_dl is None:
+            if isinstance(self.test_dl, _BitfountDataLoader):
+                test_dl = self.test_dl
+            else:
+                raise ValueError("There is no test data to evaluate the model on.")
+        test_df: _DataFrameType = test_dl.get_x_dataframe()
+        test_target = (
+            test_dl.get_y_dataframe()[self._databunch.target]
+            .to_numpy()
+            .astype(np.float32)
+        )
+        test_preds = self._evaluate_model_tree(self._model, test_df, k)
+        test_preds_aggregated = self._aggregate_model_predictions(test_preds)
+
+        return test_preds_aggregated, test_target
+
+    def _evaluate_model_tree(
+        self, model: _ModelTree, test_df: pd.DataFrame, k: int
+    ) -> List[List[float]]:
+        """Recursively run inference on ModelTree and return final predictions."""
+        if model.classifier is None:
+            self._fit_model_tree_classifier(model, k)
+        test_df = test_df[model.features]
+        tree_classes = self._eval_model_tree_classifier(model, test_df)
+        preds = np.reshape(model.predict(test_df), (len(test_df), 1)).tolist()
+
+        # Split data into positives and negatives based on classifier results
+        for indices, model_ in zip(
+            [tree_classes == 1, tree_classes == 0],
+            [model.positives, model.negatives],
+        ):
+            # If child model exists and there is at least one data point for that branch
+            # Evaluate on that branch and append predictions to parent predictions
+            if model_ is not None and sum(indices) > 0:
+                child_preds = self._evaluate_model_tree(model_, test_df[indices], k)
+                preds = self._append_model_predictions(preds, child_preds, indices)
+        predictions: List[List[float]] = preds
+        return predictions
+
+    def _aggregate_model_predictions(self, preds: List[List[float]]) -> np.ndarray:
+        """Aggregate all model predictions for each data point and return predictions.
+
+        Learning rate applied to all predictions apart from the last one. This modifies
+        the `preds` object.
+        """
+        preds_aggregated = []
+        # pred_list: List[float]
+        for pred_list in preds:
+            preds_aggregated.append(
+                pred_list[-1] + (sum(pred_list[:-1]) * self.learning_rate)
+            )
+        predictions: np.ndarray = np.asarray(preds_aggregated)
+        return predictions
 
-        for file_names_partition in self.partition(file_names, self.partition_size):
-            file_names_partition = cast(List[str], file_names_partition)
-            yield self._get_data(file_names_partition)
-            # Delete the files after they have been processed and used
-            self._cleanup(file_names_partition)
+    @staticmethod
+    def _append_model_predictions(
+        parent_preds: List[List[float]],
+        child_preds: List[List[float]],
+        indices: List[bool],
+    ) -> List[List[float]]:
+        """Appends `child_preds` to corresponding `indices` in `parent_preds`."""
+        i = 0
+        for pred, idx in zip(parent_preds, indices):
+            if idx:
+                pred.extend(child_preds[i])
+                i += 1
+        return parent_preds
+
+    def _fit_model_tree_classifier(self, model: _ModelTree, k: int) -> None:
+        """Fit ModelTree classifier with training data."""
+        if model.negatives is not None and model.positives is not None:
+            model.classifier = KNeighborsClassifier(n_neighbors=k)
+            X: _DataFrameType = self.train_dl.get_x_dataframe()
+            X = X[model.features].loc[
+                np.concatenate(
+                    [model.positives.data_indices, model.negatives.data_indices]
+                )
+            ]
+            y = [1 for _ in model.positives.data_indices] + [
+                0 for _ in model.negatives.data_indices
+            ]
+            if isinstance(model.classifier, KNeighborsClassifier):
+                model.classifier.fit(X, y)
 
-    def get_data(self, **kwargs: Any) -> Optional[pd.DataFrame]:
-        """Returns data if the datasource is not iterable, otherwise None.
+    @staticmethod
+    def _eval_model_tree_classifier(
+        model: _ModelTree, test_df: pd.DataFrame
+    ) -> np.ndarray:
+        """Run inference on ModelTree classifier to return corresponding classes.
+
+        Doesn't run the model if there is only one class available.
+        """
+        if isinstance(model.classifier, KNeighborsClassifier):
+            class_predictions = model.classifier.predict(test_df)
+        elif model.negatives is None:
+            class_predictions = [1 for _ in range(len(test_df))]
+        elif model.positives is None:
+            class_predictions = [0 for _ in range(len(test_df))]
 
-        We don't reload the data if it has already been loaded. We are assuming that
-        files are only added, not removed, meaning we can rely on simply matching the
-        number of files detected with the length of the dataframe to ascertain whether
-        the data has already been loaded.
-        """
-        if not self.stale:
-            return self.data
+        return np.array(class_predictions)
 
-        return self._get_data(**kwargs)
+    def serialize(self, filename: Union[str, os.PathLike]) -> None:
+        """Serialize model to file with provided `filename`.
 
-    def _recreate_file_structure(self, file_path: str, exist_ok: bool) -> Path:
-        """Recreates the file structure in the output directory.
+        Args:
+            filename: Path to file to save serialized model.
+        """
+        with open(filename, "wb") as f:
+            pickle.dump(self._model, f, protocol=pickle.HIGHEST_PROTOCOL)
 
-        This is used to ensure that the output directory has the same structure as the
-        input directory. This is useful for when the input data is partitioned into
-        subdirectories.
+    def deserialize(self, filename: Union[str, os.PathLike]) -> None:
+        """Deserialize model.
 
         Args:
-            file_path: The file name to recreate the structure for.
-            exist_ok: Whether to raise an error if the directory already exists.
+            filename: Path to file containing serialized model.
 
-        Returns:
-            The path to the recreated file structure.
+        :::danger
 
-        Raises:
-            FileExistsError: If the subdir already exists in the output directory and
-                `exist_ok` is False.
+        This should not be used on a model file that has been received across a
+        trust boundary due to underlying use of `pickle`.
+
+        :::
         """
-        path = Path(file_path)
-        file_id = path.stem
-        parent_path = path.parent.absolute()
-        # Relative path gets the path of the original file relative to the specified
-        # input path. This relative path is then used to recreate the file structure
-        # in the output directory with the original filename being used as a
-        # subdirectory instead containing any relevant extracted files.
-        relative_path = parent_path.relative_to(self.path.absolute())
-        save_prefix = self.out_path / relative_path / file_id
-        save_prefix.mkdir(parents=True, exist_ok=exist_ok)
-        return save_prefix
-
-    def _cleanup(self, file_names: List[str]) -> None:
-        """Remove intermediate files creating during yielding of partitions.
-
-        Intended to be called by `yield_data` every partition.
-        """
-        for file_name in file_names:
-            save_dir = self._recreate_file_structure(file_name, exist_ok=True)
-            shutil.rmtree(save_dir)
+        self._set_dataloaders()
+        with open(filename, "rb") as f:
+            # TODO: [BIT-987] Review use of pickle.load()
+            self._model = pickle.load(f)  # nosec
 
-    @staticmethod
-    def partition(iterable: Sequence, partition_size: int = 1) -> Iterable:
-        """Takes an iterable and yields partitions of size `partition_size`.
+    @property
+    def training_needed(self) -> bool:
+        """Dictates whether the model needs training.
 
-        The final partition may be less than size `partition_size` due to the variable
-        length of the iterable.
+        Returns:
+            bool: True if model is untrained, False otherwise.
         """
-        len_ = len(iterable)
-        for partition_start_idx in range(0, len_, partition_size):
-            yield iterable[
-                partition_start_idx : min(partition_start_idx + partition_size, len_)
-            ]
+        return (
+            not isinstance(self._model, RegBoostRegressor._ModelTree)
+            or self._model.is_leaf
+        )
+
+    class _Schema(_BaseModel._Schema):
+        learning_rate = fields.Float(allow_none=True)
+        max_depth = fields.Integer(allow_none=True)
+        min_data_points_per_node = fields.Integer(allow_none=True)
+        stepwise_regression = fields.String(allow_none=True)
+        stepwise_regression_threshold = fields.Float(allow_None=True)
+
+        @post_load
+        def recreate_model(self, data: _JSONDict, **kwargs: Any) -> RegBoostRegressor:
+            """Recreate model from Schema."""
+            return RegBoostRegressor(**data)
 
-    def load_data(self, **kwargs: Any) -> None:
-        """Load the data for the datasource.
+    @classmethod
+    def get_schema(cls) -> Type[RegBoostRegressor._Schema]:
+        """Get the model schema.
 
-        Raises:
-            TypeError: If data format is not supported.
+        Returns:
+            Type[RegBoostRegressor._Schema]: Schema for model.
         """
-        if self.iterable:
-            if self.stale:
-                self._load_data_iteratively(**kwargs)
-        elif (data := self.get_data(**kwargs)) is not None:
-            self.data = data
-
-    def __len__(self) -> int:
-        return len(self.file_names)
+        return cls._Schema
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `bitfount-0.5.86/bitfount/data/datastructure.py` & `bitfount-0.5.9/bitfount/data/schema.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,751 +1,741 @@
-"""Classes concerning data structures.
-
-DataStructures provide information about the columns of a BaseSource for a specific
-Modelling Job.
-"""
+"""Classes concerning data schemas."""
 from __future__ import annotations
 
-from dataclasses import dataclass
-import inspect
+import collections
+import copy
 import logging
-from types import MappingProxyType
+from os import PathLike
 from typing import (
-    TYPE_CHECKING,
     Any,
-    ClassVar,
     Dict,
     List,
     Mapping,
     MutableMapping,
     Optional,
-    Type,
+    Sequence,
+    Tuple,
     Union,
     cast,
 )
 
-import desert
-from marshmallow import fields
+from PIL import Image
+import databricks.koalas as ks
+from marshmallow import fields, post_dump, post_load, pre_dump, pre_load
+import numpy as np
+import pandas as pd
+from pandas._typing import Dtype
+from pandas.core.dtypes.common import is_numeric_dtype
+import yaml
 
-from bitfount.data.datasources.database_source import DatabaseSource
-from bitfount.data.datasplitters import DatasetSplitter
-from bitfount.data.exceptions import DataStructureError
-from bitfount.data.schema import BitfountSchema, TableSchema
+import bitfount
+from bitfount.data.datasource import DataSource
 from bitfount.data.types import (
     CategoricalRecord,
     ContinuousRecord,
-    DataSplit,
     ImageRecord,
-    SchemaOverrideMapping,
     SemanticType,
-    StrDictField,
     TextRecord,
-    _ForceStypeValue,
+    _CamelCaseSchema,
+    _FeatureDict,
     _SemanticTypeRecord,
     _SemanticTypeValue,
 )
-from bitfount.transformations.base_transformation import TRANSFORMATION_REGISTRY
-from bitfount.transformations.batch_operations import BatchTimeOperation
-from bitfount.transformations.parser import TransformationsParser
-from bitfount.types import (
-    T_FIELDS_DICT,
-    T_NESTED_FIELDS,
-    _BaseSerializableObjectMixIn,
-    _JSONDict,
-)
-from bitfount.utils import _add_this_to_list
-
-if TYPE_CHECKING:
-    from bitfount.data.datasources.base_source import BaseSource
-    from bitfount.runners.config_schemas import (
-        DataStructureAssignConfig,
-        DataStructureSelectConfig,
-        DataStructureTableConfig,
-        DataStructureTransformConfig,
-    )
+from bitfount.data.utils import _hash_str
+from bitfount.exceptions import BitfountError
+from bitfount.types import _DataFrameLib, _DataFrameType, _JSONDict
+from bitfount.utils import _add_this_to_list, _get_df_library_type
 
 logger = logging.getLogger(__name__)
 
-DEFAULT_IMAGE_TRANSFORMATIONS: List[Union[str, _JSONDict]] = [
-    {"Resize": {"height": 224, "width": 224}},
-    "Normalize",
-    "ToTensorV2",
-]
-
-_registry: Dict[str, Type[BaseDataStructure]] = {}
-registry: Mapping[str, Type[BaseDataStructure]] = MappingProxyType(_registry)
 
+class _BitfountSchemaMarshmallowMixIn:
+    """MixIn class for Schema serialization."""
 
-@dataclass
-class BaseDataStructure:
-    """Base DataStructure class."""
+    def dump(self, file_path: PathLike) -> None:
+        """Dumps the schema as a yaml file.
 
-    @classmethod
-    def __init_subclass__(cls, **kwargs: Any):
-        if not inspect.isabstract(cls):
-            logger.debug(f"Adding {cls.__name__}: {cls} to registry")
-            _registry[cls.__name__] = cls
+        Args:
+            file_path: The path where the file should be saved
 
+        Returns:
+            none
+        """
+        with open(file_path, "w") as file:
+            file.write(self.dumps())
 
-@dataclass
-class DataStructure(BaseDataStructure, _BaseSerializableObjectMixIn):
-    """Information about the columns of a BaseSource.
+    def dumps(self) -> Any:
+        """Produces the YAML representation of the schema object.
 
-    This component provides the desired structure of data
-    to be used by discriminative machine learning models.
+        Returns:
+            str: The YAML representation of the schema
+        """
+        return yaml.dump(self.to_json(), sort_keys=False)
 
-    :::note
+    def to_json(self) -> _JSONDict:
+        """Turns a schema object into a JSON compatible dictionary.
 
-    If the datastructure includes image columns, batch transformations will be applied
-    to them.
+        Returns:
+            dict: A simple JSON compatible representation of the Schema
+        """
+        # Our self._Schema() objects are dumped as JSON-compatible dicts
+        return cast(_JSONDict, self._Schema().dump(self))
 
-    :::
+    @classmethod
+    def load(cls, data: Mapping) -> BitfountSchema:
+        """Loads the schema from a dictionary.
 
-    Args:
-        table: The table in the Pod schema to be used for local data for single
-            pod tasks. If executing a remote task involving multiple pods, this
-            should be a mapping of Pod names to table names. Defaults to None.
-        query: The sql query that needs to be applied to the data.
-            It should be a string if it is used for local data or
-            single pod tasks or a mapping of Pod names to the queries
-            if multiple pods are nvolved in the task. Defaults to None.
-        schema_types_override: A mapping that defines the new data types that
-            will be returned after the sql query is executed. For a local training
-            task it will be a mapping of column names to their types, for a remote
-            task it will be a mapping of the Pod name to the new columns and types.
-            If a column is defined as "categorical", the mapping should include a
-            mapping to the categories. Required if a sql query is provided.
-            E.g. {'Pod_id': {'categorical': [{'col1': {'value_1':0, 'value_2': 1
-            }}], "continuous": ['col2']} for remote training
-            or {'categorical':[{ "col1" : {'value_1':0, 'value_2': 1}}],'continuous':
-            ['col2']} for local training. Defaults to None.
-        target: The training target column or list of columns.
-        ignore_cols: A list of columns to ignore when getting the
-            data. Defaults to None.
-        selected_cols: A list of columns to select when getting the
-            data. Defaults to None.
-        data_splitter: Approach used for splitting the data into training, test,
-            validation. Defaults to None.
-        loss_weights_col: A column name which provides a weight to be given
-            to each sample in loss function. Defaults to None.
-        multihead_col: A categorical column whereby the number of unique values
-            will determine number of heads in a Neural Network. Used
-            for multitask training. Defaults to None.
-        multihead_size: The number of uniques values in the `multihead_col`.
-            Used for multitask training. Required if `multihead_col` is
-            provided. Defaults to None.
-        ignore_classes_col: A column name denoting which classes to ignore
-            in a multilabel multiclass classification problem. Each value is
-            expected to contain a list of numbers corresponding to the indices of
-            the classes to be ignored as per the order provided in `target`.
-            E.g. [0,2,3]. An empty list can be provided (e.g. []) to avoid ignoring
-            any classes for some samples. Defaults to None.
-        image_cols: A list of columns that will be treated as images in the data.
-        batch_transforms: A dictionary of transformations to apply to batches.
-            Defaults to None.
-        dataset_transforms: A dictionary of transformations to apply to
-            the whole dataset. Defaults to None.
-        auto_convert_grayscale_images: Whether or not to automatically convert grayscale
-            images to RGB. Defaults to True.
-
-    Raises:
-        DataStructureError: If 'sql_query' is provided as well as either `selected_cols`
-            or `ignore_cols`.
-        DataStructureError: If both `ignore_cols` and `selected_cols` are provided.
-        DataStructureError: If the `multihead_col` is provided without `multihead_size`.
-        ValueError: If a batch transformation name is not recognised.
+        Args:
+            data: The data to load the schema from.
 
-    """
+        Returns:
+            BitfountSchema: A _Schema instance corresponding to the dictionary.
+        """
+        # @post_load guarantees this will be a BitfountSchema
+        schema: BitfountSchema = cls._Schema().load(data)
+        return schema
 
-    fields_dict: ClassVar[T_FIELDS_DICT] = {
-        "table": StrDictField(allow_none=True),
-        "query": StrDictField(allow_none=True),
-        "schema_types_override": fields.Dict(allow_none=True),
-        "target": fields.Raw(allow_none=True),
-        # `ignore_cols` is intentionally not serialised because it can be reconstructed
-        # from the `selected_cols`. Furthermore, when it comes to deserialisation, the
-        # datastructure can only accept one of these 2 arguments
-        "selected_cols": fields.List(fields.Str(), allow_none=True),
-        "loss_weights_col": fields.Str(allow_none=True),
-        "multihead_col": fields.Str(allow_none=True),
-        "multihead_size": fields.Int(allow_none=True),
-        "ignore_classes_col": fields.Str(allow_none=True),
-        "batch_transforms": fields.List(
-            fields.Dict(
-                keys=fields.Str(),
-                values=fields.Dict(keys=fields.Str()),
-            ),
-            allow_none=True,
-        ),
-        "dataset_transforms": fields.List(
-            fields.Dict(
-                keys=fields.Str(),
-                values=fields.Dict(keys=fields.Str()),
-            ),
-            allow_none=True,
-        ),
-        "auto_convert_grayscale_images": fields.Boolean(),
-    }
-    nested_fields: ClassVar[T_NESTED_FIELDS] = {}
-
-    table: Optional[Union[str, Mapping[str, str]]] = None
-    query: Optional[Union[str, Mapping[str, str]]] = None
-    schema_types_override: Optional[
-        Union[SchemaOverrideMapping, Mapping[str, SchemaOverrideMapping]]
-    ] = None
-    target: Optional[Union[str, List[str]]] = None
-    ignore_cols: List[str] = desert.field(
-        fields.List(fields.String()), default_factory=list
-    )
-    selected_cols: List[str] = desert.field(
-        fields.List(fields.String()), default_factory=list
-    )
-    data_splitter: Optional[DatasetSplitter] = None
-    loss_weights_col: Optional[str] = None
-    multihead_col: Optional[str] = None
-    multihead_size: Optional[int] = None
-    ignore_classes_col: Optional[str] = None
-    image_cols: Optional[List[str]] = None
-    batch_transforms: Optional[List[Dict[str, _JSONDict]]] = None
-    dataset_transforms: Optional[List[Dict[str, _JSONDict]]] = None
-    auto_convert_grayscale_images: bool = True
-
-    def __post_init__(self) -> None:
-        self.class_name = type(self).__name__
-        if self.table and self.query:
-            raise DataStructureError(
-                "Invalid parameter specification. "
-                "Please provide either table name (table) or "
-                "query to execute (query), not both. "
-            )
-        if not self.table and not self.query:
-            raise DataStructureError(
-                "Invalid parameter specification. "
-                "Please provide one of table name (table) or "
-                "query to execute (query). "
-            )
-        if self.query and (self.selected_cols or self.ignore_cols):
-            raise DataStructureError(
-                "If a query is specified, the columns needed for training "
-                "should be defined in the query."
-            )
-        if self.query and self.schema_types_override is None:
-            raise DataStructureError(
-                "Invalid parameter specification. "
-                "Please provide the schema override mapping along with the query."
-            )
-        if self.schema_types_override:
-            self._validate_schema_features()
+    @classmethod
+    def load_from_file(cls, file_path: Union[str, PathLike]) -> BitfountSchema:
+        """Loads the schema from a yaml file.
 
-        if self.selected_cols and self.ignore_cols:
-            raise DataStructureError(
-                "Invalid parameter specification. "
-                "Please provide either columns to select (selected_cols) or "
-                "to ignore (ignore_cols), not both."
-            )
-        if self.multihead_col and self.multihead_size is None:
-            raise DataStructureError("Please provide the size of the multihead column.")
-        if self.dataset_transforms is not None:
-            self.set_columns_after_transformations(self.dataset_transforms)
-        self._force_stype: MutableMapping[
-            Union[_ForceStypeValue, _SemanticTypeValue], List[str]
-        ] = {}
-        if self.image_cols:
-            self._force_stype["image"] = self.image_cols
-
-        if self.batch_transforms is None and self.image_cols:
-            default_image_transformations = []
-            for col in self.image_cols:
-                for step in DataSplit:
-                    default_image_transformations.append(
-                        {
-                            "albumentations": {
-                                "arg": col,
-                                "output": True,
-                                "transformations": DEFAULT_IMAGE_TRANSFORMATIONS,
-                                "step": step.value,
-                            }
-                        }
-                    )
-            self.batch_transforms = default_image_transformations
+        This contains validation errors to help fix an invalid YAML file.
+        """
+        with open(file_path, "r") as f:
+            schema_as_yaml = yaml.safe_load(f)
+        return cls.load(schema_as_yaml)
+
+    class _Schema(_CamelCaseSchema):
+        categorical_features = fields.List(fields.Nested(CategoricalRecord._Schema))
+        continuous_features = fields.List(fields.Nested(ContinuousRecord._Schema))
+        image_features = fields.List(fields.Nested(ImageRecord._Schema))
+        text_features = fields.List(fields.Nested(TextRecord._Schema))
+
+        # TODO: [BIT-1057] Consider moving metadata to be a separate part of the
+        #       output YAML.
+        # To maintain backwards compatibility with schemas that may not contain
+        # metadata we use a default value.
+        metadata = fields.Method(
+            serialize="dump_metadata", deserialize="load_metadata", load_default=dict
+        )
 
-        # Ensure specified batch transformations are all valid transformations
-        if self.batch_transforms is not None:
-            invalid_batch_transforms = []
-            for _dict in self.batch_transforms:
-                for tfm in _dict:
-                    if tfm not in TRANSFORMATION_REGISTRY:
-                        invalid_batch_transforms.append(tfm)
-            if invalid_batch_transforms:
-                raise ValueError(
-                    f"The following batch transformations are not recognised: "
-                    f"{', '.join(sorted(invalid_batch_transforms))}."
+        @staticmethod
+        def dump_metadata(obj: BitfountSchema) -> Dict[str, str]:
+            """Creates and dumps metadata for the schema."""
+            return {"bitfount_version": bitfount.__version__, "hash": obj.hash}
+
+        @staticmethod
+        def load_metadata(value: Dict[str, str]) -> Dict[str, str]:
+            """Loads the metadata dict."""
+            return value
+
+        @pre_dump
+        def dump_feature_values(
+            self, data: BitfountSchema, **_kwargs: Any
+        ) -> BitfountSchema:
+            """Modifies features to dump features as a list instead of dictionaries.
+
+            To ensure our YAML is clear, we pre-process our object into lists before
+            dumping it. We don't want to modify the actual schema object, as it will
+            affect its use, so we create a temporary one just for dumping to YAML.
+            """
+            temp_schema = copy.deepcopy(data)
+            for stype in data.features:
+                setattr(
+                    temp_schema,
+                    f"{stype}_features",
+                    list(data.features[cast(_SemanticTypeValue, stype)].values()),
                 )
 
-        # Create mapping of all feature names used in training together with the
-        # corresponding semantic type. This is the final mapping that will be used
-        # to decide which features will be actually be used.
-        self.selected_cols_w_types: Dict[_SemanticTypeValue, List[str]] = {}
+            return temp_schema
 
-    @classmethod
-    def create_datastructure(
-        cls,
-        table_config: DataStructureTableConfig,
-        select: DataStructureSelectConfig,
-        transform: DataStructureTransformConfig,
-        assign: DataStructureAssignConfig,
-    ) -> DataStructure:
-        """Creates a datastructure based on the yaml config.
-
-        Args:
-            table_config: The table in the Pod schema to be used for local data.
-                If executing a remote task, this should a mapping of Pod names
-                to table names.
-            select: The configuration for columns to be included/excluded
-                from the `DataStructure`.
-            transform: The configuration for dataset and batch transformations
-                to be applied to the data.
-            assign: The configuration for special columns in the `DataStructure`.
-
-        Returns:
-              A `DataStructure` object.
-        """
-        if select.include and select.exclude:
-            raise DataStructureError(
-                "Please provide either columns to include or to exclude from data"
-                ", not both."
+        @post_load
+        def recreate_schema(self, data: _JSONDict, **_kwargs: Any) -> BitfountSchema:
+            """Recreates Schema."""
+            new_schema = BitfountSchema()
+
+            for key in data:
+                if key.endswith("_features"):
+                    stype = key.replace("_features", "")
+                    new_schema.features[cast(_SemanticTypeValue, stype)] = {
+                        feature.feature_name: feature for feature in data[key]
+                    }
+
+            # Ensure existing datasources hash is loaded if present
+            new_schema._orig_hash = data["metadata"].get("hash")
+
+            return new_schema
+
+        @post_dump
+        def combine_features(self, data: _JSONDict, **kwargs: Any) -> _JSONDict:
+            """Combines features belonging to different semantic types under one key.
+
+            After combining the features into one list, it also sorts all the features
+            by featureName.
+            """
+            new_data = {}
+            new_data["features"] = [
+                item for key in data if key.endswith("Features") for item in data[key]
+            ]
+            # sort features alphabetically
+            # Type ignore due to this bug: https://github.com/python/mypy/issues/9656
+            new_data["features"] = sorted(
+                new_data["features"], key=lambda d: d["featureName"]  # type: ignore[no-any-return] # Reason: see comment above # noqa: B950
             )
-        ignore_cols = select.exclude if select.exclude is not None else []
-        selected_cols = select.include if select.include is not None else []
-        return cls(
-            table=table_config.table,
-            query=table_config.query,
-            schema_types_override=table_config.schema_types_override,
-            target=assign.target,
-            ignore_cols=ignore_cols,
-            selected_cols=selected_cols,
-            loss_weights_col=assign.loss_weights_col,
-            multihead_col=assign.multihead_col,
-            ignore_classes_col=assign.ignore_classes_col,
-            image_cols=assign.image_cols,
-            batch_transforms=transform.batch,
-            dataset_transforms=transform.dataset,
-            auto_convert_grayscale_images=transform.auto_convert_grayscale_images,
-        )
+            new_data["metadata"] = data["metadata"]
+            return new_data
 
-    def get_table_name(self, data_identifier: Optional[str] = None) -> str:
-        """Returns the relevant table name of the `DataStructure`.
+        @pre_load
+        def split_features(self, data: _JSONDict, **kwargs: Any) -> _JSONDict:
+            """Splits features back into a dictionary of lists by semantic type."""
+            result = collections.defaultdict(list)
+            features: List[_JSONDict] = data.pop("features")
+            for d in features:
+                result[d.pop("semanticType")].append(d)
 
-        Args:
-            data_identifier: The identifier of the pod/logical pod/datasource to
-                retrieve the table of.
+            for semantic_type in result:
+                data[f"{semantic_type}Features"] = result[semantic_type]
 
-        Returns:
-            The table name of the `DataStructure` corresponding to the `pod_identifier`
-            provided or just the local table name if running locally.
+            return data
 
-        Raises:
-            ValueError: If the `data_identifier` is not provided and there are different
-                table names for different pods.
-            KeyError: If the `data_identifier` is not in the collection of tables
-                specified for different pods.
-        """
-        if isinstance(self.table, str):
-            return self.table
-        elif isinstance(self.table, dict) and data_identifier:
-            return cast(str, self.table[data_identifier])
 
-        raise ValueError("No pod identifier provided for multi-pod datastructure.")
+class BitfountSchema(_BitfountSchemaMarshmallowMixIn):
+    """A schema that defines the features of a dataframe.
+
+    It lists all the (categorical, continuous, image, and
+    text) features found in the dataframe.
+
+    Args:
+        datasource: A `DataSource` to be provided to the
+            constructor instead of to the `add_datasource_features()`
+            method for brevity. Optional argument. Defaults to None.
+        **datasource_kwargs: Additional keyword arguments to pass to
+            `add_datasource_features()` alongside the datasource.
+
+    Attributes:
+           features: An ordered dictionary of features (column names).
+    """
 
-    def get_pod_identifiers(self) -> Optional[List[str]]:
-        """Returns a list of pod identifiers specified in the `table` attribute.
+    def __init__(
+        self, datasource: Optional[DataSource] = None, **datasource_kwargs: Any
+    ):
+        # ordered dictionaries of features (column names)
+        self.features: _FeatureDict = _FeatureDict()
+
+        # self._orig_hash is used to store the hash when loading a previously
+        # generated schema.
+        self._orig_hash: Optional[str] = None
+        self._datasource_hashes: List[str] = []
+        # Used to stop any more datasources from being added
+        self._frozen: bool = False
+
+        if datasource is not None:
+            self.add_datasource_features(datasource, **datasource_kwargs)
+
+    @property
+    def hash(self) -> str:
+        """The hash of this schema.
 
-        These may actually be logical pods, or datasources.
+        This relates to the DataSource(s) that were used in the generation of this
+        schema to assure that this schema is used against compatible data sources.
 
-        If there are no pod identifiers specified, returns None.
+        Returns:
+            A sha256 hash of the `_datasource_hashes`.
         """
-        if self.table is not None:
-            if isinstance(self.table, str):
-                return None
-            else:
-                pod_identifiers = list(self.table)
-        elif self.query is not None:
-            if isinstance(self.query, str):
-                return None
-            else:
-                pod_identifiers = list(self.query)
+        # Must be sorted to ensure ordering of DataSources being added doesn't
+        # change things.
+        frozen_hashes: str = str(sorted(self._datasource_hashes))
+        return _hash_str(frozen_hashes)
+
+    @staticmethod
+    def _dtype_based_stype_split(
+        data: _DataFrameType, ignore_cols: Optional[Sequence[str]] = None
+    ) -> Dict[SemanticType, List[str]]:
+        """Returns dictionary of Semantic types and the corresponding columns in `data`.
+
+        We first call `convert_dtypes` on the data to convert older pandas dtypes to
+        newer ones. This call is idempotent and allows us to identify the data type of
+        columns with the `object` dtype.
+        """
+        converted_data = data
+        if ignore_cols:
+            converted_data = converted_data.drop(columns=ignore_cols, errors="ignore")
+        dataframe_lib = _get_df_library_type(data)
+
+        if dataframe_lib == _DataFrameLib.PANDAS:
+            converted_data = converted_data.convert_dtypes()
         else:
-            return None
-        return pod_identifiers
+            # TODO: [BIT-1095] have a pyspark workaround for this `convert_dtypes` call
+            # when we switch to pyspark from koalas
+            # https://issues.apache.org/jira/browse/SPARK-37334
+            pass
+
+        semantic_types: Dict[SemanticType, List[str]] = {
+            stype: [] for stype in SemanticType
+        }
+
+        for col, typ in converted_data.dtypes.items():
+            if isinstance(typ, pd.StringDtype):
+                semantic_types[SemanticType.TEXT].append(col)
+            elif isinstance(typ, pd.BooleanDtype) or typ == bool:
+                semantic_types[SemanticType.CATEGORICAL].append(col)
+            elif is_numeric_dtype(typ):
+                # Booleans get interpereted as continuous so we must define them as
+                # categorical before this function is called
+                semantic_types[SemanticType.CONTINUOUS].append(col)
+            else:
+                # By default everything else will be interpreted as categorical.
+                # This should only happen for columns which remain as `object` because
+                # pandas is having trouble deciphering their true type
+                semantic_types[SemanticType.CATEGORICAL].append(col)
 
-    def get_columns_ignored_for_training(self, table_schema: TableSchema) -> List[str]:
-        """Adds all the extra columns that will not be used in model training.
+        return {k: v for k, v in semantic_types.items() if len(v) > 0}
+
+    def feature_names(self, semantic_type: Optional[SemanticType] = None) -> List[str]:
+        """Returns the names of all the features in the schema.
 
         Args:
-            table_schema: The schema of the table.
+            semantic_type (SemanticType, optional): if semantic type is provided, only
+                the feature names corresponding to the semantic type are returned.
+                Defaults to None.
 
         Returns:
-            ignore_cols_aux: A list of columns that will be ignored when
-                training a model.
+            features: A list of feature names.
         """
-        if self.selected_cols:
-            self.ignore_cols = [
-                feature
-                for feature in table_schema.get_feature_names()
-                if feature not in self.selected_cols
-            ]
-        ignore_cols_aux = self.ignore_cols[:]
-        ignore_cols_aux = _add_this_to_list(self.target, ignore_cols_aux)
-        ignore_cols_aux = _add_this_to_list(self.loss_weights_col, ignore_cols_aux)
-        ignore_cols_aux = _add_this_to_list(self.ignore_classes_col, ignore_cols_aux)
-        return ignore_cols_aux
-
-    def set_training_input_size(self, schema: TableSchema) -> None:
-        """Get the input size for model training.
-
-        Args:
-            schema: The schema of the table.
-            table_name: The name of the table.
-        """
-        self.input_size = len(
-            [
-                col
-                for col in schema.get_feature_names()
-                if col not in self.get_columns_ignored_for_training(schema)
-                and col not in schema.get_feature_names(SemanticType.TEXT)
-            ]
-        )
+        if semantic_type is not None:
+            stype = cast(_SemanticTypeValue, semantic_type.value)
+            if stype in self.features:
+                features = list(self.features[stype])
+            else:
+                logger.debug(f"There are no features with semantic type {stype}")
+                features = []
 
-    def set_training_column_split_by_semantic_type(self, schema: TableSchema) -> None:
-        """Sets the column split by type from the schema.
+        else:
+            features = [
+                feature_name
+                for stype in self.features
+                for feature_name in self.features[cast(_SemanticTypeValue, stype)]
+            ]
+        return features
 
-        This method splits the selected columns from the dataset
-        based on their semantic type.
+    def get_categorical_feature_size(self, var: Union[str, List[str]]) -> int:
+        """Gets the column dimensions.
 
         Args:
-            schema: The `TableSchema` for the data.
+            var: A column name or a list of column names for which
+                to get the dimensions.
+
+        Returns:
+            The number of unique value in the categorical column.
         """
-        if not self.selected_cols and not self.ignore_cols:
-            # If neither selected_cols or ignore_cols are provided,
-            # select all columns from schema,
-            self.selected_cols = schema.get_feature_names()
-        elif self.selected_cols:
-            # Make sure we set self.ignore_cols
-            self.ignore_cols = [
-                feature
-                for feature in schema.get_feature_names()
-                if feature not in self.selected_cols
-            ]
-        else:
-            # Make sure we set self.selected_cols
-            self.selected_cols = [
-                feature
-                for feature in schema.get_feature_names()
-                if feature not in self.ignore_cols
-            ]
-        if self.target and self.target not in self.selected_cols:
-            self.selected_cols = _add_this_to_list(self.target, self.selected_cols)
-        # Get the list of all columns ignored for training
-        ignore_cols_aux = self.get_columns_ignored_for_training(schema)
-
-        # Populate mapping of all feature names used in training
-        # together with the corresponding semantic type
-        for stype, features in schema.features.items():
-            columns_stype_list = list(cast(Dict[str, _SemanticTypeRecord], features))
-
-            # Iterating over `self.selected_cols` ensures we preserve the order that the
-            # user specified the columns
-            self.selected_cols_w_types[cast(_SemanticTypeValue, stype)] = [
-                col
-                for col in self.selected_cols
-                if (col in columns_stype_list and col not in ignore_cols_aux)
-            ]
-        # Add mapping to empty list for all stypes not present
-        # in the current datastructure
-        all_stypes = [stype.value for stype in SemanticType]
-        for stype in all_stypes:
-            if stype not in self.selected_cols_w_types:
-                self.selected_cols_w_types[cast(_SemanticTypeValue, stype)] = []
+        if isinstance(var, list):
+            var = var[0]
+
+        if "categorical" not in self.features:
+            raise ValueError("No categorical features.")
+        elif var not in self.features["categorical"]:
+            raise ValueError(f"{var} feature not found in categorical features.")
+
+        return self.features["categorical"][var].encoder.size
+
+    def _add_categorical_feature(
+        self,
+        name: str,
+        values: Union[np.ndarray, pd.Series, ks.Series],
+        dtype: Optional[Union[Dtype, np.dtype]] = None,
+        description: Optional[str] = None,
+    ) -> None:
+        """Adds the given categorical, with list of values to the schema."""
+        if (
+            "categorical" not in self.features
+            or name not in self.features["categorical"]
+        ):
+
+            CategoricalRecord.add_record_to_schema(
+                self,
+                feature_name=name,
+                dtype=dtype,
+                description=description,
+            )
+        self.features["categorical"][name].encoder.add_values(values)
 
-        # Get the number of images present in the datastructure.
-        self.number_of_images = len(self.image_cols) if self.image_cols else 0
+    def _combine_existing_stypes_with_forced_stypes(
+        self,
+        existing_stypes: MutableMapping[SemanticType, List[str]],
+        forced_stype: MutableMapping[_SemanticTypeValue, List[str]],
+        dataframe: _DataFrameType,
+    ) -> MutableMapping[SemanticType, List[str]]:
+        """Combine the exiting semantic types with the forced semantic types."""
+        for new_stype, feature_list in forced_stype.items():
+            try:
+                stype = SemanticType(new_stype)
+
+                if stype not in existing_stypes.keys():
+                    existing_stypes[stype] = []
+                existing_stypes[stype] = _add_this_to_list(
+                    feature_list, existing_stypes[stype]
+                )
+            except ValueError:
+                logger.warning(
+                    f"Given semantic type {new_stype} is not currently supported. "
+                    f"Defaulting to split based on dtype."
+                )
+                dtype_features = self._dtype_based_stype_split(
+                    dataframe[feature_list], []
+                )
+                stype = list(dtype_features.keys())[0]
+                if stype not in existing_stypes.keys():
+                    existing_stypes[stype] = []
+                existing_stypes[stype] = _add_this_to_list(
+                    feature_list, existing_stypes[stype]
+                )
+        return existing_stypes
 
-    def set_columns_after_transformations(
-        self, transforms: List[Dict[str, _JSONDict]]
+    def _add_dataframe_features(
+        self,
+        dataframe: _DataFrameType,
+        ignore_cols: List[str],
+        force_stype: MutableMapping[_SemanticTypeValue, List[str]],
+        descriptions: Mapping[str, str],
     ) -> None:
-        """Updates the selected/ignored columns based on the transformations applied.
+        """Add given dataframe to the schema.
+
+        Adds all the features in the dataframe to the schema, using the dtype to decide
+        the semantic type of the feature.
+        """
+        for item in force_stype.values():
+            ignore_cols = _add_this_to_list(item, ignore_cols)
+        inferred_semantic_types = self._dtype_based_stype_split(dataframe, ignore_cols)
+        semantic_types = self._combine_existing_stypes_with_forced_stypes(
+            inferred_semantic_types, force_stype, dataframe
+        )
+        for stype, features in semantic_types.items():
+            # Sort the list of features.
+            # This ensures they are added in deterministic order.
+            features.sort()
+            for feature_name in features:
+                dtype = dataframe.dtypes[feature_name]
+                description = descriptions.get(feature_name)
+                if stype == SemanticType.TEXT:
+                    if feature_name not in self.feature_names():
+                        TextRecord.add_record_to_schema(
+                            self,
+                            feature_name=feature_name,
+                            dtype=dtype,
+                            description=description,
+                        )
+                elif stype == SemanticType.CONTINUOUS:
+                    if feature_name not in self.feature_names():
+                        ContinuousRecord.add_record_to_schema(
+                            self,
+                            feature_name=feature_name,
+                            dtype=dtype,
+                            description=description,
+                        )
 
-        It updates `self.selected_cols` by adding on the new names of columns after
-        transformations are applied, and removing the original columns unless
-        explicitly specified to keep.
+                elif stype == SemanticType.CATEGORICAL:
+                    self._add_categorical_feature(
+                        name=feature_name,
+                        dtype=dtype,
+                        values=dataframe[feature_name],
+                        description=description,
+                    )
+                elif stype == SemanticType.IMAGE:
+                    if (
+                        "image" not in self.features
+                        or feature_name not in self.features["image"]
+                    ):
+                        ImageRecord.add_record_to_schema(
+                            self,
+                            feature_name=feature_name,
+                            dtype=dataframe.dtypes[feature_name],
+                            description=descriptions.get(feature_name),
+                        )
+                    record = self.features["image"][feature_name]
+                    for img in dataframe[feature_name]:
+                        im = Image.open(img)
+                        record.dimensions[im.size] += 1
+                        record.modes[im.mode] += 1
+                        record.formats[im.format] += 1
+
+    def categorical_feature_sizes(
+        self, ignore_cols: Optional[Union[str, List[str]]] = None
+    ) -> List[int]:
+        """Returns a list of categorical feature sizes.
 
         Args:
-            transforms: A list of transformations to be applied to the data.
+            ignore_cols: The column(s) to be ignored from the schema.
         """
-        for tfm in transforms:
-            for key, value in tfm.items():
-                if key == "convert_to":
-                    # Column name doesn't change if we only convert type.
-                    pass
-                else:
-                    # Check to see if any original columns are marked to keep
-                    original_cols_to_keep = value.get("keep_original", [])
+        if not ignore_cols:
+            ignore_cols = []
+        elif isinstance(ignore_cols, str):
+            ignore_cols = [ignore_cols]
+        return [
+            self.get_categorical_feature_size(var)
+            for var in self.feature_names(SemanticType.CATEGORICAL)
+            if var not in ignore_cols
+        ]
 
-                    # Make a list of all the columns to be discarded
-                    if isinstance(value["col"], str):
-                        value["col"] = [value["col"]]
-                    discard_columns = [
-                        col for col in value["col"] if col not in original_cols_to_keep
-                    ]
-                    new_columns = [f"{col}_{key}" for col in value["col"]]
-                    # Error raised in the pods if we set both ignore_cols
-                    # and selected_cols here.
-                    if self.selected_cols:
-                        self.selected_cols.extend(new_columns)
-                    else:
-                        self.ignore_cols.extend(discard_columns)
-                    self.selected_cols = [
-                        col for col in self.selected_cols if col not in discard_columns
-                    ]
+    def _expand_dataframe(self, dataframe: _DataFrameType) -> _DataFrameType:
+        """Expands dataframe to include missing columns specified in the schema.
 
-    def apply_dataset_transformations(self, datasource: BaseSource) -> BaseSource:
-        """Applies transformations to whole dataset.
+        Simply adds columns populated with default values: 'nan' for categorical
+        and text columns and '0' for continuous columns.
 
         Args:
-            datasource: The `BaseSource` object to be transformed.
+            dataframe (DataFrameType): dataframe without all the required columns
 
         Returns:
-            datasource: The transformed datasource.
+            DataFrameType: dataframe that includes all the required columns
+
+        Raises:
+            BitfountSchemaError: if there is a missing image column as this cannot be
+                replicated.
         """
-        if self.dataset_transforms:
-            # TODO: [BIT-1167] Process dataset transformations
-            raise NotImplementedError()
+        missing_categorical_value = "nan"
+        missing_text_value = "nan"
+        missing_continuous_value = 0
+        columns = list(dataframe.columns)
+
+        for stype in self.features:
+            # Iterate through semantic types
+            for feature_name in self.features[cast(_SemanticTypeValue, stype)]:
+                # Iterate through each feature in given semantic type
+                if feature_name not in columns:
+                    # If feature is not present in the given dataframe, add that feature
+                    # with a dummy value to the dataframe
+                    logger.debug(
+                        f"Feature present in schema but missing in data: {feature_name}"
+                    )
+                    if stype == SemanticType.IMAGE.value:
+                        raise BitfountSchemaError(
+                            f"Missing image feature {feature_name} in dataframe. "
+                            "Unable to apply schema to this dataframe"
+                        )
+                    elif stype == SemanticType.TEXT.value:
+                        dataframe[feature_name] = missing_text_value
+                    elif stype == SemanticType.CONTINUOUS.value:
+                        dataframe[feature_name] = missing_continuous_value
+                    elif stype == SemanticType.CATEGORICAL.value:
+                        dataframe[feature_name] = missing_categorical_value
+                        # adds the missing categorical value (i.e. 'nan') to the encoder
+                        # for the missing categorical feature
+                        self._add_categorical_feature(
+                            name=feature_name,
+                            values=np.array([missing_categorical_value]),
+                        )
+        return dataframe
 
-        return datasource
+    def _reduce_dataframe(
+        self, dataframe: _DataFrameType, keep_cols: Optional[List[str]] = None
+    ) -> _DataFrameType:
+        """Drops any columns that are not part of the schema.
 
-    def get_batch_transformations(self) -> Optional[List[BatchTimeOperation]]:
-        """Returns batch transformations to be performed as callables.
+        Args:
+            dataframe (DataFrameType): dataframe which includes extra columns
+            keep_cols (Optional[List[str]]): optional list of columns to keep even if
+                they are not part of the schema. Defaults to None.
 
         Returns:
-            A list of batch transformations to be passed to
-                TransformationProcessor.
+            DataFrameType: dataframe with extra columns removed
         """
-        if self.batch_transforms is not None:
-            parser = TransformationsParser()
-            transformations, _ = parser.deserialize_transformations(
-                self.batch_transforms
-            )
-            return cast(List[BatchTimeOperation], transformations)
-        return None
+        cols_to_keep = self.feature_names()
+        cols_to_keep = _add_this_to_list(keep_cols, cols_to_keep)
+        return dataframe[cols_to_keep]
 
-    def _validate_schema_features(self) -> None:
-        """Validate that the override contains encoding of categorical features."""
-        self.schema_types_override = cast(
-            Union[SchemaOverrideMapping, Mapping[str, SchemaOverrideMapping]],
-            self.schema_types_override,
-        )
-        # If we validate the schema features, then it means it must be a mapping
+    def _apply_types(self, dataframe: _DataFrameType) -> _DataFrameType:
+        """Applies the prescribed feature types to the dataframe.
 
-        if isinstance(self.query, str):
-            # That means that the datastructure is intended for local training.
-            if "categorical" in self.schema_types_override:
-                # For local training, categorical features should be
-                # defined inside a dictionary.
-                for item in self.schema_types_override["categorical"]:
-                    if not isinstance(item, dict):
-                        raise DataStructureError(
-                            "Categorical features should be defined as a dictionary "
-                            "with the encodings of each attribute defined. "
-                            "Please specify encodings."
-                        )
-        elif isinstance(self.query, dict):
-            for pod_id in self.query:
-                if pod_id not in self.schema_types_override:
-                    raise DataStructureError(
-                        f"The pod id:{pod_id} given to the query was "
-                        f"not found in the schema override. Please "
-                        f"update the schema override types with the pod_id "
-                        f"in which the schema types override will be applied. "
-                    )
-            for overrides in self.schema_types_override.values():
-                overrides = cast(SchemaOverrideMapping, overrides)
-                if "categorical" in overrides:
-                    for item in overrides["categorical"]:
-                        if not isinstance(item, dict):
-                            raise DataStructureError(
-                                "Categorical features should be defined as a "
-                                "dictionary with the encodings of each attribute "
-                                "defined. Please specify encodings."
-                            )
+        Args:
+            dataframe (DataFrameType): dataframe with varied types
 
-    def _override_schema(
-        self,
-        datasource: Optional[BaseSource] = None,
-        data_identifier: Optional[str] = None,
-    ) -> TableSchema:
-        """Method to override the pod/datasource schema when a sql query is given.
+        Returns:
+            DataFrameType: dataframe with types that are specified in the schema
+        """
+        types: Dict[str, Union[Dtype, np.dtype]] = {
+            feature_name: record.dtype
+            for stype in self.features
+            for feature_name, record in self.features[
+                cast(_SemanticTypeValue, stype)
+            ].items()
+        }
+
+        if "categorical" in self.features:
+            types.update(
+                {
+                    feature_name: record.encoder.dtype
+                    for feature_name, record in self.features["categorical"].items()
+                }
+            )
+
+        return dataframe.astype(types)
+
+    def _encode_dataframe(self, data: _DataFrameType) -> _DataFrameType:
+        """Encodes the dataframe categorical columns according to the schema.
 
         Args:
-            datasource: The datasource in question.
-            data_identifier: The pod/logical pod/datasource identifier to override
-                in the datastructure.
-        """
-        self.schema_types_override = cast(
-            Union[SchemaOverrideMapping, Mapping[str, SchemaOverrideMapping]],
-            self.schema_types_override,
-        )
-        if isinstance(self.query, str):
-            feature_overrides = cast(SchemaOverrideMapping, self.schema_types_override)
-        elif isinstance(self.query, dict) and data_identifier:
-            feature_overrides = cast(
-                SchemaOverrideMapping, self.schema_types_override[data_identifier]  # type: ignore[index] # Reason: An error will be caught by the _validate_schema_features. # noqa: B950
-            )
-        else:
-            raise ValueError(
-                "No query or dictionary of pod_identifiers to queries was given."
-            )
+            data (DataFrameType): the dataframe to be encoded
 
-        table_name = "data"
-        table_dtypes = None
-        if datasource and datasource._data_is_loaded:
-            # Data is loaded into a dataframe by the time we get here,
-            # so we use the DataFrameSource to get the dtypes.
-            table_dtypes = datasource.get_dtypes()
-
-        self.selected_cols = []
-        schema = TableSchema(name=table_name)
-        self.table = table_name
-        # At this point feature_overrides should be a mapping
-        # from column names to semantic types.
-        for s_type, feature_list in feature_overrides.items():
-            stype = SemanticType(s_type)
-            if stype == SemanticType.CONTINUOUS:
-                for feature_name in feature_list:
-                    # Feature_name should be string for continuous attributes
-                    feature_name = cast(str, feature_name)
-                    if table_dtypes:
-                        dtype = table_dtypes[feature_name]
-                    else:
-                        # This will only be used for setting the model parameters.
-                        dtype = "int"
-                    self.selected_cols.append(feature_name)
-                    ContinuousRecord.add_record_to_schema(
-                        schema,
-                        feature_name=feature_name,
-                        dtype=dtype,
+        Raises:
+            ValueError: if the encoder fails to encode a particular column
+
+        Returns:
+            DataFrameType: the dataframe with the categorical columns encoded
+        """
+        if "categorical" in self.features:
+            for feature_name, record in self.features["categorical"].items():
+                if feature_name not in data:
+                    logger.warning(
+                        f"Column {feature_name} is not in the dataframe. "
+                        "Skipping encoding"
                     )
-            elif stype == SemanticType.IMAGE:
-                for feature_name in feature_list:
-                    # Feature_name should be string for image attributes
-                    feature_name = cast(str, feature_name)
-                    if table_dtypes:
-                        dtype = table_dtypes[feature_name]
-                    else:
-                        # This will only be used for setting the model parameters.
-                        dtype = "image"
-                    self.selected_cols.append(feature_name)
-                    if (
-                        "image" not in schema.features
-                        or feature_name not in schema.features["image"]
-                    ):
-                        ImageRecord.add_record_to_schema(
-                            schema,
-                            feature_name=feature_name,
-                            dtype=dtype,
-                        )
-            elif stype == SemanticType.TEXT:
-                for feature_name in feature_list:
-                    # Feature_name should be string for text attributes
-                    feature_name = cast(str, feature_name)
-                    if table_dtypes:
-                        dtype = table_dtypes[feature_name]
-                    else:
-                        # This will only be used for setting the model parameters.
-                        dtype = "str"
-                    self.selected_cols.append(feature_name)
-                    TextRecord.add_record_to_schema(
-                        schema,
-                        feature_name=feature_name,
-                        dtype=dtype,
+                    continue
+                try:
+                    data[feature_name] = record.encoder.transform(data[feature_name])
+                except ValueError as err:
+                    raise ValueError(
+                        f"Could not encode column {feature_name}: {str(err)}"
                     )
-            elif stype == SemanticType.CATEGORICAL:
-                for feature in feature_list:
-                    # For categorical attributes, the `feature` here will be a dict.
-                    feature_name, encodings = next(iter(cast(dict, feature).items()))
-                    feature_name = cast(str, feature_name)
-                    if table_dtypes:
-                        dtype = table_dtypes[feature_name]
-                    else:
-                        # This will only be used for setting the model parameters.
-                        dtype = "str"
-                    self.selected_cols.append(feature_name)
-                    CategoricalRecord.add_record_to_schema(
-                        schema,
-                        feature_name=feature_name,
-                        dtype=dtype,
-                    )
-                    schema.features["categorical"][
-                        feature_name
-                    ].encoder.add_values_with_encoding(encodings)
-        return schema
+        else:
+            logger.info("No encoding to be done as there are no categorical features.")
 
-    def get_table_schema(
-        self,
-        schema: BitfountSchema,
-        data_identifier: Optional[str] = None,
-        datasource: Optional[BaseSource] = None,
-    ) -> TableSchema:
-        """Returns the table schema based on the datastructure arguments.
-
-        This will return either the new schema defined by the schema_types_override
-        if the datastructure has been initialised with a query, or the relevant table
-        schema if the datastructure has been initialised with a table name.
-
-        Args:
-            schema: The BitfountSchema either taken from the pod or provided by
-                the user when defining a model.
-            data_identifier: The pod/logical pod/datasource identifier on which the
-                model will be trained on. Defaults to None.
-            datasource: The datasource on which the model will be trained on.
-                Defaults to None.
+        return data
 
-        Raises:
-            BitfountSchemaError: If the table is not found.
+    def freeze(self) -> None:
+        """Freezes the schema, ensuring no more datasources can be added.
+
+        If this schema was loaded from an already generated schema, this will
+        also check that the schema is compatible with the datasources set.
         """
-        if self.query:
-            # If the datastructure is given a query, then we use the schema override.
-            if datasource and isinstance(datasource, DatabaseSource):
-                if isinstance(self.query, dict) and data_identifier:
-                    datasource.datastructure_query = self.query[data_identifier]
-                elif isinstance(self.query, str):
-                    datasource.datastructure_query = self.query
-            table_schema = self._override_schema(
-                data_identifier=data_identifier, datasource=datasource
+        self._frozen = True
+        if self._orig_hash and self.hash != self._orig_hash:
+            raise BitfountSchemaError(
+                "This schema was generated against a different set of datasources "
+                "and is incompatible with those selected. This may be due to "
+                "changing column names or types. Please generate a new schema."
             )
-        else:  # if self.table:
-            # If the datastructure is given a table name, then we get the table schema.
-            table_schema = schema.get_table_schema(self.get_table_name(data_identifier))
-        return table_schema
 
-    def _update_datastructure_with_hub_identifiers(
-        self, hub_pod_ids: List[str]
+    def unfreeze(self) -> None:
+        """Unfreezes the schema, allowing more datasources to be added."""
+        self._frozen = False
+
+    def add_datasource_features(
+        self,
+        datasource: DataSource,
+        ignore_cols: Optional[Sequence[str]] = None,
+        force_stype: Optional[MutableMapping[_SemanticTypeValue, List[str]]] = None,
+        descriptions: Optional[Mapping[str, str]] = None,
     ) -> None:
-        """Update the pod_ids with the hub ids, containing username."""
-        if self.table and isinstance(self.table, dict):
-            self.table = dict(zip(hub_pod_ids, self.table.values()))
-        elif (
-            self.query
-            and isinstance(self.query, dict)
-            and self.schema_types_override is not None
-        ):
-            # By this point if the datastructure has a query,
-            # it will include the pod_ids for both schema override and query.
-            self.query = dict(zip(hub_pod_ids, self.query.values()))
-            self.schema_types_override = cast(
-                Mapping[str, SchemaOverrideMapping],
-                dict(
-                    zip(
-                        hub_pod_ids,
-                        self.schema_types_override.values(),
-                    )
-                ),
+        """Adds datasource features to schema.
+
+        Args:
+            datasource: The `DataSource` whose features this method adds.
+            ignore_cols: Columns to ignore from the `DataSource`. Defaults to None.
+            force_stype: Columns for which to change the semantic type.
+                Format: semantictype: [columnnames]. Defaults to None.
+                Example: {'categorical': ['target_column'],
+                        'continuous': ['age', 'salary']}
+            descriptions: Descriptions of the features. Defaults to None.
+
+        Raises:
+            BitfountSchemaError: if the schema is already frozen
+        """
+        if not self._frozen:
+            # Add this datasource to the hash computation list
+            self._datasource_hashes.append(datasource.hash)
+
+            if ignore_cols is None:
+                ignore_cols_aux = []
+            else:
+                ignore_cols_aux = list(ignore_cols)
+
+            if force_stype is None:
+                force_stype = {}
+
+            for dataframe in [
+                datasource.train_set,
+                datasource.validation_set,
+                datasource.test_set,
+            ]:
+                self._add_dataframe_features(
+                    dataframe=dataframe,
+                    ignore_cols=ignore_cols_aux,
+                    force_stype=force_stype,
+                    descriptions=descriptions if descriptions is not None else {},
+                )
+        else:
+            raise BitfountSchemaError(
+                "This schema is frozen. No more datasources can be added."
             )
+
+    def apply(
+        self, dataframe: _DataFrameType, keep_cols: Optional[List[str]] = None
+    ) -> _DataFrameType:
+        """Applies the schema to a dataframe and returns the transformed dataframe.
+
+        Sequentially adds missing columns to the dataframe, removes superfluous columns
+        from the dataframe, changes the types of the columns in the dataframe and
+        finally encodes the categorical columns in the dataframe before returning the
+        transformed dataframe.
+
+        Args:
+            dataframe: The dataframe to transform.
+            keep_cols: A list of columns to keep even if
+                they are not part of the schema. Defaults to None.
+
+        Returns:
+            The dataframe with the transformations applied.
+        """
+        dataframe = self._expand_dataframe(dataframe)
+        dataframe = self._reduce_dataframe(dataframe, keep_cols=keep_cols)
+        dataframe = self._apply_types(dataframe)
+        dataframe = self._encode_dataframe(dataframe)
+
+        return dataframe
+
+    def __eq__(self, other: Any) -> bool:
+        """Compare two BitfountSchema objects for equality.
+
+        For two schemas to be equal they must have the same set of features,
+        including names and types. This does not require them to have come from
+        the same data source though (i.e. their hashes might be different).
+
+        Args:
+            other: The other object to compare against.
+
+        Returns:
+            True if equal, False otherwise.
+        """
+        # Check if exact same object
+        if self is other:
+            return True
+
+        # Check comparable types
+        if not isinstance(other, BitfountSchema):
+            return False
+
+        def extract_features_and_types(
+            schema: BitfountSchema,
+        ) -> Dict[str, Dict[str, Tuple[Union[Dtype, np.dtype], SemanticType]]]:
+            # Extract types from features
+            return {
+                feature_type: {
+                    feature_name: (record.dtype, record.semantic_type)
+                    for feature_name, record in cast(
+                        Dict[str, _SemanticTypeRecord], records_dict
+                    ).items()
+                }
+                for feature_type, records_dict in schema.features.items()
+            }
+
+        # Check features and their types
+        if extract_features_and_types(self) != extract_features_and_types(other):
+            return False
+
+        # Otherwise, equal for our purposes
+        return True
+
+
+class BitfountSchemaError(BitfountError):
+    """Errors related to BitfountSchema."""
+
+    pass
```

### Comparing `bitfount-0.5.86/bitfount/data/helper.py` & `bitfount-0.5.9/bitfount/data/helper.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 """Helper functions concerning data."""
 
-from bitfount.data.dataloaders import BitfountDataLoader
+from bitfount.data.dataloader import _BitfountDataLoader
 
 
-def convert_epochs_to_steps(epochs: int, dataloader: BitfountDataLoader) -> int:
+def convert_epochs_to_steps(epochs: int, dataloader: _BitfountDataLoader) -> int:
     """Converts number of epochs into number of steps.
 
     Each step represents a minibatch. Ensure provided dataloader supports batching.
 
     Args:
         epochs: An integer denoting the epoch number.
         dataloader: An instance of a Bitfount DataLoader.
```

### Comparing `bitfount-0.5.86/bitfount/federated/aggregators/__init__.py` & `bitfount-0.5.9/bitfount/federated/aggregators/__init__.py`

 * *Files identical despite different names*

### Comparing `bitfount-0.5.86/bitfount/federated/algorithms/column_avg.py` & `bitfount-0.5.9/bitfount/federated/algorithms/column_avg.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,99 +1,95 @@
 """Column averaging algorithm."""
 from __future__ import annotations
 
-from typing import Any, ClassVar, Dict, Mapping, Optional
+import os
+from typing import Any, Dict, List, Optional, Type, Union
 
-from marshmallow import fields
+from marshmallow import Schema as MarshmallowSchema
+from marshmallow import post_load
 import numpy as np
-import numpy.typing as npt
 
-from bitfount.data.datasources.base_source import BaseSource
+from bitfount.data.datasource import DataSource
 from bitfount.federated.algorithms.base import (
-    BaseAlgorithmFactory,
-    BaseModellerAlgorithm,
-    BaseWorkerAlgorithm,
+    _BaseAlgorithmFactory,
+    _BaseAlgorithmSchema,
+    _BaseModellerAlgorithm,
+    _BaseWorkerAlgorithm,
 )
 from bitfount.federated.logging import _get_federated_logger
 from bitfount.federated.privacy.differential import DPPodConfig
-from bitfount.types import T_FIELDS_DICT
-from bitfount.utils import delegates
 
 logger = _get_federated_logger(__name__)
 
 
-class _ModellerSide(BaseModellerAlgorithm):
+class _ModellerSide(_BaseModellerAlgorithm):
     """Modeller side of the ColumnAverage algorithm."""
 
     def initialise(
         self,
-        task_id: Optional[str] = None,
+        pretrained_file: Optional[Union[str, os.PathLike]] = None,
         **kwargs: Any,
     ) -> None:
         """Nothing to initialise here."""
         pass
 
-    def run(
-        self, results: Mapping[str, Dict[str, float]]
-    ) -> Dict[str, Dict[str, float]]:
+    def run(self, results: List[Dict[str, float]]) -> List[Dict[str, float]]:
         """Simply returns results."""
-        return dict(results)
+        return results
 
 
-class _WorkerSide(BaseWorkerAlgorithm):
+class _WorkerSide(_BaseWorkerAlgorithm):
     """Worker side of the ColumnAverage algorithm."""
 
-    def __init__(self, *, field: str, table_name: str, **kwargs: Any) -> None:
-        self.datasource: BaseSource
+    def __init__(self, *, field: str, **kwargs: Any) -> None:
+        self.datasource: DataSource
         self.field = field
-        self.table_name = table_name
         super().__init__(**kwargs)
 
     def initialise(
         self,
-        datasource: BaseSource,
+        datasource: DataSource,
         pod_dp: Optional[DPPodConfig] = None,
         **kwargs: Any,
     ) -> None:
         """Sets Datasource."""
         self.datasource = datasource
 
-    def run(self) -> Dict[str, npt.NDArray[np.float64]]:
-        """Returns the mean of the field in `BaseSource` dataframe."""
-        mean = np.mean(
-            self.datasource.get_column(col_name=self.field, table_name=self.table_name)
-        )
-        return {"mean": np.array(mean)}
+    def run(self) -> Dict[str, float]:
+        """Returns the mean of the field in `DataSource` dataframe."""
+        mean = np.mean(self.datasource.data[self.field], dtype=np.float64)
+        return {"mean": float(mean)}
 
 
-@delegates()
-class ColumnAverage(BaseAlgorithmFactory):
+class ColumnAverage(_BaseAlgorithmSchema, _BaseAlgorithmFactory):
     """Simple algorithm for taking the arithmetic mean of a column in a table.
 
     Args:
         field: The name of the column to take the mean of.
-        table_name: The name of the table on which column
-            average will be performed on.
 
     Attributes:
+        name: The name of the algorithm.
         field: The name of the column to take the mean of.
-        table_name: The name of the table on which column
-            average will be performed on.
     """
 
-    fields_dict: ClassVar[T_FIELDS_DICT] = {
-        "field": fields.Str(required=True),
-        "table_name": fields.Str(required=True),
-    }
-
-    def __init__(self, *, field: str, table_name: str, **kwargs: Any):
-        super().__init__(**kwargs)
+    def __init__(self, field: str):
+        super().__init__()
         self.field = field
-        self.table_name = table_name
 
     def modeller(self, **kwargs: Any) -> _ModellerSide:
         """Returns the modeller side of the ColumnAverage algorithm."""
         return _ModellerSide(**kwargs)
 
     def worker(self, **kwargs: Any) -> _WorkerSide:
         """Returns the worker side of the ColumnAverage algorithm."""
-        return _WorkerSide(field=self.field, table_name=self.table_name, **kwargs)
+        return _WorkerSide(field=self.field, **kwargs)
+
+    @staticmethod
+    def get_schema(**kwargs: Any) -> Type[MarshmallowSchema]:
+        """Returns the schema for ColumnAverage."""
+
+        class Schema(_BaseAlgorithmFactory._Schema):
+            @post_load
+            def recreate_factory(self, data: dict, **_kwargs: Any) -> ColumnAverage:
+                return ColumnAverage(**data)
+
+        return Schema
```

### Comparing `bitfount-0.5.86/bitfount/federated/algorithms/model_algorithms/base.py` & `bitfount-0.5.9/bitfount/federated/algorithms/model_algorithms/federated_training.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,232 +1,207 @@
-"""Base classes for all model-based algorithms.
-
-Attributes:
-    registry: A read-only dictionary of model algorithm factory names to their
-        implementation classes.
-"""
+"""Algorithm to train a model remotely and return its parameters."""
 from __future__ import annotations
 
-from abc import ABC
-import inspect
-import os
-from types import MappingProxyType
-from typing import (
-    TYPE_CHECKING,
-    Any,
-    ClassVar,
-    Dict,
-    Generic,
-    Mapping,
-    Optional,
-    Type,
-    Union,
-    cast,
-)
+from typing import TYPE_CHECKING, Any, Dict, Mapping, Optional, Tuple, Type, cast
+
+from marshmallow import Schema as MarshmallowSchema
+from marshmallow import fields, post_load
 
-from bitfount.data.datasources.base_source import BaseSource
-from bitfount.federated.algorithms.base import (
-    BaseAlgorithmFactory,
-    BaseModellerAlgorithm,
-    BaseWorkerAlgorithm,
-    _BaseAlgorithm,
+from bitfount.data.datasource import DataSource
+from bitfount.federated.algorithms.base import _BaseAlgorithmFactory
+from bitfount.federated.algorithms.model_algorithms.base import (
+    _BaseModelAlgorithm,
+    _BaseModelAlgorithmFactory,
+    _BaseModellerModelAlgorithm,
+    _BaseWorkerModelAlgorithm,
 )
-from bitfount.federated.helper import TaskContext
 from bitfount.federated.logging import _get_federated_logger
-from bitfount.federated.model_reference import BitfountModelReference
-from bitfount.federated.privacy.differential import DPPodConfig
-from bitfount.models.base_models import MAIN_MODEL_REGISTRY
-from bitfount.schemas.utils import bf_dump
-from bitfount.types import (
-    T_DTYPE,
-    T_FIELDS_DICT,
-    T_NESTED_FIELDS,
-    DistributedModelProtocol,
-    _BaseSerializableObjectMixIn,
-    _SerializedWeights,
-    _StrAnyDict,
-)
+from bitfount.federated.shim import BackendTensorShim
+from bitfount.hub.api import BitfountHub
+from bitfount.types import DistributedModelProtocol, _SerializedWeights, _WeightDict
 
 if TYPE_CHECKING:
-    from bitfount.hub.api import BitfountHub
     from bitfount.types import _DistributedModelTypeOrReference
 
-
 logger = _get_federated_logger(__name__)
 
 
-class _BaseModelAlgorithm(Generic[T_DTYPE], _BaseAlgorithm, ABC):
-    """Blueprint for either the modeller side or the worker side of ModelAlgorithm."""
+class _BaseModelTrainingMixIn(_BaseModelAlgorithm):
+    """Shared methods/attributes for both modeller and worker."""
 
-    def __init__(
-        self,
-        *,
-        model: DistributedModelProtocol[T_DTYPE],
-        pretrained_file: Optional[Union[str, os.PathLike]] = None,
-        **kwargs: Any,
-    ):
+    # This is set in the base model algorithm
+    model: DistributedModelProtocol
+
+    def __init__(self, **kwargs: Any):
         super().__init__(**kwargs)
-        self.model = model
-        self.pretrained_file = pretrained_file
 
     @property
-    def tensor_precision(self) -> T_DTYPE:
-        """Returns model tensor precision."""
-        return cast(T_DTYPE, self.model.tensor_precision())
+    def epochs(self) -> Optional[int]:
+        """Returns model epochs."""
+        return self.model.epochs
 
-
-class _BaseModellerModelAlgorithm(_BaseModelAlgorithm, BaseModellerAlgorithm, ABC):
-    """Modeller side of the algorithm."""
+    @property
+    def steps(self) -> Optional[int]:
+        """Returns model steps."""
+        return self.model.steps
+
+    def get_param_states(self) -> _WeightDict:
+        """Returns the current parameters of the underlying model."""
+        return self.model.get_param_states()
+
+    def apply_update(self, update: _WeightDict) -> _WeightDict:
+        """Applies a parameter update to the underlying model."""
+        return self.model.apply_weight_updates([update])
+
+    def update_params(self, params: _WeightDict) -> None:
+        """Updates model parameters."""
+        return self.model.update_params(params)
+
+
+class _ModellerSide(
+    _BaseModelTrainingMixIn,
+    _BaseModellerModelAlgorithm,
+):
+    """Modeller side of the FederatedModelTraining algorithm."""
 
     def __init__(
         self,
         *,
         model: DistributedModelProtocol,
-        modeller_checkpointing: bool = True,
-        checkpoint_filename: Optional[str] = None,
-        pretrained_file: Optional[Union[str, os.PathLike]] = None,
         **kwargs: Any,
     ):
-        super().__init__(model=model, pretrained_file=pretrained_file, **kwargs)
-        self.modeller_checkpointing = modeller_checkpointing
-        self.checkpoint_filename = checkpoint_filename
+        super().__init__(model=model, **kwargs)
 
-    def initialise(
+    def run(
         self,
-        task_id: Optional[str] = None,
-        **kwargs: Any,
-    ) -> None:
-        """Initialises the algorithm as required."""
-        if not self.model.initialised:
-            self.model.initialise_model(context=TaskContext.MODELLER)
-        if not self.checkpoint_filename:
-            self.checkpoint_filename = task_id
-        # This needs to occur AFTER model initialization so the model is correctly
-        # created. deserialize() may cause initialization but we can not rely on it
-        # in this instance because we need to pass in context information.
-        # This should be reviewed as part of [BIT-536].
-        if self.pretrained_file is not None:
-            logger.info(f"Deserializing model from {self.pretrained_file}.")
-            self.model.deserialize(self.pretrained_file, **kwargs)
-
+        update: Optional[_WeightDict] = None,
+        validation_metrics: Optional[Mapping[str, float]] = None,
+    ) -> _SerializedWeights:
+        """Takes a weight update, applies it and returns the new model parameters."""
+        if update is not None:
+            self.apply_update(update)
+        nn_params: Dict[str, Any] = self.get_param_states()
+        backend_tensor_shim: BackendTensorShim = self.model.backend_tensor_shim()
+        for name, param in nn_params.items():
+            nn_params[name] = backend_tensor_shim.to_list(param)
+        if validation_metrics:
+            for key, value in validation_metrics.items():
+                self.model.log_(key, value, on_epoch=True, prog_bar=True, logger=True)
+        return cast(_SerializedWeights, nn_params)
+
+
+class _WorkerSide(
+    _BaseModelTrainingMixIn,
+    _BaseWorkerModelAlgorithm,
+):
+    """Worker side of the FederatedModelTraining algorithm."""
 
-class _BaseWorkerModelAlgorithm(_BaseModelAlgorithm, BaseWorkerAlgorithm, ABC):
-    """Worker side of the algorithm."""
-
-    def __init__(self, *, model: DistributedModelProtocol, **kwargs: Any):
-        super().__init__(model=model, **kwargs)
-
-    def initialise(
+    def __init__(
         self,
-        datasource: BaseSource,
-        pod_dp: Optional[DPPodConfig] = None,
-        model_params: Optional[_SerializedWeights] = None,
+        *,
+        model: DistributedModelProtocol,
         **kwargs: Any,
-    ) -> None:
-        """Initialises the algorithm as required."""
-        # Apply pod DP settings if needed. Needs to occur before model
-        # initialization so the right DP settings are applied during initialization.
-        self._apply_pod_dp(pod_dp)
-        self.model.initialise_model(data=datasource, context=TaskContext.WORKER)
-        if model_params:
-            tensor_model_params = self.model.deserialize_params(model_params)
-            self.model.update_params(tensor_model_params)
+    ):
+        super().__init__(model=model, **kwargs)
 
-    def _apply_pod_dp(self, pod_dp: Optional[DPPodConfig]) -> None:
-        """Applies pod-level Differential Privacy constraints if supported.
+    def run(
+        self,
+        data: DataSource,
+        model_params: _SerializedWeights,
+        iterations: int,
+    ) -> Tuple[_WeightDict, Optional[Dict[str, str]]]:
+        """Takes the model parameters, trains and returns the parameter update."""
+        backend_tensor_shim: BackendTensorShim = self.model.backend_tensor_shim()
+        for name, param in model_params.items():
+            model_params[name] = backend_tensor_shim.to_tensor(param)
+
+        tensor_model_params = cast(_WeightDict, model_params)
+        self.update_params(tensor_model_params)
+
+        # Train for one federated round - `iterations` many steps or epochs
+        # TODO: [BIT-1228] look into combining these two method calls when we upgrade
+        # pytorch lightning
+        self.model.set_model_training_iterations(iterations)
+        self.model.reset_trainer()
+        validation_metrics: Optional[Dict[str, str]] = self.model.fit(data)
+        return self.get_param_states(), validation_metrics
 
-        The model must inherit from `DifferentiallyPrivate` for DP to be supported.
+    def save_final_parameters(self, model_params: _SerializedWeights) -> None:
+        """Saves the final global model parameters.
 
         Args:
-            pod_dp: The pod DP constraints to apply or None if no constraints.
-        """
-        try:
-            # only applied if model supports DP so can ignore attr-defined
-            self.model.apply_pod_dp(pod_dp)  # type: ignore[attr-defined]  # Reason: caught by try-except  # noqa: B950
-        except AttributeError:
-            pass
+            model_params: The final global model parameters.
 
+        :::note
 
-# The mutable underlying dict that holds the registry information
-_registry: Dict[str, Type[_BaseModelAlgorithmFactory]] = {}
-# The read-only version of the registry that is allowed to be imported
-registry: Mapping[str, Type[_BaseModelAlgorithmFactory]] = MappingProxyType(_registry)
+        This method saves the final global model to a file called `model.pt`.
 
+        :::
+        """
+        backend_tensor_shim: BackendTensorShim = self.model.backend_tensor_shim()
+        for name, param in model_params.items():
+            model_params[name] = backend_tensor_shim.to_tensor(param)
+
+        tensor_model_params = cast(_WeightDict, model_params)
+        self.update_params(tensor_model_params)
+        # TODO: [BIT-1043]: pass filename for serialization
+        self.model.serialize("model.pt")
+
+
+class FederatedModelTraining(
+    _BaseModelAlgorithmFactory,
+):
+    """Algorithm for training a model remotely and returning its updated parameters.
 
-class _BaseModelAlgorithmFactory(BaseAlgorithmFactory, ABC):
-    """Base factory for algorithms involving an underlying model.
+    This algorithm is designed to be compatible with the `FederatedAveraging` protocol.
 
     Args:
-        model: The model for the federated algorithm.
-        pretrained_file: A file path or a string containing a
-            pre-trained model. Defaults to None.
+        model: The model to train on remote data.
 
     Attributes:
-        model: The model for the federated algorithm.
-        pretrained_file: A file path or a string containing a
-            pre-trained model. Defaults to None.
+        name: The name of the algorithm.
+        model: The model to train on remote data.
     """
 
-    fields_dict: ClassVar[T_FIELDS_DICT] = {}
-    nested_fields: ClassVar[T_NESTED_FIELDS] = {"model": MAIN_MODEL_REGISTRY}
-
     def __init__(
         self,
         *,
         model: _DistributedModelTypeOrReference,
-        pretrained_file: Optional[Union[str, os.PathLike]] = None,
-        project_id: Optional[str] = None,
         **kwargs: Any,
     ):
-        # TODO: [NO_TICKET: Consideration only] Consider if project_id is required
-        #       on the algorithm or if it should be something inherent on the
-        #       model_reference (which is all it's currently used for).
-        super().__init__(**kwargs)
-        self.model = model
-        self.pretrained_file = pretrained_file
-        self.project_id = project_id
-
-    @classmethod
-    def __init_subclass__(cls, **kwargs: Any):
-        super().__init_subclass__(**kwargs)
-        if not inspect.isabstract(cls):
-            logger.debug(f"Adding {cls.__name__}: {cls} to Model Algorithm registry")
-            _registry[cls.__name__] = cls
+        super().__init__(model=model, **kwargs)
 
-    @property
-    def model_schema(self) -> _StrAnyDict:
-        """Returns underlying model Schema."""
-        # Assertion for mypy since DistributedModelProtocol is a protocol rather than
-        # a concrete class.
-        assert isinstance(self.model, _BaseSerializableObjectMixIn)  # nosec assert_used
-        return bf_dump(self.model)
-
-    def _get_model_from_reference(
-        self, hub: Optional[BitfountHub] = None, project_id: Optional[str] = None
-    ) -> DistributedModelProtocol:
-        """Returns underlying model if BitfountModelReference.
+    def modeller(self, **kwargs: Any) -> _ModellerSide:
+        """Returns the modeller side of the FederatedModelTraining algorithm."""
+        model = self._get_model_from_reference()
+        return _ModellerSide(model=model, **kwargs)
 
-        If not, just returns self.model.
+    def worker(self, hub: BitfountHub, **kwargs: Any) -> _WorkerSide:
+        """Returns the worker side of the FederatedModelTraining algorithm.
+
+        Args:
+            hub: `BitfountHub` object to use for communication with the hub.
+        """
+        model = self._get_model_from_reference(hub=hub)
+        return _WorkerSide(model=model, **kwargs)
+
+    @staticmethod
+    def get_schema(
+        model_schema: Type[MarshmallowSchema], **kwargs: Any
+    ) -> Type[MarshmallowSchema]:
+        """Returns the schema for FederatedModelTraining.
+
+        Args:
+            model_schema: The schema for the underlying model.
         """
-        # TODO: [BIT-890] perhaps move this logic one level higher so that the algorithm
-        # factory always takes a DistributedModelProtocol
-        if isinstance(self.model, BitfountModelReference):
-            if hub is not None:
-                self.model.hub = hub
-            model_cls = self.model.get_model(project_id=project_id)
-            model = model_cls(
-                datastructure=self.model.datastructure,
-                schema=self.model.schema,
-                **self.model.hyperparameters,
-            )
-            # If there is a weights file associated with the model then
-            # initialise the model with these weights
-            # TODO: [BIT-3019] Getting the weights here should not be called on both
-            # modeller and worker sides
-            if self.model.model_version:
-                if weights_bytes := self.model.get_weights(project_id=project_id):
-                    logger.info("Applying weights..")
-                    model.deserialize(weights_bytes)
-            self.model = cast(DistributedModelProtocol, model)
-            return self.model
-        else:
-            return self.model
+
+        class Schema(_BaseAlgorithmFactory._Schema):
+
+            model = fields.Nested(model_schema)
+
+            @post_load
+            def recreate_factory(
+                self, data: dict, **_kwargs: Any
+            ) -> FederatedModelTraining:
+                return FederatedModelTraining(**data)
+
+        return Schema
```

### Comparing `bitfount-0.5.86/bitfount/federated/algorithms/model_algorithms/evaluate.py` & `bitfount-0.5.9/bitfount/federated/algorithms/model_algorithms/evaluate.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,73 +1,89 @@
 """Algorithm to evaluate a model on remote data."""
 from __future__ import annotations
 
-from typing import Any, Dict, Mapping, cast
+from typing import Any, Dict, List, Type
 
-import numpy as np
+from marshmallow import Schema as MarshmallowSchema
+from marshmallow import fields, post_load
 
+from bitfount.federated.algorithms.base import _BaseAlgorithmFactory
 from bitfount.federated.algorithms.model_algorithms.base import (
     _BaseModelAlgorithmFactory,
     _BaseModellerModelAlgorithm,
     _BaseWorkerModelAlgorithm,
 )
 from bitfount.federated.logging import _get_federated_logger
 from bitfount.hub.api import BitfountHub
 from bitfount.metrics import MetricCollection
-from bitfount.utils import delegates
 
 logger = _get_federated_logger(__name__)
 
 
 class _ModellerSide(_BaseModellerModelAlgorithm):
     """Modeller side of the ModelEvaluation algorithm."""
 
-    def run(
-        self, results: Mapping[str, Dict[str, float]]
-    ) -> Dict[str, Dict[str, float]]:
+    def run(self, results: List[Dict[str, float]]) -> List[Dict[str, float]]:
         """Simply returns results."""
-        return dict(results)
+        return results
 
 
 class _WorkerSide(_BaseWorkerModelAlgorithm):
     """Worker side of the ModelEvaluation algorithm."""
 
     def run(self) -> Dict[str, float]:
         """Runs evaluation and returns metrics."""
-        preds, target = self.model.evaluate()
-        # TODO: [BIT-1604] Remove these cast statements once they become superfluous.
-        preds = cast(np.ndarray, preds)
-        target = cast(np.ndarray, target)
-        m = MetricCollection.create_from_model(self.model, self.model.metrics)
-        return m.compute(target, preds)
+        preds, targs = self.model.evaluate()
+        m = MetricCollection.create_from_model(self.model)
+        return m.compute(targs, preds)
 
 
-@delegates()
 class ModelEvaluation(_BaseModelAlgorithmFactory):
     """Algorithm for evaluating a model and returning metrics.
 
-    :::note
-
-    The metrics cannot currently be specified by the user.
-
-    :::
-
     Args:
         model: The model to evaluate on remote data.
 
     Attributes:
+        name: The name of the algorithm.
         model: The model to evaluate on remote data.
+
+    :::note
+
+    The metrics cannot currently be specified by the user.
+
+    :::
     """
 
     def modeller(self, **kwargs: Any) -> _ModellerSide:
         """Returns the modeller side of the ModelEvaluation algorithm."""
-        model = self._get_model_from_reference(project_id=self.project_id)
+        model = self._get_model_from_reference()
         return _ModellerSide(model=model, **kwargs)
 
     def worker(self, hub: BitfountHub, **kwargs: Any) -> _WorkerSide:
         """Returns the worker side of the ModelEvaluation algorithm.
 
         Args:
             hub: `BitfountHub` object to use for communication with the hub.
         """
-        model = self._get_model_from_reference(hub=hub, project_id=self.project_id)
+        model = self._get_model_from_reference(hub=hub)
         return _WorkerSide(model=model, **kwargs)
+
+    @staticmethod
+    def get_schema(
+        model_schema: Type[MarshmallowSchema], **kwargs: Any
+    ) -> Type[MarshmallowSchema]:
+        """Returns the schema for ModelTrainingAndEvaluation.
+
+        Args:
+            model_schema: The schema for the underlying model.
+        """
+
+        class Schema(_BaseAlgorithmFactory._Schema):
+
+            model = fields.Nested(model_schema)
+
+            @post_load
+            def recreate_factory(self, data: dict, **_kwargs: Any) -> ModelEvaluation:
+                return ModelEvaluation(**data)
+
+        return Schema
```

### Comparing `bitfount-0.5.86/bitfount/federated/algorithms/model_algorithms/train_and_evaluate.py` & `bitfount-0.5.9/tests/bitfount/federated/algorithms/model_algorithms/test_evaluate.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,75 +1,109 @@
-"""Algorithm to train and evaluate a model on remote data."""
-from __future__ import annotations
-
-from typing import Any, Dict, Mapping, cast
-
-import numpy as np
-
-from bitfount.data.datasources.base_source import BaseSource
+"""Tests for the model evaluation algorithm."""
+from typing import TYPE_CHECKING
+from unittest.mock import Mock, create_autospec
+
+from marshmallow import Schema as MarshmallowSchema
+from pytest_mock import MockerFixture
+
+from bitfount.federated.algorithms.base import (
+    _BaseAlgorithm,
+    _BaseModellerAlgorithm,
+    _BaseWorkerAlgorithm,
+)
+from bitfount.federated.algorithms.model_algorithms import evaluate
 from bitfount.federated.algorithms.model_algorithms.base import (
-    _BaseModelAlgorithmFactory,
+    _BaseModelAlgorithm,
     _BaseModellerModelAlgorithm,
     _BaseWorkerModelAlgorithm,
 )
-from bitfount.federated.logging import _get_federated_logger
-from bitfount.hub.api import BitfountHub
-from bitfount.metrics import MetricCollection
-from bitfount.utils import delegates
-
-logger = _get_federated_logger(__name__)
-
-
-class _ModellerSide(_BaseModellerModelAlgorithm):
-    """Modeller side of the ModelTrainingAndEvaluation algorithm."""
-
-    def run(
-        self, results: Mapping[str, Dict[str, float]]
-    ) -> Dict[str, Dict[str, float]]:
-        """Simply returns results."""
-        return dict(results)
-
-
-class _WorkerSide(_BaseWorkerModelAlgorithm):
-    """Worker side of the ModelTrainingAndEvaluation algorithm."""
-
-    def run(self, data: BaseSource) -> Dict[str, float]:
-        """Runs training and evaluation and returns metrics."""
-        self.model.fit(data)
-        preds, target = self.model.evaluate()
-        # TODO: [BIT-1604] Remove these cast statements once they become superfluous.
-        preds = cast(np.ndarray, preds)
-        target = cast(np.ndarray, target)
-        m = MetricCollection.create_from_model(self.model)
-        return m.compute(target, preds)
-
-
-@delegates()
-class ModelTrainingAndEvaluation(_BaseModelAlgorithmFactory):
-    """Algorithm for training a model, evaluating it and returning metrics.
-
-    :::note
-
-    The metrics cannot currently be specified by the user.
-
-    :::
-
-    Args:
-        model: The model to train and evaluate on remote data.
-
-    Attributes:
-        model: The model to train and evaluate on remote data.
-    """
+from bitfount.federated.algorithms.model_algorithms.evaluate import (
+    ModelEvaluation,
+    _ModellerSide,
+    _WorkerSide,
+)
+from bitfount.hub import BitfountHub
+from tests.utils.helper import unit_test
 
-    def modeller(self, **kwargs: Any) -> _ModellerSide:
-        """Returns the modeller side of the ModelTrainingAndEvaluation algorithm."""
-        model = self._get_model_from_reference(project_id=self.project_id)
-        return _ModellerSide(model=model, **kwargs)
 
-    def worker(self, hub: BitfountHub, **kwargs: Any) -> _WorkerSide:
-        """Returns the worker side of the ModelTrainingAndEvaluation algorithm.
+class TestModelEvaluation:
+    """Test Evaluate algorithm."""
 
-        Args:
-            hub: `BitfountHub` object to use for communication with the hub.
-        """
-        model = self._get_model_from_reference(hub=hub, project_id=self.project_id)
-        return _WorkerSide(model=model, **kwargs)
+    @unit_test
+    def test_modeller_types(self, model: Mock) -> None:
+        """Test modeller method."""
+        algorithm_factory = ModelEvaluation(model=model)
+        algorithm = algorithm_factory.modeller()
+        for type_ in [
+            _BaseAlgorithm,
+            _BaseModellerAlgorithm,
+            _BaseModelAlgorithm,
+            _BaseModellerModelAlgorithm,
+        ]:
+            assert isinstance(algorithm, type_)
+
+    @unit_test
+    def test_worker_types(self, model: Mock) -> None:
+        """Test worker method."""
+        algorithm_factory = ModelEvaluation(model=model)
+        algorithm = algorithm_factory.worker(
+            hub=create_autospec(BitfountHub, instance=True)
+        )
+        for type_ in [
+            _BaseAlgorithm,
+            _BaseWorkerAlgorithm,
+            _BaseModelAlgorithm,
+            _BaseWorkerModelAlgorithm,
+        ]:
+            assert isinstance(algorithm, type_)
+
+    @unit_test
+    def test_worker_run(self, mocker: MockerFixture, model: Mock) -> None:
+        """Tests that worker run does metric calculation."""
+        worker = evaluate._WorkerSide(model=model)
+        mock_metrics = Mock()
+        mocker.patch(
+            "bitfount.metrics.MetricCollection.create_from_model", mock_metrics
+        )
+        worker.run()
+        mock_metrics.assert_called_once()
+
+    @unit_test
+    def test_modeller_run(self, model: Mock) -> None:
+        """Tests that modeller run returns results."""
+        modeller = evaluate._ModellerSide(model=model)
+        results = [{"AUC": 0.5}]
+        assert results == modeller.run(results=results)
+
+    @unit_test
+    def test_schema(self, model: Mock) -> None:
+        """Tests that schema returns parent class."""
+        schema_cls = ModelEvaluation.get_schema(create_autospec(MarshmallowSchema))
+        schema = schema_cls()
+        factory = schema.recreate_factory(data={"model": model})  # type: ignore[attr-defined]  # Reason: test will fail if wrong type  # noqa: B950
+        assert isinstance(factory, ModelEvaluation)
+
+
+# Static tests for algorithm-protocol compatibility
+if TYPE_CHECKING:
+    from typing import cast
+
+    from bitfount.federated.protocols.results_only import (
+        _ResultsOnlyCompatibleModelAlgoFactory,
+        _ResultsOnlyCompatibleModeller,
+        _ResultsOnlyDataIncompatibleWorker,
+    )
+    from bitfount.types import (
+        DistributedModelProtocol,
+        _DistributedModelTypeOrReference,
+    )
+
+    # Check compatible with ResultsOnly
+    _algo_factory: _ResultsOnlyCompatibleModelAlgoFactory = ModelEvaluation(
+        model=cast(_DistributedModelTypeOrReference, object())
+    )
+    _modeller_side: _ResultsOnlyCompatibleModeller = _ModellerSide(
+        model=cast(DistributedModelProtocol, object())
+    )
+    _worker_side: _ResultsOnlyDataIncompatibleWorker = _WorkerSide(
+        model=cast(DistributedModelProtocol, object())
+    )
```

### Comparing `bitfount-0.5.86/bitfount/federated/algorithms/transformer_perplexity.py` & `bitfount-0.5.9/bitfount/federated/algorithms/model_algorithms/base.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,188 +1,181 @@
-"""HuggingFace Perplexity Algorithm.
+"""Base classes for all model-based algorithms.
 
-Reference:
-https://huggingface.co/docs/transformers/perplexity#example-calculating-perplexity-with-gpt2-in-transformers
+Attributes:
+    registry: A read-only dictionary of model algorithm factory names to their
+        implementation classes.
 """
-from typing import Any, ClassVar, Dict, Mapping, Optional
+from __future__ import annotations
 
-from marshmallow import fields
-import torch
-from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed
+from abc import ABC, abstractmethod
+import inspect
+import os
+from types import MappingProxyType
+from typing import (
+    TYPE_CHECKING,
+    Any,
+    Dict,
+    Generic,
+    Mapping,
+    Optional,
+    Type,
+    Union,
+    cast,
+)
+
+from marshmallow import Schema as MarshmallowSchema
 
-from bitfount.data.datasources.base_source import BaseSource
+from bitfount.data.datasource import DataSource
 from bitfount.federated.algorithms.base import (
-    BaseAlgorithmFactory,
-    BaseModellerAlgorithm,
-    BaseWorkerAlgorithm,
+    _BaseAlgorithm,
+    _BaseAlgorithmFactory,
+    _BaseModellerAlgorithm,
+    _BaseWorkerAlgorithm,
 )
 from bitfount.federated.logging import _get_federated_logger
+from bitfount.federated.model_reference import BitfountModelReference
 from bitfount.federated.privacy.differential import DPPodConfig
-from bitfount.types import T_FIELDS_DICT
+from bitfount.models.base_models import ModelContext
+from bitfount.types import T_DTYPE, DistributedModelProtocol
+
+if TYPE_CHECKING:
+    from bitfount.hub.api import BitfountHub
+    from bitfount.types import _DistributedModelTypeOrReference
 
-DEFAULT_STRIDE = 512
-DEFAULT_SEED = 42
 
 logger = _get_federated_logger(__name__)
 
 
-class _ModellerSide(BaseModellerAlgorithm):
-    """Modeller side of the TransformerPerplexity algorithm."""
+class _BaseModelAlgorithm(Generic[T_DTYPE], _BaseAlgorithm, ABC):
+    """Blueprint for either the modeller side or the worker side of ModelAlgorithm."""
 
-    def initialise(
+    def __init__(
         self,
-        task_id: Optional[str] = None,
+        *,
+        model: DistributedModelProtocol,
         **kwargs: Any,
-    ) -> None:
-        """Nothing to initialise here."""
-        pass
+    ):
+        super().__init__(**kwargs)
+        self.model = model
 
-    def run(self, results: Mapping[str, Any]) -> Dict[str, Any]:
-        """Simply returns results."""
-        return dict(results)
+    @property
+    def tensor_precision(self) -> T_DTYPE:
+        """Returns model tensor precision."""
+        return self.model.tensor_precision()
 
 
-class _WorkerSide(BaseWorkerAlgorithm):
-    """Worker side of the TransformerPerplexity algorithm."""
+class _BaseModellerModelAlgorithm(_BaseModelAlgorithm, _BaseModellerAlgorithm, ABC):
+    """Modeller side of the algorithm."""
 
-    def __init__(
+    def __init__(self, *, model: DistributedModelProtocol, **kwargs: Any):
+        super().__init__(model=model, **kwargs)
+
+    def initialise(
         self,
-        model_id: str,
-        text_column_name: str,
-        stride: int,
-        seed: int,
+        pretrained_file: Optional[Union[str, os.PathLike]] = None,
         **kwargs: Any,
-    ):
-        super().__init__(**kwargs)
-        self.model_id = model_id
-        self.text_column_name = text_column_name
-        self.stride = stride
-        self.seed = seed
+    ) -> None:
+        """Initialises the algorithm as required."""
+        self.model.initialise_model(context=ModelContext.MODELLER)
+
+        # This needs to occur AFTER model initialization so the model is correctly
+        # created. deserialize() may cause initialization but we can not rely on it
+        # in this instance because we need to pass in context information.
+        # This should be reviewed as part of [BIT-536].
+        if pretrained_file is not None:
+            logger.info(f"Deserializing model from {pretrained_file}.")
+            self.model.deserialize(pretrained_file)
+
+
+class _BaseWorkerModelAlgorithm(_BaseModelAlgorithm, _BaseWorkerAlgorithm, ABC):
+    """Worker side of the algorithm."""
+
+    def __init__(self, *, model: DistributedModelProtocol, **kwargs: Any):
+        super().__init__(model=model, **kwargs)
 
     def initialise(
         self,
-        datasource: BaseSource,
+        datasource: DataSource,
         pod_dp: Optional[DPPodConfig] = None,
         **kwargs: Any,
     ) -> None:
-        """Initialises the model and tokenizer."""
-        # TODO: [BIT-3097] Resolve initialise without DP
-        if pod_dp:
-            logger.warning("The use of DP is not supported, ignoring set `pod_dp`.")
-        self.datasource = datasource
-        set_seed(self.seed)
-        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)
-        self.model = AutoModelForCausalLM.from_pretrained(
-            self.model_id, trust_remote_code=True
-        )
-
-    def run(self) -> Any:
-        """Runs the pipeline to compute perplexities.
-
-        The function calculates perplexity for each prompt in the
-        provided data source. Perplexity is the average exponentiated
-        loss obtained from the model. To handle fixed-length causal language
-        models with a maximum context, we utilize a sliding window strategy.
-        This strategy breaks the sequence into subsequences with a sliding
-        context window, preventing poor approximation of the fully-factorized
-        perplexity. This approach ensures that the model has sufficient
-        context when making each prediction, leading to more accurate results.
+        """Initialises the algorithm as required."""
+        # Apply pod DP settings if needed. Needs to occur before model
+        # initialization so the right DP settings are applied during initialization.
+
+        self._apply_pod_dp(pod_dp)
+        self.model.initialise_model(data=datasource, context=ModelContext.WORKER)
+
+    def _apply_pod_dp(self, pod_dp: Optional[DPPodConfig]) -> None:
+        """Applies pod-level Differential Privacy constraints if supported.
+
+        The model must inherit from `DifferentiallyPrivate` for DP to be supported.
+
+        Args:
+            pod_dp: The pod DP constraints to apply or None if no constraints.
         """
-        perplexities = []
-        for prompt in self.datasource.get_column(self.text_column_name).tolist():
-            encodings = self.tokenizer(prompt, return_tensors="pt")
-            # model's maximum context size (tokens)
-            max_length = self.model.config.n_positions
-            # The number of tokens as context when calculating conditional likelihood
-            # of any one token (see DEFAULT_STRIDE)
-            stride = self.stride
-            seq_len = encodings.input_ids.size(1)
-
-            nlls = []
-            prev_end_loc = 0
-            # Sliding window strategy
-            for begin_loc in range(0, seq_len, stride):
-                end_loc = min(begin_loc + max_length, seq_len)
-                trg_len = end_loc - prev_end_loc
-                input_ids = encodings.input_ids[:, begin_loc:end_loc]
-                target_ids = input_ids.clone()
-                # Avoid token overlap from influencing loss
-                # by setting to -100.
-                target_ids[:, :-trg_len] = -100
-
-                with torch.no_grad():
-                    outputs = self.model(input_ids, labels=target_ids)
-                    # Loss = trg_len - 1 (internal shift of labels to the left by 1)
-                    neg_log_likelihood = outputs.loss
-
-                nlls.append(neg_log_likelihood)
-
-                prev_end_loc = end_loc
-                if end_loc == seq_len:
-                    break
-
-            ppl = torch.exp(torch.stack(nlls).mean()).item()
-            perplexities.append(ppl)
-
-        return perplexities
-
-
-class TransformerPerplexity(BaseAlgorithmFactory):
-    """HuggingFace Perplexity Algorithm.
-
-    Args:
-        model_id: The model id to use for evaluating its perplexity.
-            The model id is of a pretrained model hosted inside a model
-            repo on huggingface.co. Accepts models with a causal language
-            modeling head.
-        text_column_name: The signle column to query against. Should contain
-            text for generation.
-        stride: Sets the stride of the algorithm. Defaults to 512.
-        seed: Sets the seed of the algorithm. For reproducible behavior
-            it defaults to 42.
-
-    Attributes:
-        model_id: The model id to use for evaluation.
-            The model id is of a pretrained model hosted inside a model
-            repo on huggingface.co. Accepts models with a causal language
-            modeling head.
-        text_column_name: The signle column to query against. Should contain
-            text for generation.
-        stride: Sets the stride of the algorithm. Defaults to 512.
-        seed: Sets the seed of the algorithm. For reproducible behavior
-            it defaults to 42.
-    """
+        try:
+            # only applied if model supports DP so can ignore attr-defined
+            self.model.apply_pod_dp(pod_dp)  # type: ignore[attr-defined]  # Reason: caught by try-except  # noqa: B950
+        except AttributeError:
+            pass
 
-    def __init__(
-        self,
-        model_id: str,
-        text_column_name: str,
-        stride: int = DEFAULT_STRIDE,
-        seed: int = DEFAULT_SEED,
-        **kwargs: Any,
-    ):
+
+# The mutable underlying dict that holds the registry information
+_registry: Dict[str, Type[_BaseModelAlgorithmFactory]] = {}
+# The read-only version of the registry that is allowed to be imported
+registry: Mapping[str, Type[_BaseModelAlgorithmFactory]] = MappingProxyType(_registry)
+
+
+class _BaseModelAlgorithmSchema(ABC):
+    """Mixin for model-based algorithm get_schema calls."""
+
+    @staticmethod
+    @abstractmethod
+    def get_schema(
+        model_schema: Type[MarshmallowSchema], **kwargs: Any
+    ) -> Type[MarshmallowSchema]:
+        """Get a schema for BaseAlgorithmFactory subclass."""
+        raise NotImplementedError
+
+
+class _BaseModelAlgorithmFactory(_BaseModelAlgorithmSchema, _BaseAlgorithmFactory, ABC):
+    """Base factory for algorithms involving an underlying model."""
+
+    def __init__(self, *, model: _DistributedModelTypeOrReference, **kwargs: Any):
         super().__init__(**kwargs)
-        self.model_id = model_id
-        self.text_column_name = text_column_name
-        self.stride = stride
-        self.seed = seed
-
-    fields_dict: ClassVar[T_FIELDS_DICT] = {
-        "model_id": fields.Str(required=True),
-        "text_column_name": fields.Str(required=True),
-        "stride": fields.Int(required=False, missing=DEFAULT_STRIDE),
-        "seed": fields.Int(required=False, missing=DEFAULT_SEED),
-    }
-
-    def modeller(self, **kwargs: Any) -> _ModellerSide:
-        """Returns the modeller side of the TransformerPerplexity algorithm."""
-        return _ModellerSide(**kwargs)
-
-    def worker(self, **kwargs: Any) -> _WorkerSide:
-        """Returns the worker side of the TransformerPerplexity algorithm."""
-        return _WorkerSide(
-            model_id=self.model_id,
-            text_column_name=self.text_column_name,
-            stride=self.stride,
-            seed=self.seed,
-            **kwargs,
-        )
+        self.model = model
+
+    @classmethod
+    def __init_subclass__(cls, **kwargs: Any):
+        super().__init_subclass__(**kwargs)
+        if not inspect.isabstract(cls):
+            logger.debug(f"Adding {cls.__name__}: {cls} to Model Algorithm registry")
+            _registry[cls.__name__] = cls
+
+    @property
+    def model_schema(self) -> Type[MarshmallowSchema]:
+        """Returns underlying model Schema."""
+        self.model = cast(DistributedModelProtocol, self.model)
+        return self.model.get_schema()
+
+    def _get_model_from_reference(
+        self, hub: Optional[BitfountHub] = None
+    ) -> DistributedModelProtocol:
+        """Returns underlying model if BitfountModelReference.
+
+        If not, just returns self.model.
+        """
+        # TODO: [BIT-890] perhaps move this logic one level higher so that the algorithm
+        # factory always takes a DistributedModelProtocol
+        if isinstance(self.model, BitfountModelReference):
+            if hub is not None:
+                self.model.hub = hub
+            model = self.model.get_model()(
+                datastructure=self.model.datastructure,
+                schema=self.model.schema,
+                **self.model.hyperparameters,
+            )
+            self.model = cast(DistributedModelProtocol, model)
+            return self.model
+        else:
+            return self.model
```

### Comparing `bitfount-0.5.86/bitfount/federated/early_stopping.py` & `bitfount-0.5.9/bitfount/federated/early_stopping.py`

 * *Files identical despite different names*

### Comparing `bitfount-0.5.86/bitfount/federated/model_reference.py` & `bitfount-0.5.9/bitfount/federated/model_reference.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,280 +1,249 @@
 """References to custom models."""
 from __future__ import annotations
 
 from pathlib import Path
-from typing import TYPE_CHECKING, ClassVar, Optional, Type, Union, cast
+from typing import TYPE_CHECKING, Any, Dict, Optional, Type, Union, cast
 
-from marshmallow import fields
+from marshmallow import Schema as MarshmallowSchema
+from marshmallow import fields, post_load
 
-from bitfount.data.datastructure import (
-    DataStructure,
-    registry as datastructure_registry,
-)
+from bitfount.data.datastructure import DataStructure
 from bitfount.data.schema import BitfountSchema
 from bitfount.federated.logging import _get_federated_logger
-from bitfount.hub.exceptions import ModelUploadError
-from bitfount.hub.helper import _default_bitfounthub
-from bitfount.hub.utils import hash_file_contents
-from bitfount.models.base_models import _BaseModelRegistryMixIn
-from bitfount.types import (
-    T_FIELDS_DICT,
-    T_NESTED_FIELDS,
-    DistributedModelProtocol,
-    _BaseSerializableObjectMixIn,
-    _StrAnyDict,
-)
-from bitfount.utils import (
-    _get_non_abstract_classes_from_module,
-    _handle_fatal_error,
-    delegates,
-)
+from bitfount.federated.shim import BackendTensorShim
+from bitfount.types import DistributedModelProtocol
+from bitfount.utils import _get_non_abstract_classes_from_module
 
 if TYPE_CHECKING:
     from bitfount.hub.api import BitfountHub
-    from bitfount.hub.types import _ModelUploadResponseJSON
     from bitfount.models.bitfount_model import BitfountModel
-    from bitfount.runners.config_schemas import JWT, APIKeys
+
 
 logger = _get_federated_logger(__name__)
 
 
-@delegates()
-class BitfountModelReference(_BaseModelRegistryMixIn, _BaseSerializableObjectMixIn):
+class BitfountModelReference:
     """Describes a local or remote reference to a `BitfountModel` class.
 
-    :::tip
-
-    To use another user's custom model, simply provide that user's username instead of
-    your own (along with the name of the model as the `model_ref` argument).
-
-    :::
-
     Args:
         model_ref: Either path to model file or name of model on hub.
-        datastructure: `DataStructure` to be passed to the model when initialised. This
-            is an optional argument as it is only required for `get_model` to perform
-            validation on the model before uploading it to the hub. Ensure that you
-            provide this argument if you want to use `get_model` to upload your model.
-        model_version: The version of the model you wish to use. Defaults to
-            the latest version.
-        schema: The `BitfountSchema` object associated with the datasource
-            on which the model will be trained on.
+        datastructure: `DataStructure` to be passed to the model when initialised.
+        schema: `BitfountSchema` to be passed to the model when initialised.
         username: The username of the model owner. Defaults to bitfount session username
             if not provided.
         hub: Required for upload/download of model. This attribute is set after
             initialisation on the worker side as the hub is not serialized. Defaults to
             None.
         hyperparameters: Hyperparameters to be passed to the model constructor after it
             has been loaded from file or hub. Defaults to None.
-        private: Boolean flag to set the model to be private to control useage or
-            publicly accessible to all users.
-        new_version: Whether to upload a new version of the model to the hub.
-            Defaults to False.
-        secrets: The secrets to use when creating a `BitfountHub` instance. Defaults to
-            None.
 
     Raises:
         ValueError: If `username` is not provided and `hub` is not provided.
-    """
 
-    datastructure: DataStructure
-    fields_dict: ClassVar[T_FIELDS_DICT] = {
-        "model_ref": fields.Method(
-            serialize="get_model_ref", deserialize="load_model_ref"
-        ),
-        "model_version": fields.Int(allow_none=True),
-        # The hub should not be serialized but can be deserialized if provided
-        "hub": fields.Raw(allow_none=True, load_only=True),
-        "username": fields.Str(allow_none=True),
-        "hyperparameters": fields.Dict(keys=fields.Str()),
-        "param_clipping": fields.Dict(
-            keys=fields.String(), values=fields.Integer(), allow_none=True
-        ),
-        "schema": fields.Nested(BitfountSchema._Schema),
-        "private": fields.Bool(allow_none=True),
-        "new_version": fields.Bool(allow_none=True),
-    }
-    # TODO: [BIT-1954] Maybe this should actually fall under the hyperparameters
-    #  rather than populating the top level with additional fields. Or at least
-    #  a `other_kwargs` (terrible name, don't use that) so that we can add more
-    #  to it in the future.
-    nested_fields: ClassVar[T_NESTED_FIELDS] = {"datastructure": datastructure_registry}
+    :::tip
+
+    To use another user's custom model, simply provide that user's username instead of
+    your own (along with the name of the model as the `model_ref` argument).
+
+    :::
+    """
 
     def __init__(
         self,
         model_ref: Union[Path, str],
-        datastructure: Optional[DataStructure] = None,
-        model_version: Optional[int] = None,
+        datastructure: DataStructure,
         schema: Optional[BitfountSchema] = None,
         username: Optional[str] = None,
         hub: Optional[BitfountHub] = None,
-        hyperparameters: Optional[_StrAnyDict] = None,
-        private: bool = False,
-        new_version: bool = False,
-        secrets: Optional[Union[APIKeys, JWT]] = None,
+        hyperparameters: Optional[Dict[str, Any]] = None,
     ):
-        self.class_name = type(self).__name__
+        self.name = type(self).__name__
         self.model_ref = model_ref
-        self.model_version = model_version
+        self.datastructure = datastructure
         self.schema = schema if schema else BitfountSchema()
-        self.hub = _default_bitfounthub(hub, username=username, secrets=secrets)
+        self.username: str
+        self.hub = hub
         self.hyperparameters = hyperparameters if hyperparameters is not None else {}
-        self.username = username or self.hub.username
-        self.private = private
-        self.new_version = new_version
-        if datastructure:
-            self.datastructure = datastructure
+
+        if username is not None:
+            self.username = username
+        else:
+            if self.hub is not None:
+                self.username = self.hub.username
+            else:
+                raise ValueError("One of hub or username must be set.")
 
     def _get_model_from_path(self) -> Type[BitfountModel]:
         """Returns model class from path.
 
         Returns:
             The model class.
         """
         self.model_ref = cast(Path, self.model_ref)
         return _get_non_abstract_classes_from_module(self.model_ref)[
             self.model_ref.stem
         ]
 
-    def _upload_model_to_hub(self) -> Optional[_ModelUploadResponseJSON]:
-        """Uploads model to hub under the logged-in user's account."""
+    def _upload_model_to_hub(self) -> None:
+        """Uploads model to hub.
+
+        Raises:
+            ValueError: if `self.hub` has not been set or if there was a communication
+                error with the hub
+        """
         # model_ref is path to model code file
+        if not self.hub:
+            raise ValueError("Please a provide a BitfountHub instance to upload model.")
+
         self.model_ref = cast(Path, self.model_ref)
-        try:
-            response = self.hub.send_model(self.model_ref, self.private)
-            logger.info("Model has been uploaded to the hub.")
-            return response
-        except ModelUploadError as ex:
-            _handle_fatal_error(ex)
-
-    def _get_model_from_hub(
-        self, project_id: Optional[str] = None
-    ) -> Type[BitfountModel]:
-        """Returns model class from hub from user denoted by `self.username`.
+        model_sent_status = self.hub.send_model(self.model_ref)
+        if not model_sent_status:
+            raise ValueError(
+                "Could not send model to Hub. Please address issues and try again"
+            )
+
+    def _get_model_from_hub(self) -> Type[BitfountModel]:
+        """Returns model class from hub.
+
+        Raises:
+            ValueError: if `self.hub` has not been set or if the model was not
+                successfully received
 
         Returns:
             The model class.
         """
         # model_ref is the name of a model on the hub
+        if not self.hub:
+            raise ValueError(
+                "Please a provide a BitfountHub instance to download the model."
+            )
         self.model_ref = cast(str, self.model_ref)
-        model_cls = self.hub.get_model(
-            self.username, self.model_ref, self.model_version, project_id
-        )
+        model_cls = self.hub.get_model(self.username, self.model_ref)
 
         # Check that the model has been retrieved correctly
         if not model_cls:
             raise ValueError(
                 "Unable to retrieve model from hub, check logs for details."
             )
         return model_cls
 
-    def get_weights(self, project_id: Optional[str] = None) -> Optional[bytes]:
-        """Gets weights file uploaded for the model if one exists.
-
-        Returns:
-            The weights file as a byte stream.
-        """
-        if isinstance(self.model_ref, Path):
-            raise TypeError(
-                "Invalid model reference. get_weights can only be"
-                "called on uploaded models and you have specified "
-                f"a Path as model_ref: {self.model_ref}."
-            )
-        if not self.model_version:
-            raise ValueError(
-                "You must specify model_version in BitfountModelReference "
-                "constructor to get model weights file."
-            )
-        return self.hub.get_weights(
-            self.username, self.model_ref, self.model_version, project_id
-        )
-
-    def get_model(self, project_id: Optional[str] = None) -> Type[BitfountModel]:
+    def get_model(self) -> Type[BitfountModel]:
         """Gets the model referenced.
 
         If the model is a Path to a `BitfountModel`, it will upload it to BitfountHub
         and return the model class. If it is a name of a model on the hub, it will
         download the model from the hub and return the model class.
 
         Returns:
             The model class.
 
         Raises:
             TypeError: If the model is not a Path or a string.
             TypeError: If the model does not implement `DistributedModelProtocol`.
             ValueError: If a `BitfountHub` instance has not been provided or if there
                 was a communication error with the hub.
-            ValueError: If a datastructure has not been provided.
         """
         if isinstance(self.model_ref, Path):
             model_cls = self._get_model_from_path()
-            hash = hash_file_contents(self.model_ref)
 
             # Check that chosen model is compatible with federation by checking if it
             # implements `DistributedModelProtocol`. The only way to do this is to
             # instantiate the model and perform an `isinstance` check.
-            if self.datastructure is None:
-                raise ValueError(
-                    "Datastructure must be provided to instantiate model "
-                    "so that the type of the model can be validated."
-                )
             model = model_cls(
                 datastructure=self.datastructure,
                 schema=self.schema,
                 **self.hyperparameters,
             )
-            if not isinstance(model, DistributedModelProtocol):
+            # Mypy thinks that a subclass of "BitfountModel" and
+            # "DistributedModelProtocol" cannot exist but it can so we can ignore the
+            # errors.
+            if not isinstance(model, DistributedModelProtocol):  # type: ignore[unreachable] # Reason: see above # noqa: B950
                 raise TypeError(
                     f"Model {self.model_ref.stem} does not implement "
                     f"DistributedModelProtocol."
                 )
-            # Try to get the given (or latest if not provided)
-            # model version from the hub
-            model_response = self.hub._get_model_response(
-                username=self.username,
-                model_name=self.model_ref.stem,
-                model_version=self.model_version,
-            )
 
-            # Check hash of the last or given version before uploading,
-            # and only upload new version if they are different.
-            # Also upload model if new_version is `True`
-            if (
-                model_response is None
-                or model_response["modelHash"] != hash
-                or self.new_version
-            ):
-                self._upload_model_to_hub()
+            self._upload_model_to_hub()  # type: ignore[unreachable] # Reason: see above
 
             # self.model_ref is set to the name of the model so that the model doesn't
             # get unnecessarily re-uploaded if `get_model` is called multiple times
             self.model_ref = self.model_ref.stem
         elif isinstance(self.model_ref, str):
-            model_cls = self._get_model_from_hub(project_id=project_id)
+            model_cls = self._get_model_from_hub()
         else:
             raise TypeError(f"Model of type {type(self.model_ref)} not recognised.")
 
         return model_cls
 
-    def send_weights(self, pretrained_file: Union[Path, str]) -> None:
-        """Sends the model weights from a pretrained file to Hub.
+    def backend_tensor_shim(self) -> BackendTensorShim:
+        """Returns backend tensor shim from underlying model.
 
-        Args:
-            pretrained_file: The path to the pretrained model file.
+        This method is exposed here so that the shim can be used to create the
+        Aggregator (if required by the Protocol) when the Protocol is deserialized on
+        the Worker side.
+
+        Returns:
+            The shim for performing tensor operations.
 
         Raises:
-            ValueError: If `model_version` has not been set on BitfountModelReference
-            instance.
+            TypeError: If referenced model doesn't support backend_tensor_shim().
         """
-        if isinstance(self.model_ref, Path):
-            model_name = self.model_ref.stem
+        model = self.get_model()
+        if not hasattr(model, "backend_tensor_shim"):
+            raise TypeError("Referenced model doesn't support backend_tensor_shim().")
         else:
-            model_name = self.model_ref
-        if not self.model_version:
-            raise ValueError(
-                "You must specify model_version in BitfountModelReference "
-                "constructor to upload model weights file."
-            )
-        if isinstance(pretrained_file, str):
-            pretrained_file = Path(pretrained_file)
-        return self.hub.send_weights(model_name, self.model_version, pretrained_file)
+            # Above check ensures we have backend_tensor_shim() attribute
+            return cast(BackendTensorShim, model.backend_tensor_shim())  # type: ignore[attr-defined] # Reason: hasattr check # noqa: B950
+
+    @classmethod
+    def get_schema(cls) -> Type[BitfountModelReference._Schema]:
+        """Returns the Schema for BitfountModelReference."""
+        return cls._Schema
+
+    class _Schema(MarshmallowSchema):
+        name = fields.Str()
+        username = fields.Str(allow_none=True)
+        model_ref = fields.Method(
+            serialize="get_model_ref", deserialize="load_model_ref"
+        )
+        datastructure = fields.Nested(DataStructure._Schema)
+        hyperparameters = fields.Dict(keys=fields.Str())
+        # BitfountModelReference.{hub,schema} should not be serialized; will be need to
+        # be manually set on the created instance later.
+
+        @staticmethod
+        def get_model_ref(bfmr: BitfountModelReference) -> str:
+            """Returns the model_ref, ready for serialization."""
+            model_ref = bfmr.model_ref
+            # Try as path first
+            try:
+                return model_ref.stem  # type: ignore[union-attr]  # Reason: captured by AttributeError below  # noqa: B950
+            except AttributeError as ae:
+                # Check if class name only, return if is
+                if Path(model_ref).stem == str(model_ref):
+                    return str(model_ref)
+                # Otherwise error
+                raise TypeError(
+                    f"Unable to serialise model_ref; "
+                    f"expected python file path Path or model name str, "
+                    f"got {type(model_ref)} with value {model_ref}"
+                ) from ae
+
+        @staticmethod
+        def load_model_ref(value: str) -> Union[Path, str]:
+            """Deserialize the model_ref value."""
+            try:
+                new_value = Path(value).expanduser()
+                if new_value.stem == str(new_value):  # i.e. is just a class name
+                    return str(value)
+                return new_value
+            except TypeError:
+                return str(value)
+
+        @post_load
+        def recreate_bitfountmodelreference(
+            self, data: dict, **_kwargs: Any
+        ) -> BitfountModelReference:
+            """Recreates a BitfountModelReference from this schema."""
+            # Note that hub won't be set, will need to be set by the controlling code.
+            return BitfountModelReference(**data)
```

### Comparing `bitfount-0.5.86/bitfount/federated/modeller.py` & `bitfount-0.5.9/bitfount/federated/pod.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,632 +1,657 @@
-"""Modeller for dispatching tasks."""
-from __future__ import annotations
-
+"""Pods for responding to tasks."""
 import asyncio
 import os
-from pathlib import Path
+import threading
 from typing import (
-    TYPE_CHECKING,
-    Any,
-    Dict,
-    Iterable,
-    Literal,
-    Mapping,
+    Callable,
+    Coroutine,
+    List,
+    MutableSequence,
     Optional,
-    Type,
+    Tuple,
     Union,
     cast,
 )
 
 from cryptography.hazmat.primitives.asymmetric.rsa import RSAPrivateKey, RSAPublicKey
+from pydantic import AnyUrl
+from requests import HTTPError, RequestException
 
-from bitfount.config import BITFOUNT_DEFAULT_BATCHED_EXECUTION, BITFOUNT_STORAGE_PATH
-from bitfount.federated.aggregators.secure import SecureAggregator
+from bitfount.config import BITFOUNT_STORAGE_PATH
+from bitfount.data.datasource import DataSource
+from bitfount.data.datasplitters import _DatasetSplitter
+from bitfount.data.schema import BitfountSchema
+from bitfount.data.utils import DatabaseConnection
 from bitfount.federated.authorisation_checkers import (
     _IDENTITY_VERIFICATION_METHODS_MAP,
     IdentityVerificationMethod,
+    _AuthorisationChecker,
+    _CheckAccessRequests,
+    _OIDCAuthorisationCode,
+    _OIDCDeviceCode,
+    _SAMLAuthorisation,
     _SignatureBasedAuthorisation,
     check_identity_verification_method,
 )
-from bitfount.federated.encryption import _RSAEncryption
-from bitfount.federated.exceptions import PodResponseError
-from bitfount.federated.helper import (
-    TaskContext,
-    _check_and_update_pod_ids,
-    _create_message_service,
-    _get_idp_url,
-)
-from bitfount.federated.keys_setup import (
-    RSAKeyPair,
-    _get_key_id,
-    _get_modeller_keys,
-    _store_key_id,
-)
+from bitfount.federated.exceptions import PodRegistrationError
+from bitfount.federated.helper import _create_and_connect_pod_mailbox
 from bitfount.federated.logging import _get_federated_logger
-from bitfount.federated.model_reference import BitfountModelReference
-from bitfount.federated.monitoring import task_monitor_context
+from bitfount.federated.pod_keys_setup import PodKeys, _get_pod_keys
+from bitfount.federated.pod_response_message import _PodResponseMessage
+from bitfount.federated.pod_vitals import _PodVitals, _PodVitalsHandler
+from bitfount.federated.privacy.differential import DPPodConfig
+from bitfount.federated.task_requests import _ProtocolDetails, _TaskRequestMessage
 from bitfount.federated.transport.base_transport import _run_func_and_listen_to_mailbox
-from bitfount.federated.transport.identity_verification.oidc import (
-    _OIDCAuthFlowChallengeHandler,
-    _OIDCDeviceCodeHandler,
-)
-from bitfount.federated.transport.identity_verification.saml import (
-    _SAMLChallengeHandler,
-)
-from bitfount.federated.transport.identity_verification.types import (
-    _HasWebServer,
-    _ResponseHandler,
-)
+from bitfount.federated.transport.config import MessageServiceConfig
 from bitfount.federated.transport.message_service import (
+    _BitfountMessage,
     _BitfountMessageType,
-    _MessageService,
 )
-from bitfount.federated.transport.modeller_transport import _ModellerMailbox
-from bitfount.federated.types import _TaskRequestMessageGenerator
-from bitfount.hub.api import BitfountHub
-from bitfount.hub.authentication_flow import _get_auth_environment
-from bitfount.hub.exceptions import PodDoesNotExistError
-from bitfount.hub.helper import _default_bitfounthub, _get_pod_public_keys
-
-if TYPE_CHECKING:
-    from bitfount.federated.protocols.base import BaseProtocolFactory
-
+from bitfount.federated.transport.pod_transport import _PodMailbox
+from bitfount.federated.transport.worker_transport import _WorkerMailbox
+from bitfount.federated.types import AggregatorType, _PodResponseType
+from bitfount.federated.utils import _validate_pod_identifiers
+from bitfount.federated.worker import _Worker
+from bitfount.hub.api import BitfountAM, BitfountHub, PodPublicMetadata
+from bitfount.hub.authentication_flow import (
+    _DEFAULT_USER_DIRECTORY,
+    _get_auth_environment,
+)
+from bitfount.hub.helper import _create_bitfounthub
+from bitfount.runners.config_schemas import PodDataConfig, PodDetailsConfig
+from bitfount.transformations.dataset_operations import (
+    CleanDataTransformation,
+    NormalizeDataTransformation,
+)
+from bitfount.transformations.processor import TransformationProcessor
+from bitfount.types import _DataFrameType
+from bitfount.utils import _is_notebook
 
 logger = _get_federated_logger(__name__)
 
 
-class _Modeller:
-    """Dispatches tasks to pods and runs the modeller side of the provided protocol.
+class Pod:
+    """Makes data and computation available remotely and responds to tasks.
+
+    The basic component of the Bitfount network is the `Pod` (Processor of Data). Pods
+    are co-located with data, check users are authorized to do given operations on the
+    data and then do any approved computation. Creating a `Pod` will register the pod
+    with Bitfount Hub.
 
     ```python title="Example usage:"
     import bitfount as bf
 
-    modeller=bf.Modeller(
-        protocol=bf.FederatedAveraging(...),
+    pod = bf.Pod(
+        name="really_cool_data",
+        data="/path/to/data",
     )
-    modeller.run(pod_identifiers=["bitfount/example-pod-1", "bitfount/example-pod-2"])
+    pod.start()
     ```
 
     Args:
-        protocol: The protocol to use for the task.
-        message_service: The message service to use for communication with pods.
-            Defaults to None, in which case a new message service will be created.
-        bitfounthub: Hub instance for Bitfount Hub communication. Defaults to None,
-            in which case a new `BitfountHub` instance will be created.
-        pod_public_key_paths: Optional. Mapping of pod identifiers to public
-            key files for existing pod public keys. Expired or non-existent
-            keys will be downloaded from the Hub.
-        identity_verification_method: The identity verification method to use.
-            Defaults to OIDC_DEVICE_CODE.
-        private_key: This modeller's private key either as an `RSAPrivateKey`
-            instance or a path to a private key file. If a non-key-based identity
-            verification method is used, this is ignored. Defaults to None.
-        idp_url: URL of identity provider, used for the SAML Identity
-            Verification Method.
+        name: Name of the pod. This will appear on `Bitfount Hub` and `Bitfount AM`.
+        data: Data to be made available to the pod. This can be a Path, URL, Database
+            connection or a DataFrame.
+        username: Username of the user who is registering the pod. Defaults to None.
+        data_config: Configuration for the data. Defaults to None.
+        schema: Schema for the data. This can be a `BitfountSchema` object or a Path
+            to a serialized `BitfountSchema`. This will generated automatically if not
+            provided. Defaults to None.
+        pod_details_config: Configuration for the pod details. Defaults to None.
+        bitfount_hub: Bitfount Hub to register the pod with. Defaults to None.
+        ms_config: Configuration for the message service. Defaults to None.
+        access_manager: Access manager to use for checking access. Defaults to None.
+        pod_keys: Keys for the pod. Defaults to None.
+        approved_pods: List of other pod identifiers this pod is happy
+            to share a training task with. Required if the protocol uses the
+            `SecureAggregator` aggregator.
+        pod_dp: Differential privacy configuration for the pod. Defaults to None.
+        auto_tidy_data: Whether to tidy the data automatically (cleaning and
+            normalizing). Defaults to True.
+        koalas: Whether the data should be loaded using koalas (True) or pandas (False).
+            Defaults to False.
 
     Attributes:
-        protocol: The protocol to use for the task.
+        data: Data that the pod encapsulates.
+        name: Name of the pod.
+        pod_identifier: Identifier of the pod.
+        private_key: Private key of the pod.
+        public_key: Public key of the pod.
+        public_metadata: Public metadata about the pod.
+        schema: Schema for the data.
 
     Raises:
-        ValueError: If key-based identity verification is selected but no
-            private key is provided.
+        PodRegistrationError: If the pod could not be registered for any reason.
+
+    :::tip
+
+    Once you start a `Pod`, you can just leave it running in the background. It will
+    automatically respond to any tasks without any intervention required.
+
+    :::
     """
 
     def __init__(
         self,
-        protocol: BaseProtocolFactory,
-        message_service: Optional[_MessageService] = None,
+        name: str,
+        data: Union[
+            os.PathLike,
+            AnyUrl,
+            DatabaseConnection,
+            _DataFrameType,
+        ],
+        username: Optional[str] = None,
+        data_config: Optional[PodDataConfig] = None,
+        schema: Optional[Union[str, os.PathLike, BitfountSchema]] = None,
+        pod_details_config: Optional[PodDetailsConfig] = None,
         bitfounthub: Optional[BitfountHub] = None,
-        pod_public_key_paths: Optional[Mapping[str, Path]] = None,
-        identity_verification_method: Union[
-            str, IdentityVerificationMethod
-        ] = IdentityVerificationMethod.DEFAULT,
-        private_key: Optional[Union[RSAPrivateKey, Path]] = None,
-        idp_url: Optional[str] = None,
+        ms_config: Optional[MessageServiceConfig] = None,
+        access_manager: Optional[BitfountAM] = None,
+        pod_keys: Optional[PodKeys] = None,
+        approved_pods: Optional[List[str]] = None,
+        pod_dp: Optional[DPPodConfig] = None,
+        auto_tidy_data: bool = True,
+        koalas: bool = False,
     ):
-        self.protocol = protocol
-        self._hub = _default_bitfounthub(hub=bitfounthub)
-        self._message_service = (
-            message_service
-            if message_service is not None
-            else _create_message_service(session=self._hub.session)
+        self.name = name
+
+        if data_config is None:
+            data_config = PodDataConfig()
+
+        # Load schema if necessary
+        if schema and not isinstance(schema, BitfountSchema):
+            schema = BitfountSchema.load_from_file(schema)
+        # Schema will be loaded or None by here
+        schema = cast(Optional[BitfountSchema], schema)
+
+        self.data, self.schema = self._setup_data(
+            data, data_config, schema=schema, auto_tidy=auto_tidy_data, koalas=koalas
         )
-        self._pod_public_key_paths: Mapping[str, Path] = pod_public_key_paths or dict()
 
-        self._identity_verification_method: IdentityVerificationMethod = (
-            check_identity_verification_method(identity_verification_method)
+        pod_details_config = (
+            pod_details_config
+            if pod_details_config is not None
+            else self._get_default_pod_details_config()
+        )
+        self.public_metadata = self._get_public_metadata(
+            pod_details_config, self.schema
         )
 
-        self._private_key = _Modeller._process_private_key(
-            private_key, self._identity_verification_method, self._hub
+        self._hub = (
+            bitfounthub
+            if bitfounthub is not None
+            else _create_bitfounthub(username=username)
         )
+        self._session = self._hub.session
 
-        self._idp_url: Optional[str]
-        if self._identity_verification_method == IdentityVerificationMethod.SAML:
-            # We also need to set the idp_url based on the environment if using SAML
-            self._idp_url = idp_url if idp_url is not None else _get_idp_url()
-        else:
-            self._idp_url = None
+        self._access_manager = (
+            access_manager if access_manager is not None else BitfountAM(self._session)
+        )
+        self._access_manager_public_key = self._access_manager.get_access_manager_key()
 
-    @staticmethod
-    def _is_public_key_registered(
-        username: str, public_key: RSAPublicKey, hub: BitfountHub
-    ) -> bool:
-        key_id = None
-        try:
-            key_id = _get_key_id(_Modeller._get_modeller_key_storage_path(username))
-        except IOError:
-            logger.debug(
-                "Key present, but no key ID file found. "
-                "Will re-register key with the hub"
-            )
-
-        if key_id:
-            # Validate against registered public key or register if not already present
-            logger.debug(
-                f"Checking hub for registered modeller public key"
-                f" for modeller {username} with ID {key_id}"
-            )
-            key_with_metadata = hub.check_public_key_registered_and_active(key_id)
-            if key_with_metadata:
-                if _RSAEncryption.public_keys_equal(
-                    public_key, key_with_metadata["public_key"]
-                ):
-                    logger.debug(
-                        f"Public key already registered for modeller {username}"
-                    )
-                    return True
-                else:
-                    logger.debug(
-                        f"Key provided doesn't match "
-                        f"the one stored on the hub with ID {key_id}"
-                    )
-            else:
-                logger.debug("Key provided, but it's not registered on the hub")
-        return False
+        self.private_key, self.pod_public_key = self._get_default_pod_keys(pod_keys)
 
-    @staticmethod
-    def _process_private_key(
-        private_key: Optional[Union[RSAPrivateKey, Path]],
-        identity_verification_method: IdentityVerificationMethod,
-        hub: BitfountHub,
-    ) -> Optional[RSAPrivateKey]:
-        # NOTE: This method is static to avoid the risk of it being called during
-        #       _Modeller initialisation before the identity_verification_method
-        #       or hub attributes have been set.
-        # Fail out fast if not using key-based authorisation
-        if identity_verification_method != IdentityVerificationMethod.KEYS:
-            if private_key:
-                logger.warning(
-                    f"Private key provided but identity verification method "
-                    f'"{identity_verification_method.value}" was chosen. '
-                    f"Private key will be ignored."
-                )
-            return None
+        self.pod_identifier = f"{self._session.username}/{self.name}"
+        if approved_pods is None:
+            approved_pods = []
+        _validate_pod_identifiers([*approved_pods, self.pod_identifier])
+        self.approved_pods = approved_pods
+
+        self._pod_dp = pod_dp
+        self._pod_vitals = _PodVitals()
+
+        # Connecting the pod to the message service must happen AFTER registering
+        # it on the hub as the message service uses hub information to verify that
+        # the relevant message queue is available.
+        self._register_pod()
+        self._ms_config: Optional[MessageServiceConfig] = ms_config
+        self._mailbox: Optional[_PodMailbox] = None
+
+        # Marker for when initialization is complete
+        self._initialised: bool = False
 
-        username = hub.username
+    @staticmethod
+    def _setup_data(
+        data: Union[
+            os.PathLike,
+            AnyUrl,
+            DatabaseConnection,
+            _DataFrameType,
+        ],
+        pod_data_config: PodDataConfig,
+        schema: Optional[BitfountSchema] = None,
+        auto_tidy: bool = True,
+        koalas: bool = False,
+    ) -> Tuple[DataSource, BitfountSchema]:
+        # Create splitter from config
+        data_splitter = _DatasetSplitter.create(
+            pod_data_config.data_split.data_splitter, **pod_data_config.data_split.args
+        )
 
-        if isinstance(private_key, Path):
-            # Load private key file if needed
-            logger.debug(
-                f"Loading private key for modeller {username} from {str(private_key)}"
-            )
-            private_key = _RSAEncryption.load_private_key(private_key)
-            public_key = private_key.public_key()
-            # If we loaded it from a file, it may already be registered
-            if _Modeller._is_public_key_registered(username, public_key, hub):
-                return private_key
+        # Extract actual data reference depending on type
+        data_ref: Union[os.PathLike, AnyUrl, DatabaseConnection, _DataFrameType] = data
 
-        elif private_key is None:
-            # Generate/load stored keys
-            logger.info(
-                f"No keys provided for modeller {username}; loading/generating keys"
+        # Create datasource
+        datasource = DataSource(
+            data_ref,
+            data_splitter=data_splitter,
+            koalas=koalas,
+            ignore_cols=pod_data_config.ignore_cols,
+            modifiers=pod_data_config.modifiers,
+            **pod_data_config.datasource_args,
+        )
+        # Create schema if not provided
+        if not schema:
+            schema = BitfountSchema(
+                datasource,
+                force_stype=pod_data_config.force_stype,
             )
-            key_pair = _Modeller._get_modeller_keys(username)
-            private_key = key_pair.private
-            public_key = key_pair.public
-
-            # If the key was loaded it might already be registered
-            if _Modeller._is_public_key_registered(username, public_key, hub):
-                return private_key
-        elif isinstance(private_key, RSAPrivateKey):
-            # Just use private key as is
-            # We can't feasibly check this is registered without iterating
-            # through them all, so we're going to register it
-            public_key = private_key.public_key()
+        # Otherwise add to existing schema
         else:
-            # Must be invalid type
-            raise TypeError(
-                f"Error processing private key; expected a Path, RSAPrivateKey"
-                f" or None, got {type(private_key)}"
-            )
-
-        # Register new public key
-        logger.info(f"Registering public key for modeller {username}")
-        _store_key_id(
-            _Modeller._get_modeller_key_storage_path(username),
-            str(hub.register_user_public_key(public_key)),
+            schema.add_datasource_features(
+                datasource,
+                force_stype=pod_data_config.force_stype,
+            )
+
+        # Freeze schema
+        schema.freeze()
+
+        # Transform dataset
+        if auto_tidy:
+            clean_data = CleanDataTransformation()
+            # TODO: [BIT-962] data normalizing should not be done here as this converts
+            #       all integer columns to floats
+            normalize = NormalizeDataTransformation()
+            processor = TransformationProcessor([clean_data, normalize], schema)
+            datasource.data = processor.transform(datasource.data)
+
+        return datasource, schema
+
+    def _get_default_pod_details_config(self) -> PodDetailsConfig:
+        return PodDetailsConfig(display_name=self.name, description=self.name)
+
+    def _get_public_metadata(
+        self, pod_details_config: PodDetailsConfig, schema: BitfountSchema
+    ) -> PodPublicMetadata:
+        return PodPublicMetadata(
+            self.name,
+            pod_details_config.display_name,
+            pod_details_config.description,
+            schema.to_json(),
+            pod_details_config.is_public,
         )
 
-        return private_key
+    def _get_default_pod_keys(
+        self, pod_keys: Optional[PodKeys]
+    ) -> Tuple[RSAPrivateKey, RSAPublicKey]:
+        if pod_keys is None:
+            user_storage_path = BITFOUNT_STORAGE_PATH / _DEFAULT_USER_DIRECTORY
+            pod_directory = user_storage_path / "pods" / self.name
+            pod_keys = _get_pod_keys(pod_directory)
+        return pod_keys.private, pod_keys.public
 
-    @staticmethod
-    def _get_modeller_key_storage_path(username: str) -> Path:
-        """Get the appropriate storage directory for the modeller keys."""
-        # NOTE: We explicitly construct the storage path (rather than relying on
-        #       the user_storage_path from the hub/session) to guarantee that the
-        #       key storage location is explicitly linked to the username and avoids
-        #       using `_default`.
-        return Path(BITFOUNT_STORAGE_PATH / username / "modeller" / "keys")
+    def _register_pod(self) -> None:
+        """Register pod with Bitfount Hub.
 
-    @staticmethod
-    def _get_modeller_keys(username: str) -> RSAKeyPair:
-        """Load existing modeller keys or generate new ones.
+        If Pod is already registered, will update pod details if anything has changed.
 
-        Args:
-            username: The user to generate/load the keys for.
+        Raises:
+            PodRegistrationError: if registration fails for any reason
+        """
+        try:
+            logger.info("Registering/Updating details on Bitfount Hub.")
+            self._hub.register_pod(
+                self.public_metadata,
+                self.pod_public_key,
+                self._access_manager_public_key,
+            )
+        except HTTPError as ex:
+            logger.critical(f"Failed to register with hub: {ex}")
+            raise PodRegistrationError("Failed to register with hub") from ex
+        except RequestException as ex:
+            logger.critical(f"Could not connect to hub: {ex}")
+            raise PodRegistrationError("Could not connect to hub") from ex
 
-        Returns:
-            The loaded/generated keys.
+    async def _initialise(self) -> None:
+        """Initialises the pod.
+
+        Sets any attributes that could not be created at creation time.
         """
-        key_storage_path = _Modeller._get_modeller_key_storage_path(username)
-        keys = _get_modeller_keys(key_storage_path)
-        return keys
+        if not self._initialised:
+            # Create mailbox. Cannot be done in __init__ due to async nature.
+            self._mailbox = await _create_and_connect_pod_mailbox(
+                pod_name=self.name, session=self._session, ms_config=self._ms_config
+            )
 
-    async def _send_task_requests(
-        self,
-        pod_identifiers: Iterable[str],
-        project_id: Optional[str],
-        run_on_new_data_only: bool = False,
-        batched_execution: Optional[bool] = None,
-    ) -> _ModellerMailbox:
-        """Sends task requests to pods.
+            # Set initialised state
+            self._initialised = True
+        else:
+            logger.warning("Pod._initialise() called twice. This is not allowed.")
+
+    def _secure_aggregation_other_workers_response(
+        self, other_worker_names: MutableSequence[str]
+    ) -> Optional[List[str]]:
+        """Checks if secure aggregation can be performed with given other workers.
 
         Args:
-            pod_identifiers: The group of pods to run the task with.
-            project_id: Project Id the task belongs to.
-            run_on_new_data_only: Whether to run the task on new datapoints only.
-                Defaults to False.
-            batched_execution: Whether to run the task in batched mode. Defaults to
-                False.
+            other_worker_names (List[str]): list of other worker names
 
         Returns:
-            The created ModellerMailbox for the task.
+            Optional[List[str]]:
+                unapproved workers (if they exist in other_worker_names)
         """
-        if batched_execution is None:
-            batched_execution = BITFOUNT_DEFAULT_BATCHED_EXECUTION
-        # Load pod public keys
-        loaded_pod_public_keys: Dict[str, RSAPublicKey] = _get_pod_public_keys(
-            pod_identifiers=pod_identifiers,
-            hub=self._hub,
-            pod_public_key_paths=self._pod_public_key_paths,
-        )
-
-        task_request_msg_gen = self._get_task_request_msg_gen()
-        # Send task requests to chosen pods. We haven't attached a handler for the
-        # responses yet but this is handled below. There is no issue due to the
-        # fact that the listening task hasn't been started yet, and the inherent
-        # handler backoff.
-        modeller_mailbox: _ModellerMailbox = await _ModellerMailbox.send_task_requests(
-            serialized_protocol=self.protocol.dump(),
-            pod_public_keys=loaded_pod_public_keys,
-            task_request_msg_gen=task_request_msg_gen,
-            message_service=self._message_service,
-            project_id=project_id,
-            run_on_new_data_only=run_on_new_data_only,
-            batched_execution=batched_execution,
-        )
-
-        return modeller_mailbox
-
-    def _get_task_request_msg_gen(self) -> _TaskRequestMessageGenerator:
-        """Construct correct TaskRequestMessageGenerator object for the auth type."""
-        authorization_checker_cls = _IDENTITY_VERIFICATION_METHODS_MAP[
-            self._identity_verification_method
+        unapproved_pods = [
+            worker for worker in other_worker_names if worker not in self.approved_pods
         ]
+        logger.debug(
+            f"Modeller requested aggregation"
+            f" with non-approved pods: {unapproved_pods}"
+        )
 
-        if self._identity_verification_method == IdentityVerificationMethod.KEYS:
-            authorization_checker_cls = cast(
-                Type[_SignatureBasedAuthorisation], authorization_checker_cls
-            )
-            assert isinstance(self._private_key, RSAPrivateKey)  # nosec assert_used
-            return authorization_checker_cls.create_task_request_message_generator(
-                self._private_key
+        if unapproved_pods:
+            logger.info(
+                "Modeller requested aggregation with"
+                " pods that this pod has not approved."
             )
-        else:
-            return authorization_checker_cls.create_task_request_message_generator()
+            return unapproved_pods
 
-    async def _modeller_run(
+        logger.debug("All pods requested by modeller for aggregation are approved.")
+        return None
+
+    def _check_for_unapproved_pods(
         self,
-        modeller_mailbox: _ModellerMailbox,
-        pod_identifiers: Iterable[str],
-        require_all_pods: bool = False,
-        response_handler: Optional[_ResponseHandler] = None,
-        batched_execution: Optional[bool] = None,
-    ) -> Union[Literal[False], Optional[Any]]:
-        """Waits for pod responses and handles any who don't respond in time.
+        pods_involved_in_task: List[str],
+        task_protocol_details: _ProtocolDetails,
+    ) -> Optional[List[str]]:
+        """Returns the pods that we're not happy to work with.
+
+        If secure aggregation has been requested then this will
+        identify any pods that we've not approved.
+
+        In any other case it returns None, as there's no concern
+        around security with other pods.
 
-        Runs the modeller side of the protocol if some pods accepted the task.
+        Args:
+            pods_involved_in_task (List[str]): A list of other pods
+                that have been contacted by the modeller for this task
+            task_protocol_details (ProtocolDetails): The decrypted protocol details
+                portion of the task request.
+
+        Returns:
+            Optional[List[str]]
+                - List of unapproved pods
+                    or None if all are approved
+                    or None if secure aggregation not in use
         """
-        # If additional response handling is specified, we call the handler
-        if response_handler:
-            await response_handler.handle(modeller_mailbox)
-        if batched_execution is None:
-            batched_execution = BITFOUNT_DEFAULT_BATCHED_EXECUTION
-        # Process job accept/reject
-        await modeller_mailbox.process_task_request_responses()
-
-        # Fail-fast if no pods accepted the task request.
-        if not modeller_mailbox.accepted_worker_mailboxes:
-            logger.error(
-                "No workers with which to train. "
-                "Please ensure you have sent task requests at least some of "
-                "which have been accepted."
-            )
-            return False
-
-        # Otherwise run the protocol
-        unaccepted_worker_ids = set(pod_identifiers).difference(
-            set(modeller_mailbox.accepted_worker_mailboxes)
-        )
-        if unaccepted_worker_ids:
-            unaccepted_worker_msg = (
-                f"Pods {', '.join(unaccepted_worker_ids)} "
-                "rejected task request or failed to respond. "
-            )
-            if require_all_pods:
-                raise PodResponseError(
-                    unaccepted_worker_msg
-                    + "Task requires all pods accept the task request."
-                )
-            else:
-                logger.warning(
-                    unaccepted_worker_msg + "Continuing task without these pods ..."
-                )
+        unapproved_workers = None
 
-        await modeller_mailbox.send_task_start_message()
+        # We don't need to check if we're approved to work with ourself.
+        try:
+            pods_involved_in_task.remove(self.pod_identifier)
+        except ValueError:  # if not in list to remove
+            pass
 
-        return await self._run_modeller_protocol(modeller_mailbox, batched_execution)
+        if task_protocol_details.aggregator == AggregatorType.SecureAggregator.value:
+            logger.info(
+                "Secure aggregation is in use, checking responses from other pods."
+            )
+            unapproved_workers = self._secure_aggregation_other_workers_response(
+                pods_involved_in_task
+            )
 
-    async def _run_modeller_protocol(
-        self,
-        modeller_mailbox: _ModellerMailbox,
-        batched_execution: Optional[bool] = None,
-    ) -> Optional[Any]:
-        """Runs the modeller-side of the protocol."""
-        if batched_execution is None:
-            batched_execution = BITFOUNT_DEFAULT_BATCHED_EXECUTION
-        protocol = self.protocol.modeller(mailbox=modeller_mailbox)
-        # Run the task
-        result = await protocol.run(
-            batched_execution=batched_execution, context=TaskContext.MODELLER
-        )
-        # Send task complete message
-        await modeller_mailbox.send_task_complete_message()
-        return result
+        return unapproved_workers
+
+    async def _new_task_request_handler(self, message: _BitfountMessage) -> None:
+        """Called on new task request being received from message service."""
+        logger.info(f"Training task request received from '{message.sender}'")
+        try:
+            await self._create_and_run_worker(message)
+        except TimeoutError as te:
+            logger.warning("Training timed out, modeller was too slow!")
+            logger.warning(te)
+            # TODO: [BIT-841][BIT-788] Send modeller a message?
+            return
+
+    async def _create_and_run_worker(self, message: _BitfountMessage) -> None:
+        """Creates and runs a worker instance."""
+        # `_initialise` is always called before this method, so we can assume
+        # that the mailbox is initialised. Reassuring mypy that this is True.
+        assert isinstance(self._mailbox, _PodMailbox)  # nosec
+
+        # Unpack task details from received message
+        task_request_message: _TaskRequestMessage = _TaskRequestMessage.deserialize(
+            message.body
+        )
+        auth_type: IdentityVerificationMethod = check_identity_verification_method(
+            task_request_message.auth_type
+        )
+        authoriser_cls = _IDENTITY_VERIFICATION_METHODS_MAP[auth_type]
+        task_request = authoriser_cls.unpack_task_request(
+            message.body, self.private_key
+        )
+
+        unapproved_workers = self._check_for_unapproved_pods(
+            list(message.pod_mailbox_ids.keys()), task_request.protocol_details
+        )
+
+        # We construct the mailbox regardless of whether or not
+        # there are unapproved workers.
+        # This is because we need to use AES encryption to
+        # respond to the modeller on the task specific mailbox.
+        worker_mailbox = _WorkerMailbox(
+            pod_identifier=self.pod_identifier,
+            modeller_mailbox_id=message.sender_mailbox_id,
+            modeller_name=message.sender,
+            aes_encryption_key=task_request.aes_key,
+            message_service=self._mailbox.message_service,
+            pod_mailbox_ids=message.pod_mailbox_ids,
+        )
+
+        # TODO: [BIT-1045] Move the secure aggregation allowed check to the access
+        #       manager once we support configuring or storing it there.
+        authorisation_errors = _PodResponseMessage(message.sender, self.pod_identifier)
+        if unapproved_workers:
+            # There are pods we're explicitly not happy to work with
+            # It'll cause the task to be rejected
+            logger.info(f"Task from '{message.sender}' rejected.")
+            authorisation_errors.add(
+                _PodResponseType.SECURE_AGGREGATION_WORKERS_NOT_AUTHORISED,
+                unapproved_workers,
+            )
+            await worker_mailbox.reject_task(authorisation_errors.messages)
+            return
+
+        authorisation_checker = self._create_authorisation_checker(
+            task_request_message=task_request_message,
+            sender=message.sender,
+            worker_mailbox=worker_mailbox,
+        )
+
+        # Create worker
+        worker = _Worker(
+            data=self.data,
+            mailbox=worker_mailbox,
+            bitfounthub=self._hub,
+            authorisation=authorisation_checker,
+            pod_vitals=self._pod_vitals,
+            pod_dp=self._pod_dp,
+        )
 
-    async def run_async(
+        # Run the worker and the mailbox listening simultaneously
+        await _run_func_and_listen_to_mailbox(worker.run(), worker_mailbox)
+
+    def _create_authorisation_checker(
         self,
-        pod_identifiers: Iterable[str],
-        require_all_pods: bool,
-        project_id: Optional[str] = None,
-        run_on_new_data_only: bool = False,
-        batched_execution: Optional[bool] = None,
-    ) -> Union[Literal[False], Optional[Any]]:
-        """Runs the modeller's task with a set of pods.
+        task_request_message: _TaskRequestMessage,
+        sender: str,
+        worker_mailbox: _WorkerMailbox,
+    ) -> _AuthorisationChecker:
+        """Create appropriate Authorisation Checker.
 
-        Will send a task request before commencing the task itself and only pods that
-        accept the task request will be used.
+        Determines checker to create based on supplied auth_type.
 
         Args:
-            pod_identifiers: The identifiers for the pods to run the task on.
-            require_all_pods: Only run task if all pods are online.
-            project_id: Project Id the task belongs to. Defaults to None.
-            run_on_new_data_only: Whether to run the task on new datapoints only.
-                Defaults to False.
-            batched_execution: Whether to run the task in batched mode. Defaults to
-                False.
+            task_request_message: The full task request message.
+            sender: The sender (i.e. modeller) of the request.
+            worker_mailbox: Worker mailbox for communication with modeller.
 
         Returns:
-            Whatever the protocol's return value is.
-
-        Raises:
-            PodResponseError: If require_all_pods is True and at least one
-                pod_identifier rejects or fails to respond to the task request.
+            An authorisation checker.
         """
-        if batched_execution is None:
-            batched_execution = BITFOUNT_DEFAULT_BATCHED_EXECUTION
-        try:
-            # We do this early to give any handler components time to spin up
-            response_handler = self._get_response_handler()
-
-            logger.info("Sending task requests...")
-            modeller_mailbox: _ModellerMailbox = await self._send_task_requests(
-                pod_identifiers, project_id, run_on_new_data_only, batched_execution
-            )
-
-            with task_monitor_context(
-                hub=self._hub,
-                task_id=modeller_mailbox.task_id,
-                sender_id=modeller_mailbox.mailbox_id,
-            ):
-                run_result: Union[
-                    Literal[False], Optional[Any]
-                ] = await _run_func_and_listen_to_mailbox(
-                    self._modeller_run(
-                        modeller_mailbox=modeller_mailbox,
-                        pod_identifiers=pod_identifiers,
-                        require_all_pods=require_all_pods,
-                        response_handler=response_handler,
-                        batched_execution=batched_execution,
-                    ),
-                    modeller_mailbox,
-                )
-                modeller_mailbox.delete_all_handlers(_BitfountMessageType.LOG_MESSAGE)
+        auth_type: IdentityVerificationMethod = check_identity_verification_method(
+            task_request_message.auth_type
+        )
+        authorisation_checker_cls = _IDENTITY_VERIFICATION_METHODS_MAP[auth_type]
 
-                logger.info("Task complete")
-                return run_result
-        except PodDoesNotExistError as e:
-            logger.exception(e)
-            logger.error("Aborted task request.")
-            return False
-        finally:
-            # Stop any response handler web servers that may be running still
-            try:
-                # noinspection PyUnboundLocalVariable
-                await cast(_HasWebServer, response_handler).stop_server()
-            except NameError:
-                logger.warning("Tried to shutdown non-existent response handler")
-            except AttributeError:
-                # Didn't have a stop_server() method
-                pass
+        task_request = authorisation_checker_cls.unpack_task_request(
+            task_request_message, self.private_key
+        )
+        task_protocol_details = task_request.protocol_details
 
-    def _get_response_handler(self) -> Optional[_ResponseHandler]:
-        """Construct a response handler for additional task messages if required.
+        pod_response_message = _PodResponseMessage(
+            modeller_name=sender,
+            pod_identifier=self.pod_identifier,
+        )
 
-        This may spin-up web servers if the response handler requires it. It is
-        the responsibility of the handler to shut these down when no longer needed.
-        """
-        response_handler: Optional[_ResponseHandler] = None
+        authorisation_checker: _AuthorisationChecker
+        if auth_type == IdentityVerificationMethod.KEYS:
+            # Public Key Signature authorisation
+            packed_request = (
+                authorisation_checker_cls.extract_from_task_request_message(
+                    task_request_message
+                )
+            )
 
-        # SAML
-        if self._identity_verification_method == IdentityVerificationMethod.SAML:
-            logger.debug("Setting up SAML challenge listener")
-            # If SAML is used, the url is set in the modeller init
-            self._idp_url = cast(str, self._idp_url)
-            response_handler = _SAMLChallengeHandler(self._idp_url)
-
-        # OIDC_ACF_PKCE
-        elif (
-            self._identity_verification_method
-            == IdentityVerificationMethod.OIDC_ACF_PKCE
-        ):
+            authorisation_checker = _SignatureBasedAuthorisation(
+                pod_response_message=pod_response_message,
+                access_request_checker=_CheckAccessRequests(
+                    hub=self._hub,
+                    access_manager_public_key=self._access_manager_public_key,
+                ),
+                encrypted_task_request=packed_request.encrypted_request,
+                signature=packed_request.signature,
+                task_protocol_details=task_protocol_details,
+            )
+        elif auth_type == IdentityVerificationMethod.OIDC_ACF_PKCE:
+            # OIDC Authorization Code Flow
             auth_env = _get_auth_environment()
-            logger.debug(
-                f"Setting up OIDC Authorization Code Flow challenge listener against "
-                f"{auth_env.name} authorization environment."
-            )
-            response_handler = _OIDCAuthFlowChallengeHandler(
-                auth_domain=auth_env.auth_domain
+            authorisation_checker = _OIDCAuthorisationCode(
+                pod_response_message=pod_response_message,
+                access_manager=self._access_manager,
+                mailbox=worker_mailbox,
+                task_protocol_details=task_protocol_details,
+                _auth_domain=auth_env.auth_domain,
+                _client_id=auth_env.client_id,
             )
-
-        # OIDC_DEVICE_CODE
-        elif (
-            self._identity_verification_method
-            == IdentityVerificationMethod.OIDC_DEVICE_CODE
-        ):
+        elif auth_type == IdentityVerificationMethod.OIDC_DEVICE_CODE:
+            # OIDC Device Code flow
             auth_env = _get_auth_environment()
-            response_handler = _OIDCDeviceCodeHandler(auth_domain=auth_env.auth_domain)
+            authorisation_checker = _OIDCDeviceCode(
+                pod_response_message=pod_response_message,
+                access_manager=self._access_manager,
+                mailbox=worker_mailbox,
+                task_protocol_details=task_protocol_details,
+                _auth_domain=auth_env.auth_domain,
+                _client_id=auth_env.client_id,
+            )
+        else:
+            # Default to SAML Authorisation
+            authorisation_checker = _SAMLAuthorisation(
+                pod_response_message=pod_response_message,
+                access_manager=self._access_manager,
+                mailbox=worker_mailbox,
+                task_protocol_details=task_protocol_details,
+            )
+        return authorisation_checker
+
+    @staticmethod
+    async def _repeat(interval: int, func: Callable[..., Coroutine]) -> None:
+        """Run coroutine func every interval seconds.
 
-        # Start web server if needed
-        # cast()s are needed to allow mypy to infer this as reachable
-        if isinstance(cast(_HasWebServer, response_handler), _HasWebServer):
-            # Start response handler web server in background as it takes some time
-            cast(_HasWebServer, response_handler).start_server()
-
-        return response_handler
-
-    def _serialize(self, model_out: Union[str, os.PathLike]) -> None:
-        """Save any models to file.
-
-        Save any algorithms which have models to the filename specified. If there is
-        more than one algorithm with a model, the model name will be be appended with a
-        number, starting from 1.
+        If func has not finished before *interval*, will run again
+        immediately when the previous iteration finished.
 
         Args:
-            model_out: The path to save the model out to.
+            interval: run interval in seconds
+            func: function to call which returns a coroutine to await
         """
-        model_algorithms = [i for i in self.protocol.algorithms if hasattr(i, "model")]
+        while True:
+            await asyncio.gather(func(), asyncio.sleep(interval))
 
-        for i, algo in enumerate(model_algorithms, start=1):
-            model_out = (
-                f"{str(model_out)}_{i}" if len(model_algorithms) > 1 else str(model_out)
-            )
-            try:
-                algo.model.serialize(model_out)  # type: ignore[attr-defined] # Reason: Captured by try clause  # noqa: B950
-                logger.info(f'Model saved successfully to "{model_out}"')
-            except Exception as e:
-                logger.exception(e)
+    async def _pod_heartbeat(self) -> None:
+        """Makes a pod heartbeat to the hub."""
+        try:
+            self._hub.do_pod_heartbeat(self.name, self.pod_public_key)
+        except HTTPError as ex:
+            logger.warning(f"Failed to reach hub for status: {ex}")
+        except RequestException as ex:
+            logger.warning(f"Could not connect to hub for status: {ex}")
+
+    def _run_pod_heartbeat_task(self) -> None:
+        """Makes 10-second interval pod heartbeats to the hub."""
+        if _is_notebook():
+            # We need to create a new event loop here for jupyter
+            # As it's run in a new thread and can't be patched by nest_asyncio
+            asyncio.set_event_loop(asyncio.new_event_loop())
+        asyncio.run(self._repeat(10, self._pod_heartbeat))
+
+    def _run_pod_vitals_server(self, vitals_handler: _PodVitalsHandler) -> None:
+        """Run _PodVitals webserver."""
+        # The Pod Vitals webserver should run until the
+        # pod itself it shut down. asyncio.run would handle
+        # the event loop for us however it would also
+        # shutdown the loop (and the webserver) on completion
+        # so instead we directly interact with the
+        # event loop here to ensure it is run_forever.
+        pod_vitals_loop = asyncio.new_event_loop()
+        asyncio.set_event_loop(pod_vitals_loop)
+        vitals_handler.start(pod_vitals_loop)
+        pod_vitals_loop.run_forever()
+
+    async def start_async(self) -> None:
+        """Starts a pod instance, listening for tasks.
+
+        Whenever a task is received, a worker is created to handle it. Runs continuously
+        and asynchronously orchestrates training whenever a task arrives i.e. multiple
+        tasks can run concurrently.
+        """
+        # Do post-init initialization work
+        await self._initialise()
 
-    def run(
-        self,
-        pod_identifiers: Iterable[str],
-        require_all_pods: bool = False,
-        model_out: Optional[Union[Path, str]] = None,
-        project_id: Optional[str] = None,
-        run_on_new_data_only: bool = False,
-        batched_execution: bool = False,
-    ) -> Union[Literal[False], Optional[Any]]:
-        """Runs the modeller's task with a set of pods.
+        # `_initialise` has just been called which sets the mailbox so we can assume
+        # that the mailbox is initialised. Reassuring mypy that this is True.
+        assert isinstance(self._mailbox, _PodMailbox)  # nosec
+
+        # Setup heartbeat to hub
+        logger.info(f"Starting pod {self.name}...")
+        threading.Thread(daemon=True, target=self._run_pod_heartbeat_task).start()
+
+        # Setup vitals webserver
+        vitals_handler = _PodVitalsHandler(self._pod_vitals)
+        logger.debug("Starting Pod Vitals interface...")
+        threading.Thread(
+            daemon=True, target=self._run_pod_vitals_server, args=(vitals_handler,)
+        ).start()
+
+        # Attach handler for new tasks
+        self._mailbox.register_handler(
+            _BitfountMessageType.JOB_REQUEST, self._new_task_request_handler
+        )
 
-        Will send a task request before commencing the task itself and only pods
-        that accept the task request will be used.
+        # Start pod listening for messages
+        logger.info("Pod started... press Ctrl+C to stop")
+        await self._mailbox.listen_for_messages()
 
-        Args:
-            pod_identifiers: The identifiers for the pods to run the task on.
-            require_all_pods: Only run task if all pods are online.
-            model_out: If specified, path to save the model out to.
-            project_id: Project_id the task belongs to.
-            run_on_new_data_only: Whether to run the task on new datapoints only.
-                Defaults to False.
-            batched_execution: Whether to run the task in batched mode. Defaults to
-                False.
+        logger.info(f"Pod {self.name} stopped.")
 
-        Returns:
-            Whatever the protocol's return value is.
+        # Shutdown pod vitals webserver
+        vitals_handler.runner.cleanup()
 
-        Raises:
-            PodResponseError: If require_all_pods is True and at least one
-                pod_identifier rejects or fails to respond to the task request.
-        """
-        pod_identifiers = _check_and_update_pod_ids(pod_identifiers, self._hub)
-
-        # We need to pass the `SecureShare` parameters to the
-        # model when SecureAggregation is in use.
-        # First we check the protocol's model and aggregators.
-        # if steps_between_parameter_updates is 1, then we do
-        # the clipping at the 'SecureShare' level.
-        if (
-            hasattr(self.protocol, "aggregator")
-            and isinstance(self.protocol.aggregator, SecureAggregator)
-        ) and (
-            hasattr(self.protocol, "steps_between_parameter_updates")
-            and self.protocol.steps_between_parameter_updates != 1
-        ):
-            if any(hasattr(algo, "model") for algo in self.protocol.algorithms):
-                logger.warning(
-                    "SecureAggregation in use. We recommend normalization "
-                    "of continuous features prior to training."
-                )
+    def start(self) -> None:
+        """Starts a pod instance, listening for tasks.
 
-            # Show warning to user if using custom models that
-            # they might have to implement clipping in their custom model.
-            if any(
-                hasattr(algo, "model")
-                and isinstance(algo.model, BitfountModelReference)
-                for algo in self.protocol.algorithms
-            ):
-                logger.warning(
-                    "You are using a custom model with Secure Aggregation."
-                    "We recommend clipping the model parameters."
-                )
-
-            # Pass the `SecureShare` parameters to the model for clipping
-            for algo in self.protocol.algorithms:
-                if hasattr(algo, "model"):
-                    algo.model.param_clipping = {
-                        "prime_q": self.protocol.aggregator._secure_share.prime_q,
-                        "precision": self.protocol.aggregator._secure_share.precision,
-                        "num_workers": len(pod_identifiers),
-                    }
-        result = asyncio.run(
-            self.run_async(
-                pod_identifiers,
-                require_all_pods,
-                project_id,
-                run_on_new_data_only,
-                batched_execution,
-            )
-        )
-        if model_out and result is not False:
-            self._serialize(model_out)
-        return result
+        Whenever a task is received, a worker is created to handle it. Runs continuously
+        and asynchronously orchestrates training whenever a task arrives i.e. multiple
+        tasks can run concurrently.
+        """
+        asyncio.run(self.start_async())
```

### Comparing `bitfount-0.5.86/bitfount/federated/pod_response_message.py` & `bitfount-0.5.9/bitfount/federated/pod_response_message.py`

 * *Files identical despite different names*

### Comparing `bitfount-0.5.86/bitfount/federated/protocols/model_protocols/federated_averaging.py` & `bitfount-0.5.9/bitfount/federated/protocols/fed_avg.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,171 +1,184 @@
 """Federated Averaging protocol."""
 from __future__ import annotations
 
 import asyncio
 import os
+from pathlib import Path
 import time
 from typing import (
     TYPE_CHECKING,
     Any,
-    ClassVar,
     Dict,
+    Iterable,
     List,
     Mapping,
     Optional,
     Protocol,
     Tuple,
+    Type,
     Union,
     cast,
     runtime_checkable,
 )
 
-from marshmallow import fields
+from cryptography.hazmat.primitives.asymmetric.rsa import RSAPrivateKey
+from marshmallow import Schema as MarshmallowSchema
+from marshmallow import fields, post_load
 
-from bitfount.data.datasources.base_source import BaseSource
+from bitfount.data.datasource import DataSource
 from bitfount.federated.aggregators.base import (
     _AggregatorWorkerFactory,
     _BaseAggregator,
     _BaseAggregatorFactory,
     _BaseModellerAggregator,
     _BaseWorkerAggregator,
-    registry as aggregators_registry,
 )
 from bitfount.federated.aggregators.secure import _InterPodAggregatorWorkerFactory
-from bitfount.federated.algorithms.model_algorithms.base import (
-    registry as algorithms_registry,
+from bitfount.federated.algorithms.model_algorithms.federated_training import (
+    FederatedModelTraining,
+)
+from bitfount.federated.authorisation_checkers import (
+    IdentityVerificationMethod,
+    check_identity_verification_method,
 )
 from bitfount.federated.early_stopping import FederatedEarlyStopping
-from bitfount.federated.helper import _create_aggregator
+from bitfount.federated.helper import (
+    _create_aggregator,
+    _create_federated_averaging_protocol_factory,
+    _create_message_service,
+)
 from bitfount.federated.logging import _get_federated_logger
-from bitfount.federated.model_reference import BitfountModelReference
 from bitfount.federated.pod_vitals import _PodVitals
 from bitfount.federated.privacy.differential import DPPodConfig
 from bitfount.federated.protocols.base import (
-    BaseCompatibleAlgoFactory,
-    BaseModellerProtocol,
-    BaseProtocolFactory,
-    BaseWorkerProtocol,
+    _BaseCompatibleAlgoFactory,
+    _BaseModellerProtocol,
+    _BaseProtocolFactory,
+    _BaseWorkerProtocol,
+    _ProtocolAggregatorSchema,
+    _run_protocol,
 )
+from bitfount.federated.transport.config import MessageServiceConfig
+from bitfount.federated.transport.message_service import _MessageService
 from bitfount.federated.transport.modeller_transport import (
     _get_parameter_updates_from_workers,
     _get_training_metrics_from_workers,
     _ModellerMailbox,
     _send_model_parameters,
 )
 from bitfount.federated.transport.worker_transport import (
     _get_model_parameters,
-    _InterPodWorkerMailbox,
     _send_parameter_update,
     _send_training_metrics,
     _WorkerMailbox,
 )
-from bitfount.hub.api import BitfountHub
-from bitfount.types import (
-    T_DTYPE,
-    T_FIELDS_DICT,
-    T_NESTED_FIELDS,
-    DistributedModelProtocol,
-    _SerializedWeights,
-    _Weights,
-)
-from bitfount.utils import delegates
+from bitfount.hub.api import _PRODUCTION_IDP_URL, BitfountHub
+from bitfount.hub.helper import _create_bitfounthub
+from bitfount.types import T_DTYPE, _JSONDict, _SerializedWeights, _WeightDict
 
 if TYPE_CHECKING:
     from bitfount.types import _DistributedModelTypeOrReference
 
 logger = _get_federated_logger(__name__)
 
 
 @runtime_checkable
 class _FederatedAveragingCompatibleAlgo(Protocol[T_DTYPE]):
     @property
     def epochs(self) -> Optional[int]:
-        """Number of epochs."""
         ...
 
     @property
     def steps(self) -> Optional[int]:
-        """Number of steps."""
         ...
 
     @property
     def tensor_precision(self) -> T_DTYPE:
-        """Tensor precision."""
         ...
 
 
 @runtime_checkable
 class _FederatedAveragingCompatibleWorker(_FederatedAveragingCompatibleAlgo, Protocol):
     """Defines worker-side algorithm compatibility."""
 
     def initialise(
         self,
-        datasource: BaseSource,
+        datasource: DataSource,
         pod_dp: Optional[DPPodConfig] = None,
         **kwargs: Any,
     ) -> None:
-        """Initialises the worker-side algorithm."""
+        """Initialise the worker-side algorithm."""
         ...
 
     def run(
         self,
-        data: BaseSource,
+        data: DataSource,
         model_params: _SerializedWeights,
         iterations: int,
-    ) -> Tuple[_Weights, Optional[Dict[str, str]]]:
-        """Runs the worker-side algorithm."""
+    ) -> Tuple[_WeightDict, Optional[Dict[str, str]]]:
+        """Run the worker-side algorithm."""
         ...
 
     def save_final_parameters(self, model_params: _SerializedWeights) -> None:
-        """Saves the weights from the worker-side algorithm."""
+        """Save the weights from the worker-side algorithm."""
         ...
 
 
 @runtime_checkable
 class _FederatedAveragingCompatibleModeller(
     _FederatedAveragingCompatibleAlgo, Protocol
 ):
     """Defines modeller-side algorithm compatibility."""
 
     def initialise(
         self,
-        task_id: Optional[str] = None,
+        pretrained_file: Optional[Union[str, os.PathLike]] = None,
         **kwargs: Any,
     ) -> None:
-        """Initialises the modeller-side algorithm."""
+        """Initialise the modeller-side algorithm."""
         ...
 
     def run(
         self,
-        iteration: int = 0,
-        update: Optional[_Weights] = None,
+        update: Optional[_WeightDict] = None,
         validation_metrics: Optional[Mapping[str, float]] = None,
     ) -> _SerializedWeights:
-        """Runs the modeller-side algorithm."""
+        """Run the modeller-side algorithm."""
         ...
 
 
 @runtime_checkable
-class _FederatedAveragingCompatibleAlgoFactory(BaseCompatibleAlgoFactory, Protocol):
+class _FederatedAveragingCompatibleAlgoFactory(_BaseCompatibleAlgoFactory, Protocol):
     """Defines algorithm factory compatibility."""
 
     model: _DistributedModelTypeOrReference
-    pretrained_file: Optional[Union[str, os.PathLike]] = None
+
+    @property
+    def model_schema(self) -> Type[MarshmallowSchema]:
+        """Returns the schema of the underlying model."""
+        ...
 
     def modeller(self, **kwargs: Any) -> _FederatedAveragingCompatibleModeller:
         """Returns a modeller-side algorithm."""
         ...
 
     def worker(
         self, hub: BitfountHub, **kwargs: Any
     ) -> _FederatedAveragingCompatibleWorker:
         """Returns a worker-side algorithm."""
         ...
 
+    @staticmethod
+    def get_schema(
+        model_schema: Type[MarshmallowSchema], **kwargs: Any
+    ) -> Type[MarshmallowSchema]:
+        """Returns the schema for this algorithm."""
+        ...
+
 
 class _BaseFederatedAveragingMixIn:
     """Shared behaviour for the `FederatedAveraging` classes."""
 
     # This is set in the base protocol
     algorithm: _FederatedAveragingCompatibleAlgo
 
@@ -188,15 +201,17 @@
             ValueError: if there is a mismatch between model iterations and
                 algorithm iterations.
         """
         if bool(self.steps_between_parameter_updates) == bool(
             self.epochs_between_parameter_updates
         ):
             raise ValueError("You must specify one (and only one) of steps or epochs.")
-        if bool(self.steps_between_parameter_updates) != bool(self.algorithm.steps):
+        if not (
+            bool(self.steps_between_parameter_updates) == bool(self.algorithm.steps)
+        ):
             raise ValueError(
                 "Parameter update method must match model training method"
                 + " i.e. steps or epochs."
             )
         if (
             # If steps_between_parameter_updates then algorithm.steps is not None
             self.steps_between_parameter_updates
@@ -223,15 +238,15 @@
         )
         num_iterations = cast(int, self.algorithm.epochs or self.algorithm.steps)
 
         # floor division rounds the result down to the nearest whole number
         return num_iterations // num_iterations_between_updates
 
 
-class _ModellerSide(BaseModellerProtocol, _BaseFederatedAveragingMixIn):
+class _ModellerSide(_BaseModellerProtocol, _BaseFederatedAveragingMixIn):
     """Modeller side of the FederatedAveraging protocol."""
 
     aggregator: _BaseModellerAggregator
     algorithm: _FederatedAveragingCompatibleModeller
 
     def __init__(
         self,
@@ -261,57 +276,53 @@
     async def _send_parameters(self, new_parameters: _SerializedWeights) -> None:
         """Sends central model parameters to workers."""
         logger.debug("Sending global parameters to workers")
         await _send_model_parameters(new_parameters, self.mailbox)
 
     async def _receive_parameter_updates(
         self,
-    ) -> Dict[str, _SerializedWeights]:
-        """Receives parameter updates from every worker.
-
-        Returns:
-            A dictionary of the form {worker_name: weight_update}.
-        """
+    ) -> List[_SerializedWeights]:
+        """Receives parameter updates from every worker."""
         return await _get_parameter_updates_from_workers(self.mailbox)
 
     async def _get_training_metrics_updates(
         self,
     ) -> Dict[str, float]:
         return await _get_training_metrics_from_workers(self.mailbox)
 
     async def run(
         self,
+        pretrained_file: Optional[Union[str, os.PathLike]] = None,
         **kwargs: Any,
     ) -> List[Dict[str, float]]:
         """Receives updates and sends new parameters in a loop."""
-        self.algorithm.initialise(task_id=self.mailbox._task_id)
+        self.algorithm.initialise(pretrained_file=pretrained_file)
         self.perform_iterations_checks()
         initial_parameters = self.algorithm.run(update=None)
-        logger.federated_info("Sending model parameters to pods...")
         await self._send_parameters(initial_parameters)
-        logger.info("Sent model parameters to Pods")
         num_federated_iterations = self.get_num_federated_iterations()
         for i in range(1, num_federated_iterations + 1):
             if self.algorithm.epochs:
                 logger.info(f"Federated Epoch {i}")
             else:
                 logger.info(f"Federated Step {i}")
 
+            logger.info("Sending model parameters to Pods")
             if self.auto_eval:
                 # We create this as a task so that it can process TRAINING_METRICS
                 # messages in the background without blocking TRAINING_UPDATE
                 # messages.
                 validation_metrics_task = asyncio.create_task(
                     self._get_training_metrics_updates()
                 )
 
-            # from worker(s)
             weight_updates = await self._receive_parameter_updates()
+            logger.federated_info("Aggregating parameter updates")
             parameter_update = self.aggregator.run(
-                algorithm_outputs=weight_updates,
+                parameter_updates=weight_updates,
                 tensor_dtype=self.algorithm.tensor_precision,
             )
             if self.auto_eval:
                 # This is guaranteed to be bound as it's creation is also in a
                 # `if self.auto_eval:` block.
                 # noinspection PyUnboundLocalVariable
                 await validation_metrics_task
@@ -321,46 +332,42 @@
                 )
                 # Each item in the list is the average results from every worker
                 # for a given iteration. New results are appended to the list
                 # such that the final item is always the latest.
                 self.validation_results.append(validation_metrics)
 
                 new_parameters = self.algorithm.run(
-                    update=parameter_update,
-                    validation_metrics=validation_metrics,
-                    iteration=i,
+                    update=parameter_update, validation_metrics=validation_metrics
                 )
                 # Send the latest averaged validation metrics only at each iteration
             else:
                 new_parameters = self.algorithm.run(update=parameter_update)
 
             logger.federated_info("Sending updated parameters")
-            logger.info("Sending updated parameters")
             await self._send_parameters(
                 new_parameters
             )  # Workers end up with final model
-            logger.info("Sent updated parameters")
 
             # TODO: [BIT-970] consider moving early stopping to be handled in a side
             #       channel as part of handler based approach
             training_complete = False
             if self.early_stopping is not None:
                 training_complete = self.early_stopping.check(self.validation_results)
             await self.mailbox.send_training_iteration_complete_update(
                 training_complete
             )
             if training_complete:
                 logger.info("Early stopping criterion met. Stopping training.")
                 break
-
+        await self.mailbox.send_task_complete_message()
         modeller_results = self.validation_results
         return modeller_results
 
 
-class _WorkerSide(BaseWorkerProtocol, _BaseFederatedAveragingMixIn):
+class _WorkerSide(_BaseWorkerProtocol, _BaseFederatedAveragingMixIn):
     """Worker side of the FederatedAveraging protocol."""
 
     aggregator: _BaseWorkerAggregator
     algorithm: _FederatedAveragingCompatibleWorker
 
     def __init__(
         self,
@@ -403,175 +410,131 @@
     ) -> None:
         """Sends parameter update."""
         logger.debug("Sending parameter update to modeller")
         await _send_parameter_update(parameter_update, self.mailbox)
 
     async def run(
         self,
-        datasource: BaseSource,
+        datasource: DataSource,
         pod_dp: Optional[DPPodConfig] = None,
         pod_vitals: Optional[_PodVitals] = None,
-        pod_identifier: Optional[str] = None,
         **kwargs: Any,
-    ) -> Any:
+    ) -> None:
         """Receives parameters and sends updates in a loop."""
         self.algorithm.initialise(datasource=datasource, pod_dp=pod_dp)
         self.perform_iterations_checks()
-        logger.debug("Waiting on initial parameters")
-        serialized_model_params = await self._receive_parameters()
-        logger.debug("Received initial parameters")
+        model_params = await self._receive_parameters()
 
         num_federated_iterations = self.get_num_federated_iterations()
         for i in range(1, num_federated_iterations + 1):
             if self.algorithm.epochs:
                 logger.info(f"Federated Epoch {i}")
                 iterations = self.epochs_between_parameter_updates
             else:
                 logger.info(f"Federated Step {i}")
                 iterations = self.steps_between_parameter_updates
             iterations = cast(int, iterations)
             logger.federated_info("Running algorithm")
             if pod_vitals:
                 pod_vitals.last_task_execution_time = time.time()
             parameter_update, validation_metrics = self.algorithm.run(
-                datasource, serialized_model_params, iterations
+                datasource, model_params, iterations
             )
 
-            # Aggregator returns the difference between the new and old model parameters
-            aggregated_parameter_update = await self.aggregator.run(parameter_update)
-
+            aggregated_parameter_update = await self.aggregator.run(
+                parameter_update=parameter_update
+            )
             if self.auto_eval:
                 logger.info(
                     f"Validation Metrics at iteration {i}: {validation_metrics}"
                 )
                 await self._send_training_metrics(validation_metrics)
             await self._send_parameter_update(aggregated_parameter_update)
-            serialized_model_params = await self._receive_parameters()
+            model_params = await self._receive_parameters()
 
             # TODO: [BIT-970] consider moving early stopping to be handled in a side
-            # channel as part of handler based approach
+            #       channel as part of handler based approach
             training_complete = (
                 await self.mailbox.get_training_iteration_complete_update()
             )
             if training_complete:
                 logger.info(
                     "Modeller reporting early stopping criterion met. "
                     + "Stopping training.",
                 )
                 break
-
-        self.algorithm.save_final_parameters(serialized_model_params)
+        await self.mailbox.get_task_complete_update()
+        self.algorithm.save_final_parameters(model_params)
 
 
-@delegates()
-class FederatedAveraging(BaseProtocolFactory):
+class FederatedAveraging(_ProtocolAggregatorSchema, _BaseProtocolFactory):
     """Original Federated Averaging algorithm by McMahan et al. (2017).
 
     This protocol performs a predetermined number of epochs or steps of training on
     each remote Pod before sending the updated model parameters to the modeller. These
     parameters are then averaged and sent back to the Pods for as many federated
     iterations as the Modeller specifies.
 
-    :::tip
-
-    For more information, take a look at the seminal paper:
-    https://arxiv.org/abs/1602.05629
-
-    :::
-
     Args:
         algorithm: The algorithm to use for training. This must be compatible with the
             `FederatedAveraging` protocol.
         aggregator: The aggregator to use for updating the model parameters across all
-            Pods participating in the task. This argument takes priority over the
-            `secure_aggregation` argument.
+            Pods participating in the task.
         steps_between_parameter_updates: The number of steps between parameter updates,
             i.e. the number of rounds of local training before parameters are updated.
             If `epochs_between_parameter_updates` is provided,
             `steps_between_parameter_updates` cannot be provided. Defaults to None.
         epochs_between_parameter_updates: The number of epochs between parameter
             updates, i.e. the number of rounds of local training before parameters are
             updated. If `steps_between_parameter_updates` is provided,
             `epochs_between_parameter_updates` cannot be provided. Defaults to None.
         auto_eval: Whether to automatically evaluate the model on the validation
             dataset. Defaults to True.
-        secure_aggregation: Whether to use secure aggregation. This argument is
-            overridden by the `aggregator` argument. Defaults to False.
 
     Attributes:
         name: The name of the protocol.
         algorithm: The algorithm to use for training
         aggregator: The aggregator to use for updating the model parameters.
         steps_between_parameter_updates: The number of steps between parameter updates.
         epochs_between_parameter_updates: The number of epochs between parameter
             updates.
         auto_eval: Whether to automatically evaluate the model on the validation
             dataset.
 
     Raises:
         TypeError: If the `algorithm` is not compatible with the protocol.
+
+    :::tip
+
+    For more information, take a look at the seminal paper:
+    https://arxiv.org/abs/1602.05629
+
+    :::
     """
 
     algorithm: _FederatedAveragingCompatibleAlgoFactory
-    fields_dict: ClassVar[T_FIELDS_DICT] = {
-        "steps_between_parameter_updates": fields.Integer(allow_none=True),
-        "epochs_between_parameter_updates": fields.Integer(allow_none=True),
-        "auto_eval": fields.Boolean(),
-    }
-    nested_fields: ClassVar[T_NESTED_FIELDS] = {
-        "algorithm": algorithms_registry,
-        "aggregator": aggregators_registry,
-    }
 
     def __init__(
         self,
         *,
         algorithm: _FederatedAveragingCompatibleAlgoFactory,
-        aggregator: Optional[_BaseAggregatorFactory] = None,
+        aggregator: _BaseAggregatorFactory,
         steps_between_parameter_updates: Optional[int] = None,
         epochs_between_parameter_updates: Optional[int] = None,
         auto_eval: bool = True,
-        secure_aggregation: bool = False,
         **kwargs: Any,
     ) -> None:
-        if kwargs.get("model"):
-            logger.warning("Ignoring provided model. Algorithm already has a model.")
         super().__init__(algorithm=algorithm, **kwargs)
-        if aggregator:
-            self.aggregator = aggregator
-            logger.warning(
-                "Aggregator provided, ignoring 'secure_aggregation' argument."
-            )
-        else:
-            self.aggregator = _create_aggregator(secure_aggregation=secure_aggregation)
+        self.aggregator = aggregator
         self.steps_between_parameter_updates = steps_between_parameter_updates
         self.epochs_between_parameter_updates = epochs_between_parameter_updates
         self.auto_eval = auto_eval
-        if not steps_between_parameter_updates and not epochs_between_parameter_updates:
-            logger.info(
-                "Neither steps_between_parameter_updates or "
-                "epochs_between_parameter_updates were set ..."
-            )
-            if isinstance(algorithm.model, DistributedModelProtocol):
-                if bool(algorithm.model.epochs):
-                    logger.info("Setting epochs_between_parameter_updates to 1.")
-                    self.epochs_between_parameter_updates = 1
-                else:
-                    logger.info("Setting steps_between_parameter_updates to 1.")
-                    self.steps_between_parameter_updates = 1
-            elif isinstance(self.algorithm.model, BitfountModelReference):
-                if self.algorithm.model.hyperparameters.get("epochs"):
-                    logger.info("Setting epochs_between_parameter_updates to 1.")
-                    self.epochs_between_parameter_updates = 1
-                else:
-                    logger.info("Setting steps_between_parameter_updates to 1.")
-                    self.steps_between_parameter_updates = 1
 
     @classmethod
-    def _validate_algorithm(cls, algorithm: BaseCompatibleAlgoFactory) -> None:
+    def _validate_algorithm(cls, algorithm: _BaseCompatibleAlgoFactory) -> None:
         """Checks that `algorithm` is compatible with the protocol.
 
         Raises:
             TypeError: If the `algorithm` is not compatible with the protocol.
         """
         if not isinstance(algorithm, _FederatedAveragingCompatibleAlgoFactory):
             raise TypeError(
@@ -583,17 +546,15 @@
         self,
         mailbox: _ModellerMailbox,
         early_stopping: Optional[FederatedEarlyStopping] = None,
         **kwargs: Any,
     ) -> _ModellerSide:
         """Returns the modeller side of the FederatedAveraging protocol."""
         return _ModellerSide(
-            algorithm=self.algorithm.modeller(
-                pretrained_file=self.algorithm.pretrained_file
-            ),
+            algorithm=self.algorithm.modeller(),
             aggregator=self.aggregator.modeller(),
             steps_between_parameter_updates=self.steps_between_parameter_updates,
             epochs_between_parameter_updates=self.epochs_between_parameter_updates,
             auto_eval=self.auto_eval,
             mailbox=mailbox,
             early_stopping=early_stopping,
             **kwargs,
@@ -601,26 +562,18 @@
 
     def worker(
         self,
         mailbox: _WorkerMailbox,
         hub: BitfountHub,
         **kwargs: Any,
     ) -> _WorkerSide:
-        """Returns the worker side of the FederatedAveraging protocol.
-
-        Raises:
-            TypeError: If the mailbox is not compatible with the aggregator.
-        """
+        """Returns the worker side of the FederatedAveraging protocol."""
         if isinstance(self.aggregator, _AggregatorWorkerFactory):
             worker_agg = self.aggregator.worker()
         elif isinstance(self.aggregator, _InterPodAggregatorWorkerFactory):
-            if not isinstance(mailbox, _InterPodWorkerMailbox):
-                raise TypeError(
-                    "Inter-pod aggregators require an inter-pod worker mailbox."
-                )
             worker_agg = self.aggregator.worker(mailbox=mailbox)
         else:
             raise TypeError(
                 f"Unrecognised aggregator factory ({type(self.aggregator)}); "
                 f"unable to determine how to call .worker() factory method."
             )
         return _WorkerSide(
@@ -628,7 +581,173 @@
             aggregator=worker_agg,
             steps_between_parameter_updates=self.steps_between_parameter_updates,
             epochs_between_parameter_updates=self.epochs_between_parameter_updates,
             auto_eval=self.auto_eval,
             mailbox=mailbox,
             **kwargs,
         )
+
+    @classmethod
+    def run(
+        cls,
+        pod_identifiers: Iterable[str],
+        username: Optional[str] = None,
+        model: Optional[_DistributedModelTypeOrReference] = None,
+        algorithm: Optional[_FederatedAveragingCompatibleAlgoFactory] = None,
+        aggregator: Optional[_BaseAggregatorFactory] = None,
+        steps_between_parameter_updates: Optional[int] = None,
+        epochs_between_parameter_updates: Optional[int] = None,
+        hub: Optional[BitfountHub] = None,
+        ms_config: Optional[MessageServiceConfig] = None,
+        message_service: Optional[_MessageService] = None,
+        pod_public_key_paths: Optional[Mapping[str, Path]] = None,
+        pretrained_file: Optional[Union[str, os.PathLike]] = None,
+        secure_aggregation: bool = False,
+        auto_eval: bool = True,
+        identity_verification_method: Union[
+            str, IdentityVerificationMethod
+        ] = IdentityVerificationMethod.DEFAULT,
+        private_key_or_file: Optional[Union[RSAPrivateKey, Path]] = None,
+        idp_url: Optional[str] = None,
+    ) -> Optional[Any]:
+        """Quickstart method for running the protocol as a `Modeller`.
+
+        Running this method avoids the need to create a `Modeller` or any of the other
+        instances required to run the protocol. However, these can still be provided
+        to override the default values.
+
+        ```python title="Example usage:"
+        import bitfount as bf
+
+        bf.FederatedAveraging.run(
+            pod_identifiers=["bitfount/example-pod-1", "bitfount/example-pod-2"],
+            algorithm=bf.FederatedModelTraining(
+                model=bf.PyTorchTabularClassifier(...),
+            ),
+        )
+        ```
+
+        Args:
+            pod_identifiers: The identifiers of the pods to participate in the
+                task as a list of strings.
+            username: The username of the Modeller. This is only necessary if you are
+                switching between multiple accounts. Defaults to None.
+            model: The model to train. If `algorithm` is provided, this `model` is
+                ignored. Defaults to None.
+            algorithm: The algorithm to use for training. If `model` is not provided,
+                this must be provided. Defaults to None.
+            aggregator: The aggregator to use for updating the model parameters.
+                Defaults to None.
+            steps_between_parameter_updates: The number of steps between parameter
+                updates. If neither `steps_between_parameter_updates` nor
+                `epochs_between_parameter_updates` are provided,
+                `steps_between_parameter_updates` is set to 1. Defaults to None.
+            epochs_between_parameter_updates: The number of epochs between parameter
+                updates. If neither `steps_between_parameter_updates` nor
+                `epochs_between_parameter_updates` are provided,
+                `steps_between_parameter_updates` is set to 1. Defaults to None.
+            hub: The `BitfountHub` object to use for communication with the hub.
+                Defaults to None.
+            ms_config: The `MessageServiceConfig` used to create an instance of
+                `MessageService` if not provided. Defaults to None.
+            message_service: The `MessageService` object to use for communication
+                with other Pods and Modeller. Defaults to None.
+            pod_public_key_paths: The paths to the public keys of the Pods involved in
+                the task. Defaults to None.
+            pretrained_file: The path to a file containing a pretrained model. Defaults
+                to None.
+            secure_aggregation: Whether to use secure aggregation. Defaults to False.
+            auto_eval: Whether to automatically evaluate the model on the validation
+                dataset. Defaults to True.
+            identity_verification_method: The method used to verify the identity of
+                the Modeller. Defaults to SAML.
+            private_key_or_file: The private key used to sign the messages.
+            idp_url: The URL of the Identity Provider. Defaults to Bitfount's identity
+                provider.
+
+        Raises:
+            ValueError: If neither `model` nor `algorithm` are provided.
+
+        :::tip
+
+        The `pod_identifiers` and one of `model` or `algorithm` are the only arguments
+        that are necessary. Everything else can be created on the fly.
+
+        :::
+        """
+        if model is not None and algorithm is not None:
+            logger.warning("Ignoring provided model. Algorithm already has a model.")
+        elif model is not None:
+            algorithm = FederatedModelTraining(model=model)
+        elif model is None and algorithm is None:
+            raise ValueError(
+                "Must provide either the model or the algorithm containing the model."
+            )
+
+        if not hub:
+            hub = _create_bitfounthub(username=username)
+
+        if not message_service:
+            message_service = _create_message_service(
+                session=hub.session, ms_config=ms_config
+            )
+        if not aggregator:
+            aggregator = _create_aggregator(
+                model=algorithm.model, secure_aggregation=secure_aggregation
+            )
+
+        protocol = _create_federated_averaging_protocol_factory(
+            protocol_cls=cls,
+            algorithm=algorithm,
+            aggregator=aggregator,
+            steps_between_parameter_updates=steps_between_parameter_updates,
+            epochs_between_parameter_updates=epochs_between_parameter_updates,
+            auto_eval=auto_eval,
+        )
+
+        return _run_protocol(
+            protocol=protocol,
+            pod_identifiers=pod_identifiers,
+            hub=hub,
+            message_service=message_service,
+            pod_public_key_paths=pod_public_key_paths,
+            pretrained_file=pretrained_file,
+            identity_verification_method=check_identity_verification_method(
+                identity_verification_method
+            ),
+            private_key_or_file=private_key_or_file,
+            idp_url=idp_url if idp_url is not None else _PRODUCTION_IDP_URL,
+        )
+
+    @staticmethod
+    def get_schema(
+        algorithm_schema: Type[MarshmallowSchema],
+        aggregator_schema: Type[MarshmallowSchema],
+        **kwargs: Any,
+    ) -> MarshmallowSchema:
+        """Returns the schema for FederatedAveraging."""
+
+        class Schema(_BaseProtocolFactory._Schema):
+            algorithm = fields.Nested(algorithm_schema)
+            aggregator = fields.Nested(aggregator_schema)
+            steps_between_parameter_updates = fields.Integer(allow_none=True)
+            epochs_between_parameter_updates = fields.Integer(allow_none=True)
+            auto_eval = fields.Boolean()
+
+            @post_load
+            def recreate_factory(
+                self, data: dict, **_kwargs: Any
+            ) -> FederatedAveraging:
+                return FederatedAveraging(**data)
+
+        return Schema()
+
+    def dump(self) -> _JSONDict:
+        """Returns the JSON-serializable representation of the protocol."""
+        # TODO: [BIT-889] Getting schemas should not be the responsibility of
+        #       this method.
+        algorithm_schema = self.algorithm.get_schema(self.algorithm.model_schema)
+        aggregator_schema = self.aggregator.get_schema(
+            tensor_shim_factory=self.algorithm.model.backend_tensor_shim
+        )
+        schema = self.get_schema(algorithm_schema, aggregator_schema)
+        return cast(_JSONDict, schema.dump(self))
```

### Comparing `bitfount-0.5.86/bitfount/federated/roles.py` & `bitfount-0.5.9/bitfount/federated/roles.py`

 * *Files identical despite different names*

### Comparing `bitfount-0.5.86/bitfount/federated/secure.py` & `bitfount-0.5.9/bitfount/federated/secure.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,182 +1,135 @@
 """Secure aggregation."""
 from __future__ import annotations
 
-import inspect
-import logging
 import secrets
-from types import MappingProxyType
-from typing import TYPE_CHECKING, Any, ClassVar, Dict, List, Mapping, Type, Union
+from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Type, Union
 
-from marshmallow import fields
+from marshmallow import Schema as MarshmallowSchema
+from marshmallow import fields, post_load
 import numpy as np
 
-from bitfount.federated.shim import _load_default_tensor_shim
+from bitfount.federated.exceptions import SecureShareError
+from bitfount.federated.shim import BackendTensorShim
 from bitfount.federated.transport.worker_transport import (
     _get_worker_secure_shares,
-    _InterPodWorkerMailbox,
     _send_secure_shares_to_others,
-)
-from bitfount.types import (
-    T_FIELDS_DICT,
-    T_NESTED_FIELDS,
-    _BaseSerializableObjectMixIn,
-    _Weights,
+    _WorkerMailbox,
 )
 
 if TYPE_CHECKING:
-    from bitfount.types import _TensorLike
+    from bitfount.types import T_DTYPE, _TensorLike, _WeightDict, _WeightMapping
 
 # Can't be larger than 2^64 -1 (largest unsigned 64 bit integer). Otherwise we get:
 # “OverflowError: Python int too large to convert to C long"
-LARGE_PRIME_NUMBER: int = (2**61) - 1  # Largest possible Mersenne prime number.
+LARGE_PRIME_NUMBER: int = (2**59) - 1  # Largest possible Mersenne prime number.
 
 # Precision does not need to be greater than this in order be able to perform lossless
 # computation on IEEE 754 32-bit floating point values
 FLOAT_32_BIT_PRECISION: int = 10**10
 
-_secure_share_registry: Dict[str, Type[_BaseSecureShare]] = {}
-secure_share_registry: Mapping[str, Type[_BaseSecureShare]] = MappingProxyType(
-    _secure_share_registry
-)
-logger = logging.getLogger(__name__)
-
-
-class _BaseSecureShare:
-    @classmethod
-    def __init_subclass__(cls, **kwargs: Any):
-        if not inspect.isabstract(cls):
-            logger.debug(f"Adding {cls.__name__}: {cls} to registry")
-            _secure_share_registry[cls.__name__] = cls
-
 
-class SecureShare(_BaseSecureShare, _BaseSerializableObjectMixIn):
+class SecureShare:
     """Additive, replicated, secret sharing algorithm responsible for secure averaging.
 
     This secret sharing implementation is 'additive' because the secret can be
     reconstructed by taking the sum of all the shares it is split into, and 'replicated'
     because each party receives more than one share.
 
     The algorithm works as follows:
         1. First every worker shares a securely generated random number (between 0 and
         `prime_q`) with every other worker such that every worker ends up with one
         number from every other worker. These numbers are known as shares as they will
-        form part of the secret (the state dictionary) which will be shared.
-        2. The values in the state dictionary are then converted to positive integer
-        field elements of a finite field bounded by `prime_q`.
+        form part of the secret (the weight update) which will be shared.
+        2. The tensors in the weight update are then converted to positive integer field
+        elements of a finite field bounded by `prime_q`.
         3. The random numbers generated are used to compute a final share for every
-        value in the state dictionary. This final share has the same shape as the secret
-        state dictionary.
+        tensor in the weight update. This final share has the same shape as the secret
+        tensor.
         4. This final share is then reconstructed using the shares retrieved from the
         other workers. At this point, the final share from each worker is meaningless
-        until averaged with every other state dictionary.
+        until averaged with every other weight update.
         5. This final share is sent to the modeller where it will be averaged with the
-        state dictionaries from all the other workers (all the while in the finite field
-        space).
-        6. After averaging, the state dictionaries are finally decoded back to floating
-        point numpy arrays.
-
-    :::note
-
-    The relationships between individual elements in the tensors are preserved in
-    this implementation since our shares are scalars rather than vectors. Therefore,
-    whilst the secret itself cannot be reconstructed, some properties of the secret can
-    be deciphered e.g. which element is the largest/smallest, etc.
-
-    :::
+        updates from all the other workers (all the while in the finite field space).
+        6. After averaging, the updates are finally decoded back to floating point
+        tensors.
 
     Args:
+        tensor_shim: Provides backend-specific tensor methods.
         prime_q: Large prime number used in secure aggregation. This should be a
             few orders of magnitude larger than the precision so that when we add
             encoded finite field elements with one another, we do not breach the limits
             of the finite field. A `SecureShareError` is raised if this occurs. Defaults
-            to 2^61 -1 (the largest Mersenne 64 bit Mersenne prime number - for ease).
+            to 2^59 -1 (the largest Mersenne 64 bit Mersenne prime number - for ease).
         precision: Degree of precision for floating points in secure aggregation
             i.e. the number of digits after the decimal point that we want to keep.
             Defaults to 10^10.
 
     Attributes:
         prime_q: Large prime number used in secure aggregation.
         precision: Degree of precision for floating points in secure aggregation.
+
+    :::note
+
+    The relationships between individual elements in the tensors are preserved in
+    this implementation since our shares are scalars rather than vectors. Therefore,
+    whilst the secret itself cannot be reconstructed, some properties of the secret can
+    be deciphered e.g. which element is the largest/smallest, etc.
+
+    :::
     """
 
     # TODO: [BIT-423] Review security
-    fields_dict: ClassVar[T_FIELDS_DICT] = {
-        "prime_q": fields.Integer(),
-        "precision": fields.Integer(),
-    }
-    nested_fields: ClassVar[T_NESTED_FIELDS] = {}
 
     def __init__(
         self,
+        tensor_shim: BackendTensorShim,
         prime_q: int = LARGE_PRIME_NUMBER,
         precision: int = FLOAT_32_BIT_PRECISION,
     ):
-        self.class_name = type(self).__name__
         self.prime_q = prime_q
         self.precision = precision
 
-        self._tensor_shim = _load_default_tensor_shim()
+        self._tensor_shim = tensor_shim
         self._own_shares: List[int] = []
         self._other_worker_shares: List[int] = []
 
-    def _encode_finite_field(
-        self, rational: Union[_TensorLike, np.ndarray]
-    ) -> np.ndarray:
-        """Converts `rational` to integer in finite field.
+    def _encode_finite_field(self, rational: _TensorLike) -> np.ndarray:
+        """Converts tensor `rational` to integer in finite field.
 
         Raises:
             SecureShareError: if finite field limit is breached. This is raised if there
                 are not enough integers to represent all the possible floating point
                 numbers in `rational`.
         """
+        upscaled = self._tensor_shim.to_numpy(rational * self.precision).astype(int)
         total_num_workers = len(self._own_shares) + 1
-        # Warning! on Windows astype(int) returns the min long value.
-        # We use astype(np.int64) in this module to avoid this.
-
-        # This branch is only for tensors. Clamping parameters does not apply to
-        # non-model state dictionaries.
-        if not isinstance(rational, np.ndarray):
-            # Auxiliary value for checking finite field limit
-            _upscaled_param_values = (
-                self._tensor_shim.to_numpy(rational * self.precision).astype(np.int64)
-                * total_num_workers
-            )
-
-            # Check that _upscaled_param_values are all within finite field limits
-            if (
-                (_upscaled_param_values > self.prime_q / 2)
-                | (_upscaled_param_values < -self.prime_q / 2)
-            ).sum() != 0:
-                logger.warning(
-                    "Parameter weights have been clipped. If you want to avoid this, "
-                    "choose a larger `prime_q` or a smaller `precision` for "
-                    "the `SecureShare` or normalize continuous features prior "
-                    "to training."
-                )
-                rational = self._tensor_shim.clamp_params(
-                    rational, self.prime_q, self.precision, total_num_workers
-                )
-
-            rational = self._tensor_shim.to_numpy(rational)
-
-        upscaled = (rational * self.precision).astype(np.int64)
+        if (
+            ((upscaled * total_num_workers) > self.prime_q / 2)
+            | ((upscaled * total_num_workers) < -self.prime_q / 2)
+        ).sum() != 0:
+            raise SecureShareError("Choose a larger `prime_q` or a smaller `precision`")
         field_element = upscaled % self.prime_q
         return field_element
 
-    def _decode_finite_field(self, field_element: np.ndarray) -> np.ndarray:
-        """Converts finite field array back into a rational array."""
+    def _decode_finite_field(
+        self, field_element: np.ndarray
+    ) -> Union[float, np.ndarray]:
+        """Converts finite field array back into tensor."""
         field_element = np.where(
             field_element > (self.prime_q / 2),
             field_element - self.prime_q,
             field_element,
         )
-        return (field_element / self.precision).astype(float)
+        rational = field_element / self.precision
+        if isinstance(rational, np.ndarray):
+            rational = rational.astype(float)
+        return rational
 
-    def _encode_secret(self, secret: Union[_TensorLike, np.ndarray]) -> np.ndarray:
+    def _encode_secret(self, secret: _TensorLike) -> np.ndarray:
         """Encodes the provided secret using `self.own_shares` and returns it.
 
         Secret is first moved to the finite field space and then split into n shares
         where n is the number of all workers participating in training. All but one
         shares (integers) are shared with the other workers and the final share
         (a dictionary of tensors) is returned. The sum of all these will yield the
         original (encoded) secret.
@@ -189,54 +142,50 @@
         """Reconstructs the shares into a secret.
 
         This secret is not the same secret as originally encoded and shared. This secret
         is useless unless averaged with the secret outputs from all the other workers.
         """
         return np.asarray(sum(shares) % self.prime_q)
 
-    def _encode_and_reconstruct_state_dict(
-        self, secret_state_dict: Union[_Weights, Mapping[str, np.ndarray]]
+    def _encode_and_reconstruct_update(
+        self, secret_update: _WeightMapping
     ) -> Dict[str, np.ndarray]:
-        """Encodes and reconstructs state dict from own and worker shares.
+        """Encodes and reconstructs update from own and worker shares.
 
-        Encrypts `secret_state_dict` using `own_shares` then reconstructs it using
-        `self._other_worker_shares`.
+        Encrypts `secret_update` using `own_shares` then reconstructs it using
+        provided `other_worker_shares`.
         """
-        logger.debug("Encoding state dict...")
-        encrypted_state_dict = [
-            self._encode_secret(v) for _, v in secret_state_dict.items()
-        ]
-        logger.debug("Reconstructing state dict...")
+        encrypted_params = [self._encode_secret(v) for _, v in secret_update.items()]
         reconstructed = [
-            self._reconstruct_secret([*self._other_worker_shares, value])
-            for value in encrypted_state_dict
+            self._reconstruct_secret([*self._other_worker_shares, param])
+            for param in encrypted_params
         ]
-        return dict(zip(list(secret_state_dict), reconstructed))
+        return dict(zip(list(secret_update), reconstructed))
 
     def _get_random_number(self) -> int:
         """Generate a random number, append it to `self.own_shares` and also return it.
 
         Random number generator is cryptographically secure.
         """
         rand_num = secrets.randbelow(self.prime_q)
         self._own_shares.append(rand_num)
         return rand_num
 
-    async def _share_own_shares(self, mailbox: _InterPodWorkerMailbox) -> None:
+    async def _share_own_shares(self, mailbox: _WorkerMailbox) -> None:
         """Sends own secure aggregation shares to other workers (one each).
 
         A random number is securely generated and sent to each worker such that each
         worker receives a different random number from every other worker. Each worker
         keeps a copy of the random numbers they generated which later become 'shares'
-        as they can be used to encode a secret (the state dictionary).
+        as they can be used to encode a secret (the parameter update).
         """
         self._own_shares = []
         await _send_secure_shares_to_others(self._get_random_number, mailbox)
 
-    async def _receive_worker_shares(self, mailbox: _InterPodWorkerMailbox) -> None:
+    async def _receive_worker_shares(self, mailbox: _WorkerMailbox) -> None:
         """Receives secure aggregation shares from other workers."""
         self._other_worker_shares = await _get_worker_secure_shares(mailbox)
 
     def _add(self, arrays: List[np.ndarray]) -> np.ndarray:
         """Add multiple encoded numpy arrays element-wise and return a numpy array.
 
         All arrays must have the same shape.
@@ -245,65 +194,74 @@
             return np.array(
                 [np.sum(arrs, axis=0) % self.prime_q for arrs in zip(*arrays)]
             )
         else:
             return np.asarray(np.sum(arrays, axis=0) % self.prime_q)
 
     def average_and_decode_state_dicts(
-        self,
-        state_dicts: List[Dict[str, np.ndarray]],
-    ) -> Dict[str, np.ndarray]:
-        """Averages and decodes multiple encrypted state dictionaries.
+        self, state_dicts: List[Dict[str, np.ndarray]], dtype: Optional[T_DTYPE] = None
+    ) -> _WeightDict:
+        """Averages and decodes multiple encryped parameter dictionaries.
 
         Computes the mean of all the `state_dicts` before decoding the averaged result
-        and returning it. This is called on the Modeller side after receiving the state
-        dictionaries from all the workers.
+        and returning it.
 
         Args:
-            state_dicts: List of encoded state dictionaries as numpy arrays.
+            state_dicts: List of dictionaries of model parameters as numpy arrays.
+            dtype: Optional dtype of the tensors in the returned tensor parameters.
 
         Returns:
-            A dictionary of averaged and decoded state dictionaries.
+            A dictionary of averaged and decoded model parameters.
+
         """
-        average_state_dict: Dict[str, np.ndarray] = {}
-        for field in state_dicts[0]:
-            summed_value = self._add([state_dict[field] for state_dict in state_dicts])
-            average_decoded_value = self._decode_finite_field(summed_value) / len(
+        average_state_dict: _WeightDict = {}
+        for param in state_dicts[0]:
+            summed_param = self._add([state_dict[param] for state_dict in state_dicts])
+            average_decoded_param = self._decode_finite_field(summed_param) / len(
                 state_dicts
             )
-            average_state_dict[field] = average_decoded_value
-
+            average_state_dict[param] = self._tensor_shim.to_tensor(
+                average_decoded_param, dtype=dtype
+            ).squeeze()
         return average_state_dict
 
     async def do_secure_aggregation(
         self,
-        state_dict: Union[_Weights, Mapping[str, np.ndarray]],
-        mailbox: _InterPodWorkerMailbox,
+        param_update: _WeightMapping,
+        mailbox: _WorkerMailbox,
     ) -> Dict[str, np.ndarray]:
         """Performs secure aggregation.
 
-        This is called on the Pod side before sending the state dict to the
-        Modeller.
-
         Args:
-            state_dict: A dictionary of tensors or numpy arrays to be securely
-                aggregated.
+            param_update: A dictionary of tensors to be securely aggregated.
             mailbox: A mailbox to send and receive messages from other workers.
 
         Returns:
-            A dictionary of encoded numpy arrays.
+            A dictionary of encoded parameters as numpy arrays.
 
         Raises:
-            SecureShareError: If finite field limit is breached. This is raised if there
+            SecureShareError: if finite field limit is breached. This is raised if there
                 are not enough integers to represent all the possible floating point
                 numbers.
         """
-        logger.info("Performing secure aggregation...")
         await self._share_own_shares(mailbox)
-        logger.debug("Shared own shares with other workers.")
         await self._receive_worker_shares(mailbox)
-        logger.debug("Received shares from other workers.")
-        encoded_reconstructued_update = self._encode_and_reconstruct_state_dict(
-            state_dict
-        )
-        logger.info("Secure aggregation complete.")
-        return encoded_reconstructued_update
+        return self._encode_and_reconstruct_update(param_update)
+
+    @staticmethod
+    def get_schema(
+        tensor_shim_factory: Callable[[], BackendTensorShim]
+    ) -> Type[MarshmallowSchema]:
+        """Gets the schema for the `SecureShare` instance."""
+        # The backend tensor shim is not (and should not be) serialized so a way of
+        # creating it every time a SecureShare is created from Schema is needed. By
+        # passing in a factory method we can ensure that the post_load always creates
+        # a new instance of the shim.
+        class Schema(MarshmallowSchema):
+            prime_q = fields.Integer()
+            precision = fields.Integer()
+
+            @post_load
+            def recreate_secure_share(self, data: dict, **_kwargs: Any) -> SecureShare:
+                return SecureShare(tensor_shim=tensor_shim_factory(), **data)
+
+        return Schema
```

### Comparing `bitfount-0.5.86/bitfount/federated/transport/config.py` & `bitfount-0.5.9/bitfount/federated/transport/config.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,29 +1,23 @@
 """Message Service configuration."""
 from dataclasses import dataclass
 import logging
-from typing import Optional
 
-from grpc import aio, ssl_channel_credentials
+# gRPC stubs don't cover grpc.aio yet
+from grpc import aio  # type: ignore[attr-defined] # Reason: see comment
+from grpc import ssl_channel_credentials
 
-from bitfount.config import (
-    _DEVELOPMENT_ENVIRONMENT,
-    _SANDBOX_ENVIRONMENT,
-    _STAGING_ENVIRONMENT,
-    _get_environment,
-)
 from bitfount.federated.transport import _MESSAGE_SERVICE_GRPC_OPTIONS
 from bitfount.federated.transport.protos.messages_pb2_grpc import MessageServiceStub
 
 logger = logging.getLogger(__name__)
 
 #: Production message service URL.
 PRODUCTION_MESSAGE_SERVICE_URL = "messaging.bitfount.com"
 _STAGING_MESSAGE_SERVICE_URL = "messaging.staging.bitfount.com"
-_SANDBOX_MESSAGE_SERVICE_URL = "messaging.sandbox.bitfount.com"
 _DEV_MESSAGE_SERVICE_URL = "localhost"
 _DEV_MESSAGE_SERVICE_PORT = 5001
 _DEV_MESSAGE_SERVICE_TLS = False
 
 
 @dataclass
 class MessageServiceConfig:
@@ -38,43 +32,27 @@
             message service if both parties are on the same device. This can be used to
             remove the overhead of communication. Defaults to False.
 
     Raises:
         ValueError: If `tls` is False and `url` is a Bitfount URL.
     """
 
-    url: Optional[str] = None
+    url: str = PRODUCTION_MESSAGE_SERVICE_URL
     port: int = 443
     tls: bool = True  # only used for development
     use_local_storage: bool = False
 
     def __post_init__(self) -> None:
-        if not self.url:
-            # get the correct URL based on environment
-            environment = _get_environment()
-            if environment == _STAGING_ENVIRONMENT:
-                self.url = _STAGING_MESSAGE_SERVICE_URL
-            elif environment == _DEVELOPMENT_ENVIRONMENT:
-                self.url = _DEV_MESSAGE_SERVICE_URL
-                self.port = _DEV_MESSAGE_SERVICE_PORT
-                self.tls = _DEV_MESSAGE_SERVICE_TLS
-            elif environment == _SANDBOX_ENVIRONMENT:
-                self.url = _SANDBOX_MESSAGE_SERVICE_URL
-            else:
-                self.url = PRODUCTION_MESSAGE_SERVICE_URL
         if not self.tls and ".bitfount.com" in self.url:
             raise ValueError(
                 "TLS disabled. Message service communication must be with TLS."
             )
         elif not self.tls:
             logger.warning("Message service communication without TLS.")
 
-        # Log the config for easier debugging.
-        logger.debug(f"Message service configuration: {vars(self)}")
-
     # @cached_property can't be used because it returns the _exact_ same Awaitable
     # which cannot be awaited more than once.
     @property
     async def stub(self) -> MessageServiceStub:
         """Creates and returns message service stub from this config."""
         # This method must be kept as async def to ensure that the channel
         # creation occurs on the correct event loop.
```

### Comparing `bitfount-0.5.86/bitfount/federated/transport/identity_verification/oidc.py` & `bitfount-0.5.9/bitfount/federated/transport/identity_verification/oidc.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,65 +1,50 @@
 """Handling of OIDC challenges."""
 from __future__ import annotations
 
 import asyncio
-from asyncio import Task, create_task
 import base64
 from collections import defaultdict
 from datetime import datetime, timedelta, timezone
 import hashlib
 import secrets
 from typing import DefaultDict, Dict, Final, Iterator, List
 from urllib.parse import urlencode
 import webbrowser
 
 from aiohttp import web
-from aiohttp.web import (
-    Application,
-    AppRunner,
-    Request as AioRequest,
-    Response as AioResponse,
-    TCPSite,
-)
+from aiohttp.web import Application, AppRunner
+from aiohttp.web import Request as AioRequest
+from aiohttp.web import Response as AioResponse
+from aiohttp.web import TCPSite
 import requests
 from requests import HTTPError
 
 from bitfount.federated.logging import _get_federated_logger
-from bitfount.federated.transport.identity_verification import (
-    _BITFOUNT_MODELLER_PORT,
-    _PORT_WAIT_TIMEOUT,
-)
-from bitfount.federated.transport.identity_verification.types import (
-    _HasWebServer,
-    _ResponseHandler,
-)
+from bitfount.federated.transport.identity_verification import _BITFOUNT_MODELLER_PORT
 from bitfount.federated.transport.modeller_transport import _ModellerMailbox
 from bitfount.federated.transport.types import (
     _DeviceCodeDetailsPair,
     _ModellerDeviceCodeDetails,
     _OIDCAuthEndpointResponse,
     _OIDCAuthFlowResponse,
     _OIDCClientID,
     _PodDeviceCodeDetails,
 )
+from bitfount.federated.types import _ResponseHandler
 from bitfount.hub.authentication_flow import (
     _PRODUCTION_AUTH_DOMAIN,
     AuthEnvironmentError,
     _get_auth_environment,
 )
 from bitfount.hub.types import _DeviceCodeRequestDict, _DeviceCodeResponseJSON
 from bitfount.types import _JSONDict
-from bitfount.utils import web_utils
 
 logger = _get_federated_logger(__name__)
 
-# This forces `requests` to make IPv4 connections
-# TODO: [BIT-1443] Remove this once Hub/AM support IPv6
-requests.packages.urllib3.util.connection.HAS_IPV6 = False  # type: ignore[attr-defined] # Reason: see above # noqa: B950
-
 _OIDC_IDENTITY_VERIFICATION_SCOPES: Final[str] = "execute:task"
 _OIDC_IDENTITY_VERIFICATION_AUDIENCE: Final[
     str
 ] = "https://am.bitfount.com/api/access/modellerProofOfIdentity"
 
 _CALLBACK_ROUTE: Final[str] = "/api/auth/callback/auth0"
 
@@ -209,15 +194,15 @@
         raise AuthEnvironmentError(
             f"Authorisation environments do not match. "
             f'Expected client_id "{our_auth_env.client_id}" but the following '
             f"pods mismatched: {errors_str}"
         )
 
 
-class _OIDCAuthFlowChallengeHandler(_ResponseHandler, _HasWebServer):
+class _OIDCAuthFlowChallengeHandler(_ResponseHandler):
     """Perform OIDC flow from the modeller-side.
 
     This flow is based on:
     https://auth0.com/docs/get-started/authentication-and-authorization-flow/authorization-code-flow-with-proof-key-for-code-exchange-pkce  # noqa: B950
     """
 
     def __init__(
@@ -237,170 +222,125 @@
 
         # Create web application for callbacks
         self._oidc_endpoint = _OIDCWebEndpoint()
         app = Application()
         app.add_routes([web.get(_CALLBACK_ROUTE, self._oidc_endpoint.process_callback)])
         self._runner = AppRunner(app)
 
-        # This task is used to ensure we don't try to
+        # This event is used to ensure we don't try to
         # perform authentication before the server has started
-        self._server_start_task: Task
+        self._server_started = asyncio.Event()
 
-    def start_server(self) -> Task:
+    async def start(self) -> None:
         """Sets up and starts the web server."""
-        self._server_start_task = create_task(self._start())
-        return self._server_start_task
+        await self._runner.setup()
+        # This site is accessed by the modeller (who is running this code)
+        # in their own browser.
+        # Their OIDC IdP provider will redirect them to:
+        # `http://localhost:{BITFOUNT_MODELLER_PORT}/{_CALLBACK_ROUTE}`
+        # So they should not have any issues accessing it.
+        site = TCPSite(self._runner, "localhost", _BITFOUNT_MODELLER_PORT)
+        await site.start()
+        self._server_started.set()
 
-    async def _start(self) -> None:
-        """Sets up and starts the web server."""
-        logger.debug("Starting OIDC web server")
-        host = "localhost"
-        port = _BITFOUNT_MODELLER_PORT
+    async def handle(self, modeller_mailbox: _ModellerMailbox) -> None:
+        """Receive and perform OIDC verification."""
+        # Await Client ID information from pods
+        oidc_details = await modeller_mailbox.get_oidc_client_ids()
 
-        # Try to create web server for endpoint, failing out if binding to the
-        # host and port isn't possible.
-        try:
-            await self._runner.setup()
-            # This site is accessed by the modeller (who is running this code)
-            # in their own browser.
-            # Their OIDC IdP provider will redirect them to:
-            # `http://localhost:{BITFOUNT_MODELLER_PORT}/{_CALLBACK_ROUTE}`
-            # So they should not have any issues accessing it.
-            site = TCPSite(self._runner, host, port)
-            # wait_for() use ensures we don't hang indefinitely on waiting for the
-            # port to be available
-            await asyncio.wait_for(site.start(), _PORT_WAIT_TIMEOUT)
-        except TimeoutError:
-            logger.critical(
-                f"Timeout reached whilst trying to bind OIDC web endpoint to "
-                f"http://{host}:{port}"
-            )
-            raise
-        except OSError:
-            # Raises OSError: [Errno 48] if address already in use
-            logger.critical(f"Unable to bind OIDC web endpoint to http://{host}:{port}")
-            raise
+        # Verify Client IDs are as expected
+        self._verify_client_ids(oidc_details)
 
-        logger.debug("OIDC web server started successfully")
+        # Generate /authorize URL for each authorization request
+        to_send_dicts: DefaultDict[str, _JSONDict] = defaultdict(dict)
+        urls: Dict[str, str] = {}
+        states: Dict[str, str] = {}
+        for pod_id, details in oidc_details.items():
+            # Generate code verifier
+            # Should be between 43 and 128 characters; recommended to use
+            # base64url-encoding which secrets.token_urlsafe does for us. 60 bytes
+            # will produce ~78 characters.
+            # See: https://datatracker.ietf.org/doc/html/rfc7636#section-4.1
+            code_verifier: str = secrets.token_urlsafe(nbytes=60)
+            # Add to details
+            to_send_dicts[pod_id]["code_verifier"] = code_verifier
+
+            # Generate code challenge
+            # This is the b64url-encoded SHA256 hash of the ascii-encoded code_verifier.
+            # We also encode the bytes-output of the b64 encoding with UTF-8 to
+            # make it a string.
+            # See: https://datatracker.ietf.org/doc/html/rfc7636#section-4.2
+            code_challenge: str = _get_urlsafe_hash(
+                code_verifier, initial_encoding="ascii"
+            )
+
+            # Request PKCE authorization
+            # Generate random state
+            state = secrets.token_hex(nbytes=32)
+            params = urlencode(
+                {
+                    "audience": self._audience,
+                    "scope": self._scopes,
+                    "response_type": "code",
+                    "client_id": details.client_id,
+                    "state": state,
+                    "redirect_uri": self._redirect_uri,
+                    "code_challenge_method": "S256",
+                    "code_challenge": code_challenge,
+                }
+            )
+            url = self._authorize_endpoint + f"?{params}"
+
+            urls[pod_id] = url
+            states[pod_id] = state
+
+        # Set generated URLs on web endpoint for later iteration
+        urls_list: List[str] = list(urls.values())
+        self._oidc_endpoint.initialise(urls_list, states)
+
+        # Wait for server if necessary
+        if not self._server_started.is_set():
+            logger.info("Waiting for OIDC challenge handler to start")
+            await self._server_started.wait()
+
+        # Start URL processing
+        self._oidc_endpoint.start_processing()
+
+        # Wait for all authorization to be done
+        responses = await self._oidc_endpoint.get_responses()
+
+        # Validate the states are the same
+        for pod_id, response in responses.items():
+            if response.state != states[pod_id]:
+                raise ValueError(f"Unable to validate response intended for {pod_id}")
+
+        # Add auth_code and redirect_uri to details
+        for pod_id in oidc_details.keys():
+            to_send_dicts[pod_id]["auth_code"] = responses[pod_id].auth_code
+            to_send_dicts[pod_id]["redirect_uri"] = self._redirect_uri
+
+        # Send details to pods
+        to_send: Dict[str, _OIDCAuthFlowResponse] = {
+            pod_id: _OIDCAuthFlowResponse(**d) for pod_id, d in to_send_dicts.items()
+        }
+        await modeller_mailbox.send_oidc_auth_flow_responses(to_send)
 
-    async def handle(self, modeller_mailbox: _ModellerMailbox) -> None:
-        """Receive and perform OIDC verification."""
-        try:
-            # Check that start_server() has been called
-            try:
-                self._server_start_task
-            except AttributeError as e:
-                raise RuntimeError(
-                    "OIDC server has not been started; "
-                    "ensure start_server() has been called."
-                ) from e
-
-            # Await Client ID information from pods
-            oidc_details = await modeller_mailbox.get_oidc_client_ids()
-
-            # Verify Client IDs are as expected
-            self._verify_client_ids(oidc_details)
-
-            # Generate /authorize URL for each authorization request
-            to_send_dicts: DefaultDict[str, _JSONDict] = defaultdict(dict)
-            urls: Dict[str, str] = {}
-            states: Dict[str, str] = {}
-            for pod_id, details in oidc_details.items():
-                # Generate code verifier
-                # Should be between 43 and 128 characters; recommended to use
-                # base64url-encoding which secrets.token_urlsafe does for us. 60 bytes
-                # will produce ~78 characters.
-                # See: https://datatracker.ietf.org/doc/html/rfc7636#section-4.1
-                code_verifier: str = secrets.token_urlsafe(nbytes=60)
-                # Add to details
-                to_send_dicts[pod_id]["code_verifier"] = code_verifier
-
-                # Generate code challenge
-                # This is the b64url-encoded SHA256 hash of the ascii-encoded
-                # code_verifier.
-                # We also encode the bytes-output of the b64 encoding with UTF-8 to
-                # make it a string.
-                # See: https://datatracker.ietf.org/doc/html/rfc7636#section-4.2
-                code_challenge: str = _get_urlsafe_hash(
-                    code_verifier, initial_encoding="ascii"
-                )
-
-                # Request PKCE authorization
-                # Generate random state
-                state = secrets.token_hex(nbytes=32)
-                params = urlencode(
-                    {
-                        "audience": self._audience,
-                        "scope": self._scopes,
-                        "response_type": "code",
-                        "client_id": details.client_id,
-                        "state": state,
-                        "redirect_uri": self._redirect_uri,
-                        "code_challenge_method": "S256",
-                        "code_challenge": code_challenge,
-                    }
-                )
-                url = self._authorize_endpoint + f"?{params}"
-
-                urls[pod_id] = url
-                states[pod_id] = state
-
-            # Set generated URLs on web endpoint for later iteration
-            urls_list: List[str] = list(urls.values())
-            self._oidc_endpoint.initialise(urls_list, states)
-
-            # Wait for server if necessary
-            if not self._server_start_task.done():
-                logger.info("Waiting for OIDC challenge handler to start")
-                await self._server_start_task
-            # If an exception was thrown in the task,
-            # Task.result() will re-raise it for us.
-            self._server_start_task.result()
-
-            # Start URL processing
-            self._oidc_endpoint.start_processing()
-
-            # Wait for all authorization to be done
-            responses = await self._oidc_endpoint.get_responses()
-
-            # Validate the states are the same
-            for pod_id, response in responses.items():
-                if response.state != states[pod_id]:
-                    raise ValueError(
-                        f"Unable to validate response intended for {pod_id}"
-                    )
-
-            # Add auth_code and redirect_uri to details
-            for pod_id in oidc_details:
-                to_send_dicts[pod_id]["auth_code"] = responses[pod_id].auth_code
-                to_send_dicts[pod_id]["redirect_uri"] = self._redirect_uri
-
-            # Send details to pods
-            to_send: Dict[str, _OIDCAuthFlowResponse] = {
-                pod_id: _OIDCAuthFlowResponse(**d)
-                for pod_id, d in to_send_dicts.items()
-            }
-            await modeller_mailbox.send_oidc_auth_flow_responses(to_send)
-        finally:
-            await self.stop_server()
+        # Shutdown web server in background
+        asyncio.create_task(self._runner.cleanup())
 
     def _verify_client_ids(self, oidc_details: Dict[str, _OIDCClientID]) -> None:
         """Checks that all received client IDs match the expected one.
 
         Raises:
             AuthEnvironmentError:
                 If the Client IDs for this auth environment do not match those
                 received from the pods.
         """
         return _verify_client_ids(oidc_details, expected_auth_domain=self._auth_domain)
 
-    async def stop_server(self) -> None:
-        """Stop the web server."""
-        await self._runner.cleanup()
-
 
 class _OIDCDeviceCodeHandler(_ResponseHandler):
     """OIDC Device Authorisation Flow from modeller-side.
 
     See: https://auth0.com/docs/get-started/authentication-and-authorization-flow/device-authorization-flow  # noqa: B950
     """
 
@@ -414,15 +354,14 @@
         self._device_code_endpoint = f"https://{self._auth_domain}/oauth/device/code"
 
         self._scopes: str = scopes
         self._audience: str = audience
 
     async def handle(self, modeller_mailbox: _ModellerMailbox) -> None:
         """See parent class for more information."""
-        logger.debug("Getting oidc client ids")
         # Await Client ID information from pods
         oidc_details = await modeller_mailbox.get_oidc_client_ids()
 
         # Verify Client IDs are as expected
         self._verify_client_ids(oidc_details)
 
         # Get device code details for each pod
@@ -486,22 +425,21 @@
             AuthEnvironmentError:
                 If the Client IDs for this auth environment do not match those
                 received from the pods.
         """
         return _verify_client_ids(oidc_details, expected_auth_domain=self._auth_domain)
 
     def _get_device_code(self, client_id: str) -> _DeviceCodeDetailsPair:
-        logger.debug(f"Retrieving device code for client id: {client_id}")
         # Make request and raise for error if needed
         request_data: _DeviceCodeRequestDict = {
             "audience": self._audience,
             "scope": self._scopes,
             "client_id": client_id,
         }
-        response = web_utils.post(
+        response = requests.post(
             self._device_code_endpoint,
             data=request_data,
         )
 
         # Handle 4XX/5XX errors
         response.raise_for_status()
```

### Comparing `bitfount-0.5.86/bitfount/federated/transport/identity_verification/saml.py` & `bitfount-0.5.9/bitfount/federated/transport/identity_verification/saml.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,31 +1,22 @@
 """Handling of SAML Challenges."""
 import asyncio
-from asyncio import Task, create_task
 from typing import Awaitable, Callable, List, Mapping, MutableSequence, Optional, Tuple
-import urllib.parse
+import urllib
 import webbrowser
 
 from aiohttp import web
 from aiohttp.web import Application, AppRunner, Request, Response, TCPSite
 
 from bitfount.federated.logging import _get_federated_logger
-from bitfount.federated.transport.identity_verification import (
-    _BITFOUNT_MODELLER_PORT,
-    _PORT_WAIT_TIMEOUT,
-)
-from bitfount.federated.transport.identity_verification.types import (
-    _HasWebServer,
-    _ResponseHandler,
-)
+from bitfount.federated.transport.base_transport import _WorkerMailboxDetails
+from bitfount.federated.transport.identity_verification import _BITFOUNT_MODELLER_PORT
 from bitfount.federated.transport.message_service import _DecryptedBitfountMessage
-from bitfount.federated.transport.modeller_transport import (
-    _ModellerMailbox,
-    _WorkerMailboxDetails,
-)
+from bitfount.federated.transport.modeller_transport import _ModellerMailbox
+from bitfount.federated.types import _ResponseHandler
 from bitfount.types import _SAMLResponse
 
 logger = _get_federated_logger(__name__)
 
 
 class _SAMLWebEndpoint:
     """SAML Web Listener Endpoint.
@@ -114,17 +105,15 @@
 
         # Extract SAML response from request
         saml_response: _SAMLResponse = dict(await request.post())
         # Find where we need to send SAML response
         # And remove the challenge from the list,
         # so that next iteration we process a different SAML challenge
         _, worker_mailbox = self.challenges_with_mailbox_details.pop(0)
-        logger.info(
-            f"Responding to SAML challenge from: {worker_mailbox.pod_identifier}"
-        )
+        logger.info(f"Handling SAML challenge from: {worker_mailbox.pod_identifier}")
         # Send SAML response Pod that issued challenge
         await self.send(saml_response, worker_mailbox)
 
         if len(self.challenges_with_mailbox_details) == 0:
             # All challenges have been processed
             # So we can allow server clean up
             # And show a message in the browser
@@ -150,15 +139,15 @@
             f"If something goes wrong with your authentication "
             f"then visit this URL to try again: {saml_url}"
         )
         # aiohttp uses an exception to return a redirect response
         raise web.HTTPFound(saml_url)
 
 
-class _SAMLChallengeHandler(_ResponseHandler, _HasWebServer):
+class _SAMLChallengeHandler(_ResponseHandler):
     """Manages SAML user authentication.
 
     This is used to configure and perform SAML auth
     when challenges are received from resources.
 
     It can start, handle & shutdown a server for
     receiving SAML authentication information.
@@ -170,67 +159,42 @@
     ):
         self.idp_url = idp_url
 
         # This event is used to prevent us from shutting down the
         # web server until all SAML challenges have been processed
         self.all_saml_challenges_handled = asyncio.Event()
 
-        # This task is used to ensure we don't try to
+        # This event is used to ensure we don't try to
         # perform SAML authentication before the server has started
-        self._server_start_task: Task
+        self.server_started = asyncio.Event()
 
         app = Application()
         # We have to create this now so that it can be added as a route
         # As Routes cannot be added in aiohttp after the runner setup
         # has been called
         self.saml_endpoint = _SAMLWebEndpoint(
             self.all_saml_challenges_handled,
             self.idp_url,
         )
         app.add_routes(
             [web.post("/api/saml", self.saml_endpoint.handle_saml_idp_response)]
         )
         self.runner = AppRunner(app)
 
-    def start_server(self) -> Task:
-        """Sets up and starts the web server."""
-        self._server_start_task = create_task(self._start())
-        return self._server_start_task
-
-    async def _start(self) -> None:
+    async def start(self) -> None:
         """Sets up and starts the web server."""
-        logger.debug("Starting SAML web server")
-        host = "localhost"
-        port = _BITFOUNT_MODELLER_PORT
-
-        # Try to create web server for endpoint, failing out if binding to the
-        # host and port isn't possible.
-        try:
-            await self.runner.setup()
-            # This site is access by the modeller (who is running this code)
-            # in their own browser.
-            # Their SAML IdP provider will redirect them to:
-            # `http://localhost:{BITFOUNT_MODELLER_PORT}/{PATH}`
-            # So they should not have any issues accessing it
-            site = TCPSite(self.runner, host, port)
-            # wait_for() use ensures we don't hang indefinitely on waiting for the
-            # port to be available
-            await asyncio.wait_for(site.start(), _PORT_WAIT_TIMEOUT)
-        except TimeoutError:
-            logger.critical(
-                f"Timeout reached whilst trying to bind SAML web endpoint to "
-                f"http://{host}:{port}"
-            )
-            raise
-        except OSError:
-            # Raises OSError: [Errno 48] if address already in use
-            logger.critical(f"Unable to bind SAML web endpoint to http://{host}:{port}")
-            raise
-
-        logger.debug("SAML web server started successfully")
+        await self.runner.setup()
+        # This site is access by the modeller (who is running this code)
+        # in their own browser.
+        # Their SAML IdP provider will redirect them to:
+        # `http://localhost:{BITFOUNT_MODELLER_PORT}/{PATH}`
+        # So they should not have any issues accessing it
+        site = TCPSite(self.runner, "localhost", _BITFOUNT_MODELLER_PORT)
+        await site.start()
+        self.server_started.set()
 
     async def _handle_saml_challenges(
         self,
         send: Callable[[_SAMLResponse, _WorkerMailboxDetails], Awaitable[None]],
         saml_challenges: MutableSequence[_DecryptedBitfountMessage],
         worker_mailbox_details: Mapping[str, _WorkerMailboxDetails],
     ) -> None:
@@ -240,63 +204,45 @@
         Opens the first challenge in the user's browser.
 
         Args:
             send: Function for sending SAML response to pod.
             saml_challenges: SAML Challenges from pods.
             worker_mailbox_details: Mailbox details for each pod.
         """
-        # Check that start_server() has been called
-        try:
-            self._server_start_task
-        except AttributeError as e:
-            raise RuntimeError(
-                "SAML server has not been started; "
-                "ensure start_server() has been called."
-            ) from e
-
         # We only open the first one, then use
         # the browser to redirect us to the other pages
         # We pair the challenges with the mailbox details
         # so that we have easy access to the keys later
         challenges_with_mailbox_details: List[
             Tuple[_DecryptedBitfountMessage, _WorkerMailboxDetails]
         ] = [
             (saml_challenge, worker_mailbox_details[saml_challenge.sender])
             for saml_challenge in saml_challenges
         ]
         self.saml_endpoint.set_saml_challenges(challenges_with_mailbox_details, send)
 
-        if not self._server_start_task.done():
-            logger.info("Waiting for SAML challenge handler to start")
-            await self._server_start_task
-        # If an exception was thrown in the task,
-        # Task.result() will re-raise it for us.
-        self._server_start_task.result()
+        logger.info("Waiting for SAML challenge handler to start")
+        await self.server_started.wait()
 
         saml_url = f"{self.idp_url}{urllib.parse.quote_plus(saml_challenges[0].body)}"
         logger.info(
             f"Attempting to open browser. "
             f"Running a headless client? "
             f"You'll need to open this link in a browser: {saml_url}"
         )
         webbrowser.open(saml_url)
 
         # Wait for all SAML challenges to be handled by the listener
         await self.all_saml_challenges_handled.wait()
+        # Shutdown web server in background
+        asyncio.create_task(self.runner.cleanup())
 
     async def handle(self, modeller_mailbox: _ModellerMailbox) -> None:
-        try:
-            # SAML is in use, so we are expecting SAML challenges from the pods
-            saml_challenges = await modeller_mailbox.get_saml_challenges()
-
-            # Process the SAML challenges & respond to them
-            await self._handle_saml_challenges(
-                modeller_mailbox.send_saml_responses,
-                saml_challenges,
-                modeller_mailbox.worker_mailboxes,
-            )
-        finally:
-            await self.stop_server()
+        # SAML is in use, so we are expecting SAML challenges from the pods
+        saml_challenges = await modeller_mailbox.get_saml_challenges()
 
-    async def stop_server(self) -> None:
-        """Stop the web server."""
-        await self.runner.cleanup()
+        # Process the SAML challenges & respond to them
+        await self._handle_saml_challenges(
+            modeller_mailbox.send_saml_responses,
+            saml_challenges,
+            modeller_mailbox.worker_mailboxes,
+        )
```

### Comparing `bitfount-0.5.86/bitfount/federated/transport/modeller_transport.py` & `bitfount-0.5.9/bitfount/federated/transport/modeller_transport.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,276 +1,217 @@
 """Handles Modeller sending training requests and receiving pod responses."""
 from __future__ import annotations
 
-from dataclasses import dataclass
 import logging
 from typing import (
     Any,
+    Callable,
     Dict,
     Final,
-    Iterable,
     List,
     Mapping,
     MutableMapping,
     MutableSequence,
     Optional,
     Set,
     Tuple,
-    Union,
     cast,
 )
 
 from cryptography.hazmat.primitives.asymmetric.rsa import RSAPublicKey
 from grpc import RpcError
 
-from bitfount import config
-from bitfount.config import BITFOUNT_DEFAULT_BATCHED_EXECUTION
 from bitfount.federated.encryption import _AESEncryption
 from bitfount.federated.exceptions import BitfountTaskStartError
 from bitfount.federated.logging import _federate_logger, _get_federated_logger
+from bitfount.federated.task_requests import _ProtocolDetails
 from bitfount.federated.transport.base_transport import (
     Handler,
     MessageRetrievalError,
-    SyncHandler,
     _BaseMailbox,
     _send_aes_encrypted_message,
+    _WorkerMailboxDetails,
 )
 from bitfount.federated.transport.handlers import _AsyncMultipleResponsesHandler
 from bitfount.federated.transport.message_service import (
     _BitfountMessage,
     _BitfountMessageType,
     _DecryptedBitfountMessage,
     _MessageService,
 )
-from bitfount.federated.transport.protos.messages_pb2 import TaskMetadata
 from bitfount.federated.transport.types import (
     _OIDCAuthFlowResponse,
     _OIDCClientID,
     _PodDeviceCodeDetails,
 )
 from bitfount.federated.transport.utils import _average_training_metrics
 from bitfount.federated.types import (
     _RESPONSE_MESSAGES,
-    SerializedProtocol,
     _PodResponseType,
     _TaskRequestMessageGenerator,
 )
-from bitfount.types import _JSONDict, _SAMLResponse, _SerializedWeights, _StrAnyDict
+from bitfount.types import _JSONDict, _SAMLResponse, _SerializedWeights
 
 logger = _get_federated_logger(__name__)
 
-_DEFAULT_TASK_RESPONSE_TIMEOUT: Final[int] = 5 * 60  # 5 minutes
-_DEFAULT_OIDC_CLIENT_IDS_TIMEOUT: Final[int] = 5 * 60  # 5 minutes
-_SOFT_LIMIT_MESSAGE_TIMEOUT: Final[int] = config.BITFOUNT_ONLINE_CHECK_SOFT_LIMIT
-
-
-@dataclass
-class _WorkerMailboxDetails:
-    """Mailbox details for a specific task/worker on a pod.
-
-    Used by the modeller to encapsulate details of worker mailboxes.
-
-    Attributes:
-        pod_identifier: The parent pod's identifier.
-        public_key: The parent pod's public key.
-        mailbox_id: The mailbox ID for this specific task/worker.
-        aes_encryption_key: The encryption key to use for this specific task/worker.
-    """
-
-    pod_identifier: str
-    public_key: RSAPublicKey
-    mailbox_id: str
-    aes_encryption_key: bytes
+_DEFAULT_TASK_RESPONSE_TIMEOUT: Final[int] = 5 * 60
+_DEFAULT_OIDC_CLIENT_IDS_TIMEOUT: Final[int] = 5 * 60
 
 
 class _ModellerMailbox(_BaseMailbox):
     """Handles message interactions with pods."""
 
     def __init__(
         self,
         mailbox_id: str,
         worker_mailboxes: Mapping[str, _WorkerMailboxDetails],
-        task_id: str,
         message_service: _MessageService,
-        handlers: Optional[
-            Mapping[_BitfountMessageType, Union[Handler, Iterable[Handler]]]
-        ] = None,
+        handlers: Optional[Mapping[_BitfountMessageType, Handler]] = None,
     ):
         """Creates a new ModellerMailbox.
 
         Note that the preferred way to get a new ModellerMailbox is by calling
         ModellerMailbox.send_task_requests() which will instantiate the correct
         ModellerMailbox for you.
 
         Args:
             mailbox_id: The mailbox ID for this modeller mailbox.
             worker_mailboxes: A mapping of pod identifiers to worker mailbox details
                               for the pods/workers that will be involved in this task.
-            task_id: The ID for the task this mailbox belongs to.
             message_service: The underlying message service.
             handlers: Optional. A set of handlers to initialise with.
         """
         super().__init__(
             mailbox_id=mailbox_id, message_service=message_service, handlers=handlers
         )
         self.worker_mailboxes: Dict[str, _WorkerMailboxDetails] = dict(worker_mailboxes)
         self._pod_identifiers: Set[str] = set(worker_mailboxes.keys())
-        self._task_id = task_id
 
         self.accepted_worker_mailboxes: Dict[str, _WorkerMailboxDetails] = {}
 
         self._setup_federated_logging()
-        self._setup_online_status_handler()
-
-    @property
-    def task_id(self) -> str:
-        """The task ID of the task associated with this mailbox."""
-        return self._task_id
 
     ############################
     # Task Setup Phase Methods #
     ############################
     @classmethod
     async def send_task_requests(
         cls,
-        serialized_protocol: SerializedProtocol,
+        protocol_details: _ProtocolDetails,
         pod_public_keys: Mapping[str, RSAPublicKey],
         task_request_msg_gen: _TaskRequestMessageGenerator,
         message_service: _MessageService,
-        project_id: Optional[str] = None,
-        run_on_new_data_only: bool = False,
-        batched_execution: Optional[bool] = None,
     ) -> _ModellerMailbox:
         """Sends task requests, such as training requests, to pods.
 
         Appropriate mailboxes will be created for the modeller and worker mailboxes
         for the pods.
 
         Args:
-            serialized_protocol: The serialized protocol to use for the task.
-            pod_public_keys: A mapping of pod identifiers to their public keys for all
-                the pods involved in this task request.
-            task_request_msg_gen: A callable which will generate a task request message
-                appropriate to the chosen verification method.
+            protocol_details:
+                A tuple of protocol name, algorithm name and optionally model and
+                aggregator names.
+            pod_public_keys:
+                A mapping of pod identifiers to their public keys for all the pods
+                involved in this task request.
+            task_request_msg_gen:
+                A callable which will generate a task request message appropriate
+                to the chosen verification method.
             message_service: The underlying message service to use.
-            project_id: The project Id the task belongs to. Defaults to None.
-            run_on_new_data_only: Whether to run the task on new datapoints only.
-                Defaults to False.
-            batched_execution: Whether to run the task in batched execution mode.
-                Defaults to False.
 
         Returns:
             The created modeller mailbox for this task.
         """
-        if batched_execution is None:
-            batched_execution = BITFOUNT_DEFAULT_BATCHED_EXECUTION
-        modeller_mailbox_id, worker_mailboxes, task_id = await cls._send_task_requests(
-            serialized_protocol,
+        modeller_mailbox_id, worker_mailboxes = await cls._send_task_requests(
+            protocol_details,
             pod_public_keys,
             task_request_msg_gen=task_request_msg_gen,
             message_service=message_service,
-            project_id=project_id,
-            run_on_new_data_only=run_on_new_data_only,
-            batched_execution=batched_execution,
         )
         modeller_mailbox = cls(
             mailbox_id=modeller_mailbox_id,
             worker_mailboxes=worker_mailboxes,
-            task_id=task_id,
             message_service=message_service,
         )
         return modeller_mailbox
 
     @classmethod
     async def _send_task_requests(
         cls,
-        serialized_protocol: SerializedProtocol,
+        protocol_details: _ProtocolDetails,
         pod_public_keys: Mapping[str, RSAPublicKey],
         task_request_msg_gen: _TaskRequestMessageGenerator,
         message_service: _MessageService,
-        project_id: Optional[str],
-        run_on_new_data_only: bool = False,
-        batched_execution: Optional[bool] = None,
-    ) -> Tuple[str, Dict[str, _WorkerMailboxDetails], str]:
+    ) -> Tuple[str, Dict[str, _WorkerMailboxDetails]]:
         """Manage sending of task requests, such as training requests, to pods.
 
         Args:
-            serialized_protocol: The serialized protocol to use for the task.
-            pod_public_keys: A mapping of pod identifiers to their public keys for all
-                the pods involved in this task request.
-            task_request_msg_gen: A callable which will generate a task request message
-                appropriate to the chosen verification method.
+            protocol_details:
+                A tuple of protocol name, algorithm name and optionally model and
+                aggregator names.
+            pod_public_keys:
+                A mapping of pod identifiers to their public keys for all the pods
+                involved in this task request.
+            task_request_msg_gen:
+                A callable which will generate a task request message appropriate
+                to the chosen verification method.
             message_service: The underlying message service to use.
-            project_id: The project Id the task belongs to.
-            run_on_new_data_only: Whether to run the task on new datapoints only.
-                Defaults to False.
-            batched_execution: Whether to run the task in batched execution mode.
-                Defaults to False.
 
         Returns:
             Tuple of:
                 - (str) modeller mailbox ID
                 - (dict) of pod identifier to worker mailbox details
-                - (str) the task ID
         """
         # Shorthand the pod identifiers for ease of use
-        pod_identifiers: List[str] = list(pod_public_keys)
-        if batched_execution is None:
-            batched_execution = BITFOUNT_DEFAULT_BATCHED_EXECUTION
+        pod_identifiers: List[str] = list(pod_public_keys.keys())
+
         # Generate encryption keys and then task request messages for each pod.
         aes_key_per_pod = {
             pod_identifier: _AESEncryption.generate_key()
             for pod_identifier in pod_identifiers
         }
         task_request_per_pod: Dict[str, bytes] = {
             pod_identifier: task_request_msg_gen(
-                serialized_protocol,
+                protocol_details,
                 pod_identifiers,
                 aes_key_per_pod[pod_identifier],
                 pod_public_key,
-                project_id,
-                run_on_new_data_only,
-                batched_execution,
             )
             for pod_identifier, pod_public_key in pod_public_keys.items()
         }
 
         # Send this message to each pod and receive the modeller's mailbox ID
         # and the mailbox IDs of all the pods in the task.
         try:
             (
                 modeller_mailbox_id,
                 worker_mailbox_ids,
-                task_id,
-            ) = await message_service.setup_task(
-                task_request_per_pod,
-                TaskMetadata(protocol=serialized_protocol["class_name"]),
-                project_id,
+            ) = await message_service.setup_communication_with_pods(
+                task_request_per_pod
             )
             logger.info(f"Sent task requests to {pod_identifiers}")
         except RpcError as err:
             logger.error(
                 f"Failed to start task with pods: {pod_identifiers}. Error: {err}"
             )
             raise BitfountTaskStartError(
                 f"Failed to start task with pods: {pod_identifiers}"
-            ) from err
+            )
 
         return (
             modeller_mailbox_id,
             {
                 pod_identifier: _WorkerMailboxDetails(
                     pod_identifier,
                     pod_public_keys[pod_identifier],
                     worker_mailbox_id,
                     aes_key_per_pod[pod_identifier],
                 )
                 for pod_identifier, worker_mailbox_id in worker_mailbox_ids.items()
             },
-            task_id,
         )
 
     async def get_oidc_client_ids(
         self, timeout: Optional[int] = _DEFAULT_OIDC_CLIENT_IDS_TIMEOUT
     ) -> Dict[str, _OIDCClientID]:
         """Receive OIDC client ID responses from pods.
 
@@ -303,21 +244,15 @@
                 # timeout expires.
                 await multi_response.wait_for_responses(timeout=timeout)
             except MessageRetrievalError as err:
                 logger.error(
                     f"Error receiving responses from all pods to the OIDC phase "
                     f"of the task request: {err}"
                 )
-                raise BitfountTaskStartError(
-                    "Failed to start task with all pods."
-                ) from err
-
-        if not oidc_client_ids:
-            logger.error("No OIDC client id retreived from message handler")
-            raise ValueError("No OIDC client id retreived from message handler")
+                raise BitfountTaskStartError("Failed to start task with all pods.")
 
         return oidc_client_ids
 
     async def send_oidc_auth_flow_responses(
         self,
         oidc_responses: Dict[str, _OIDCAuthFlowResponse],
     ) -> None:
@@ -329,15 +264,14 @@
                 pod_mailbox.aes_encryption_key,
                 self.message_service,
                 message_type=_BitfountMessageType.OIDC_AFC_PKCE_RESPONSE,
                 recipient=pod_mailbox.pod_identifier,
                 recipient_mailbox_id=pod_mailbox.mailbox_id,
                 sender=self.message_service.username,
                 sender_mailbox_id=self.mailbox_id,
-                task_id=self._task_id,
             )
 
     async def send_oidc_device_code_responses(
         self, device_code_details: Dict[str, _PodDeviceCodeDetails]
     ) -> None:
         """Send response for OIDC Device Code Flow."""
         for pod_id, details in device_code_details.items():
@@ -347,28 +281,26 @@
                 pod_mailbox.aes_encryption_key,
                 self.message_service,
                 message_type=_BitfountMessageType.OIDC_DEVICE_CODE_RESPONSE,
                 recipient=pod_mailbox.pod_identifier,
                 recipient_mailbox_id=pod_mailbox.mailbox_id,
                 sender=self.message_service.username,
                 sender_mailbox_id=self.mailbox_id,
-                task_id=self._task_id,
             )
 
     async def get_saml_challenges(
         self, timeout: int = _DEFAULT_TASK_RESPONSE_TIMEOUT
     ) -> List[_DecryptedBitfountMessage]:
         """Process incoming SAML Challenges.
 
         Incoming responses are awaited as a group until all are received or timeout
         is reached. Responses are expected from all pods assigned at mailbox init.
 
         Returns:
-            A list of received SAML challenges. Note that there is no notion of
-            "pod order" in this list, the elements are in the order they are received.
+            A list of received SAML challenges.
         """
         saml_challenges = []
 
         def _saml_challenge_message_handler(message: _BitfountMessage) -> None:
             """Simple handler that saves messages to closured dict."""
             logger.info(
                 f"Received message with type: {message.message_type} "
@@ -390,17 +322,15 @@
                 # timeout expires.
                 await multi_response.wait_for_responses(timeout=timeout)
             except MessageRetrievalError as err:
                 logger.error(
                     f"Error receiving responses from all pods to the task request: "
                     f"{err}"
                 )
-                raise BitfountTaskStartError(
-                    "Failed to start task with all pods."
-                ) from err
+                raise BitfountTaskStartError("Failed to start task with all pods.")
 
         return saml_challenges
 
     async def process_task_request_responses(
         self, timeout: int = _DEFAULT_TASK_RESPONSE_TIMEOUT
     ) -> Dict[str, _WorkerMailboxDetails]:
         """Process incoming responses to a task request.
@@ -443,17 +373,15 @@
                 # timeout expires.
                 await multi_response.wait_for_responses(timeout=timeout)
             except MessageRetrievalError as err:
                 logger.error(
                     f"Error receiving responses from all pods to the task request: "
                     f"{err}"
                 )
-                raise BitfountTaskStartError(
-                    "Failed to start task with all pods."
-                ) from err
+                raise BitfountTaskStartError("Failed to start task with all pods.")
 
         accepted_mailbox_details = await self._handle_task_responses(response_messages)
 
         # Set on the attribute and return as well
         self.accepted_worker_mailboxes = accepted_mailbox_details
         return accepted_mailbox_details
 
@@ -489,18 +417,29 @@
 
                 # Handle REJECT response (regardless of what form that reject takes)
                 else:
                     rejected_tasks.append(response)
 
                     # Process different forms of rejection
                     for response_type in response.body:
-                        logger.error(
-                            f"Received rejection from {pod_identifier}. "
-                            f"{_RESPONSE_MESSAGES[_PodResponseType[response_type]]}"  # noqa: B950
-                        )
+                        if (
+                            response_type
+                            == _PodResponseType.SECURE_AGGREGATION_WORKERS_NOT_AUTHORISED.name  # noqa: B950
+                        ):
+                            unapproved_pods = ", ".join(response.body[response_type])
+                            logger.error(
+                                f"Received rejection from {pod_identifier}. "
+                                f"{_RESPONSE_MESSAGES[_PodResponseType[response_type]]}: "  # noqa: B950
+                                f"{unapproved_pods}"
+                            )
+                        else:
+                            logger.error(
+                                f"Received rejection from {pod_identifier}. "
+                                f"{_RESPONSE_MESSAGES[_PodResponseType[response_type]]}"  # noqa: B950
+                            )
 
             # Handle cases where response didn't arrive
             else:
                 response = cast(None, response)
                 ignored_tasks.append(response)
 
         logger.info(
@@ -520,15 +459,14 @@
             pod_mailbox.aes_encryption_key,
             self.message_service,
             message_type=_BitfountMessageType.SAML_RESPONSE,
             recipient=pod_mailbox.pod_identifier,
             recipient_mailbox_id=pod_mailbox.mailbox_id,
             sender=self.message_service.username,
             sender_mailbox_id=self.mailbox_id,
-            task_id=self._task_id,
         )
 
     ################################
     # End Task Setup Phase Methods #
     ################################
 
     ##############################
@@ -549,39 +487,39 @@
                 mailbox.aes_encryption_key,
                 self.message_service,
                 message_type=message_type,
                 recipient=mailbox.pod_identifier,
                 recipient_mailbox_id=mailbox.mailbox_id,
                 sender_mailbox_id=self.mailbox_id,
                 sender=self.message_service.username,
-                task_id=self._task_id,
             )
 
+    async def send_task_details(self, task_details: _JSONDict) -> None:
+        """Sends the task details (protocol, algorithm, etc) to the workers.
+
+        Args:
+            task_details: The serialized task details.
+        """
+        await self._send_to_all_pods_aes_encrypt(
+            task_details, _BitfountMessageType.ALGORITHM_EXCHANGE
+        )
+
     async def send_training_iteration_complete_update(
         self, training_complete: bool
     ) -> None:
         """Sends whether training is complete or not to the workers."""
-        logger.debug(f"Sending TRAINING_COMPLETE from {self.mailbox_id}")
         await self._send_to_all_pods_aes_encrypt(
             training_complete, _BitfountMessageType.TRAINING_COMPLETE
         )
 
-    async def send_task_start_message(self) -> None:
-        """Sends task start message to the workers.
-
-        Note: The message is not important here, the message type is.
-        """
-        await self._send_to_all_pods_aes_encrypt(None, _BitfountMessageType.TASK_START)
-
     async def send_task_complete_message(self) -> None:
         """Sends task complete message to the workers.
 
         Note: The message is not important here, the message type is.
         """
-        logger.info(f"Sending TASK_COMPLETE message to workers from {self.mailbox_id}")
         await self._send_to_all_pods_aes_encrypt(
             None, _BitfountMessageType.TASK_COMPLETE
         )
 
     def _decrypt_message(self, message: _BitfountMessage) -> _DecryptedBitfountMessage:
         """Decrypt received message using this mailbox's AES keys.
 
@@ -591,24 +529,23 @@
         Returns:
             The decrypted message body.
         """
         return message.decrypt(self.worker_mailboxes[message.sender].aes_encryption_key)
 
     async def get_evaluation_results_from_workers(
         self, timeout: Optional[int] = None
-    ) -> _StrAnyDict:
+    ) -> List[Dict[str, float]]:
         """Get evaluation results from workers."""
-        logger.info("Waiting to receive results from Pods...")
-        all_eval_results: _StrAnyDict = {}
+        all_eval_results: List[Dict[str, float]] = []
 
         # Create light-weight handler to append to shared list
         def evaluation_results_handler(message: _BitfountMessage) -> None:
             logger.debug(f"Receiving evaluation results from worker {message.sender}")
             eval_results = self._decrypt_message(message).body
-            all_eval_results[message.sender] = eval_results
+            all_eval_results.append(eval_results)
 
         # We use `self` rather than `self.modeller_mailbox` as the mailbox below
         # because this is ensures things are correctly delegated.
         with _AsyncMultipleResponsesHandler(
             handler=evaluation_results_handler,
             message_types=_BitfountMessageType.EVALUATION_RESULTS,
             mailbox=self,
@@ -623,121 +560,28 @@
         await self._send_to_all_pods_aes_encrypt(
             message, _BitfountMessageType.LOG_MESSAGE
         )
 
     def _setup_federated_logging(self) -> None:
         """Set up federated logging."""
         _federate_logger(self)
-        self.register_handler(
-            _BitfountMessageType.LOG_MESSAGE, self._get_log_message_handler()
-        )
-
-    def _get_log_message_handler(self) -> SyncHandler:
-        """Create the appropriate handler for LOG_MESSAGE messages."""
 
         def log_message_handler(message: _BitfountMessage) -> None:
-            """Locally logs the log message that has been received from a pod."""
             log_message_wrapper: _DecryptedBitfountMessage = self._decrypt_message(
                 message
             )
-            log_message: _JSONDict = log_message_wrapper.body
-
+            log_message: Dict[str, Any] = log_message_wrapper.body
             # We prepend the name of the pod to the log message
-            log_message["msg"] = f"<FROM POD {message.sender}>: {log_message['msg']}"
-
-            # Modify processName and threadName to indicate these are non-local
-            try:
-                log_message["processName"] = f"<{log_message['processName']}>"
-            except KeyError:
-                pass
-            try:
-                log_message["threadName"] = f"<{log_message['threadName']}>"
-            except KeyError:
-                pass
-
+            log_message["msg"] = f"{message.sender} Pod: {log_message['msg']}"
             # We remove the `federated` key to avoid recursively sending a federated
             # log message on both the Modeller and Worker sides
             log_message.pop("federated")
             logger.handle(logging.makeLogRecord(log_message))
 
-        return log_message_handler
-
-    def _setup_online_status_handler(self) -> None:
-        """Respond to online status requests from Pods."""
-
-        async def status_request_handler(message: _BitfountMessage) -> None:
-            """Responds to an ONLINE_CHECK request with an ONLINE_RESPONSE."""
-            logger.info(f"Informing {message.sender} that we are still online.")
-
-            # We use the message service sending directly as we don't want to
-            # re-encrypt the already encrypted body, we just want to send it back
-            # to the worker.
-            await self.message_service.send_message(
-                _BitfountMessage(
-                    message_type=_BitfountMessageType.ONLINE_RESPONSE,
-                    body=message.body,
-                    recipient=message.sender,
-                    recipient_mailbox_id=message.sender_mailbox_id,
-                    sender=self.message_service.username,
-                    sender_mailbox_id=self.mailbox_id,
-                    task_id=self._task_id,
-                ),
-                already_packed=True,
-            )
-
-        self.register_handler(
-            _BitfountMessageType.ONLINE_CHECK,
-            status_request_handler,
-            high_priority=True,
-        )
-
-    async def get_num_batches_message(self, timeout: Optional[int] = None) -> int:
-        """Get number of batches from worker for batched execution.
-
-        This is intended to be used for batched execution, where the number of batches
-        is not known in advance by the modeller so the modeller must get it from the
-        worker. Batched execution is only supported in cases where there is only one
-        worker.
-
-        Args:
-            modeller_mailbox: The modeller mailbox.
-            timeout: The timeout for the request.
-
-        Returns:
-            A number of batches.
-
-        Raises:
-            ValueError: If the number of responses is not 1.
-        """
-        num_batches_list: List[int] = []
-
-        def batched_execution_handler(message: _BitfountMessage) -> None:
-            logger.debug(
-                f"Receiving number of batches update from worker {message.sender}"
-            )
-            # Deliberate access to private method here as that method shouldn't be used
-            # in any other context than transport layer access.
-            # noinspection PyProtectedMember
-            num_batches: int = self._decrypt_message(message).body
-            num_batches_list.append(num_batches)
-
-        with _AsyncMultipleResponsesHandler(
-            handler=batched_execution_handler,
-            message_types=_BitfountMessageType.NUMBER_OF_BATCHES,
-            mailbox=self,
-            responders=self.accepted_worker_mailboxes.keys(),
-        ) as response_handler:
-            await response_handler.wait_for_responses(timeout=timeout)
-
-        if len(num_batches_list) != 1:
-            raise ValueError(
-                f"Expected one response from worker for number of batches, "
-                f"got {len(num_batches_list)}"
-            )
-        return num_batches_list[0]
+        self.register_handler(_BitfountMessageType.LOG_MESSAGE, log_message_handler)
 
     ##################################
     # End Task Running Phase Methods #
     ##################################
 
 
 async def _send_model_parameters(
@@ -748,148 +592,39 @@
     # any other context than transport layer access.
     # noinspection PyProtectedMember
     await modeller_mailbox._send_to_all_pods_aes_encrypt(
         model_parameters, _BitfountMessageType.MODEL_PARAMETERS
     )
 
 
-async def _send_psi_dataset_modeller(
-    dataset: List[str], modeller_mailbox: _ModellerMailbox
-) -> None:
-    """Sends psi datasets to the worker."""
-    await modeller_mailbox._send_to_all_pods_aes_encrypt(
-        dataset, _BitfountMessageType.PSI_DATASET
-    )
-
-
-def _psi_dataset_handler(
-    modeller_mailbox: _ModellerMailbox,
-    psi_datasets_str: List[Tuple[List[str], List[str]]],
-) -> SyncHandler:
-    """Handle receiving of PSI datasets from workers.
-
-    Appends them to the `psi_datasets_str` list.
-
-    Note that there is no notion of "worker order" in this list, the elements are
-    in the order they are received.
-    """
-
-    def psi_dataset_handler(message: _BitfountMessage) -> None:
-        # Create light-weight handler to append to shared list
-        logger.debug(f"Receiving psi dataset from worker {message.sender}")
-        # Deliberate access to private method here as that method shouldn't be used in
-        # any other context than transport layer access.
-        # noinspection PyProtectedMember
-        psi_results = modeller_mailbox._decrypt_message(message).body
-        psi_datasets_str.append(psi_results)
-
-    return psi_dataset_handler
-
-
-async def _get_psi_datasets_from_workers(
-    modeller_mailbox: _ModellerMailbox, timeout: Optional[int] = None
-) -> List[Tuple[List[str], List[str]]]:
-    """Get psi datasets from workers.
-
-    Note that there is no notion of "worker order" in this list, the elements are
-    in the order they are received.
-    """
-    psi_datasets_str: List[Tuple[List[str], List[str]]] = []
-    psi_dataset_handler = _psi_dataset_handler(modeller_mailbox, psi_datasets_str)
-    # We use `self` rather than `self.modeller_mailbox` as the mailbox below
-    # because this is ensures things are correctly delegated.
-    with _AsyncMultipleResponsesHandler(
-        handler=psi_dataset_handler,
-        message_types=_BitfountMessageType.PSI_DATASET,
-        mailbox=modeller_mailbox,
-        responders=modeller_mailbox.accepted_worker_mailboxes.keys(),
-    ) as response_handler:
-        await response_handler.wait_for_responses(timeout=timeout)
-    return psi_datasets_str
-
-
-def _public_key_handler(
-    modeller_mailbox: _ModellerMailbox, public_key: List[bytes]
-) -> SyncHandler:
-    """Public key handler.
-
-    Will mutate the passed in sequence by appending responses to it.
-    Should only be used for the PrivateSetIntersection protocol.
-
-    Note that there is no notion of "worker order" in this list, the elements are
-    in the order they are received.
-    """
-
-    def public_key_handler(message: _BitfountMessage) -> None:
-        logger.debug(f"Receiving the public key from worker {message.sender}")
-        # Deliberate access to private method here as that method shouldn't be used in
-        # any other context than transport layer access.
-        # noinspection PyProtectedMember
-        public_key_ = modeller_mailbox._decrypt_message(message).body
-        public_key.append(public_key_)
-
-    return public_key_handler
-
-
-async def _get_public_key(
-    modeller_mailbox: _ModellerMailbox, timeout: Optional[int] = None
-) -> List[bytes]:
-    """Get public key from worker.
-
-    Used for the PrivateSetIntersection protocol.
-
-    Note that there is no notion of "pod order" in this list, the elements are in
-    the order they are received.
-    """
-    public_key_lst: List[bytes] = []
-    public_key_handler = _public_key_handler(modeller_mailbox, public_key_lst)
-    with _AsyncMultipleResponsesHandler(
-        handler=public_key_handler,
-        message_types=_BitfountMessageType.KEY_EXCHANGE,
-        mailbox=modeller_mailbox,
-        responders=modeller_mailbox.accepted_worker_mailboxes.keys(),
-    ) as response_handler:
-        non_responders = await response_handler.wait_for_responses(timeout=timeout)
-        if non_responders:
-            logger.info(
-                f"The following did not send training metrics in time: {non_responders}"
-            )
-    return public_key_lst
-
-
 def _training_metrics_handler(
     modeller_mailbox: _ModellerMailbox,
     training_metrics: MutableSequence[Mapping[str, str]],
-) -> SyncHandler:
+) -> Callable[[_BitfountMessage], None]:
     """Training metrics handler.
 
     Will mutate the passed in sequence by appending responses to it.
-
-    Note that there is no notion of "worker order" in this list, the elements are
-    in the order they are received.
     """
 
     def training_metrics_handler(message: _BitfountMessage) -> None:
         logger.debug(f"Receiving training metrics update from worker {message.sender}")
         # Deliberate access to private method here as that method shouldn't be used in
         # any other context than transport layer access.
         # noinspection PyProtectedMember
-        single_training_metrics: Mapping[str, str] = modeller_mailbox._decrypt_message(
-            message
-        ).body
+        single_training_metrics = modeller_mailbox._decrypt_message(message).body
         training_metrics.append(single_training_metrics)
 
     return training_metrics_handler
 
 
 async def _get_training_metrics_from_workers(
     modeller_mailbox: _ModellerMailbox,
     timeout: Optional[int] = None,
 ) -> Dict[str, float]:
-    """Get average training metrics from workers."""
+    """Get training metrics from workers."""
     training_metrics: List[Mapping[str, str]] = []
     training_metrics_handler = _training_metrics_handler(
         modeller_mailbox, training_metrics
     )
     with _AsyncMultipleResponsesHandler(
         handler=training_metrics_handler,
         message_types=_BitfountMessageType.TRAINING_METRICS,
@@ -905,46 +640,37 @@
     # Find the average metrics for those who responded and return
     averaged_training_metrics = _average_training_metrics(training_metrics)
     return averaged_training_metrics
 
 
 def _parameter_updates_handler(
     modeller_mailbox: _ModellerMailbox,
-    weight_updates: MutableMapping[str, _SerializedWeights],
-) -> SyncHandler:
+    weight_updates: MutableSequence[_SerializedWeights],
+) -> Callable[[_BitfountMessage], None]:
     """Parameter update handler.
 
-    Will mutate the passed in mapping by appending responses to it.
+    Will mutate the passed in sequence by appending responses to it.
     """
 
     def parameter_update_handler(message: _BitfountMessage) -> None:
         logger.debug(f"Receiving parameter update from worker {message.sender}")
         # Deliberate access to private method here as that method shouldn't be used in
         # any other context than transport layer access.
         # noinspection PyProtectedMember
         weight_update = modeller_mailbox._decrypt_message(message).body
-        sender = message.sender
-        weight_updates[sender] = weight_update
+        weight_updates.append(weight_update)
 
     return parameter_update_handler
 
 
 async def _get_parameter_updates_from_workers(
     modeller_mailbox: _ModellerMailbox, timeout: Optional[int] = None
-) -> Dict[str, _SerializedWeights]:
-    """Get model parameter updates from workers.
-
-    Args:
-        modeller_mailbox: The modeller mailbox.
-        timeout: The timeout for the request.
-
-    Returns:
-        A dictionary of the form {worker_name: weight_update}.
-    """
-    weight_updates: Dict[str, _SerializedWeights] = {}
+) -> List[_SerializedWeights]:
+    """Get model parameter updates from workers."""
+    weight_updates: List[_SerializedWeights] = []
     parameter_updates_handler = _parameter_updates_handler(
         modeller_mailbox, weight_updates
     )
 
     with _AsyncMultipleResponsesHandler(
         handler=parameter_updates_handler,
         message_types=_BitfountMessageType.TRAINING_UPDATE,
```

### Comparing `bitfount-0.5.86/bitfount/federated/transport/pod_transport.py` & `bitfount-0.5.9/bitfount/federated/transport/pod_transport.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 """Handling incoming tasks for a Pod."""
 from __future__ import annotations
 
-from typing import Iterable, List, Mapping, Optional, Union
+from typing import Mapping, Optional
 
 from grpc import RpcError
 
 from bitfount.federated.exceptions import PodConnectFailedError
 from bitfount.federated.logging import _get_federated_logger
 from bitfount.federated.transport.base_transport import Handler, _BaseMailbox
 from bitfount.federated.transport.message_service import (
@@ -22,17 +22,15 @@
     """Handling of incoming training tasks from modellers."""
 
     def __init__(
         self,
         pod_name: str,
         mailbox_id: str,
         message_service: _MessageService,
-        handlers: Optional[
-            Mapping[_BitfountMessageType, Union[Handler, Iterable[Handler]]]
-        ] = None,
+        handlers: Optional[Mapping[_BitfountMessageType, Handler]] = None,
     ):
         """Creates a new pod mailbox.
 
         Note that the preferred way to get a new PodMailbox is by calling
         PodMailbox.connect_pod() which will instantiate the correct PodMailbox
         for you.
 
@@ -46,38 +44,35 @@
             mailbox_id=mailbox_id, message_service=message_service, handlers=handlers
         )
         self.pod_name = pod_name
         self.pod_identifier = f"{self.message_service.username}/{self.pod_name}"
 
     @classmethod
     async def connect_pod(
-        cls,
-        pod_name: str,
-        dataset_names: Optional[List[str]],
-        message_service: _MessageService,
+        cls, pod_name: str, message_service: _MessageService
     ) -> _PodMailbox:
         """Registers with message service and created mailbox.
 
         Returns:
             The created mailbox.
 
         Raises:
             RuntimeError: If a GRPC error occurs or the message service is
                           unable to connect the pod.
         """
         logger.info(f"Connecting to messaging service as: {pod_name}")
         try:
-            mailbox_id = await message_service.connect_pod(pod_name, dataset_names)
+            mailbox_id = await message_service.connect_pod(pod_name)
         except (RpcError, PodConnectFailedError) as err:
             logger.critical(
                 "Error occurred when trying to call PodConnect on messaging service"
             )
             raise PodConnectFailedError(
                 f"Failed to connect to messaging service "
                 f"as pod: {pod_name}. Error: {err}"
-            ) from err
+            )
 
         pod_mailbox = cls(
             pod_name=pod_name, mailbox_id=mailbox_id, message_service=message_service
         )
 
         return pod_mailbox
```

### Comparing `bitfount-0.5.86/bitfount/federated/transport/protos/messages_pb2_grpc.py` & `bitfount-0.5.9/bitfount/federated/transport/protos/messages_pb2_grpc.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,396 +1,264 @@
 # Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!
 """Client and server classes corresponding to protobuf-defined services."""
 import grpc
 
-from bitfount.federated.transport.protos import (
-    messages_pb2 as bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2,
-)
+from bitfount.federated.transport.protos import messages_pb2 as bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2
 
 
 class MessageServiceStub(object):
     """Missing associated documentation comment in .proto file."""
 
     def __init__(self, channel):
         """Constructor.
 
         Args:
             channel: A grpc.Channel.
         """
         self.PodConnect = channel.unary_unary(
-            "/messages.MessageService/PodConnect",
-            request_serializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.PodData.SerializeToString,
-            response_deserializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.SuccessResponse.FromString,
-        )
+                '/messages.MessageService/PodConnect',
+                request_serializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.PodData.SerializeToString,
+                response_deserializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.SuccessResponse.FromString,
+                )
         self.SetupTaskMailboxes = channel.unary_unary(
-            "/messages.MessageService/SetupTaskMailboxes",
-            request_serializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.BitfountTasks.SerializeToString,
-            response_deserializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.CommunicationDetails.FromString,
-        )
+                '/messages.MessageService/SetupTaskMailboxes',
+                request_serializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.BitfountTasks.SerializeToString,
+                response_deserializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.CommunicationDetails.FromString,
+                )
+        self.StartTraining = channel.unary_unary(
+                '/messages.MessageService/StartTraining',
+                request_serializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.BitfountTasks.SerializeToString,
+                response_deserializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.CommunicationDetails.FromString,
+                )
         self.AcknowledgeMessage = channel.unary_unary(
-            "/messages.MessageService/AcknowledgeMessage",
-            request_serializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.Acknowledgement.SerializeToString,
-            response_deserializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.SuccessResponse.FromString,
-        )
+                '/messages.MessageService/AcknowledgeMessage',
+                request_serializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.Acknowledgement.SerializeToString,
+                response_deserializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.SuccessResponse.FromString,
+                )
         self.GetBitfountMessage = channel.unary_unary(
-            "/messages.MessageService/GetBitfountMessage",
-            request_serializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.CommunicationDetails.SerializeToString,
-            response_deserializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.BitfountMessage.FromString,
-        )
+                '/messages.MessageService/GetBitfountMessage',
+                request_serializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.CommunicationDetails.SerializeToString,
+                response_deserializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.BitfountMessage.FromString,
+                )
         self.SendBitfountMessage = channel.unary_unary(
-            "/messages.MessageService/SendBitfountMessage",
-            request_serializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.BitfountMessage.SerializeToString,
-            response_deserializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.SuccessResponse.FromString,
-        )
+                '/messages.MessageService/SendBitfountMessage',
+                request_serializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.BitfountMessage.SerializeToString,
+                response_deserializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.SuccessResponse.FromString,
+                )
         self.GetLargeObjectStorage = channel.unary_unary(
-            "/messages.MessageService/GetLargeObjectStorage",
-            request_serializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.LargeStorageRequest.SerializeToString,
-            response_deserializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.BlobStorageData.FromString,
-        )
-        self.SetupTask = channel.unary_unary(
-            "/messages.MessageService/SetupTask",
-            request_serializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.TaskTransferRequests.SerializeToString,
-            response_deserializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.TaskTransferMetadata.FromString,
-        )
-        self.InitiateTask = channel.unary_unary(
-            "/messages.MessageService/InitiateTask",
-            request_serializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.BitfountTasks.SerializeToString,
-            response_deserializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.CommunicationDetails.FromString,
-        )
+                '/messages.MessageService/GetLargeObjectStorage',
+                request_serializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.LargeStorageRequest.SerializeToString,
+                response_deserializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.BlobStorageData.FromString,
+                )
 
 
 class MessageServiceServicer(object):
     """Missing associated documentation comment in .proto file."""
 
     def PodConnect(self, request, context):
         """Missing associated documentation comment in .proto file."""
         context.set_code(grpc.StatusCode.UNIMPLEMENTED)
-        context.set_details("Method not implemented!")
-        raise NotImplementedError("Method not implemented!")
+        context.set_details('Method not implemented!')
+        raise NotImplementedError('Method not implemented!')
 
     def SetupTaskMailboxes(self, request, context):
         """Missing associated documentation comment in .proto file."""
         context.set_code(grpc.StatusCode.UNIMPLEMENTED)
-        context.set_details("Method not implemented!")
-        raise NotImplementedError("Method not implemented!")
+        context.set_details('Method not implemented!')
+        raise NotImplementedError('Method not implemented!')
+
+    def StartTraining(self, request, context):
+        """Missing associated documentation comment in .proto file."""
+        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
+        context.set_details('Method not implemented!')
+        raise NotImplementedError('Method not implemented!')
 
     def AcknowledgeMessage(self, request, context):
         """Missing associated documentation comment in .proto file."""
         context.set_code(grpc.StatusCode.UNIMPLEMENTED)
-        context.set_details("Method not implemented!")
-        raise NotImplementedError("Method not implemented!")
+        context.set_details('Method not implemented!')
+        raise NotImplementedError('Method not implemented!')
 
     def GetBitfountMessage(self, request, context):
         """Missing associated documentation comment in .proto file."""
         context.set_code(grpc.StatusCode.UNIMPLEMENTED)
-        context.set_details("Method not implemented!")
-        raise NotImplementedError("Method not implemented!")
+        context.set_details('Method not implemented!')
+        raise NotImplementedError('Method not implemented!')
 
     def SendBitfountMessage(self, request, context):
         """Missing associated documentation comment in .proto file."""
         context.set_code(grpc.StatusCode.UNIMPLEMENTED)
-        context.set_details("Method not implemented!")
-        raise NotImplementedError("Method not implemented!")
+        context.set_details('Method not implemented!')
+        raise NotImplementedError('Method not implemented!')
 
     def GetLargeObjectStorage(self, request, context):
         """Missing associated documentation comment in .proto file."""
         context.set_code(grpc.StatusCode.UNIMPLEMENTED)
-        context.set_details("Method not implemented!")
-        raise NotImplementedError("Method not implemented!")
-
-    def SetupTask(self, request, context):
-        """These are part of the newer unified flow where all task requests go via S3."""
-        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
-        context.set_details("Method not implemented!")
-        raise NotImplementedError("Method not implemented!")
-
-    def InitiateTask(self, request, context):
-        """Missing associated documentation comment in .proto file."""
-        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
-        context.set_details("Method not implemented!")
-        raise NotImplementedError("Method not implemented!")
+        context.set_details('Method not implemented!')
+        raise NotImplementedError('Method not implemented!')
 
 
 def add_MessageServiceServicer_to_server(servicer, server):
     rpc_method_handlers = {
-        "PodConnect": grpc.unary_unary_rpc_method_handler(
-            servicer.PodConnect,
-            request_deserializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.PodData.FromString,
-            response_serializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.SuccessResponse.SerializeToString,
-        ),
-        "SetupTaskMailboxes": grpc.unary_unary_rpc_method_handler(
-            servicer.SetupTaskMailboxes,
-            request_deserializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.BitfountTasks.FromString,
-            response_serializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.CommunicationDetails.SerializeToString,
-        ),
-        "AcknowledgeMessage": grpc.unary_unary_rpc_method_handler(
-            servicer.AcknowledgeMessage,
-            request_deserializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.Acknowledgement.FromString,
-            response_serializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.SuccessResponse.SerializeToString,
-        ),
-        "GetBitfountMessage": grpc.unary_unary_rpc_method_handler(
-            servicer.GetBitfountMessage,
-            request_deserializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.CommunicationDetails.FromString,
-            response_serializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.BitfountMessage.SerializeToString,
-        ),
-        "SendBitfountMessage": grpc.unary_unary_rpc_method_handler(
-            servicer.SendBitfountMessage,
-            request_deserializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.BitfountMessage.FromString,
-            response_serializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.SuccessResponse.SerializeToString,
-        ),
-        "GetLargeObjectStorage": grpc.unary_unary_rpc_method_handler(
-            servicer.GetLargeObjectStorage,
-            request_deserializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.LargeStorageRequest.FromString,
-            response_serializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.BlobStorageData.SerializeToString,
-        ),
-        "SetupTask": grpc.unary_unary_rpc_method_handler(
-            servicer.SetupTask,
-            request_deserializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.TaskTransferRequests.FromString,
-            response_serializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.TaskTransferMetadata.SerializeToString,
-        ),
-        "InitiateTask": grpc.unary_unary_rpc_method_handler(
-            servicer.InitiateTask,
-            request_deserializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.BitfountTasks.FromString,
-            response_serializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.CommunicationDetails.SerializeToString,
-        ),
+            'PodConnect': grpc.unary_unary_rpc_method_handler(
+                    servicer.PodConnect,
+                    request_deserializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.PodData.FromString,
+                    response_serializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.SuccessResponse.SerializeToString,
+            ),
+            'SetupTaskMailboxes': grpc.unary_unary_rpc_method_handler(
+                    servicer.SetupTaskMailboxes,
+                    request_deserializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.BitfountTasks.FromString,
+                    response_serializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.CommunicationDetails.SerializeToString,
+            ),
+            'StartTraining': grpc.unary_unary_rpc_method_handler(
+                    servicer.StartTraining,
+                    request_deserializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.BitfountTasks.FromString,
+                    response_serializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.CommunicationDetails.SerializeToString,
+            ),
+            'AcknowledgeMessage': grpc.unary_unary_rpc_method_handler(
+                    servicer.AcknowledgeMessage,
+                    request_deserializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.Acknowledgement.FromString,
+                    response_serializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.SuccessResponse.SerializeToString,
+            ),
+            'GetBitfountMessage': grpc.unary_unary_rpc_method_handler(
+                    servicer.GetBitfountMessage,
+                    request_deserializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.CommunicationDetails.FromString,
+                    response_serializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.BitfountMessage.SerializeToString,
+            ),
+            'SendBitfountMessage': grpc.unary_unary_rpc_method_handler(
+                    servicer.SendBitfountMessage,
+                    request_deserializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.BitfountMessage.FromString,
+                    response_serializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.SuccessResponse.SerializeToString,
+            ),
+            'GetLargeObjectStorage': grpc.unary_unary_rpc_method_handler(
+                    servicer.GetLargeObjectStorage,
+                    request_deserializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.LargeStorageRequest.FromString,
+                    response_serializer=bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.BlobStorageData.SerializeToString,
+            ),
     }
     generic_handler = grpc.method_handlers_generic_handler(
-        "messages.MessageService", rpc_method_handlers
-    )
+            'messages.MessageService', rpc_method_handlers)
     server.add_generic_rpc_handlers((generic_handler,))
 
 
-# This class is part of an EXPERIMENTAL API.
+ # This class is part of an EXPERIMENTAL API.
 class MessageService(object):
     """Missing associated documentation comment in .proto file."""
 
     @staticmethod
-    def PodConnect(
-        request,
-        target,
-        options=(),
-        channel_credentials=None,
-        call_credentials=None,
-        insecure=False,
-        compression=None,
-        wait_for_ready=None,
-        timeout=None,
-        metadata=None,
-    ):
-        return grpc.experimental.unary_unary(
-            request,
+    def PodConnect(request,
             target,
-            "/messages.MessageService/PodConnect",
+            options=(),
+            channel_credentials=None,
+            call_credentials=None,
+            insecure=False,
+            compression=None,
+            wait_for_ready=None,
+            timeout=None,
+            metadata=None):
+        return grpc.experimental.unary_unary(request, target, '/messages.MessageService/PodConnect',
             bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.PodData.SerializeToString,
             bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.SuccessResponse.FromString,
-            options,
-            channel_credentials,
-            insecure,
-            call_credentials,
-            compression,
-            wait_for_ready,
-            timeout,
-            metadata,
-        )
+            options, channel_credentials,
+            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)
 
     @staticmethod
-    def SetupTaskMailboxes(
-        request,
-        target,
-        options=(),
-        channel_credentials=None,
-        call_credentials=None,
-        insecure=False,
-        compression=None,
-        wait_for_ready=None,
-        timeout=None,
-        metadata=None,
-    ):
-        return grpc.experimental.unary_unary(
-            request,
+    def SetupTaskMailboxes(request,
             target,
-            "/messages.MessageService/SetupTaskMailboxes",
+            options=(),
+            channel_credentials=None,
+            call_credentials=None,
+            insecure=False,
+            compression=None,
+            wait_for_ready=None,
+            timeout=None,
+            metadata=None):
+        return grpc.experimental.unary_unary(request, target, '/messages.MessageService/SetupTaskMailboxes',
             bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.BitfountTasks.SerializeToString,
             bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.CommunicationDetails.FromString,
-            options,
-            channel_credentials,
-            insecure,
-            call_credentials,
-            compression,
-            wait_for_ready,
-            timeout,
-            metadata,
-        )
+            options, channel_credentials,
+            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)
 
     @staticmethod
-    def AcknowledgeMessage(
-        request,
-        target,
-        options=(),
-        channel_credentials=None,
-        call_credentials=None,
-        insecure=False,
-        compression=None,
-        wait_for_ready=None,
-        timeout=None,
-        metadata=None,
-    ):
-        return grpc.experimental.unary_unary(
-            request,
+    def StartTraining(request,
             target,
-            "/messages.MessageService/AcknowledgeMessage",
+            options=(),
+            channel_credentials=None,
+            call_credentials=None,
+            insecure=False,
+            compression=None,
+            wait_for_ready=None,
+            timeout=None,
+            metadata=None):
+        return grpc.experimental.unary_unary(request, target, '/messages.MessageService/StartTraining',
+            bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.BitfountTasks.SerializeToString,
+            bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.CommunicationDetails.FromString,
+            options, channel_credentials,
+            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)
+
+    @staticmethod
+    def AcknowledgeMessage(request,
+            target,
+            options=(),
+            channel_credentials=None,
+            call_credentials=None,
+            insecure=False,
+            compression=None,
+            wait_for_ready=None,
+            timeout=None,
+            metadata=None):
+        return grpc.experimental.unary_unary(request, target, '/messages.MessageService/AcknowledgeMessage',
             bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.Acknowledgement.SerializeToString,
             bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.SuccessResponse.FromString,
-            options,
-            channel_credentials,
-            insecure,
-            call_credentials,
-            compression,
-            wait_for_ready,
-            timeout,
-            metadata,
-        )
+            options, channel_credentials,
+            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)
 
     @staticmethod
-    def GetBitfountMessage(
-        request,
-        target,
-        options=(),
-        channel_credentials=None,
-        call_credentials=None,
-        insecure=False,
-        compression=None,
-        wait_for_ready=None,
-        timeout=None,
-        metadata=None,
-    ):
-        return grpc.experimental.unary_unary(
-            request,
+    def GetBitfountMessage(request,
             target,
-            "/messages.MessageService/GetBitfountMessage",
+            options=(),
+            channel_credentials=None,
+            call_credentials=None,
+            insecure=False,
+            compression=None,
+            wait_for_ready=None,
+            timeout=None,
+            metadata=None):
+        return grpc.experimental.unary_unary(request, target, '/messages.MessageService/GetBitfountMessage',
             bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.CommunicationDetails.SerializeToString,
             bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.BitfountMessage.FromString,
-            options,
-            channel_credentials,
-            insecure,
-            call_credentials,
-            compression,
-            wait_for_ready,
-            timeout,
-            metadata,
-        )
+            options, channel_credentials,
+            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)
 
     @staticmethod
-    def SendBitfountMessage(
-        request,
-        target,
-        options=(),
-        channel_credentials=None,
-        call_credentials=None,
-        insecure=False,
-        compression=None,
-        wait_for_ready=None,
-        timeout=None,
-        metadata=None,
-    ):
-        return grpc.experimental.unary_unary(
-            request,
+    def SendBitfountMessage(request,
             target,
-            "/messages.MessageService/SendBitfountMessage",
+            options=(),
+            channel_credentials=None,
+            call_credentials=None,
+            insecure=False,
+            compression=None,
+            wait_for_ready=None,
+            timeout=None,
+            metadata=None):
+        return grpc.experimental.unary_unary(request, target, '/messages.MessageService/SendBitfountMessage',
             bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.BitfountMessage.SerializeToString,
             bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.SuccessResponse.FromString,
-            options,
-            channel_credentials,
-            insecure,
-            call_credentials,
-            compression,
-            wait_for_ready,
-            timeout,
-            metadata,
-        )
+            options, channel_credentials,
+            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)
 
     @staticmethod
-    def GetLargeObjectStorage(
-        request,
-        target,
-        options=(),
-        channel_credentials=None,
-        call_credentials=None,
-        insecure=False,
-        compression=None,
-        wait_for_ready=None,
-        timeout=None,
-        metadata=None,
-    ):
-        return grpc.experimental.unary_unary(
-            request,
+    def GetLargeObjectStorage(request,
             target,
-            "/messages.MessageService/GetLargeObjectStorage",
+            options=(),
+            channel_credentials=None,
+            call_credentials=None,
+            insecure=False,
+            compression=None,
+            wait_for_ready=None,
+            timeout=None,
+            metadata=None):
+        return grpc.experimental.unary_unary(request, target, '/messages.MessageService/GetLargeObjectStorage',
             bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.LargeStorageRequest.SerializeToString,
             bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.BlobStorageData.FromString,
-            options,
-            channel_credentials,
-            insecure,
-            call_credentials,
-            compression,
-            wait_for_ready,
-            timeout,
-            metadata,
-        )
-
-    @staticmethod
-    def SetupTask(
-        request,
-        target,
-        options=(),
-        channel_credentials=None,
-        call_credentials=None,
-        insecure=False,
-        compression=None,
-        wait_for_ready=None,
-        timeout=None,
-        metadata=None,
-    ):
-        return grpc.experimental.unary_unary(
-            request,
-            target,
-            "/messages.MessageService/SetupTask",
-            bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.TaskTransferRequests.SerializeToString,
-            bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.TaskTransferMetadata.FromString,
-            options,
-            channel_credentials,
-            insecure,
-            call_credentials,
-            compression,
-            wait_for_ready,
-            timeout,
-            metadata,
-        )
-
-    @staticmethod
-    def InitiateTask(
-        request,
-        target,
-        options=(),
-        channel_credentials=None,
-        call_credentials=None,
-        insecure=False,
-        compression=None,
-        wait_for_ready=None,
-        timeout=None,
-        metadata=None,
-    ):
-        return grpc.experimental.unary_unary(
-            request,
-            target,
-            "/messages.MessageService/InitiateTask",
-            bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.BitfountTasks.SerializeToString,
-            bitfount_dot_federated_dot_transport_dot_protos_dot_messages__pb2.CommunicationDetails.FromString,
-            options,
-            channel_credentials,
-            insecure,
-            call_credentials,
-            compression,
-            wait_for_ready,
-            timeout,
-            metadata,
-        )
+            options, channel_credentials,
+            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)
```

### Comparing `bitfount-0.5.86/bitfount/federated/transport/types.py` & `bitfount-0.5.9/bitfount/federated/transport/types.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,31 +1,29 @@
 """Types related to transport layer sending and receiving."""
 from __future__ import annotations
 
 import dataclasses
 from dataclasses import dataclass
 from datetime import datetime
-from typing import Dict, NamedTuple, Type, TypeVar
+from typing import NamedTuple, Type, TypeVar
 
 from bitfount.types import _JSONDict
 
 _T = TypeVar("_T")
 
 
 @dataclass
 class _SerializationMixin:
     """Mixin to provide (de)serialization functionality."""
 
     def serialize(self) -> _JSONDict:
-        """Serialize the dataclass to a JSON-compatible dictionary."""
         return dataclasses.asdict(self)
 
     @classmethod
     def deserialize(cls: Type[_T], data: _JSONDict) -> _T:
-        """Deserialize the dataclass from a JSON-compatible dictionary."""
         return cls(**data)
 
 
 @dataclass
 class _OIDCClientID(_SerializationMixin):
     """Form of Client ID response from pods to initial request."""
 
@@ -61,37 +59,27 @@
     """The pod-related device code response details."""
 
     device_code: str
     expires_at: datetime
     interval: int
 
     def serialize(self) -> _JSONDict:
-        """Serialize the dataclass to a JSON-compatible dictionary."""
         # Create basic serialization dictionary
         d = super().serialize()
         # Replace datetime with ISO format string
         d["expires_at"] = d["expires_at"].isoformat()
         return d
 
     @classmethod
     def deserialize(cls, data: _JSONDict) -> _PodDeviceCodeDetails:
-        """Deserialize the dataclass from a JSON-compatible dictionary."""
         # Reconstruct datetime object
         data["expires_at"] = datetime.fromisoformat(data["expires_at"])
         return cls(**data)
 
 
 @dataclass
 class _ModellerDeviceCodeDetails:
     """The modeller-related device code response details."""
 
     user_code: str
     verification_uri: str
     verification_uri_complete: str
-
-
-class CommunicationDetails(NamedTuple):
-    """Container for received communication details."""
-
-    mailbox_id: str
-    pod_mailbox_ids: Dict[str, str]
-    task_id: str
```

### Comparing `bitfount-0.5.86/bitfount/federated/transport/worker_transport.py` & `bitfount-0.5.9/bitfount/federated/transport/worker_transport.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,149 +1,90 @@
 """Handles messages for a Pods and Modellers at the protocol level during a task."""
-import asyncio
 import logging
-from typing import (
-    Any,
-    Callable,
-    Dict,
-    Final,
-    Iterable,
-    List,
-    Mapping,
-    Optional,
-    Tuple,
-    Union,
-)
-import warnings
+from typing import Any, Callable, Dict, Final, Iterable, List, Mapping, Optional
 
-from cryptography.hazmat.primitives.asymmetric.rsa import RSAPrivateKey, RSAPublicKey
 import msgpack
 
-from bitfount import config
-from bitfount.federated.encryption import _RSAEncryption
 from bitfount.federated.logging import _federate_logger, _get_federated_logger
 from bitfount.federated.transport.base_transport import (
     Handler,
-    SyncHandler,
     _BaseMailbox,
+    _decrypt_aes_message,
     _send_aes_encrypted_message,
 )
 from bitfount.federated.transport.handlers import (
     _AsyncMultipleResponsesHandler,
     _get_message_awaitable,
-    _OnlineResponseHandling,
 )
 from bitfount.federated.transport.message_service import (
     _BitfountMessage,
     _BitfountMessageType,
-    _DecryptedBitfountMessage,
     _MessageService,
 )
 from bitfount.federated.transport.types import (
     _OIDCAuthFlowResponse,
     _OIDCClientID,
     _PodDeviceCodeDetails,
 )
 from bitfount.federated.types import _PodResponseType
 from bitfount.types import _JSONDict, _SAMLResponse, _SerializedWeights
 
 logger = _get_federated_logger(__name__)
 
-# How long to wait for Modeller authentication response
-_DEFAULT_AUTHENTICATION_MODELLER_RESPONSE_TIMEOUT: Final[int] = 5 * 60  # 5 minutes
-# How long in seconds to wait for a response from the modeller before checking if
-# the modeller is still online.
-_SOFT_LIMIT_MESSAGE_TIMEOUT: Final[int] = config.BITFOUNT_ONLINE_CHECK_SOFT_LIMIT
-# How long in seconds to wait for a response from the modeller regarding their
-# online status before aborting the task. The hard limit can only be reached after
-# the soft limit has already been reached.
-_HARD_LIMIT_MESSAGE_TIMEOUT: Final[int] = config.BITFOUNT_ONLINE_CHECK_HARD_LIMIT
+_DEFAULT_OIDC_MODELLER_RESPONSE_TIMEOUT: Final[int] = 5 * 60
 
 
 class _WorkerMailbox(_BaseMailbox):
     """Used by a pod for handling messages during a task."""
 
     def __init__(
         self,
         pod_identifier: str,
         modeller_mailbox_id: str,
         modeller_name: str,
         aes_encryption_key: bytes,
         message_service: _MessageService,
         pod_mailbox_ids: Mapping[str, str],
-        task_id: Optional[str] = None,
-        handlers: Optional[
-            Mapping[_BitfountMessageType, Union[Handler, Iterable[Handler]]]
-        ] = None,
+        handlers: Optional[Mapping[_BitfountMessageType, Handler]] = None,
     ):
         """Create new worker mailbox for a specific task.
 
         Args:
             pod_identifier: identifier for the pod that contains this worker.
             modeller_mailbox_id: mailbox id for modeller involved in the task.
             modeller_name: name of the modeller involved in the task.
             aes_encryption_key: encryption key for task messages.
             message_service: the underlying message service.
             pod_mailbox_ids: mapping of pod_identifier to worker mailbox IDs for
                              all pods involved in the task.
-            task_id: The ID of the task that this mailbox is associated with.
             handlers: an optional mapping of message types to handlers to initialise
                       with.
         """
         # Our own mailbox ID is stored in the pods mailbox dict.
         super().__init__(
             mailbox_id=pod_mailbox_ids[pod_identifier],
             message_service=message_service,
             handlers=handlers,
         )
         self.pod_identifier = pod_identifier
         self.modeller_mailbox_id = modeller_mailbox_id
         self.modeller_name = modeller_name
         self.aes_encryption_key = aes_encryption_key
         self.pod_mailbox_ids: Dict[str, str] = dict(pod_mailbox_ids)
-        self._task_id = task_id
-        if self._task_id is None:
-            warnings.warn(
-                "Worker mailboxes should be instantiated with task IDs;"
-                " ask the Modeller to upgrade their bitfount version if possible.",
-                FutureWarning,
-            )
-            logger.warning(
-                "Worker mailboxes should be instantiated with task IDs;"
-                " ask the Modeller to upgrade their bitfount version if possible."
-            )
-
-        # Create modeller online checker and set it to monitor all messages and
-        # specifically ONLINE_RESPONSE messages
-        self._online_response_handler = _OnlineResponseHandling(
-            self.modeller_name, self.aes_encryption_key
-        )
-        self.register_universal_handler(
-            self._online_response_handler.response_handler, high_priority=True
-        )
-        self.register_handler(
-            _BitfountMessageType.ONLINE_RESPONSE,
-            self._online_response_handler.response_handler,
-            high_priority=True,
-        )
 
         # Gather list of other pods. Iteration order is important here (to ensure
         # consistency between share generation order), so we use List rather than
         # another collection.
         self.other_pods: List[str] = [
             pod_identifier
-            for pod_identifier in pod_mailbox_ids
+            for pod_identifier in pod_mailbox_ids.keys()
             if pod_identifier != self.pod_identifier
         ]
-        self._setup_federated_logging()
 
-    @property
-    def task_id(self) -> Optional[str]:
-        """The task ID of the task associated with this mailbox."""
-        return self._task_id
+        self._setup_federated_logging()
 
     async def accept_task(self) -> None:
         """Sends an acceptance message to the modeller."""
         await self._send_aes_encrypted_message(
             {_PodResponseType.ACCEPT.name: self.pod_identifier},
             _BitfountMessageType.JOB_ACCEPT,
         )
@@ -154,15 +95,15 @@
     ) -> None:
         """Send a rejection of a training request to a modeller.
 
         Args:
             error_messages: Error messages to send
 
         Returns:
-            True if message sent successfully, else False
+            True if message sent successfully, else false
 
         """
         logger.info(f"Rejecting task from {self.modeller_name}")
         logger.debug(
             f"Rejected task from {self.modeller_name} "
             f"at mailbox: {self.modeller_mailbox_id}"
         )
@@ -176,17 +117,15 @@
     ) -> None:
         """Send SAML challenge to a modeller."""
         logger.info(f"Issuing SAML Challenge to: '{self.modeller_name}'.")
         await self._send_aes_encrypted_message(
             saml_request, _BitfountMessageType.SAML_REQUEST
         )
 
-    async def get_saml_response(
-        self, timeout: Optional[int] = _DEFAULT_AUTHENTICATION_MODELLER_RESPONSE_TIMEOUT
-    ) -> _SAMLResponse:
+    async def get_saml_response(self, timeout: Optional[int] = None) -> _SAMLResponse:
         """Awaits for the SAML Response from the modeller."""
         saml_response: _SAMLResponse = await self._get_message_and_decrypt(
             _BitfountMessageType.SAML_RESPONSE, timeout
         )
         return saml_response
 
     async def send_oidc_client_id(self, client_id: str) -> None:
@@ -194,84 +133,115 @@
         logger.info("Sending client ID to modeller for OIDC authentication.")
         await self._send_aes_encrypted_message(
             _OIDCClientID(client_id).serialize(),
             _BitfountMessageType.OIDC_CHALLENGE,
         )
 
     async def get_oidc_auth_flow_response(
-        self, timeout: Optional[int] = _DEFAULT_AUTHENTICATION_MODELLER_RESPONSE_TIMEOUT
+        self, timeout: Optional[int] = _DEFAULT_OIDC_MODELLER_RESPONSE_TIMEOUT
     ) -> _OIDCAuthFlowResponse:
         """Get OIDC Auth Code Flow response from the modeller.
 
         Response will contain an authorization code, code verifier, and redirect URI.
 
         Args:
-            timeout: How long to wait for response, in seconds. If `None`, will wait
-                indefinitely. Defaults to 5 minutes.
+            timeout:
+                How long to wait for response, in seconds. If `None`, will wait
+                indefinitely.
 
         Returns:
             Tuple of authorization code, code verifier, and redirect URI.
         """
         logger.info(f"Awaiting OIDC response from '{self.modeller_name}'")
         decrypted: _JSONDict = await self._get_message_and_decrypt(
             _BitfountMessageType.OIDC_AFC_PKCE_RESPONSE, timeout
         )
 
         # Check contents
         try:
             return _OIDCAuthFlowResponse.deserialize(decrypted)
-        except (KeyError, TypeError) as e:
+        except (KeyError, TypeError):
             try:
                 # Assume it's a dict but that we just couldn't deserialize it
                 raise KeyError(
                     f"Expected auth_code, code_verifier, and redirect_uri to be in "
                     f"OIDC response; got {decrypted.keys()}"
-                ) from e
-            except AttributeError as ae:
+                )
+            except AttributeError:
                 # If not, raise a TypeError
                 raise TypeError(
                     f"Unable to access OIDC response contents; expected dict, "
                     f"got {type(decrypted)}"
-                ) from ae
+                )
 
     async def get_oidc_device_code_response(
-        self, timeout: Optional[int] = _DEFAULT_AUTHENTICATION_MODELLER_RESPONSE_TIMEOUT
+        self, timeout: Optional[int] = _DEFAULT_OIDC_MODELLER_RESPONSE_TIMEOUT
     ) -> _PodDeviceCodeDetails:
         """Get OIDC Device Code Flow response from the modeller.
 
         Response will contain a device code, and polling details.
 
         Args:
-            timeout: How long to wait for response, in seconds. If `None`, will wait
-                indefinitely. Defaults to 5 minutes.
+            timeout:
+                How long to wait for response, in seconds. If `None`, will wait
+                indefinitely.
 
         Returns:
             Tuple device code, when the code expires, and the polling interval to use.
         """
         logger.info(f"Awaiting OIDC device response from '{self.modeller_name}'")
-        decrypted: _JSONDict = await self._get_message_and_decrypt(
+        decrypted: Dict[str, Any] = await self._get_message_and_decrypt(
             _BitfountMessageType.OIDC_DEVICE_CODE_RESPONSE, timeout
         )
 
         # Check contents and decode as needed
         try:
             return _PodDeviceCodeDetails.deserialize(decrypted)
-        except (KeyError, TypeError) as e:
+        except (KeyError, TypeError):
             try:
                 # Assume it's a dict but that we just couldn't deserialize it
                 raise KeyError(
                     f"Expected device_code, expires_at, and interval to be in "
                     f"OIDC response; got {decrypted.keys()}"
-                ) from e
-            except AttributeError as ae:
+                )
+            except AttributeError:
                 # If not, raise a TypeError
                 raise TypeError(
                     f"Unable to access OIDC response contents; expected dict, "
                     f"got {type(decrypted)}"
-                ) from ae
+                )
+
+    async def _send_pod_to_pod_message(
+        self,
+        recipient: str,
+        recipient_mailbox_id: str,
+        object_to_send: Any,
+        message_type: _BitfountMessageType,
+    ) -> None:
+        """Send message to other pod/worker, unencrypted.
+
+        Args:
+            recipient: The name of the worker/pod.
+            recipient_mailbox_id: The mailbox to send message to.
+            object_to_send: Body of the message (not encrypted).
+            message_type: The type of the message to send.
+        """
+        # TODO: [BIT-961] This shouldn't exist and should be replaced with an RSA
+        #       pod-to-pod encryption method.
+        await self.message_service.send_message(
+            _BitfountMessage(
+                message_type=message_type,
+                body=msgpack.dumps(object_to_send),
+                recipient=recipient,
+                recipient_mailbox_id=recipient_mailbox_id,
+                sender=self.pod_identifier,
+                sender_mailbox_id=self.mailbox_id,
+            ),
+            already_packed=True,
+        )
 
     async def _send_aes_encrypted_message(
         self, object_to_send: Any, message_type: _BitfountMessageType
     ) -> None:
         """Send message to modeller, AES encrypted.
 
         Args:
@@ -283,227 +253,107 @@
             self.aes_encryption_key,
             self.message_service,
             message_type=message_type,
             recipient=self.modeller_name,
             recipient_mailbox_id=self.modeller_mailbox_id,
             sender=self.pod_identifier,
             sender_mailbox_id=self.mailbox_id,
-            task_id=self._task_id,
         )
 
     async def send_evaluation_results(self, eval_results: Mapping[str, float]) -> None:
         """Sends evaluation results to the modeller."""
-        logger.info("Sending results to modeller...")
         await self._send_aes_encrypted_message(
             eval_results, _BitfountMessageType.EVALUATION_RESULTS
         )
 
     async def _get_message(
-        self, message_type: _BitfountMessageType, timeout: Optional[int]
+        self, message_type: _BitfountMessageType, timeout: Optional[int] = None
     ) -> _BitfountMessage:
         """Generic handler for single message retrieval.
 
         Args:
             message_type: The type of message to wait on.
             timeout: How long to wait before timing out.
 
         Returns:
             The message.
-
-        Raises:
-            asyncio.TimeoutError: If the message is not received within the timeout.
         """
-        logger.debug(
-            f"Waiting for message ({message_type}) retrieval in"
-            f" {self.__class__.__name__}._get_message() from mailbox {self.mailbox_id}"
-        )
-
-        # Registers the handler for the expected message type
         async_awaitable = _get_message_awaitable()
         self.register_temp_handler(message_type, async_awaitable)
-
-        # Waits for the message until a `TimeoutError` occurs. If a message is received
-        # before the timeout, it is simply returned. Otherwise, a message is sent to the
-        # modeller to check that they are still online. If they are online, the timeout
-        # resets and we wait for the original message again. If they are not online,
-        # this will raise another `TimeoutError` waiting for the Modeller's response. On
-        # the second timeout, the task is cancelled by sending a `TASK_ABORT` message to
-        # the modeller and re-raising the original `TimeoutError`.
-        while True:
-            try:
-                message = await async_awaitable.result(timeout)
-                break
-            except asyncio.TimeoutError as te:
-                logger.debug(
-                    f"Soft limit timeout hit waiting for message {message_type.name}."
-                )
-
-                logger.info("Checking if Modeller is still online...")
-                online_check_uuid = self._online_response_handler.get_online_check_id()
-                await self.check_modeller_online(online_check_uuid)
-
-                # Want to wait for _either_ the original message to come through or
-                # for the modeller to indicate it is online.
-                message_result_task = asyncio.create_task(async_awaitable.result())
-                online_response_task = asyncio.create_task(
-                    self._online_response_handler.wait_for_response(online_check_uuid)
-                )
-                joint_wait = asyncio.wait(
-                    (message_result_task, online_response_task),
-                    return_when=asyncio.FIRST_COMPLETED,
-                )
-
-                try:
-                    done, pending = await asyncio.wait_for(
-                        joint_wait, timeout=_HARD_LIMIT_MESSAGE_TIMEOUT
-                    )
-                except asyncio.TimeoutError:
-                    logger.warning("Modeller is offline. Aborting task.")
-                    await self.send_task_abort_message()
-
-                    # Double-check these tasks are cancelled
-                    message_result_task.cancel()
-                    online_response_task.cancel()
-
-                    raise te  # want to raise ORIGINAL timeout error here
-                else:
-                    if message_result_task in done:
-                        # We have our message, cancel the response waiter and return
-                        logger.info(
-                            "Modeller is online, responded with expected message."
-                        )
-                        online_response_task.cancel()
-                        self._online_response_handler.remove_waiter(online_check_uuid)
-                        return message_result_task.result()
-                    else:  # online_response_task in done
-                        logger.info("Modeller is online, continuing to wait.")
-
-                        # One final check to see if it's there
-                        if not message_result_task.done():
-                            # Cancel message result task as we're going to start the
-                            # loop again and await on it there
-                            message_result_task.cancel()
-                        else:
-                            return message_result_task.result()
-
+        message = await async_awaitable.result(timeout)
         return message
 
     def _aes_decrypt(self, message: _BitfountMessage) -> Any:
         """Decrypt message from modeller.
 
         Args:
             message: Encrypted message to decrypt.
 
         Returns:
             Decrypted message body.
         """
-        return message.decrypt(self.aes_encryption_key).body
+        return _decrypt_aes_message(message, self.aes_encryption_key)
 
     async def _get_message_and_decrypt(
-        self,
-        message_type: _BitfountMessageType,
-        timeout: Optional[int] = _SOFT_LIMIT_MESSAGE_TIMEOUT,
+        self, message_type: _BitfountMessageType, timeout: Optional[int] = None
     ) -> Any:
         """Generic handler for single encrypted message retrieval.
 
         Args:
             message_type: The type of message to wait on.
             timeout: How long to wait before timing out.
 
         Returns:
             The decrypted message contents.
         """
         message = await self._get_message(message_type, timeout)
         return self._aes_decrypt(message)
 
+    async def get_task_details(self, timeout: Optional[int] = None) -> _JSONDict:
+        """Awaits serialized task details (protocol, etc.) from the modeller."""
+        task_details: _JSONDict = await self._get_message_and_decrypt(
+            _BitfountMessageType.ALGORITHM_EXCHANGE, timeout=timeout
+        )
+        return task_details
+
     async def get_training_iteration_complete_update(
-        self, timeout: Optional[int] = _SOFT_LIMIT_MESSAGE_TIMEOUT
+        self, timeout: Optional[int] = None
     ) -> bool:
         """Awaits for the next training complete update from the modeller."""
         training_complete: bool = await self._get_message_and_decrypt(
             _BitfountMessageType.TRAINING_COMPLETE, timeout=timeout
         )
         return training_complete
 
-    async def get_task_start_update(
-        self, timeout: Optional[int] = _SOFT_LIMIT_MESSAGE_TIMEOUT
-    ) -> None:
-        """Awaits for the task start message from the modeller."""
-        await self._get_message_and_decrypt(
-            _BitfountMessageType.TASK_START, timeout=timeout
-        )
-
-    async def get_task_complete_update(
-        self, timeout: Optional[int] = _SOFT_LIMIT_MESSAGE_TIMEOUT
-    ) -> None:
+    async def get_task_complete_update(self, timeout: Optional[int] = None) -> None:
         """Awaits for the task complete message from the modeller."""
-        logger.debug(
-            f"Awaiting TASK_COMPLETE message"
-            f" from {self.modeller_name}"
-            f" in mailbox {self.mailbox_id}"
-        )
         await self._get_message_and_decrypt(
             _BitfountMessageType.TASK_COMPLETE, timeout=timeout
         )
 
-    async def check_modeller_online(self, online_check_uuid: str) -> None:
-        """Send message to Modeller to check if the Modeller is online."""
-        await self._send_aes_encrypted_message(
-            online_check_uuid, _BitfountMessageType.ONLINE_CHECK
-        )
-
-    async def send_task_abort_message(self) -> None:
-        """Send task abort message to Modeller."""
-        await self._send_aes_encrypted_message(None, _BitfountMessageType.TASK_ABORT)
-
-    async def send_num_batches_message(self, num_batches: int) -> None:
-        """Send number of batches message to Modeller for batched execution."""
-        await self._send_aes_encrypted_message(
-            num_batches, _BitfountMessageType.NUMBER_OF_BATCHES
-        )
-
     async def log(self, message: Mapping[str, object]) -> None:
         """Log message to Modeller."""
         await self._send_aes_encrypted_message(
             message, _BitfountMessageType.LOG_MESSAGE
         )
 
     def _setup_federated_logging(self) -> None:
         """Set up federated logging."""
         _federate_logger(self)
-        self.register_handler(
-            _BitfountMessageType.LOG_MESSAGE, self._get_log_message_handler()
-        )
-
-    def _get_log_message_handler(self) -> SyncHandler:
-        """Create the appropriate handler for LOG_MESSAGE messages."""
 
         def log_message_handler(message: _BitfountMessage) -> None:
-            """Locally logs the log message that has been received from the modeller."""
-            log_message: _JSONDict = self._aes_decrypt(message)
-
+            log_message: Dict[str, Any] = self._aes_decrypt(message)
             # We prepend the log message to show it's come from the Modeller
-            log_message["msg"] = f"<FROM MODELLER>: {log_message['msg']}"
-
-            # Modify processName and threadName to indicate these are non-local
-            try:
-                log_message["processName"] = f"<{log_message['processName']}>"
-            except KeyError:
-                pass
-            try:
-                log_message["threadName"] = f"<{log_message['threadName']}>"
-            except KeyError:
-                pass
-
+            log_message["msg"] = f"Modeller: {log_message['msg']}"
             # We remove the `federated` key to avoid recursively sending a federated
             # log message on both the Modeller and Worker sides
             log_message.pop("federated")
             logger.handle(logging.makeLogRecord(log_message))
 
-        return log_message_handler
+        self.register_handler(_BitfountMessageType.LOG_MESSAGE, log_message_handler)
 
 
 async def _send_training_metrics(
     validation_metrics: Mapping[str, str],
     worker_mailbox: _WorkerMailbox,
 ) -> None:
     """Sends a model parameter update to the modeller.
@@ -528,56 +378,28 @@
     Args:
         parameter_update: The parameter update to send, already serialized.
         worker_mailbox: The worker mailbox to use to send the parameter update.
     """
     # Deliberate access to private method here as that method shouldn't be used in
     # any other context than transport layer access.
     # noinspection PyProtectedMember
-    logger.debug(f"Sending TRAINING_UPDATE from {worker_mailbox.mailbox_id}")
     await worker_mailbox._send_aes_encrypted_message(
         parameter_update, _BitfountMessageType.TRAINING_UPDATE
     )
 
 
-async def _send_public_key(
-    public_key: RSAPublicKey, worker_mailbox: _WorkerMailbox
-) -> None:
-    """Sends the public key to the modeller.
-
-    Used for the PSI protocol.
-
-    Args:
-        public_key: The public key to use in the PSI.
-        worker_mailbox: The worker mailbox to use to send the parameter update.
-    """
-    # Serialize the public key first
-    serialized_key = _RSAEncryption.serialize_public_key(public_key)
-    await worker_mailbox._send_aes_encrypted_message(
-        serialized_key, _BitfountMessageType.KEY_EXCHANGE
-    )
-
-
-async def _send_psi_dataset_worker(
-    dataset: Tuple[List[str], List[str]], worker_mailbox: _WorkerMailbox
-) -> None:
-    """Sends psi datasets to the modeller."""
-    await worker_mailbox._send_aes_encrypted_message(
-        dataset, _BitfountMessageType.PSI_DATASET
-    )
-
-
 async def _get_model_parameters(
-    worker_mailbox: _WorkerMailbox, timeout: Optional[int] = _SOFT_LIMIT_MESSAGE_TIMEOUT
+    worker_mailbox: _WorkerMailbox, timeout: Optional[int] = None
 ) -> _SerializedWeights:
     """Awaits for the next model parameter update from the modeller.
 
     Args:
         worker_mailbox: The worker mailbox that the message will be sent to.
         timeout: Optional. The time to wait in seconds for the next model
-            parameter update.
+                 parameter update.
 
     Returns:
         The received parameter update.
     """
     # Await messages for updates sent modeller->worker
     # Deliberate access to private method here as that method shouldn't be used in
     # any other context than transport layer access.
@@ -585,121 +407,17 @@
     weights: _SerializedWeights = await worker_mailbox._get_message_and_decrypt(
         _BitfountMessageType.MODEL_PARAMETERS, timeout=timeout
     )
 
     return weights
 
 
-async def _get_psi_dataset(
-    worker_mailbox: _WorkerMailbox, timeout: Optional[int] = _SOFT_LIMIT_MESSAGE_TIMEOUT
-) -> List[str]:
-    """Awaits for the psi dataset from the modeller.
-
-    Args:
-        worker_mailbox: The worker mailbox that the message will be sent to.
-        timeout: Optional. The time to wait in seconds for the next model
-            parameter update.
-
-    Returns:
-        The psi dataset.
-    """
-    psi_dataset: List[str] = await worker_mailbox._get_message_and_decrypt(
-        _BitfountMessageType.PSI_DATASET, timeout=timeout
-    )
-    return psi_dataset
-
-
-class _InterPodWorkerMailbox(_WorkerMailbox):
-    """A worker mailbox that also handles pod-to-pod communication.
-
-    In particular ensures that other pod RSA keys are stored and used.
-    """
-
-    def __init__(
-        self,
-        pod_public_keys: Mapping[str, RSAPublicKey],
-        private_key: RSAPrivateKey,
-        *args: Any,
-        **kwargs: Any,
-    ):
-        """Create new inter-pod worker mailbox for a specific task.
-
-        Args:
-            pod_public_keys: Mapping of pod_identifier to the pod's RSA public
-                             key for all pods involved in the task. This is for
-                             inter-pod communication.
-            private_key: The RSA private key for this pod.
-            *args: Positional arguments as per _WorkerMailbox.
-            **kwargs: Keyword arguments as per _WorkerMailbox.
-        """
-        super().__init__(*args, **kwargs)
-        self._pod_public_keys = pod_public_keys
-
-        # Check that we have public keys for all other pods
-        missing_keys = set(self.other_pods) - set(self._pod_public_keys.keys())
-        if missing_keys:
-            missing_keys_str = ", ".join(missing_keys)
-            raise ValueError(
-                f"We are missing public keys for the following pods: "
-                f"{missing_keys_str}. "
-                f"Unable to continue inter-pod communication."
-            )
-
-        self._private_key = private_key
-
-    async def _send_pod_to_pod_message(
-        self,
-        recipient: str,
-        recipient_mailbox_id: str,
-        object_to_send: Any,
-        message_type: _BitfountMessageType,
-    ) -> None:
-        """Send message to other pod/worker, unencrypted.
-
-        Args:
-            recipient: The identifier of the worker/pod.
-            recipient_mailbox_id: The mailbox to send message to.
-            object_to_send: Body of the message (not encrypted).
-            message_type: The type of the message to send.
-        """
-        try:
-            recipient_key = self._pod_public_keys[recipient]
-        except KeyError:
-            logging.error(
-                f"Unable to find public key for pod {recipient}. "
-                f"Unable to send pod-to-pod message."
-            )
-            return None
-
-        message_body: bytes = _RSAEncryption.encrypt(
-            msgpack.dumps(object_to_send), recipient_key
-        )
-        await self.message_service.send_message(
-            _BitfountMessage(
-                message_type=message_type,
-                body=message_body,
-                recipient=recipient,
-                recipient_mailbox_id=recipient_mailbox_id,
-                sender=self.pod_identifier,
-                sender_mailbox_id=self.mailbox_id,
-                task_id=self._task_id,
-            ),
-            already_packed=True,
-        )
-
-    def _pod_to_pod_message_handler(
-        self, message: _BitfountMessage
-    ) -> _DecryptedBitfountMessage:
-        """Handler for decrypting pod-to-pod messages."""
-        return message.decrypt_rsa(self._private_key)
-
-
 async def _send_secure_shares_to_others(
     secure_share_generator: Callable[[], int],
-    worker_mailbox: _InterPodWorkerMailbox,
+    worker_mailbox: _WorkerMailbox,
 ) -> None:
     """Sends result of `secure_share_generator` to all other pods in training task.
 
     Args:
         secure_share_generator: The function to be called which returns
                                 the secure share to be sent.
         worker_mailbox: The worker mailbox to send the shares with.
@@ -713,46 +431,43 @@
             f"{worker_mailbox_id}"
         )
 
         share = secure_share_generator()
         # Deliberate access to private method here as that method shouldn't be used in
         # any other context than transport layer access.
         # noinspection PyProtectedMember
+        # TODO: [BIT-961] These should be RSA encrypted.
         await worker_mailbox._send_pod_to_pod_message(
             recipient=worker_identifier,
             recipient_mailbox_id=worker_mailbox_id,
             object_to_send=share,
             message_type=_BitfountMessageType.SECURE_SHARE,
         )
 
 
 async def _get_worker_secure_shares(
-    worker_mailbox: _InterPodWorkerMailbox, timeout: Optional[int] = None
+    worker_mailbox: _WorkerMailbox, timeout: Optional[int] = None
 ) -> List[int]:
     """Awaits the set of secure shares from the other workers.
 
     Args:
         worker_mailbox: The worker mailbox the shares will be received at.
         timeout: Optional. The number of seconds to wait for secure shares to arrive.
 
     Returns:
-        The list of received shares. Note that there is no notion of "worker order"
-        in this list, the shares will be in the order they were received.
+        The list of received shares.
     """
     shares: List[int] = []
 
     # Create light-weight handler to append to shared list.
     # Note that the secure shares are NOT received as encrypted messages.
     def worker_secure_share_handler(message: _BitfountMessage) -> None:
         logger.debug(f"Receiving secure share from worker {message.sender}")
-        # Deliberate access to private method here as that method shouldn't be used in
-        # any other context than transport layer access.
-        # noinspection PyProtectedMember
-        decrypted_message = worker_mailbox._pod_to_pod_message_handler(message)
-        share: int = decrypted_message.body
+        # TODO: [BIT-961] These should be RSA encrypted.
+        share: int = msgpack.loads(message.body)
         shares.append(share)
 
     # Await on all the other workers to send their shares, which will be
     # appended to the list above.
     with _AsyncMultipleResponsesHandler(
         handler=worker_secure_share_handler,
         message_types=_BitfountMessageType.SECURE_SHARE,
```

### Comparing `bitfount-0.5.86/bitfount/federated/utils.py` & `bitfount-0.5.9/bitfount/federated/utils.py`

 * *Files 14% similar despite different names*

```diff
@@ -6,44 +6,44 @@
 """
 from __future__ import annotations
 
 import importlib
 import inspect
 import pkgutil
 from types import ModuleType
-from typing import Dict, Mapping, Type, Union, cast
+from typing import Dict, List, Mapping, Type, Union, cast
 
 import bitfount.federated.aggregators.base as aggregators
 import bitfount.federated.algorithms.base as algorithms
 from bitfount.federated.logging import _get_federated_logger
 from bitfount.federated.mixins import _DistributedModelMixIn
 import bitfount.federated.protocols.base as protocols
+import bitfount.models.base_models as model_structures
 from bitfount.models.base_models import (
     NeuralNetworkModelStructure,
     NeuralNetworkPredefinedModel,
     _BaseModel,
 )
-import bitfount.models.base_models as model_structures
 import bitfount.models.models as core_models
 from bitfount.types import DistributedModelProtocol
 
 logger = _get_federated_logger(__name__)
 
 # This is a read-only dictionary mapping the name of an aggregator to the class itself
 _AGGREGATORS: Mapping[
     str, Type[aggregators._BaseAggregatorFactory]
 ] = aggregators.registry
 
 
 # This is a read-only dictionary mapping the name of an algorithm to the class itself
-_ALGORITHMS: Mapping[str, Type[algorithms.BaseAlgorithmFactory]] = algorithms.registry
+_ALGORITHMS: Mapping[str, Type[algorithms._BaseAlgorithmFactory]] = algorithms.registry
 
 
 # This is a read-only dictionary mapping the name of a protocol to the class itself
-_PROTOCOLS: Mapping[str, Type[protocols.BaseProtocolFactory]] = protocols.registry
+_PROTOCOLS: Mapping[str, Type[protocols._BaseProtocolFactory]] = protocols.registry
 
 
 _MODEL_STRUCTURES: Dict[
     str, Type[Union[NeuralNetworkPredefinedModel, NeuralNetworkModelStructure]]
 ] = {
     name: class_
     for name, class_ in vars(model_structures).items()
@@ -121,7 +121,25 @@
 
 # We take a subset of MODELS that additionally subclass DistributedModelMixIn
 for model_name, model_class in _MODELS.items():
     if issubclass(model_class, _DistributedModelMixIn):
         _DISTRIBUTED_MODELS[model_name] = cast(
             Type[DistributedModelProtocol], model_class
         )
+
+
+def _validate_pod_identifiers(pod_identifiers: List[str]) -> None:
+    """Validate pod identifier format.
+
+    Ensure they follow the format "namespace/podname",
+    where namespace is currently username.
+    """
+    separator = "/"
+    for identifier in pod_identifiers:
+        if (
+            identifier.count(separator) != 1
+            or identifier[0] == separator
+            or identifier[-1] == separator
+        ):
+            raise ValueError(
+                'Pod identifiers must follow the format: "<pod_namespace>/<pod_name>"'
+            )
```

### Comparing `bitfount-0.5.86/bitfount/hub/api.py` & `bitfount-0.5.9/bitfount/hub/api.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,81 +1,47 @@
 """Typed wrappers around the Bitfount REST apis."""
 from __future__ import annotations
 
 import base64
+import dataclasses
 from dataclasses import dataclass
+import json
 import logging
 from pathlib import Path
-import time
-from typing import (
-    Any,
-    Callable,
-    Final,
-    List,
-    Mapping,
-    Optional,
-    Tuple,
-    Type,
-    TypeVar,
-    Union,
-    cast,
-)
+from typing import TYPE_CHECKING, Any, Final, List, Mapping, Optional, Tuple, Type, cast
 import warnings
 
 from cryptography.exceptions import InvalidSignature
 from cryptography.hazmat.primitives import hashes
 from cryptography.hazmat.primitives.asymmetric import padding
 from cryptography.hazmat.primitives.asymmetric.rsa import RSAPublicKey
+from cryptography.hazmat.primitives.serialization import Encoding, PublicFormat
 import requests
 from requests import HTTPError, RequestException, Response
 from requests.exceptions import InvalidJSONError
-from typing_extensions import ParamSpec
 
-from bitfount.config import (
-    _DEVELOPMENT_ENVIRONMENT,
-    _PRODUCTION_ENVIRONMENT,
-    _SANDBOX_ENVIRONMENT,
-    _STAGING_ENVIRONMENT,
-    _get_environment,
-)
 from bitfount.data.schema import BitfountSchema
 from bitfount.federated.encryption import _RSAEncryption
-from bitfount.federated.types import SerializedProtocol, _PodResponseType
+from bitfount.federated.types import _PodResponseType
 from bitfount.hub.authentication_flow import BitfountSession
-from bitfount.hub.exceptions import (
-    ModelTooLargeError,
-    ModelUploadError,
-    ModelValidationError,
-    SchemaUploadError,
-)
 from bitfount.hub.types import (
-    _DEV_AM_URL,
-    _SANDBOX_AM_URL,
-    _STAGING_AM_URL,
-    PRODUCTION_AM_URL,
-    PRODUCTION_HUB_URL,
+    _AccessJSON,
     _AccessManagerKeyResponseJSON,
-    _ActivePublicKey,
+    _AccessRequestResponseJSON,
     _AMAccessCheckResponseJSON,
-    _CreatedResourceResponseJSON,
     _ModelDetailsResponseJSON,
     _ModelUploadResponseJSON,
-    _MonitorPostJSON,
     _MultiModelDetailsResponseJSON,
     _MultiPodDetailsResponseJSON,
     _OIDCAccessCheckPostJSON,
     _PodDetailsResponseJSON,
     _PodRegistrationResponseJSON,
-    _PublicKeyJSON,
-    _RegisterUserPublicKeyPOSTJSON,
     _SAMLAdditionalInfoPOSTJSON,
     _SAMLChallengeResponseJSON,
-    _SignatureBasedAccessCheckPostJSON,
 )
-from bitfount.hub.utils import hash_file_contents
 from bitfount.models.bitfount_model import BitfountModel
 from bitfount.storage import (
     _download_data_from_s3,
     _download_file_from_s3,
     _get_packed_data_object_size,
     _upload_data_to_s3,
     _upload_file_to_s3,
@@ -83,56 +49,65 @@
 from bitfount.types import (
     _JSONDict,
     _S3PresignedPOSTFields,
     _S3PresignedPOSTURL,
     _S3PresignedURL,
     _SAMLResponse,
 )
-from bitfount.utils import (
-    _get_mb_from_bytes,
-    _get_non_abstract_classes_from_module,
-    web_utils,
-)
+from bitfount.utils import _get_mb_from_bytes, _get_non_abstract_classes_from_module
+
+if TYPE_CHECKING:
+    from bitfount.federated.task_requests import _ProtocolDetails
 
 logger = logging.getLogger(__name__)
 
-# This forces `requests` to make IPv4 connections
-# TODO: [BIT-1443] Remove this once Hub/AM support IPv6
-requests.packages.urllib3.util.connection.HAS_IPV6 = False  # type: ignore[attr-defined] # Reason: see above # noqa: B950
 
+PRODUCTION_HUB_URL: Final[str] = "https://hub.bitfount.com"
+PRODUCTION_AM_URL: Final[str] = "https://am.bitfount.com"
+_STAGING_HUB_URL: Final[str] = "https://hub.staging.bitfount.com"
+_STAGING_AM_URL: Final[str] = "https://am.staging.bitfount.com"
+_DEV_HUB_URL: Final[str] = "http://localhost:3000"
+_DEV_AM_URL: Final[str] = "http://localhost:3001"
+
+_PRODUCTION_IDP_URL: Final[str] = (
+    "https://prod-bitfount.eu.auth0.com/"
+    "samlp/XJAxDKePxvyTKYdwPZMGjTwUAkX7Hts0?SAMLRequest="
+)
+_STAGING_IDP_URL: Final[str] = (
+    "https://dev-bitfount.eu.auth0.com/"
+    "samlp/kLEmmLCZB8frJsimz6A30DZFpvAUbrDt?SAMLRequest="
+)
+_DEV_IDP_URL: Final[str] = (
+    "https://dev-bitfount.eu.auth0.com/"
+    "samlp/FUo7nqEpUlARg11KE9WbYA0trvPJJ5ms?SAMLRequest="
+)
 
 # This should match the corresponding value in
 # bitfount/bitfount-web:bf-packages/bitfount-hub/pages/api/pods.api.js
 _MAX_SCHEMA_SIZE_BYTES: Final[int] = 10 * 1024 * 1024  # 10 Megabytes
 _MAX_SCHEMA_SIZE_MEGABYTES: Final[int] = _get_mb_from_bytes(
     _MAX_SCHEMA_SIZE_BYTES
 ).whole
 # This should match the corresponding value in
 # bitfount/bitfount-web:bf-packages/bitfount-hub/pages/api/models.api.js
 _MAX_CUSTOM_MODEL_SIZE_BYTES: Final[int] = 3 * 1024 * 1024  # 3 Megabytes
 _MAX_CUSTOM_MODEL_SIZE_MEGABYTES: Final[int] = _get_mb_from_bytes(
     _MAX_CUSTOM_MODEL_SIZE_BYTES
 ).whole
-_MAX_WEIGHTS_SIZE_BYTES: Final[int] = 500 * 1024 * 1024  # 3 Megabytes
-_MAX_WEIGHTS_SIZE_MEGABYTES: Final[int] = _get_mb_from_bytes(
-    _MAX_CUSTOM_MODEL_SIZE_BYTES
-).whole
-
-_P = ParamSpec("_P")
-_R = TypeVar("_R")
 
 
 @dataclass
 class PodPublicMetadata:
     """Data about the Pod which is publicly visible on BitfountHub."""
 
     name: str
     display_name: str
     description: str
     schema: _JSONDict  # This should be set to schema.to_json()
+    is_public: bool = False
 
 
 def _verify_signature(
     public_key: RSAPublicKey, signature: bytes, message: bytes
 ) -> bool:
     """Verifies that decrypting `signature` with `public_key` == `message`."""
     try:
@@ -166,21 +141,86 @@
     return err_msg
 
 
 def _extract_response_json(response: Response) -> Any:
     """Extracts the JSON from the response, raising an HTTPError if there is none."""
     try:
         response_json = response.json()
-    except ValueError as e:  # raised if JSON extraction fails
+    except ValueError:  # raised if JSON extraction fails
         raise InvalidJSONError(
             f"Invalid JSON response ({response.status_code}): {response.text}"
-        ) from e
+        )
     return response_json
 
 
+class AccessRequest:
+    """An access request that has been made for a Pod."""
+
+    def __init__(self, record_json: _AccessRequestResponseJSON):
+        """Create a new AccessRequest instance.
+
+        Args:
+            record_json: The access request JSON received from the hub.
+        """
+        self.access_json_str: str = record_json["accessJSON"]
+        self.access_json: _AccessJSON = json.loads(self.access_json_str)
+        self.modeller_public_key_pem: bytes = self.access_json[
+            "modellerPublicKey"
+        ].encode()
+        self.modeller_public_key: RSAPublicKey = _RSAEncryption.load_public_key(
+            self.modeller_public_key_pem
+        )
+        self.signed_permission = record_json.get("signedPermission")
+        self.status = self.access_json.get("status")
+        self.protocol = self.access_json["protocol"]
+
+    def authoriser_signed(self, authoriser_key: RSAPublicKey) -> bool:
+        """Checks whether the given authoriser has signed this access request.
+
+        Args:
+            authoriser_key: public key corresponding to authoriser.
+
+        Returns:
+            True if the given authoriser_key was used to sign this request,
+            False otherwise.
+        """
+        if self.signed_permission is None:
+            logger.warning(
+                "Not authorized: "
+                "Access request received is missing authoriser signature."
+            )
+            return False
+
+        signature = self.signed_permission.encode("ascii")
+        signature = base64.b64decode(signature)
+
+        logger.info("Verifying authorizer signature.")
+        return _verify_signature(
+            authoriser_key, signature, self.access_json_str.encode("ascii")
+        )
+
+    def is_correct_modeller(self, message: bytes, signature: bytes) -> bool:
+        """Checks that the message signature matches the expected modeller.
+
+        Checks if the signature and message combination proves that the
+        modeller is the modeller associated with this access request.
+
+        Args:
+            message: The message to verify.
+            signature: The signature on the message to compare to the modeller's
+                       public key.
+
+        Returns:
+            True if the modeller is the one who created the signature, False otherwise.
+        """
+        # Use modeller public key to verify their identity
+        logger.info("Verifying that modeller has correct signature.")
+        return _verify_signature(self.modeller_public_key, signature, message)
+
+
 def _check_pod_id_details(
     pod_identifier: Optional[str] = None,
     pod_namespace: Optional[str] = None,
     pod_name: Optional[str] = None,
 ) -> str:
     """Checks that either pod_identifier OR pod_namespace and pod_name are provided.
 
@@ -250,15 +290,15 @@
         session: Optional[BitfountSession] = None,
         url: str = PRODUCTION_HUB_URL,
     ):
         self.session = session if session else BitfountSession()
         if not self.session.authenticated:
             self.session.authenticate()
         self.url = url.rstrip("/")  # to ensure no trailing slash
-        self.user_storage_path = self.session.authentication_handler.user_storage_path
+        self.user_storage_path = self.session.user_storage_path
         self.username = self.session.username
 
     @staticmethod
     def _handle_hub_request_exception(
         err: RequestException, response: Optional[Response]
     ) -> str:
         """Logs out the request exception and response details, if provided."""
@@ -360,239 +400,182 @@
         return response_json
 
     def get_pod_key(
         self,
         pod_identifier: Optional[str] = None,
         pod_namespace: Optional[str] = None,
         pod_name: Optional[str] = None,
-    ) -> Optional[str]:
+    ) -> str:
         """Gets the public key of a pod.
 
         Either pod_identifier or pod_namespace and pod_name can be provided.
 
         Args:
             pod_identifier: Full pod identifier (i.e. <pod_namespace>/<pod_name>).
             pod_namespace: Pod namespace name.
             pod_name: Pod name.
 
         Returns:
-            The pod key or None if no key can be found.
+            The pod key or the empty string if no key can be found.
 
         Raises:
             HTTPError: If the response from the hub is malformed.
         """
         # Get pod details and check validity
         logger.debug(f"Requesting public key for '{pod_identifier}' from hub")
         pod_details: Optional[_PodDetailsResponseJSON] = self.get_pod(
             pod_identifier, pod_namespace, pod_name
         )
 
         if not pod_details:
-            # Return None so that the caller is aware that
-            # they aren't guaranteed a string key!
-            return None
+            # TODO: [BIT-959] We should be returning a None (Optional[str]) so
+            #       that the caller can be aware that they aren't guaranteed a
+            #       string key!
+            return ""
 
         # Extract key from response
-        pod_key: Optional[str] = pod_details.get("podPublicKey")
+        pod_key: str = pod_details.get("podPublicKey", "")
 
         if not pod_key:
             logger.error(f"Response JSON contained no public key: {pod_details}")
-            return None
 
+        # TODO: [BIT-959] We should be returning a None (Optional[str]) so that
+        #       the caller can be aware that they aren't guaranteed a string key!
         return pod_key
 
-    def send_model(
-        self, model_code_path: Path, private_model: bool = False
-    ) -> _ModelUploadResponseJSON:
+    def get_access_requests(
+        self, modeller_name: str, pod_identifier: str
+    ) -> List[AccessRequest]:
+        """Retrieves a set of access requests from the hub.
+
+        Gets access requests from the hub for the user `modeller_name` and the
+        pod `pod_identifier` along with the current status of that request
+        (approved/rejected/pending).
+
+        Args:
+            modeller_name: The modeller's username.
+            pod_identifier: The pod's identifier.
+
+        Returns:
+            A list of AccessRequest instances or an empty list if unable to
+            retrieve them.
+        """
+        _, pod_name = pod_identifier.split("/")
+        logger.info(
+            "Retrieving access requests from user: "
+            + f"'{modeller_name}' for pod: '{pod_identifier}'"
+        )
+
+        try:
+            response = self.session.get(
+                f"{self.url}/api/access/access-request",
+                params={
+                    "modellerName": modeller_name,
+                    "podName": pod_name,
+                },
+            )
+            response.raise_for_status()
+        except RequestException as err:
+            try:
+                # noinspection PyUnboundLocalVariable
+                err_response: Optional[Response] = response
+            except NameError:
+                # response variable wasn't created yet
+                err_response = None
+            self._handle_hub_request_exception(err, err_response)
+            return []
+
+        access_requests: List[_AccessRequestResponseJSON] = _extract_response_json(
+            response
+        )
+        return [AccessRequest(access) for access in access_requests]
+
+    def send_model(self, model_code_path: Path) -> bool:
         """Sends the provided `model_code` to Hub, associated to session username.
 
         The name of the model is taken from the name of the file.
 
         Args:
             model_code_path: The path to the file containing the model code.
-            private_model: Whether the model is private or publically accessible.
-
-        Returns: The response JSON from the Hub.
 
-        Raises:
-            ModelUploadError: If the model upload fails. Specific subclasses of
-                ModelUploadError are used to indicate different failure conditions.
+        Returns:
+            True if model was uploaded successfully, False otherwise.
         """
         # Check model is correctly formed
         try:
             self._verify_bitfount_model_format(model_code_path)
         except (ValueError, TypeError) as e:
             logger.error(f"Model incorrectly structured. Error: {e}")
-            raise ModelValidationError("Model incorrectly structured.") from e
+            return False
         except ImportError as ie:
             logger.error(ie)
-            raise ModelValidationError("Unable to import model.") from ie
+            return False
 
         model_name: str = model_code_path.stem
         model_code_text: bytes = model_code_path.read_text().encode("utf-8")
         model_size = len(model_code_text)
-        model_hash = hash_file_contents(model_code_path)
         if model_size > _MAX_CUSTOM_MODEL_SIZE_BYTES:
-            raise ModelTooLargeError(
+            raise ValueError(
                 f"Model is too large to upload: "
                 f"expected max {_MAX_CUSTOM_MODEL_SIZE_MEGABYTES} megabytes, "
                 f"got {_get_mb_from_bytes(model_size).fractional} megabytes."
             )
 
         # Register model details with hub
         try:
             logger.info(f"Uploading model '{model_name}' to Hub")
             response = self.session.post(
                 f"{self.url}/api/models",
                 json={
                     "modelName": model_name,
                     "modelSize": model_size,
-                    "modelHash": model_hash,
-                    "privateModel": private_model,
                 },
             )
             response.raise_for_status()
         except RequestException as err:
             try:
                 # noinspection PyUnboundLocalVariable
                 err_response: Optional[Response] = response
             except NameError:
                 # response variable wasn't created yet
                 err_response = None
             self._handle_hub_request_exception(err, err_response)
-            raise ModelUploadError(
-                "Request exception occurred when uploading model details to hub."
-            ) from err
+            return False
 
         # Extract response
         response_json: _ModelUploadResponseJSON = _extract_response_json(response)
         if not response_json["success"]:
             logger.error(
                 f"Could not send model to Bitfount Hub. "
                 f"Failed with message: {response_json['errorMessage']}"
             )
-            raise ModelUploadError(
-                f"Failed to upload model details to hub: "
-                f"{response_json['errorMessage']}"
-            )
+            return False
         logger.info(
             "Model successfully registered to Bitfount Hub, "
             "uploading to object storage."
         )
 
         # Extract URL to upload model to and upload it
         upload_url: _S3PresignedPOSTURL = response_json["uploadUrl"]
         upload_fields: _S3PresignedPOSTFields = response_json["uploadFields"]
-        try:
-            _upload_file_to_s3(
-                upload_url,
-                upload_fields,
-                file_contents=model_code_text,
-                file_name=model_code_path.name,
-            )
-        except RequestException as re:
-            logger.error(f"Failed to upload model to S3: {re}")
-            raise ModelUploadError("Failed to upload model to S3") from re
-
-        logger.info("Model successfully uploaded to object storage.")
-        return response_json
-
-    def send_weights(
-        self, model_name: str, model_version: int, pretrained_file_path: Path
-    ) -> None:
-        """Sends the provided `model_weights` to Hub.
-
-        Save trained model weights associated to session username,
-        model_name and model_version.
-
-        Args:
-            model_name: The name of the model to associate the weights with.
-            model_version: The version of the model to associate the weights with.
-            pretrained_file_path: The path to the pretrained model file.
-
-        Raises:
-            ModelUploadError: If the model upload fails. Specific subclasses of
-            ModelUploadError are used to indicate different failure conditions.
-        """
-        weights_text: bytes = pretrained_file_path.read_bytes()
-        weights_size = len(weights_text)
-        if weights_size > _MAX_WEIGHTS_SIZE_BYTES:
-            raise ModelTooLargeError(
-                f"Model weights are too large to upload: "
-                f"expected max {_MAX_WEIGHTS_SIZE_MEGABYTES} megabytes, "
-                f"got {_get_mb_from_bytes(weights_size).fractional} megabytes."
-            )
-
-        # Register model weight details with hub
-        try:
-            logger.info(
-                f"Uploading model weights to '{model_name}:{model_version}' to Hub"
-            )
-            response = self.session.put(
-                f"{self.url}/api/models",
-                json={
-                    "modelName": model_name,
-                    "modelVersion": model_version,
-                    "weightSize": weights_size,
-                },
-            )
-            response.raise_for_status()
-        except RequestException as err:
-            try:
-                # noinspection PyUnboundLocalVariable
-                err_response: Optional[Response] = response
-            except NameError:
-                # response variable wasn't created yet
-                err_response = None
-            self._handle_hub_request_exception(err, err_response)
-            raise ModelUploadError(
-                "Request exception occurred when uploading model weights to hub."
-            ) from err
-
-        # Extract response
-        response_json: _ModelUploadResponseJSON = _extract_response_json(response)
-        logger.info(
-            "Model weights successfully registered to Bitfount Hub, "
-            "uploading to object storage."
+        _upload_file_to_s3(
+            upload_url,
+            upload_fields,
+            file_contents=model_code_text,
+            file_name=model_code_path.name,
         )
+        logger.info("Model successfully uploaded to object storage.")
 
-        # Extract URL to upload model to and upload it
-        upload_url: _S3PresignedPOSTURL = response_json["uploadUrl"]
-        upload_fields: _S3PresignedPOSTFields = response_json["uploadFields"]
-        try:
-            _upload_file_to_s3(
-                upload_url,
-                upload_fields,
-                file_contents=weights_text,
-                file_name=pretrained_file_path.name,
-            )
-        except RequestException as re:
-            logger.error(f"Failed to upload model weights to S3: {re}")
-            raise ModelUploadError("Failed to upload model weights to S3") from re
-
-        logger.info("Model weights successfully uploaded to object storage.")
-        return None
+        return True
 
     @staticmethod
     def _verify_bitfount_model_format(path_to_model: Path) -> Type[BitfountModel]:
-        """Verifies that the model file is correctly formatted.
-
-        Args:
-            path_to_model: Path to the model file.
-
-        Returns:
-            The parsed model class.
-
-        Raises:
-            ImportError: If the file could not be loaded.
-            TypeError: If there is no BitfountModel subclass in the file, or it
-                is still abstract.
-            ValueError: If multiple model classes are contained in the file.
-            ValueError: If the name of the class and the name of the file differ.
+        """Verifies that the the model file is correctly formatted.
 
+        Returns the model class
         """
         models = _get_non_abstract_classes_from_module(path_to_model)
         models = {
             name: class_
             for name, class_ in models.items()
             if issubclass(class_, BitfountModel)
         }
@@ -614,196 +597,98 @@
                 "Model class name must be the same as the filename",
             )
 
         logger.debug("Model is formatted correctly")
 
         return model[1]
 
-    def _get_model_response(
-        self,
-        username: str,
-        model_name: str,
-        model_version: Optional[int] = None,
-        project_id: Optional[str] = None,
-    ) -> Optional[_ModelDetailsResponseJSON]:
-        """Gets the response for the model from the hub."""
+    def get_model(
+        self, username: str, model_name: str
+    ) -> Optional[Type[BitfountModel]]:
+        """Gets model code corresponding to `model_name` from user `username`.
+
+        Args:
+            username: The model's owner.
+            model_name: The name of the model.
+
+        Returns:
+            The loaded Bitfount model class or None if unable to retrieve it.
+        """
         # Retrieve model details from API endpoint
         try:
-            params = {
-                "modellerName": username,
-                "modelName": model_name,
-            }
-            if model_version:
-                params.update({"modelVersion": str(model_version)})
-            if project_id:
-                params.update({"projectId": project_id})
             response = self.session.get(
                 f"{self.url}/api/models",
-                params=params,
+                params={
+                    "modellerName": username,
+                    "modelName": model_name,
+                },
             )
             response.raise_for_status()
         except RequestException as err:
             try:
                 # noinspection PyUnboundLocalVariable
                 err_response: Optional[Response] = response
             except NameError:
                 # response variable wasn't created yet
                 err_response = None
             self._handle_hub_request_exception(err, err_response)
             return None
 
         # Check response contains model details
         response_json: _ModelDetailsResponseJSON = _extract_response_json(response)
-
-        return response_json
-
-    def _response_retry(
-        self, request: Callable[_P, Optional[_R]], err_msg: str
-    ) -> Callable[_P, Optional[_R]]:
-        """Retry decorator for Hub requests."""
-
-        def retried_request(*args: _P.args, **kwargs: _P.kwargs) -> Optional[_R]:
-            attempts = 5
-            response_json = request(*args, **kwargs)
-            while response_json is None and attempts > 0:
-                time.sleep(5)  # give an extra 5 seconds in between retries
-                response_json = request(*args, **kwargs)
-                attempts -= 1
-            if not response_json:  # i.e. empty JSON response
-                logger.warning(err_msg)
-                return None
-
-            return response_json
-
-        return retried_request
-
-    def get_weights(
-        self,
-        username: str,
-        model_name: str,
-        model_version: int,
-        project_id: Optional[str] = None,
-    ) -> Optional[bytes]:
-        """Gets weights byte stream corresponding to `model_name` from user `username`.
-
-        Args:
-            username: The model's owner.
-            model_name: The name of the model.
-            model_version: (Optional) The version of the model. Defaults to latest.
-
-        Returns:
-            The loaded weights as a byte stream or None if one has not been uploaded.
-        """
-        retrying_get_model_response = self._response_retry(
-            self._get_model_response,
-            f"No models registered by the name of {model_name} from user {username}",
-        )
-        response_json = retrying_get_model_response(
-            username=username,
-            model_name=model_name,
-            model_version=model_version,
-            project_id=project_id,
-        )
-        if not response_json:
-            return None
-
-        if weights_url := response_json.get("weightsDownloadUrl"):
-            weights_bytes = _download_file_from_s3(weights_url)
-            return cast(bytes, weights_bytes)
-        else:
+        if len(response_json) == 0:  # i.e. empty JSON response
             logger.warning(
-                "No weight file associated with "
-                f"{username}/{model_name}:{model_version}"
+                f"No models registered by the name of {model_name} from user {username}"
             )
             return None
 
-    def get_model(
-        self,
-        username: str,
-        model_name: str,
-        model_version: Optional[int] = None,
-        project_id: Optional[str] = None,
-    ) -> Optional[Type[BitfountModel]]:
-        """Gets model code corresponding to `model_name` from user `username`.
-
-        Args:
-            username: The model's owner.
-            model_name: The name of the model.
-            model_version: (Optional) The version of the model. Defaults to latest.
-
-        Returns:
-            The loaded Bitfount model class or None if unable to retrieve it.
-        """
-        retrying_get_model_response = self._response_retry(
-            self._get_model_response,
-            f"No models registered by the name of {model_name} from user {username}",
-        )
-        response_json: Optional[
-            _ModelDetailsResponseJSON
-        ] = retrying_get_model_response(
-            username=username,
-            model_name=model_name,
-            model_version=model_version,
-            project_id=project_id,
-        )
-        if not response_json:
-            return None
-
         # Download model code from S3
         try:
             model_url: _S3PresignedURL = response_json["modelDownloadUrl"]
-        except (KeyError, TypeError) as e:
+        except (KeyError, TypeError):
             # Either entry isn't in the dict, or it's not a dict (might be None)
             raise InvalidJSONError(
                 f"Cannot retrieve model, no model URL in pod response: "
                 f"{response_json}"
-            ) from e
+            )
 
         # Extract class from downloaded code. We save it out to file to make it
         # easier to import and verify.
         # This will be a str because we provide an encoding arg.
         model_code: str = cast(str, _download_file_from_s3(model_url, encoding="utf-8"))
-
+        # TODO: [BIT-1073] Reintroduce model hash verification at this point in
+        #  the download process.
         model_file = self._write_model_code_to_file(
-            model_code, self.user_storage_path / "models", model_name, ".py"
+            model_code, self.user_storage_path, model_name
         )
-
-        file_hash = hash_file_contents(model_file)
-        if file_hash != response_json["modelHash"]:
-            raise ModelValidationError(
-                "Stored hash does not match hashed model file contents. "
-                "Model must be re-uploaded to hub."
-            )
         try:
             model_class = self._verify_bitfount_model_format(model_file)
         except (ValueError, TypeError, ImportError) as e:
             logger.error(e)
             # Remove the model file as model is incompatible
             model_file.unlink()
             return None
-
         return model_class
 
     @staticmethod
     def _write_model_code_to_file(
-        contents: Union[bytes, str], storage_path: Path, model_name: str, suffix: str
+        model_code: str, user_storage_path: Path, model_name: str
     ) -> Path:
         """Saves downloaded model code to a Python file.
 
-        Creates a module from `contents` and saves it in a directory named after
-        `username` under storage_path
+        Creates a module from `model_code` and saves it in a directory named after
+        `username` under user_storage_path/models
         """
-        storage_path.mkdir(parents=True, exist_ok=True)
-        file = Path(storage_path / model_name).with_suffix(suffix)
-        file.touch()
-        if isinstance(contents, bytes):
-            file.write_bytes(contents)
-        else:
-            file.write_text(contents)
-        return file
+        model_store = user_storage_path / "models"
+        model_store.mkdir(parents=True, exist_ok=True)
+        model_file = Path(model_store / model_name).with_suffix(".py")
+        model_file.touch()
+        model_file.write_text(model_code)
+
+        return model_file
 
     def register_pod(
         self,
         public_metadata: PodPublicMetadata,
         pod_public_key: RSAPublicKey,
         access_manager_key: RSAPublicKey,
     ) -> None:
@@ -814,35 +699,36 @@
         Args:
             public_metadata: Details about the pod (name, etc) to register.
             pod_public_key: The public key to use for this pod.
             access_manager_key: The public key of the Access Manager.
 
         Raises:
             RequestException: If unable to connect to the hub.
-            HTTPError: If the response is not successful.
+            HTTPError: If the response if not successful.
         """
         # Need to determine how large it will be once packed and uploaded to S3
         schema_size = _get_packed_data_object_size(public_metadata.schema)
         if schema_size > _MAX_SCHEMA_SIZE_BYTES:
-            raise SchemaUploadError(
+            raise ValueError(
                 f"Schema is too large to upload: "
                 f"expected max {_MAX_SCHEMA_SIZE_MEGABYTES} megabytes, "
                 f"got {_get_mb_from_bytes(schema_size).fractional} megabytes."
             )
 
         json_post = {
             "name": public_metadata.name,
             "podDisplayName": public_metadata.display_name,
-            "podPublicKey": _RSAEncryption.serialize_public_key(
-                pod_public_key, form="SSH"
+            "podPublicKey": pod_public_key.public_bytes(
+                Encoding.OpenSSH, PublicFormat.OpenSSH
             ).decode("utf-8"),
-            "accessManagerPublicKey": _RSAEncryption.serialize_public_key(
-                access_manager_key, form="SSH"
+            "accessManagerPublicKey": access_manager_key.public_bytes(
+                Encoding.OpenSSH, PublicFormat.OpenSSH
             ).decode("utf-8"),
             "description": public_metadata.description,
+            "isPublic": public_metadata.is_public,
             "schemaSize": schema_size,
         }
 
         # Send pod registration request and get response from hub
         try:
             response = self.session.post(
                 f"{self.url}/api/pods",
@@ -887,16 +773,16 @@
 
         Args:
             pod_name: The name of the pod.
             pod_public_key: The public key of the pod.
         """
         json_patch = {
             "name": pod_name,
-            "podPublicKey": _RSAEncryption.serialize_public_key(
-                pod_public_key, form="SSH"
+            "podPublicKey": pod_public_key.public_bytes(
+                Encoding.OpenSSH, PublicFormat.OpenSSH
             ).decode("utf-8"),
         }
 
         # We don't need to catch RequestExceptions here as the calling loop
         # handles that.
         response = self.session.patch(
             f"{self.url}/api/pods",
@@ -939,188 +825,69 @@
         # Try and extract schema URL from response
         try:
             if pod_details:
                 schema_url: _S3PresignedURL = pod_details["schemaDownloadUrl"]
             else:
                 # i.e. pod_details is None
                 raise TypeError("No pod details retrieved from Hub")
-        except (KeyError, TypeError) as e:
+        except (KeyError, TypeError):
             # Either entry isn't in the dict, or it's not a dict (might be None)
             raise InvalidJSONError(
                 f"Cannot retrieve pod schema, no schema URL in pod response: "
                 f"{pod_details}"
-            ) from e
+            )
 
         # Download and parse schema
         schema_json: _JSONDict = _download_data_from_s3(schema_url)
         return BitfountSchema.load(schema_json)
 
-    def send_monitor_update(self, update: _MonitorPostJSON) -> None:
-        """Send an update to the monitor service.
-
-        Args:
-            update: The monitor service update to send as a JSON dict.
-
-        Raises:
-            RequestException: If unable to connect to the hub.
-            HTTPError: If the response is not successful.
-        """
-        try:
-            response = self.session.post(
-                f"{self.url}/api/ingest", json=update, timeout=5
-            )
-            response.raise_for_status()
-        except RequestException as err:
-            try:
-                # noinspection PyUnboundLocalVariable
-                err_response: Optional[Response] = response
-            except NameError:
-                # response variable wasn't created yet
-                err_response = None
-            err_msg = self._handle_hub_request_exception(err, err_response)
-            # Raise the same type of exception that was caught
-            raise type(err)(err_msg) from err
-
-    def register_user_public_key(self, key: RSAPublicKey) -> Union[str, int]:
-        """Register a public key for the signed-in user.
-
-        Args:
-            key: The public key to register.
-        """
-        key_json: _RegisterUserPublicKeyPOSTJSON = {
-            "key": _RSAEncryption.serialize_public_key(key, form="SSH").decode("utf-8"),
-        }
-        try:
-            response = self.session.post(
-                f"{self.url}/api/{self.username}/keys",
-                json=key_json,
-                timeout=10,
-                params={"version": 2},
-            )
-            response.raise_for_status()
-        except RequestException as err:
-            try:
-                # noinspection PyUnboundLocalVariable
-                err_response: Optional[Response] = response
-            except NameError:
-                # response variable wasn't created yet
-                err_response = None
-            err_msg = self._handle_hub_request_exception(err, err_response)
-            # Raise the same type of exception that was caught
-            raise type(err)(err_msg) from err
-
-        response_json: _CreatedResourceResponseJSON = _extract_response_json(response)
-        return response_json["id"]
-
-    def check_public_key_registered_and_active(
-        self, key_id: str, username: Optional[str] = None
-    ) -> Optional[_ActivePublicKey]:
-        """Return key details from hub if key is registered."""
-        if username is None:
-            username = self.username
-
-        try:
-            response = self.session.get(
-                f"{self.url}/api/{username}/keys/{key_id}",
-            )
-            if response.status_code == 404:
-                return None
-            response.raise_for_status()
-        except RequestException as err:
-            try:
-                # noinspection PyUnboundLocalVariable
-                err_response: Optional[Response] = response
-            except NameError:
-                # response variable wasn't created yet
-                err_response = None
-            err_msg = self._handle_hub_request_exception(err, err_response)
-            # Raise the same type of exception that was caught
-            raise type(err)(err_msg) from err
-
-        # Load key from response
-        response_json: _PublicKeyJSON = _extract_response_json(response)
-
-        if response_json["active"]:
-            return _ActivePublicKey(
-                public_key=_RSAEncryption.load_public_key(
-                    response_json["public_key"].encode()
-                ),
-                id=response_json["id"],
-                active=response_json["active"],
-            )
-        return None
-
 
 class BitfountAM:
     """Typed API for communicating with the Bitfount Access Manager.
 
     Args:
         session: Bitfount session for authentication.
         access_manager_url: URL of the access manager.
     """
 
     def __init__(
         self,
         session: Optional[BitfountSession] = None,
-        access_manager_url: Optional[str] = None,
+        access_manager_url: str = PRODUCTION_AM_URL,
     ):
-        # Set the AM url based on the environment
-        access_manager_url = (
-            access_manager_url if access_manager_url else self._get_am_url()
-        )
         self.session = session if session else BitfountSession()
         if not self.session.authenticated:
             self.session.authenticate()
         self.access_manager_url = access_manager_url.rstrip(
             "/"
         )  # to ensure no trailing slashes
 
     @staticmethod
     def _handle_am_request_exception(
         err: RequestException, response: Optional[Response]
     ) -> str:
         """Logs out the request exception and response details, if provided."""
         return _handle_request_exception(err, response, "Access Manager")
 
-    @staticmethod
-    def _get_am_url() -> str:
-        """Gets the AM url based on the environment."""
-        environment = _get_environment()
-        if environment == _STAGING_ENVIRONMENT:
-            access_manager_url = _STAGING_AM_URL
-        elif environment == _DEVELOPMENT_ENVIRONMENT:
-            access_manager_url = _DEV_AM_URL
-        elif environment == _PRODUCTION_ENVIRONMENT:
-            access_manager_url = PRODUCTION_AM_URL
-        elif environment == _SANDBOX_ENVIRONMENT:
-            access_manager_url = _SANDBOX_AM_URL
-        return access_manager_url
-
     def get_access_manager_key(self) -> RSAPublicKey:
         """Gets the Access Manager's public key.
 
         Returns:
             The access manager's public key.
 
         Raises:
             RequestException: If there is a problem communicating with the
                               access manager.
         """
+        # Doesn't need authentication as not behind authenticated endpoint
         try:
-            # No need to use `self.session` here because the endpoint does not require
-            # authentication.
-            logger.debug(
-                f"Retrieving access manager public key from "
-                f"{self.access_manager_url}/api/access-manager-key"
-            )
-            response = web_utils.get(
+            response = requests.get(
                 f"{self.access_manager_url}/api/access-manager-key",
-                timeout=5,
+                timeout=10,
             )
-            logger.debug(f"Access manager public key response received: {response}")
             response.raise_for_status()
         except RequestException as err:
             try:
                 # noinspection PyUnboundLocalVariable
                 err_response: Optional[Response] = response
             except NameError:
                 # response variable wasn't created yet
@@ -1129,20 +896,19 @@
             # Raise same type of exception as caught before
             raise type(err)(err_msg) from err
 
         response_json: _AccessManagerKeyResponseJSON = _extract_response_json(response)
 
         try:
             am_key: str = response_json["accessManagerPublicKey"]
-            logger.debug(f"Received access manager key: {am_key}")
-        except (KeyError, TypeError) as e:
+        except (KeyError, TypeError):
             raise InvalidJSONError(
                 f"Unable to extract public key from access manager response, "
                 f"no key in JSON: {response_json}"
-            ) from e
+            )
         return _RSAEncryption.load_public_key(am_key.encode())
 
     def get_saml_challenge(self) -> Tuple[str, str]:
         """Gets a fresh SAML challenge for a user.
 
         Returns:
             A tuple of:
@@ -1153,15 +919,15 @@
             RequestException: If there is a problem communicating with the
                               access manager.
             HTTPError: If it is unable to extract the SAML challenge from
                        the response.
         """
         try:
             response = self.session.get(
-                f"{self.access_manager_url}/api/saml?idp=cli", timeout=10
+                f"{self.access_manager_url}/api/saml", timeout=10
             )
             response.raise_for_status()
         except RequestException as err:
             try:
                 # noinspection PyUnboundLocalVariable
                 err_response: Optional[Response] = response
             except NameError:
@@ -1172,141 +938,98 @@
             raise type(err)(err_msg) from err
 
         response_json: _SAMLChallengeResponseJSON = _extract_response_json(response)
 
         try:
             saml_request_to_proxy_to_modeller: str = response_json["samlRequest"]
             saml_request_id: str = response_json["id"]
-        except (KeyError, TypeError) as e:
+        except (KeyError, TypeError):
             raise InvalidJSONError(
                 f"Unable to extract SAML Challenge from access manager response, "
                 f"no challenge in JSON: {response_json}"
-            ) from e
+            )
         return saml_request_to_proxy_to_modeller, saml_request_id
 
     def validate_saml_response(
         self,
         saml_response: _SAMLResponse,
         saml_request_id: str,
         pod_identifier: str,
         modeller_name: str,
-        serialized_protocol: SerializedProtocol,
+        task_protocol_details: _ProtocolDetails,
     ) -> _PodResponseType:
         """Check if modeller task should be accepted.
 
         This sends the SAML Response from a modeller,
         along with details about their training request
         to the access manager so that it can inform the pod
         whether or not to accept the job.
 
         Args:
             saml_response: SAML Response from Modeller
             saml_request_id: The ID of the original SAML request. The pod should have
                 stored this.
             pod_identifier: The pod identifier of the pod that the task is for
             modeller_name: The username of the modeller that has sent the task
-            serialized_protocol: The serialized protocol to use for the task received
-                from the modeller.
+            task_protocol_details: The protocol details portion of the task request from
+                the Modeller
 
         Returns:
             Response code from the Access manager.
 
         Raises:
             RequestException: If there is a problem communicating with the
                 access manager.
             HTTPError: If it is unable to extract the JSON from the response.
             InvalidJSONError: If JSON does not match expected format.
         """
         additional_info = _SAMLAdditionalInfoPOSTJSON(
             originalSAMLRequestID=saml_request_id,
             podIdentifier=pod_identifier,
             modellerName=modeller_name,
-            modellerProtocolRequest=serialized_protocol,
+            modellerRequest=dataclasses.astuple(task_protocol_details),
             identityProvider="SAML",
         )
         post_json: _JSONDict = dict(saml_response, **additional_info)
 
         return self._do_authorization_check(
-            f"{self.access_manager_url}/api/access?idp=cli", post_json
+            f"{self.access_manager_url}/api/access", post_json
         )
 
     def check_oidc_access_request(
         self,
         pod_identifier: str,
-        serialized_protocol: SerializedProtocol,
+        task_protocol_details: _ProtocolDetails,
         modeller_name: str,
         modeller_access_token: str,
     ) -> _PodResponseType:
         """Check access using access token from OIDC request.
 
         Args:
             pod_identifier: The pod identifier for the request.
-            serialized_protocol: The protocol received from the modeller.
-            modeller_name: The name of the modeller.
-            modeller_access_token: OAuth access token for the modeller.
-
-        Returns:
-            Response code from the Access manager.
-
-        Raises:
-            RequestException: If there is a problem communicating with the
-                access manager.
-            HTTPError: If it is unable to extract the JSON from the response.
-            InvalidJSONError: If JSON does not match expected format.
-        """
-        post_json = _OIDCAccessCheckPostJSON(
-            podIdentifier=pod_identifier,
-            modellerProtocolRequest=serialized_protocol,
-            modellerName=modeller_name,
-            modellerToken=modeller_access_token,
-            identityProvider="OIDC",
-        )
-
-        return self._do_authorization_check(
-            f"{self.access_manager_url}/api/access", post_json
-        )
-
-    def check_signature_based_access_request(
-        self,
-        unsigned_task: bytes,
-        task_signature: bytes,
-        pod_identifier: str,
-        serialized_protocol: SerializedProtocol,
-        modeller_name: str,
-    ) -> _PodResponseType:
-        """Check access by verifying the signed task against the unsigned task.
-
-        Check that when modeller_name signs the unsigned task it matches the signed
-        version of the task.
-
-        Args:
-            unsigned_task: Unsigned task.
-            task_signature: Task signature.
-            pod_identifier: The pod identifier for the request.
-            serialized_protocol: The protocol received from the modeller.
+            task_protocol_details: The details of the request.
             modeller_name: The name of the modeller.
             modeller_access_token: OAuth access token for the modeller.
 
         Returns:
             Response code from the Access manager.
 
         Raises:
             RequestException: If there is a problem communicating with the
                 access manager.
             HTTPError: If it is unable to extract the JSON from the response.
             InvalidJSONError: If JSON does not match expected format.
         """
-        post_json = _SignatureBasedAccessCheckPostJSON(
-            podIdentifier=pod_identifier,
-            modellerProtocolRequest=serialized_protocol,
-            modellerName=modeller_name,
-            unsignedTask=base64.b64encode(unsigned_task).decode("utf-8"),
-            taskSignature=base64.b64encode(task_signature).decode("utf-8"),
-            identityProvider="SIGNATURE",
-        )
+        post_json: _OIDCAccessCheckPostJSON = {
+            "podIdentifier": pod_identifier,
+            "modellerRequest": dataclasses.astuple(task_protocol_details),
+            "modellerName": modeller_name,
+            "modellerToken": modeller_access_token,
+            "identityProvider": "OIDC",
+        }
 
         return self._do_authorization_check(
             f"{self.access_manager_url}/api/access", post_json
         )
 
     def _do_authorization_check(
         self,
@@ -1348,15 +1071,15 @@
 
         try:
             response_json: _AMAccessCheckResponseJSON = _extract_response_json(response)
             logger.debug(f"JSON From access manager: {response_json}")
             try:
                 return _PodResponseType[response_json["code"]]
             except KeyError:
-                return _PodResponseType.NO_ACCESS
-            except TypeError as e:
-                raise AttributeError from e
-        except AttributeError as ae:
+                return _PodResponseType.ERROR_IN_VERIFICATION
+            except TypeError:
+                raise AttributeError()
+        except AttributeError:
             # raised if response is not a dict
             raise InvalidJSONError(
                 f"Invalid JSON response ({response.status_code}): {response.text}"
-            ) from ae
+            )
```

### Comparing `bitfount-0.5.86/bitfount/hub/authentication_handlers.py` & `bitfount-0.5.9/bitfount/hub/authentication_flow.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,330 +1,211 @@
-"""Authentication source for Bitfount services."""
-from abc import ABC, abstractmethod
+"""Authentication flow and session management."""
+from __future__ import annotations
+
+from dataclasses import dataclass
 from datetime import datetime, timedelta, timezone
-from functools import cached_property
 import json
-from json import JSONDecodeError
 import logging
 from pathlib import Path
+import threading
 import time
-from typing import Callable, Dict, Final, List, Optional, Tuple, Union, cast
+from typing import TYPE_CHECKING, Dict, Final, Optional, Tuple, Union, cast
 import webbrowser
 
 import jwt
-from requests import Response
+import requests
 from requests.exceptions import InvalidJSONError
 
-from bitfount.config import BITFOUNT_STORAGE_PATH
-from bitfount.hub.exceptions import AuthenticatedUserError
+from bitfount.config import (
+    _DEVELOPMENT_ENVIRONMENT,
+    _STAGING_ENVIRONMENT,
+    BITFOUNT_STORAGE_PATH,
+    _get_environment,
+)
+from bitfount.exceptions import BitfountError
 from bitfount.hub.types import (
     _DeviceAccessTokenFailResponseJSON,
     _DeviceAccessTokenRequestDict,
     _DeviceAccessTokenResponseJSON,
     _DeviceCodeRequestDict,
     _DeviceCodeResponseJSON,
     _TokenRefreshRequestDict,
     _TokenRefreshResponseJSON,
 )
-from bitfount.utils import web_utils
+
+if TYPE_CHECKING:
+    from requests import Response
 
 logger = logging.getLogger(__name__)
 
-_PRODUCTION_AUTH_DOMAIN: Final = "auth.bitfount.com"
+
+_PRODUCTION_AUTH_DOMAIN: Final = "auth.hub.bitfount.com"
 _STAGING_AUTH_DOMAIN: Final = "auth.staging.bitfount.com"
 _DEVELOPMENT_AUTH_DOMAIN: Final = (
     "auth.staging.bitfount.com"  # this is part of the staging tenant on auth0
 )
-_SANDBOX_AUTH_DOMAIN: Final = "auth.staging.bitfount.com"
 
 # TODO: [BIT-356] potentially remove these client ids from the codebase
 _PRODUCTION_CLIENT_ID: Final = "8iCJ33Kp6hc9ofrXTzr5GLxMRHWrlzZO"
 _STAGING_CLIENT_ID: Final = "Wk4XZHDKfY8F3OYcKdagIHETt6JYwX08"
 _DEVELOPMENT_CLIENT_ID: Final = "MP8oao6gcJd4jARwzJiJlEiK59ZeLCt3"
-_SANDBOX_CLIENT_ID: Final = "Wk4XZHDKfY8F3OYcKdagIHETt6JYwX08"  # same as staging
 
 _SCOPES: Final = "profile openid offline_access"
 _HUB_API_IDENTIFIER: Final = (
     "https://hub.bitfount.com/api"  # this is the same for staging and production
 )
 _DEVICE_CODE_GRANT_TYPE: Final = "urn:ietf:params:oauth:grant-type:device_code"
 _AUTHORIZATION_PENDING_ERROR: Final = "authorization_pending"
 _SLOW_DOWN_ERROR: Final = "slow_down"
 
-_DEFAULT_USERNAME: Final[str] = "_default"
+_DEFAULT_USER_DIRECTORY: Final = "_default"
 _USERNAME_KEY: Final = "https://www.bitfount.com/username"
 
 
-class AuthenticationHandler(ABC):
-    """Abstract Authentication Handler for use with BitfountSessions."""
-
-    def __init__(self, username: str):
-        self.user_storage_path: Path = BITFOUNT_STORAGE_PATH / username
-
-    @property
-    @abstractmethod
-    def hub_request_headers(self) -> Dict:
-        """HTTP Request headers for authenticating with the Hub."""
-        pass
-
-    @property
-    @abstractmethod
-    def am_request_headers(self) -> Dict:
-        """HTTP Request headers for authenticating with the Access Manager."""
-        pass
-
-    @property
-    @abstractmethod
-    def message_service_request_metadata(self) -> List[Tuple[str, str]]:
-        """Metadata used to authenticate with the message service."""
-        pass
-
-    @abstractmethod
-    def authenticate(self) -> None:
-        """Retrieve a valid method for authentication if managed externally.
-
-        If the authentication mechanism requires interaction with an external
-        party, or the authentication expires, then this is the method that
-        should be used to retrieve new authentication materials for
-        communicating with Bitfount services.
-        """
-        pass
-
-    @property
-    @abstractmethod
-    def authenticated(self) -> bool:
-        """Whether the handler currently has valid authentication.
-
-        Some authentication methods are valid from creation,
-        others may need refreshing intermittently.
-        """
-        pass
-
-    @property
-    @abstractmethod
-    def username(self) -> str:
-        """Authenticated user's username."""
-        pass
-
-
-class APIKeysHandler(AuthenticationHandler):
-    """Authenticate a user with API Keys."""
-
-    def __init__(self, api_key_id: str, api_key: str, username: str):
-        super().__init__(username)
-        self._api_key_id = api_key_id
-        self._api_key = api_key
-        self._username = username
-        # We do not send the Access manager portion of the key
-        # to the hub or the message service
-        self._core_api_key_id = self._api_key_id.split(":")[0]
-        self._core_api_key = self._api_key.split(":")[0]
-
-        if self._username == _DEFAULT_USERNAME:
-            # username must be explicitly set in __init__ to use API keys
-            raise AuthenticatedUserError("Must specify a username when using API Keys.")
-
-    @cached_property
-    def hub_request_headers(self) -> Dict:
-        """Header for authenticating with hub."""
-        return {
-            "x-api-key-id": self._core_api_key_id,
-            "x-api-key": self._core_api_key,
-        }
-
-    @cached_property
-    def am_request_headers(self) -> Dict:
-        """Header for authenticating with access manager."""
-        # If the URL is an AM URL, the entire API key is used because the AM
-        # also calls the Hub under the hood and therefore needs both portions
-        return {"x-api-key-id": self._api_key_id, "x-api-key": self._api_key}
-
-    @property
-    def message_service_request_metadata(self) -> List[Tuple[str, str]]:
-        """Metadata for authenticating with message service."""
-        return [
-            ("x-api-key-id", self._core_api_key_id),
-            ("x-api-key", self._core_api_key),
-        ]
-
-    def authenticate(self) -> None:
-        """Authenticates the user.
-
-        We're using API keys here which are valid from creation.
-        They do not require any additional interaction from the user here.
-        """
-        logger.debug("Using API keys, no need to authenticate.")
-
-    @property
-    def authenticated(self) -> bool:
-        """Checks the user is authenticated.
-
-        This class is using API keys which
-        cannot be checked in a meaningful way locally.
-        """
-        logger.debug("Using API keys, assuming they are valid.")
-        return True
-
-    @property
-    def username(self) -> str:
-        """Authenticated user's username.
-
-        In the case of API keys we have relied on the user providing this.
-        If it's incorrect then their API calls will fail,
-        but we can't meaningfully check this locally.
-        """
-        return self._username
+@dataclass
+class _AuthEnv:
+    """Captures the combined authorisation information."""
 
+    name: str
+    auth_domain: str
+    client_id: str
 
-class ExternallyManagedJWTHandler(AuthenticationHandler):
-    """Authenticates user via JWT from an external source.
 
-    This can provide a JWT to the `BitfountSession` that is managed
-    by another application.
+def _get_auth_environment() -> _AuthEnv:
+    """Determines the auth settings based on environment variables.
 
-    The Bitfount library hands responsibility for management of the
-    token to the external source.
-    Whenever a new token is needed it makes a call to the `get_token`
-    hook which provides one.
+    Returns:
+        A tuple containing the auth domain and client ID for the given environment.
     """
+    environment = _get_environment()
+    if environment == _STAGING_ENVIRONMENT:
+        return _AuthEnv("staging", _STAGING_AUTH_DOMAIN, _STAGING_CLIENT_ID)
+    if environment == _DEVELOPMENT_ENVIRONMENT:
+        return _AuthEnv("development", _DEVELOPMENT_AUTH_DOMAIN, _DEVELOPMENT_CLIENT_ID)
+    return _AuthEnv("production", _PRODUCTION_AUTH_DOMAIN, _PRODUCTION_CLIENT_ID)
 
-    def __init__(
-        self,
-        jwt: str,
-        expires: datetime,
-        get_token: Callable[[], Tuple[str, datetime]],
-        username: str,
-    ):
-        super().__init__(username)
-        self._jwt = jwt
-        self._expires = expires
-        self._get_token: Callable[[], Tuple[str, datetime]] = get_token
-        self._username = username
-
-        if self._username == _DEFAULT_USERNAME:
-            # username must be explicitly set in __init__ to use OAuth Application
-            raise AuthenticatedUserError(
-                "Must specify a username when using OAuth Application."
-            )
-
-    @property
-    def hub_request_headers(self) -> Dict:
-        """Header for authenticating with hub."""
-        return {"authorization": f"Bearer {self._jwt}"}
 
-    @property
-    def am_request_headers(self) -> Dict:
-        """Header for authenticating with access manager."""
-        return {"authorization": f"Bearer {self._jwt}"}
+class AuthEnvironmentError(BitfountError):
+    """Exception related to the authorization and authentication environment."""
 
-    @property
-    def message_service_request_metadata(self) -> List[Tuple[str, str]]:
-        """Metadata for authenticating with message service."""
-        return [("token", self._jwt)]
-
-    def authenticate(self) -> None:
-        """Retrieves a token from the token source.
+    pass
 
-        Calls the hook provided on object creation to retrieve a new token.
-        """
-        if not self.authenticated:
-            token, expires = self._get_token()
-            self._jwt = token
-            self._expires = expires
 
-    @property
-    def authenticated(self) -> bool:
-        """Whether the token is still valid."""
-        return (self._expires - timedelta(minutes=1)) > datetime.now(timezone.utc)
+class BitfountSession(requests.Session):
+    """Manages session-based interactions with Bitfount.
 
-    @property
-    def username(self) -> str:
-        """Username of authenticated user."""
-        return self._username
-
-
-class DeviceCodeFlowHandler(AuthenticationHandler):
-    """Manages token retrieval and refresh for interactions with Bitfount.
-
-    Extends `requests.Session`, appending an access token to the
+    Extends requests.Session, appending an access token to the
     authorization of any requests made if an access token is present
 
     When the token expires it will request a new token prior to
     sending the web request.
 
-    Attributes:
-        access_token_expires_at: The time at which the access token expires.
-        device_code: The device code returned by the Bitfount API.
-        device_code_arrival_time: The time at which the device code was issued.
-        id_token: The ID token returned by the Bitfount API.
-        refresh_token: The refresh token returned by the Bitfount API.
-        token_file: The path to the file where the token is stored.
-        token_request_interval: The time between token requests.
+    Usage:
+        `session = BitfountSession(...)`
+        # When you want the user to interact to start the session:
+        `session.authenticate()`
+        # The session can then be used as a normal requests.Session
     """
 
     def __init__(
         self,
         auth_domain: str = _PRODUCTION_AUTH_DOMAIN,
         client_id: str = _PRODUCTION_CLIENT_ID,
+        user_storage_path: Optional[Path] = None,
         scopes: str = _SCOPES,
         audience: str = _HUB_API_IDENTIFIER,
-        username: str = _DEFAULT_USERNAME,
     ):
-        super().__init__(username)
-        self._access_token: Optional[str] = None
-        self._auth_domain = auth_domain
-        self._client_id = client_id
-        self._scopes: str = scopes
-        self._audience: str = audience
-        self._device_code_endpoint: str = (
-            f"https://{self._auth_domain}/oauth/device/code"
-        )
-        self._token_endpoint: str = f"https://{self._auth_domain}/oauth/token"
-        self._username = username
+        super().__init__()
+
+        self.auth_domain = auth_domain
+        self.client_id = client_id
+        # This default value cannot be set in the signature as otherwise it will
+        # appear in the generated docs with whichever dev's path was being used.
+        if user_storage_path is None:
+            user_storage_path = BITFOUNT_STORAGE_PATH / _DEFAULT_USER_DIRECTORY
+        self.user_storage_path: Path = user_storage_path
+        self.scopes: str = scopes
+        self.audience: str = audience
+
+        self.device_code_endpoint: str = f"https://{self.auth_domain}/oauth/device/code"
+        self.token_endpoint: str = f"https://{self.auth_domain}/oauth/token"
 
-        self.access_token_expires_at: Optional[datetime] = None
         self.device_code: Optional[str] = None
         self.device_code_arrival_time: Optional[datetime] = None
         self.device_code_expires_in: Optional[int] = None
-        self.id_token: Optional[str] = None
+        self.token_request_interval: Optional[int] = None
+        self._access_token: Optional[str] = None
         self.refresh_token: Optional[str] = None
+        self.id_token: Optional[str] = None
+        self.access_token_expires_at: Optional[datetime] = None
         self.token_file: Path = self.user_storage_path / ".token"
-        self.token_request_interval: Optional[int] = None
+
+        self.reauthentication_lock = threading.Lock()
 
     @property
-    def hub_request_headers(self) -> Dict:
-        """Header for authenticating with hub."""
-        return {"authorization": f"Bearer {self._access_token}"}
+    def username(self) -> str:
+        """Returns the username of the authenticated user."""
+        return self._get_username_from_id_token()
+
+    @property
+    def access_token(self) -> Optional[str]:
+        """Returns the access token. Tries to refresh it if needed."""
+        with self.reauthentication_lock:
+            if self.expired:
+                self.authenticate()
+            return self._access_token
 
     @property
-    def am_request_headers(self) -> Dict:
-        """Header for authenticating with access manager."""
-        return {"authorization": f"Bearer {self._access_token}"}
+    def authenticated(self) -> bool:
+        """Returns true if we have an unexpired access token."""
+        return bool(self._access_token and not self.expired)
 
     @property
-    def message_service_request_metadata(self) -> List[Tuple[str, str]]:
-        """Metadata for authenticating with message service."""
-        return [("token", cast(str, self._access_token))]
+    def expired(self) -> bool:
+        """Whether or not the access token has expired.
+
+        Returns: True if the token has expired
+        """
+        # Either both attributes will be present or neither will be
+        if self._access_token and self.access_token_expires_at:
+            return self.access_token_expires_at <= datetime.now(timezone.utc)
+        else:
+            return False
+
+    def _verify_user_storage_path(self) -> None:
+        """Verifies that user storage path corresponds to username.
+
+        Raises:
+            AssertionError: if user storage path corresponds to a different username
+                from the BitfountSession.
+        """
+        if not str(self.user_storage_path).endswith(
+            (_DEFAULT_USER_DIRECTORY, self.username)
+        ):
+            provided_user = self.user_storage_path.stem
+            raise AssertionError(
+                f"BitfountSession connected to {self.username}. "
+                f"This does not match the provided user storage path: {provided_user}"
+            )
 
     def authenticate(self) -> None:
         """Authenticates user to allow protected requests.
 
         Prompts the user to login/authenticate and stores the tokens to use them
         in future requests.
 
         Raises:
-            AssertionError: If user storage path corresponds to a different username
+            AssertionError: if user storage path corresponds to a different username
                 from the BitfountSession.
-            ConnectionError: If a token cannot be retrieved.
         """
         self._load_token_from_file(self.token_file)
 
         # Refresh the loaded token if it has expired
         refreshed = False
-        if self.access_token_expires_at and not self.authenticated:
+        if self.expired:
             refreshed = self._refresh_access_token()
 
         # Force user to go through login flow if we didn't refresh the token
         # Or if we haven't loaded an authenticated token
         if not self.authenticated and not refreshed:
             user_code, verification_uri = self._fetch_device_code()
             self._do_verification(user_code, verification_uri)
@@ -334,164 +215,24 @@
         self._verify_user_storage_path()
 
         # Ensure directory path exists
         self.user_storage_path.mkdir(parents=True, exist_ok=True)
         self._save_token_to_file(self.token_file)
         logger.info(f'Logged into Bitfount as "{self.username}"')
 
-    @property
-    def authenticated(self) -> bool:
-        """Whether the access token is valid.
-
-        Returns: True if the token is valid
-        """
-        # Either both attributes will be present or neither will be
-        if self._access_token and self.access_token_expires_at:
-            # If the token expires in the next 10 minutes, refresh
-            return (
-                self.access_token_expires_at - timedelta(minutes=10)
-            ) > datetime.now(timezone.utc)
-        else:
-            return False
-
-    @property
-    def username(self) -> str:
-        """Username of the authenticated user."""
-        username_from_id_token = self._get_username_from_id_token()
-        if self._username != _DEFAULT_USERNAME:
-            if self._username != username_from_id_token:
-                raise AuthenticatedUserError(
-                    f"DeviceCodeFlowHandler object was created for {self._username} but"
-                    f" authentication was done against {username_from_id_token}"
-                )
-            return self._username
-        return username_from_id_token
-
-    def _refresh_access_token(self) -> bool:
-        """Attempts to refresh the access token.
-
-        Returns: True if the token was refreshed, false otherwise
-        """
-        token_response = self._send_token_request(refresh=True)
-
-        if token_response.status_code == 200:
-            token_response_json: _TokenRefreshResponseJSON = token_response.json()
-            self._access_token = token_response_json["access_token"]
-            self.refresh_token = token_response_json["refresh_token"]
-            self.id_token = token_response_json["id_token"]
-            self.access_token_expires_at = datetime.now(timezone.utc) + timedelta(
-                seconds=token_response_json["expires_in"]
-            )
-            return True
-        logger.warning(
-            f"Failed to refresh access token, response was: {token_response.text}"
-        )
-        return False
-
-    def _send_token_request(self, refresh: bool = False) -> Response:
-        """Sends a request to the Auth Server token endpoint to get a new token."""
-        if refresh:
-            # See: https://auth0.com/docs/api/authentication?http#refresh-token
-            # If refreshing, must have refresh_token. Reassure mypy.
-            assert self.refresh_token is not None  # nosec assert_used
-            refresh_request_data: _TokenRefreshRequestDict = {
-                "client_id": self._client_id,
-                "grant_type": "refresh_token",
-                "refresh_token": self.refresh_token,
-            }
-            return web_utils.post(self._token_endpoint, data=refresh_request_data)
-        else:
-            # See: https://auth0.com/docs/api/authentication?http#device-authorization-flow48  # noqa: B950
-            # device_code must already be set by the time we are calling this.
-            # Reassure mypy.
-            assert self.device_code is not None  # nosec assert_used
-            request_data: _DeviceAccessTokenRequestDict = {
-                "client_id": self._client_id,
-                "grant_type": _DEVICE_CODE_GRANT_TYPE,
-                "device_code": self.device_code,
-            }
-            return web_utils.post(self._token_endpoint, data=request_data)
-
-    def _save_token_to_file(self, token_file: Path) -> None:
-        """Saves authentication token to file.
-
-        Saves all fields that are necessary to reproduce this object to a file.
-        """
-        # This will be set by the time this is called. Reassure mypy.
-        assert self.access_token_expires_at is not None  # nosec assert_used
-        json.dump(
-            {
-                "access_token": self._access_token,
-                "refresh_token": self.refresh_token,
-                "id_token": self.id_token,
-                "access_token_expires_at": self.access_token_expires_at.timestamp(),
-                "auth_domain": self._auth_domain,
-                "client_id": self._client_id,
-                "scopes": self._scopes,
-                "audience": self._audience,
-            },
-            token_file.open("w"),
-        )
-
-    def _load_token_from_file(self, token_file: Path) -> None:
-        """Loads authentication token from file.
-
-        Attempts to load the data needed for authentication, when loaded it updates
-        the fields on the instance.
-
-        If the data is found but the metadata differs then it will not update
-        the fields.
-
-        If no data, or no file is found it will just return without error.
-        """
-        if token_file.exists():
-            try:
-                with token_file.open() as tf:
-                    serialized_tokens = json.load(tf)
-            except (JSONDecodeError, IOError) as e:
-                logger.warning(
-                    f"Unable to read existing token file ({str(token_file)}),"
-                    f" will require new login: {e}"
-                )
-                return
-
-            auth_domain = serialized_tokens["auth_domain"]
-            client_id = serialized_tokens["client_id"]
-            scopes = serialized_tokens["scopes"]
-            audience = serialized_tokens["audience"]
-
-            if (
-                self._auth_domain != auth_domain
-                or self._client_id != client_id
-                or self._scopes != scopes
-                or self._audience != audience
-            ):
-                print(
-                    "Stored tokens are no longer valid, "
-                    "fresh authentication is necessary"
-                )
-                return
-
-            self._access_token = serialized_tokens["access_token"]
-            self.refresh_token = serialized_tokens["refresh_token"]
-            self.id_token = serialized_tokens["id_token"]
-            self.access_token_expires_at = datetime.fromtimestamp(
-                serialized_tokens["access_token_expires_at"], tz=timezone.utc
-            )
-
     def _fetch_device_code(self) -> Tuple[str, str]:
         """Fetches device code."""
         # See: https://auth0.com/docs/api/authentication?http#device-authorization-flow
         request_data: _DeviceCodeRequestDict = {
-            "client_id": self._client_id,
-            "scope": self._scopes,
-            "audience": self._audience,
+            "client_id": self.client_id,
+            "scope": self.scopes,
+            "audience": self.audience,
         }
-        device_code_response: Response = web_utils.post(
-            self._device_code_endpoint,
+        device_code_response: Response = requests.post(
+            self.device_code_endpoint,
             data=request_data,
         )
         device_code_response.raise_for_status()
 
         response_json: _DeviceCodeResponseJSON = device_code_response.json()
         verification_uri: str = response_json["verification_uri_complete"]
         user_code: str = response_json["user_code"]
@@ -508,30 +249,30 @@
     def _do_verification(self, user_code: str, verification_uri: str) -> None:
         """Opens web browser for verification."""
         print(f"Your confirmation code is: {user_code}")
         time.sleep(1)  # Give the user a second to see the code before opening browser
         webbrowser.open(verification_uri)
         print(
             "A browser window has been opened, "
-            "please log in to confirm your identity."
+            "please login and confirm that this tool can manage your pod."
         )
         print("If no browser window has opened, then please visit the following URL:")
         print(verification_uri)
 
     def _exchange_device_code_for_token(self) -> None:
         """Exchanges device code for token."""
         token_response: Optional[
             Union[_DeviceAccessTokenResponseJSON, _DeviceAccessTokenFailResponseJSON]
         ] = None
 
         # This method should only be called after a call to _fetch_device_code
         # so these will be set. Asserts to reassure mypy.
-        assert self.device_code_arrival_time is not None  # nosec assert_used
-        assert self.device_code_expires_in is not None  # nosec assert_used
-        assert self.token_request_interval is not None  # nosec assert_used
+        assert self.device_code_arrival_time is not None  # nosec
+        assert self.device_code_expires_in is not None  # nosec
+        assert self.token_request_interval is not None  # nosec
 
         interval = self.token_request_interval
 
         while not self._device_code_expired(
             self.device_code_arrival_time, self.device_code_expires_in
         ):
             response: Response = self._send_token_request()
@@ -606,55 +347,161 @@
                 seconds=token_response["expires_in"],
             )
         else:
             raise ConnectionError(
                 "Failed to retrieve a device code from the authentication server"
             )
 
+    def _send_token_request(self, refresh: bool = False) -> Response:
+        """Sends a request to the Auth Server token endpoint to get a new token."""
+        if refresh:
+            # See: https://auth0.com/docs/api/authentication?http#refresh-token
+            # If refreshing, must have refresh_token. Reassure mypy.
+            assert self.refresh_token is not None  # nosec
+            refresh_request_data: _TokenRefreshRequestDict = {
+                "client_id": self.client_id,
+                "grant_type": "refresh_token",
+                "refresh_token": self.refresh_token,
+            }
+            return requests.post(self.token_endpoint, data=refresh_request_data)
+        else:
+            # See: https://auth0.com/docs/api/authentication?http#device-authorization-flow48  # noqa: B950
+            # device_code must already be set by the time we are calling this.
+            # Reassure mypy.
+            assert self.device_code is not None  # nosec
+            request_data: _DeviceAccessTokenRequestDict = {
+                "client_id": self.client_id,
+                "grant_type": _DEVICE_CODE_GRANT_TYPE,
+                "device_code": self.device_code,
+            }
+            return requests.post(self.token_endpoint, data=request_data)
+
+    def request(  # type: ignore[no-untyped-def,override]
+        self, method, url, params=None, data=None, headers=None, **kwargs
+    ) -> Response:
+        """Performs an HTTP request.
+
+        Overrides requests.session.request, appending our access token
+        to the request headers.
+
+        Also refreshes the token if it has expired, prior to sending
+        the request provided.
+        """
+        if self.access_token:
+            # We now have a valid access token
+            if not headers:
+                headers = {}
+            headers["authorization"] = f"Bearer {self._access_token}"
+
+        return super().request(
+            method, url, params=params, data=data, headers=headers, **kwargs
+        )
+
+    def _refresh_access_token(self) -> bool:
+        """Attempts to refresh the access token.
+
+        Returns: True if the token was refreshed, false otherwise
+
+        """
+        token_response = self._send_token_request(refresh=True)
+
+        if token_response.status_code == 200:
+            token_response_json: _TokenRefreshResponseJSON = token_response.json()
+            self._access_token = token_response_json["access_token"]
+            self.refresh_token = token_response_json["refresh_token"]
+            self.id_token = token_response_json["id_token"]
+            self.access_token_expires_at = datetime.now(timezone.utc) + timedelta(
+                seconds=token_response_json["expires_in"]
+            )
+            return True
+        logger.warning(
+            f"Failed to refresh access token, response was: {token_response.text}"
+        )
+        return False
+
     @staticmethod
     def _device_code_expired(arrival_time: datetime, expires_in_seconds: int) -> bool:
         """Checks if authorization code has expired.
 
         Checks if too much time has passed between the device code
         being issued by the Auth Server and the user approving access
         using that device code.
         """
         return datetime.now(timezone.utc) >= arrival_time + timedelta(
             seconds=expires_in_seconds
         )
 
+    def _save_token_to_file(self, token_file: Path) -> None:
+        """Saves authentication token to file.
+
+        Saves all fields that are necessary to reproduce this object to a file.
+        """
+        # This will be set by the time this is called. Reassure mypy.
+        assert self.access_token_expires_at is not None  # nosec
+        json.dump(
+            {
+                "access_token": self._access_token,
+                "refresh_token": self.refresh_token,
+                "id_token": self.id_token,
+                "access_token_expires_at": self.access_token_expires_at.timestamp(),
+                "auth_domain": self.auth_domain,
+                "client_id": self.client_id,
+                "scopes": self.scopes,
+                "audience": self.audience,
+            },
+            token_file.open("w"),
+        )
+
+    def _load_token_from_file(self, token_file: Path) -> None:
+        """Loads authentication token from file.
+
+        Attempts to load the data needed for authentication, when loaded it updates
+        the fields on the instance.
+
+        If the data is found but the metadata differs then it will not update
+        the fields.
+
+        If no data, or no file is found it will just return without error.
+        """
+        if token_file.exists():
+            serialized_tokens = json.load(token_file.open())
+
+            auth_domain = serialized_tokens["auth_domain"]
+            client_id = serialized_tokens["client_id"]
+            scopes = serialized_tokens["scopes"]
+            audience = serialized_tokens["audience"]
+
+            if (
+                self.auth_domain != auth_domain
+                or self.client_id != client_id
+                or self.scopes != scopes
+                or self.audience != audience
+            ):
+                print(
+                    "Stored tokens are no longer valid, "
+                    "fresh authentication is necessary"
+                )
+                return
+
+            self._access_token = serialized_tokens["access_token"]
+            self.refresh_token = serialized_tokens["refresh_token"]
+            self.id_token = serialized_tokens["id_token"]
+            self.access_token_expires_at = datetime.fromtimestamp(
+                serialized_tokens["access_token_expires_at"], tz=timezone.utc
+            )
+
     def _get_username_from_id_token(self) -> str:
         """Extracts the Bitfount username from the token.
 
         Note: This function performs no verification of the id_token signature
         and should only be used in situations where the username in the token
         is not used to make decisions. As this is not backend code (i.e.
         anyone can edit this) we aren't very concerned about the fact it is
         not verified.
         """
-        if self.id_token is None:
-            raise AuthenticatedUserError(
-                "User not authenticated yet, call authenticate() before accessing"
-                " the ID token"
-            )
-
+        # Calling function assures authenticated. Reassure mypy.
+        assert self.id_token is not None  # nosec
         # Decode the ID token without verification
         id_token_decode: Dict[str, str] = jwt.decode(
             self.id_token, options={"verify_signature": False}
         )
         return id_token_decode[_USERNAME_KEY]
-
-    def _verify_user_storage_path(self) -> None:
-        """Verifies that user storage path corresponds to username.
-
-        Raises:
-            AssertionError: if user storage path corresponds to a different username
-                from the BitfountSession.
-        """
-        # User storage should either be for the default username or the
-        # authenticated user
-        if not str(self.user_storage_path).endswith((_DEFAULT_USERNAME, self.username)):
-            provided_user = self.user_storage_path.stem
-            raise AuthenticatedUserError(
-                f"BitfountSession connected to {self.username}. "
-                f"This does not match the provided user storage path: {provided_user}"
-            )
```

### Comparing `bitfount-0.5.86/bitfount/hub/helper.py` & `bitfount-0.5.9/bitfount/hub/helper.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,140 +1,99 @@
 """Helper functions related to hub and AM interactions."""
 import logging
-import os
 from pathlib import Path
 from typing import Dict, Iterable, Mapping, Optional, Union
 
 from cryptography.hazmat.primitives.asymmetric.rsa import RSAPublicKey
 import yaml
 
 from bitfount.config import (
     _DEVELOPMENT_ENVIRONMENT,
-    _SANDBOX_ENVIRONMENT,
     _STAGING_ENVIRONMENT,
     BITFOUNT_KEY_STORE,
+    BITFOUNT_STORAGE_PATH,
     _get_environment,
 )
 from bitfount.data.schema import BitfountSchema
 from bitfount.federated.encryption import _RSAEncryption
-from bitfount.hub.api import BitfountAM, BitfountHub
-from bitfount.hub.authentication_flow import (
-    _DEVELOPMENT_AUTH_DOMAIN,
-    _DEVELOPMENT_CLIENT_ID,
-    _STAGING_AUTH_DOMAIN,
-    _STAGING_CLIENT_ID,
-    BitfountSession,
-)
-from bitfount.hub.authentication_handlers import (
-    _DEFAULT_USERNAME,
-    APIKeysHandler,
-    AuthenticationHandler,
-    DeviceCodeFlowHandler,
-    ExternallyManagedJWTHandler,
-)
-from bitfount.hub.exceptions import PodDoesNotExistError
-from bitfount.hub.types import (
+from bitfount.hub.api import (
     _DEV_AM_URL,
     _DEV_HUB_URL,
-    _SANDBOX_AM_URL,
-    _SANDBOX_HUB_URL,
     _STAGING_AM_URL,
     _STAGING_HUB_URL,
     PRODUCTION_AM_URL,
     PRODUCTION_HUB_URL,
+    BitfountAM,
+    BitfountHub,
+)
+from bitfount.hub.authentication_flow import (
+    _DEFAULT_USER_DIRECTORY,
+    _DEVELOPMENT_AUTH_DOMAIN,
+    _DEVELOPMENT_CLIENT_ID,
+    _STAGING_AUTH_DOMAIN,
+    _STAGING_CLIENT_ID,
+    BitfountSession,
 )
-from bitfount.runners.config_schemas import JWT, APIKeys
 
 logger = logging.getLogger(__name__)
 
 
 def _create_bitfount_session(
-    url: str,
-    username: Optional[str] = None,
-    secrets: Optional[Union[APIKeys, JWT]] = None,
+    url: str, username: Optional[str] = None
 ) -> BitfountSession:
     """Creates a relevant Bitfount Session according to the environment.
 
     Args:
         url: Bitfount hub URL.
         username: Optional. Username. Defaults to DEFAULT_USER_DIRECTORY.
-        secrets: Optional
 
     Returns:
         BitfountSession object.
     """
-    username = username if username else _DEFAULT_USERNAME
+    username = username if username else _DEFAULT_USER_DIRECTORY
+    user_storage_path = BITFOUNT_STORAGE_PATH / username
 
-    handler: AuthenticationHandler
-    if secrets:
-        if isinstance(secrets, APIKeys):
-            handler = APIKeysHandler(
-                api_key_id=secrets.access_key_id,
-                api_key=secrets.access_key,
-                username=username,
-            )
-        else:
-            handler = ExternallyManagedJWTHandler(
-                jwt=secrets.jwt,
-                expires=secrets.expires,
-                get_token=secrets.get_token,
-                username=username,
-            )
+    if url == _STAGING_HUB_URL:
+        session = BitfountSession(
+            _STAGING_AUTH_DOMAIN, _STAGING_CLIENT_ID, user_storage_path
+        )
+    elif "localhost" in url:
+        session = BitfountSession(
+            auth_domain=_DEVELOPMENT_AUTH_DOMAIN,
+            client_id=_DEVELOPMENT_CLIENT_ID,
+            user_storage_path=user_storage_path,
+        )
     else:
-        if os.getenv("BITFOUNT_API_KEY_ID") and os.getenv("BITFOUNT_API_KEY"):
-            # If no secrets are provided in the config, but API keys are set
-            # in the environment then they are used by default
-            handler = APIKeysHandler(
-                os.environ["BITFOUNT_API_KEY_ID"],
-                os.environ["BITFOUNT_API_KEY"],
-                username=username,
-            )
-        elif url == _STAGING_HUB_URL:
-            handler = DeviceCodeFlowHandler(
-                auth_domain=_STAGING_AUTH_DOMAIN,
-                client_id=_STAGING_CLIENT_ID,
-                username=username,
-            )
-        elif "localhost" in url:
-            handler = DeviceCodeFlowHandler(
-                auth_domain=_DEVELOPMENT_AUTH_DOMAIN,
-                client_id=_DEVELOPMENT_CLIENT_ID,
-                username=username,
-            )
-        else:
-            handler = DeviceCodeFlowHandler(username=username)
-    session = BitfountSession(handler)
+        session = BitfountSession(user_storage_path=user_storage_path)
+
     return session
 
 
 def _default_bitfounthub(
     hub: Optional[BitfountHub] = None,
     username: Optional[str] = None,
     url: Optional[str] = None,
-    secrets: Optional[Union[APIKeys, JWT]] = None,
 ) -> BitfountHub:
     """Gets a default BitfountHub instance if one is not specified.
 
     Args:
         hub: Optional. The BitfountHub instance to use if it exists.
         username: Optional. Username.
         url: Optional. Bitfount hub URL.
 
     Returns:
         BitfountHub object representing the hub.
     """
     if not hub:
-        return _create_bitfounthub(username, url, secrets)
+        return _create_bitfounthub(username, url)
     return hub
 
 
 def _create_bitfounthub(
-    username: Optional[str] = None,
-    url: Optional[str] = None,
-    secrets: Optional[Union[APIKeys, JWT]] = None,
+    username: Optional[str] = None, url: Optional[str] = None
 ) -> BitfountHub:
     """Creates bitfounthub object.
 
     Args:
         username: Optional. Username.
         url: Optional. Bitfount hub URL. Will use the environment default if not
              supplied.
@@ -144,26 +103,23 @@
     """
     if not url:
         environment = _get_environment()
         if environment == _STAGING_ENVIRONMENT:
             url = _STAGING_HUB_URL
         elif environment == _DEVELOPMENT_ENVIRONMENT:
             url = _DEV_HUB_URL
-        elif environment == _SANDBOX_ENVIRONMENT:
-            url = _SANDBOX_HUB_URL
         else:
             url = PRODUCTION_HUB_URL
-    session = _create_bitfount_session(url=url, username=username, secrets=secrets)
+    session = _create_bitfount_session(url=url, username=username)
     session.authenticate()
     return BitfountHub(session=session, url=url)
 
 
 def _create_access_manager(
-    session: BitfountSession,
-    url: Optional[str] = None,
+    session: BitfountSession, url: Optional[str] = None
 ) -> BitfountAM:
     """Creates and returns Bitfount Access Manager.
 
     Args:
         session: Bitfount session for authentication.
         url: Optional. URL of the access manager. Will use the environment default
              if not supplied.
@@ -173,101 +129,97 @@
     """
     if not url:
         environment = _get_environment()
         if environment == _STAGING_ENVIRONMENT:
             url = _STAGING_AM_URL
         elif environment == _DEVELOPMENT_ENVIRONMENT:
             url = _DEV_AM_URL
-        elif environment == _SANDBOX_ENVIRONMENT:
-            url = _SANDBOX_AM_URL
         else:
             url = PRODUCTION_AM_URL
 
     return BitfountAM(session, url)
 
 
-def _save_key_to_key_store(
-    key_store_path: Path, pod_identifier: str, serialized_pod_key: str
-) -> None:
-    """Save pod keys to the key store.
-
-    This will override any previously saved keys.
+def _get_access_manager_public_key(session: BitfountSession) -> RSAPublicKey:
+    """Creates access manager and returns corresponding public key.
 
     Args:
-        key_store_path: The path to where the key files are stored
-        pod_identifier: The pod identifier of the pod we are saving the key for.
-        serialized_pod_key: The serialized pod key
-    """
-    with open(key_store_path, "r") as key_store:
-        known_pod_keys: Dict[str, str] = yaml.safe_load(key_store) or {}
+        session: Bitfount session for authenticated HTTP calls.
 
-    known_pod_keys[pod_identifier] = serialized_pod_key
-
-    with open(key_store_path, "w") as key_store:
-        logger.debug(f"Saving pod public keys to {key_store_path}")
-        yaml.safe_dump(known_pod_keys, key_store)
+    Returns:
+        The access manager's public key.
+    """
+    access_manager = _create_access_manager(session)
+    return access_manager.get_access_manager_key()
 
 
-def _check_known_pods(
-    pod_name: str, pod_public_key: RSAPublicKey, key_store_path: Path
-) -> RSAPublicKey:
+def _check_known_pods(pod_name: str, pod_public_key: RSAPublicKey) -> RSAPublicKey:
     """Checks known pods still have valid public key.
 
     Checks pod public key is the same as in BITFOUNT_KEY_STORE
     If BITFOUNT_KEY_STORE does not exist, it creates it and adds the key
     If it does exist but the key is different, it prompts the user to
     accept or reject new key.
 
     Returns:
         The new or existing key (depending on which the user chooses) as a string.
     """
-    # Serialise to allow comparison with stored keys
+    key_store_path = Path(BITFOUNT_KEY_STORE).expanduser()
+
+    # Create key store file if file doesn't exist
+    if not key_store_path.is_file():
+        logger.info("Creating key store for pod public keys...")
+        key_store_path.parent.mkdir(parents=True, exist_ok=True)
+        key_store_path.touch()
+
+    # Check whether the provided key matches the existing key, request permission
+    # to update if not.
     serialized_pod_key: str = _RSAEncryption.serialize_public_key(
         pod_public_key
     ).decode()
-
-    # Check if target key already exists in the key store
     with open(key_store_path, "r") as key_store:
         known_pod_keys: Dict[str, str] = yaml.safe_load(key_store) or {}
         serialized_current_pod_key: Optional[str] = known_pod_keys.get(pod_name, None)
 
         if serialized_current_pod_key is None:
-            # No current key for this user, ok to "update" it (i.e. add it)
+            # No current key for this user, ok to "update" it
             logger.debug(f"No existing key found for pod {pod_name}")
         elif serialized_current_pod_key != serialized_pod_key:
-            # Key mismatch, check with user for how to proceed
             logger.error(
                 f"{pod_name} public key has changed. \n"
                 f"Please double check the key has really changed and this "
                 f"is not an attack."
             )
             while True:
                 # We are only using python3 so this is secure
-                response = input("Do you want to trust the new public key? (y/n)")
+                response = input(  # nosec
+                    "Do you want to trust the new public key? (y/n)"
+                )
                 if response.lower() == "y":
                     # Break out of the loop
                     break
                 elif response.lower() == "n":
                     # Use the old public key
                     return _RSAEncryption.load_public_key(
                         serialized_current_pod_key.encode()
                     )
                 else:
                     print("Didn't catch that, please type 'y' or 'n'")
-        else:
-            # Key matches, just return it
-            logger.info(f"Found public key for {pod_name} in key store.")
-            return pod_public_key
 
-    # Save known pod key with the new one. We do this here because either
+    # Update known pod key with the new one. We do this here because either
     # the user has approved or there was no key for that user in the first
     # place.
-    logger.info(f"Saving public key for {pod_name} in key store.")
-    _save_key_to_key_store(key_store_path, pod_name, serialized_pod_key)
+    known_pod_keys[pod_name] = serialized_pod_key
+
+    # Save out the keys
+    with open(key_store_path, "w") as key_store:
+        logger.debug(f"Saving pod public keys to {key_store_path}")
+        yaml.safe_dump(known_pod_keys, key_store)
 
+    logger.info(f"Found public key for {pod_name} in key store.")
     # The new pod key has been saved so we can just return it
     return pod_public_key
 
 
 def _get_pod_public_key(
     pod_identifier: str,
     hub: BitfountHub,
@@ -277,54 +229,36 @@
 
     Either loads it from disk or downloads it from BitfountHub.
 
     Args:
         pod_identifier: The pod identifier of the pod we get the public key for.
         hub: Hub to download keys from.
         pod_public_key_paths: Optional. Mapping of pod identifiers to already
-            existing key files.
+                              existing keys.
 
     Returns:
         The public key for the pod.
-
-    Raises:
-        PodDoesNotExistError: If the pod does not exist.
     """
-    key_store_path = Path(BITFOUNT_KEY_STORE).expanduser()
-
-    # Create key store file if file doesn't exist
-    if not key_store_path.is_file():
-        logger.info("Creating key store for pod public keys...")
-        key_store_path.parent.mkdir(parents=True, exist_ok=True)
-        key_store_path.touch()
-
-    # Check for the target key against explicitly provided key files
     if pod_public_key_paths:
-        logger.debug(f"Checking for public key file for {pod_identifier}")
+        logger.info(f"Checking if we already have the public key for {pod_identifier}")
         try:
             pod_public_key_path: Path = pod_public_key_paths[pod_identifier]
             existing_pod_public_key = _RSAEncryption.load_public_key(
                 pod_public_key_path
             )
-            return _check_known_pods(
-                pod_identifier, existing_pod_public_key, key_store_path
-            )
+            return _check_known_pods(pod_identifier, existing_pod_public_key)
         except KeyError:
             # We have some pod public key files, but not for this pod identifier
             logger.debug(f"No existing public key file for {pod_identifier}")
 
-    # If no key files at all, or just not for that pod, retrieve key
-    if pod_key := hub.get_pod_key(pod_identifier=pod_identifier):
-        pod_public_key = _RSAEncryption.load_public_key(pod_key.encode())
-
-        # Check that the downloaded key matches any existing one we have and save
-        # it to the key store.
-        return _check_known_pods(pod_identifier, pod_public_key, key_store_path)
-    else:
-        raise PodDoesNotExistError(f"No public key found for pod: {pod_identifier}")
+    # If no key files at all, or just not for that pod, retrieve it
+    # TODO: [BIT-1050] Does this downloaded key need to be saved in the key store?
+    return _RSAEncryption.load_public_key(
+        hub.get_pod_key(pod_identifier=pod_identifier).encode()
+    )
 
 
 def _get_pod_public_keys(
     pod_identifiers: Iterable[str],
     hub: BitfountHub,
     pod_public_key_paths: Optional[Mapping[str, Path]] = None,
 ) -> Dict[str, RSAPublicKey]:
@@ -349,36 +283,31 @@
     return loaded_pod_public_keys
 
 
 def get_pod_schema(
     pod_identifier: str,
     save_file_path: Optional[Union[str, Path]] = None,
     hub: Optional[BitfountHub] = None,
-    username: Optional[str] = None,
 ) -> BitfountSchema:
     """Get a pod's schema from the hub.
 
     Args:
         pod_identifier: The identifier of the pod. If supplied with only pod name
-            assumes the namespace is the current user.
+                        assumes the namespace is the current user.
         save_file_path: Optional. Path to save the downloaded schema to. Won't save
-            if not provided.
+                   if not provided.
         hub: Optional. The BitfountHub to connect to. The default hub will be
              used if not provided.
-        username: The username to use when connecting to the hub if a hub instance is
-            not provided.
 
     Returns:
         The loaded BitfountSchema object.
     """
     # Generate default hub if not provided
     if not hub:
-        hub = _create_bitfounthub(username=username)
-    elif username:
-        logger.warning("Ignoring username argument as hub was provided.")
+        hub = _create_bitfounthub()
 
     # Check if full pod_identifier or pod name only
     if "/" not in pod_identifier:
         # Construct full pod identifier if needed
         pod_identifier = f"{hub.username}/{pod_identifier}"
 
     schema = hub.get_pod_schema(pod_identifier)
```

### Comparing `bitfount-0.5.86/bitfount/metrics.py` & `bitfount-0.5.9/bitfount/metrics.py`

 * *Files 20% similar despite different names*

```diff
@@ -15,14 +15,16 @@
     MutableMapping,
     Optional,
     Tuple,
     Union,
     cast,
 )
 
+import matplotlib
+import matplotlib.pyplot as plt
 import numpy as np
 import pandas as pd
 from scipy.stats import kstest
 from sklearn.metrics import (
     accuracy_score,
     brier_score_loss,
     confusion_matrix,
@@ -31,181 +33,50 @@
     mean_squared_error,
     precision_score,
     r2_score,
     recall_score,
     roc_auc_score,
 )
 
-from bitfount.models.base_models import ClassifierMixIn, RegressorMixIn, _BaseModel
-from bitfount.utils import delegates
+from bitfount.models.base_models import ClassifierMixIn, _BaseModel
 
 if TYPE_CHECKING:
     from bitfount.types import DistributedModelProtocol
 
 __all__: List[str] = [
     "BINARY_CLASSIFICATION_METRICS",
     "ClassificationMetric",
     "MULTICLASS_CLASSIFICATION_METRICS",
     "MULTILABEL_CLASSIFICATION_METRICS",
     "Metric",
     "MetricCollection",
     "MetricsProblem",
     "REGRESSION_METRICS",
-    "SEGMENTATION_METRICS",
     "sum_fpr_curve",
 ]
 
-_logger = logging.getLogger(__name__)
-
-
-def _binary_dice_coefficient(target: np.ndarray, pred: np.ndarray) -> float:
-    """Computes the dice coefficient for a binary segmentation task.
-
-    Dice Coefficient is 2 * the Area of Overlap divided by the total
-    number of pixels in both images.
-
-    Args:
-        pred: The predicted segmentation.
-        target: The target segmentation.
-
-    Returns:
-        The binary dice coefficients score.
-    """
-    y_true_f = target.flatten()
-    y_pred_f = pred.flatten()
-    intersection = (y_true_f * y_pred_f).sum()
-    smooth = 0.0001
-    return cast(
-        float, (2.0 * intersection) / (y_true_f.sum() + y_pred_f.sum() + smooth)
-    )
-
-
-def multiclass_dice_coefficients(target: np.ndarray, pred: np.ndarray) -> float:
-    """Computes the dice coefficient for a multiclass segmentation task.
-
-    Args:
-        pred: The predicted segmentation.
-        target: The target segmentation of shape:
-            (Batch_size X number of classes X Height X Weight).
-
-    Returns:
-        The multiclass dice coefficients score.
-    """
-    n_classes = pred.shape[1]
-    dice = 0.0
-    for index in range(1, n_classes):
-        dice += _binary_dice_coefficient(target, pred[:, index, :, :])
-
-    return dice / n_classes  # taking average
-
-
-def _stats_score(
-    target: np.ndarray, pred: np.ndarray, n: int = 1
-) -> Tuple[int, int, int, int]:
-    """Computes the relevant statistics between targets and predictions.
-
-    More specifically, computes the true positive (tp), false positive(fp),
-    true negative (tn) and false positive (fp) rates given the targets,
-    prediction and the class.
-
-    Args:
-        pred: The predicted value.
-        target: The target value.
-
-    Returns:
-        A tuple containing (tp, fp, tn, fn).
-    """
-    cls_pred = np.argmax(pred, axis=1)
-    tp = ((cls_pred == n) * (target == n)).sum()
-    fp = ((cls_pred == n) * (target != n)).sum()
-    tn = ((cls_pred != n) * (target != n)).sum()
-    fn = ((cls_pred != n) * (target == n)).sum()
-    return tp, fp, tn, fn
-
-
-def dice_score(target: np.ndarray, pred: np.ndarray) -> float:
-    """Computes the dice score.
-
-    This is computed as 2*tp/2*tp+fp+fn.
-
-    Args:
-        pred: The predicted segmentation.
-        target: The target segmentation.
-
-    Returns:
-        The dice score.
-    """
-    n_classes = pred.shape[1]
-    score = 0.0
-    # only loop from 1 as we don't care about the background
-    for i in range(1, n_classes):
-        tp, fp, tn, fn = _stats_score(target, pred, i)
-        denom = 2 * tp + fp + fn
-        if not (target == i).any():
-            # no foreground class
-            score_cls = 0.0
-        elif np.isclose(denom, np.zeros_like(denom)).any():
-            # nan result
-            score_cls = 0.0
-        else:
-            score_cls = (2 * tp) / denom
-        score += score_cls
-    return score / (n_classes)
-
-
-def iou(target: np.ndarray, pred: np.ndarray) -> float:
-    """Computes the Intersection over union score.
-
-    This is computed as tp/tp+fp+fn. Also known as Jaccard Index.
-
-    Args:
-        pred: The predicted segmentation.
-        target: The target segmentation.
-
-    Returns:
-        The IoU score.
-    """
-    # computed as tp/tp+fp+fn
-    n_classes = pred.shape[1]
-    score = 0.0
-    # only loop from 1 as we don't care about the background
-    for i in range(1, n_classes):
-        tp, fp, tn, fn = _stats_score(target, pred, i)
-        denom = tp + fp + fn
-        if not (target == i).any():
-            # no foreground class
-            score_cls = 0.0
-        elif np.isclose(denom, np.zeros_like(denom)).any():
-            # nan result
-            score_cls = 0.0
-        else:
-            score_cls = (tp) / denom
-        score += score_cls
-    return score / (n_classes)
-
 
 @dataclass
 class Metric:
     """A metric used for assessing ML model performance.
 
     Attributes:
-        func: A function which computes the metric. Must take two arguments:
+        func: function which computes the metric. Must take two arguments:
             y_true and y_pred and return a metric as a float.
     """
 
     func: Callable[[np.ndarray, np.ndarray], float]
 
 
-@delegates()
 @dataclass
 class ClassificationMetric(Metric):
     """A classification metric used for assessing ML model performance.
 
     Args:
-        probabilities: Whether y_pred needs to be classes or probabilities.
+        probabilities: whether y_pred needs to be classes or probabilities
     """
 
     probabilities: bool
 
 
 MULTICLASS_CLASSIFICATION_METRICS: Dict[str, Metric] = {
     "AUC": ClassificationMetric(
@@ -250,130 +121,89 @@
     "MAE": Metric(mean_absolute_error),
     "MSE": Metric(mean_squared_error),
     "RMSE": Metric(_rmse_func),
     "R2": Metric(r2_score),
     "KS": Metric(kstest),
 }
 
-SEGMENTATION_METRICS: Dict[str, Metric] = {
-    "IoU": Metric(iou),
-    "DiceCoefficients": Metric(multiclass_dice_coefficients),
-    "DiceScore": Metric(dice_score),
-}
-
 
 class MetricsProblem(Enum):
     """Simple wrapper for different problem types for MetricCollection."""
 
     BINARY_CLASSIFICATION = auto()
     MULTICLASS_CLASSIFICATION = auto()
     MULTILABEL_CLASSIFICATION = auto()
     REGRESSION = auto()
-    SEGMENTATION = auto()
 
 
 class MetricCollection:
-    """Container class for metrics to calculate.
-
-    Args:
-        metrics: A list of metrics to calculate.
-        problem: The problem type. If metrics are not specified, the problem type will
-            be used to determine the metrics to calculate.
-
-    Raises:
-        ValueError: If neither one of `problem` nor `metrics` is specified.
-
-    Attributes:
-        metrics: A list of metrics to calculate.
-        problem: The problem type.
-        optimal_threshold: The optimal threshold to separate classes (only used for
-            classification problems).
-        thresholds: The thresholds to separate classes (only used for
-            classification problems).
-        threshold_metrics: The metrics for each threshold (only used for
-            classification problems).
-    """
+    """Container class for metrics to calculate."""
 
     def __init__(
         self,
+        problem: MetricsProblem,
         metrics: Optional[MutableMapping[str, Metric]] = None,
-        problem: Optional[MetricsProblem] = None,
     ):
-        if metrics is None and problem is None:
-            raise ValueError(
-                "At least one of `metrics` or `problem` must be specified."
-            )
-
-        if metrics is not None:
-            self.metrics = metrics
-        else:
-            self.metrics = self._get_metrics(cast(MetricsProblem, problem))
-
         self.problem = problem
+        self.metrics = metrics if metrics is not None else self._get_metrics()
         self.optimal_threshold: Optional[float] = None
         self.results: Dict[str, float] = {}
         self.thresholds = np.linspace(0, 100, 201, endpoint=True) / 100
         self.threshold_metrics: Optional[Mapping[str, Iterable[float]]] = None
 
     @classmethod
     def create_from_model(
         cls,
         model: Union[_BaseModel, DistributedModelProtocol],
         metrics: Optional[MutableMapping[str, Metric]] = None,
     ) -> MetricCollection:
         """Creates a MetricCollection object from a _BaseModel.
 
         Args:
-            model: A _BaseModel.
-            metrics: The metrics dictionary. Defaults to None.
+            model: _BaseModel
+            metrics: metrics dictionary.
+                Defaults to None.
 
         Returns:
-            Instance of MetricCollection.
+            Instance of MetricCollection
         """
-        problem: Optional[MetricsProblem] = None
         # The casts are to reassure mypy that a subclass of both _BaseModel and
         # ClassifierMixIn can exist (the ._Schema attribute causes problems)
         if isinstance(cast(ClassifierMixIn, model), ClassifierMixIn):
             classifier_model = cast(ClassifierMixIn, model)
             if classifier_model.multilabel:
                 problem = MetricsProblem.MULTILABEL_CLASSIFICATION
             elif classifier_model.n_classes > 2:
                 problem = MetricsProblem.MULTICLASS_CLASSIFICATION
             else:
                 problem = MetricsProblem.BINARY_CLASSIFICATION
-        elif isinstance(cast(RegressorMixIn, model), RegressorMixIn):
-            problem = MetricsProblem.REGRESSION
         else:
-            _logger.warning("Metrics problem type can't be determined. Leaving empty.")
+            problem = MetricsProblem.REGRESSION
 
         return cls(problem=problem, metrics=metrics)
 
     def compute(
         self,
         test_target: np.ndarray,
         test_preds: np.ndarray,
         metric_to_optimise: str = "F1",
         threshold: Optional[float] = None,
     ) -> Dict[str, float]:
         """Compute list of metrics and save results in self.results.
 
-        :::note
-
-        Thresholds do not apply to multiclass problems.
-
-        :::
-
         Args:
-            test_target: A list of targets.
-            test_preds: A list of predictions.
-            metric_to_optimise: What metric to optimize in order to compute the
+            test_target: list of targets
+            test_preds: list of predictions
+            metric_to_optimise: what metric to optimize in order to compute the
                 optimal threshold. This will have no effect if there aren't any metrics
-                to which a threshold is applied. Must be present in 'self.metrics'.
+                to which a threshold is applied. Must be present in 'self.metrics'
             threshold: If this argument is provided, this threshold will be
                 used instead of optimising the threshold as per 'optimise'
+
+        NOTE: thresholds do not apply to multiclass problems.
         """
         if (
             self.problem is MetricsProblem.BINARY_CLASSIFICATION
             and np.asarray(test_preds).ndim == 2
         ):
             test_preds = np.asarray([row[1] for row in test_preds])
 
@@ -397,15 +227,15 @@
                     self._round(metric.func(test_target, test_probs_or_classes), 4),
                 )
                 # metric.func always returns a float, so the _round
                 # function will return a float as well
             except (ValueError, IndexError):
                 # This usually occurs when the targets do not have all possible classes
                 self.results[metric_name] = np.inf
-                _logger.warning(
+                logging.warning(
                     "Unable to compute metric: " + metric_name + " setting to np.inf"
                 )
 
         return self.results
 
     @staticmethod
     def _round(
@@ -423,39 +253,49 @@
 
     def get_results_df(self) -> pd.DataFrame:
         """Calls get_results and returns results as a dataframe."""
         return pd.DataFrame.from_dict(
             cast(Dict, self.results), orient="index", columns=["value"]
         )
 
-    @staticmethod
-    def _get_metrics(problem: MetricsProblem) -> Dict[str, Metric]:
+    def view_thresholds(self) -> matplotlib.figure.Figure:
+        """Displays chart of how thresholds affect relevant metrics."""
+        fig, ax = plt.subplots()
+
+        for metric_name, metric_vals in cast(
+            Mapping[str, Iterable[float]], self.threshold_metrics
+        ).items():
+            plt.plot(self.thresholds, metric_vals, label=metric_name)
+        ax.legend()
+        ax.set_title("Metric values at various thresholds")
+        ax.set_ylabel("Metric value")
+        ax.set_xlabel("Threshold")
+        plt.close()
+
+        return fig
+
+    def _get_metrics(
+        self,
+    ) -> Dict[str, Metric]:
         """Returns metrics appropriate for the task, otherwise simply returns `metrics`.
 
         Args:
-            problem: The problem type to return the appropriate metrics for.
+            metrics: Optional. Metrics to be computed.
 
         Returns:
-            Dictionary of metrics to be computed.
-
-        Raises:
-            ValueError: If the problem type is not recognised.
+            Dict[str, Metric]: Dictionary of metrics to be computed
         """
-        if problem is MetricsProblem.REGRESSION:
+        if self.problem is MetricsProblem.REGRESSION:
             metrics = REGRESSION_METRICS
-        elif problem is MetricsProblem.BINARY_CLASSIFICATION:
+        elif self.problem is MetricsProblem.BINARY_CLASSIFICATION:
             metrics = BINARY_CLASSIFICATION_METRICS
-        elif problem is MetricsProblem.MULTICLASS_CLASSIFICATION:
+        elif self.problem is MetricsProblem.MULTICLASS_CLASSIFICATION:
             metrics = MULTICLASS_CLASSIFICATION_METRICS
-        elif problem is MetricsProblem.SEGMENTATION:
-            metrics = SEGMENTATION_METRICS
-        elif problem is MetricsProblem.MULTILABEL_CLASSIFICATION:
-            metrics = MULTILABEL_CLASSIFICATION_METRICS
         else:
-            raise ValueError("Problem type not recognised.")
+            metrics = MULTILABEL_CLASSIFICATION_METRICS
 
         return metrics
 
     def _get_prediction_classes(
         self,
         test_target: np.ndarray,
         test_preds: np.ndarray,
@@ -509,31 +349,31 @@
     ) -> None:
         """Optimise decision threshold by tuning metric_to_optimise.
 
         Finds the argmax of given metric by modifying the decision threshold and sets
         self.optimal_threshold
 
         Args:
-            test_target: The targets array.
-            test_preds: The predictions array.
-            metric_to_optimise: The metric to optimise.
+            test_target: targets
+            test_preds: predictions
+            metric_to_optimise: metric to optimise
 
         Raises:
             ValueError: if metric_to_optimise does not exist in self.metrics
         """
         if metric_to_optimise not in self.metrics:
             raise ValueError("Chosen optimisation metric is not defined.")
 
         optimal_threshold = 0
         max_metric: float = 0
 
         # Dictionary of relevant metrics (i.e. those where a threshold is applied)
         # and the computed value for every threshold stored as a list
         threshold_metrics: Dict[str, List[float]] = {}
-        for m in self.metrics:
+        for m in self.metrics.keys():
             if (
                 isinstance(self.metrics[m], ClassificationMetric)
                 and not cast(ClassificationMetric, self.metrics[m]).probabilities
             ):
                 threshold_metrics[m] = []
         # Loop through every threshold and apply it to our predictions
         for threshold in self.thresholds:
```

### Comparing `bitfount-0.5.86/bitfount/models/__init__.py` & `bitfount-0.5.9/bitfount/models/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -3,14 +3,15 @@
 
 from bitfount.models.base_models import (
     ClassifierMixIn,
     CNNModelStructure,
     EarlyStopping,
     FeedForwardModelStructure,
     LoggerConfig,
+    ModelContext,
     NeuralNetworkMixIn,
     NeuralNetworkModelStructure,
     NeuralNetworkPredefinedModel,
     Optimizer,
     RegressorMixIn,
     Scheduler,
 )
@@ -21,14 +22,15 @@
     "BitfountModel",
     "ClassifierMixIn",
     "CNNModelStructure",
     "EarlyStopping",
     "FeedForwardModelStructure",
     "LoggerConfig",
     "LogisticRegressionClassifier",
+    "ModelContext",
     "NeuralNetworkMixIn",
     "NeuralNetworkModelStructure",
     "NeuralNetworkPredefinedModel",
     "Optimizer",
     "RegBoostRegressor",
     "RegressorMixIn",
     "Scheduler",
```

### Comparing `bitfount-0.5.86/bitfount/models/nn.py` & `bitfount-0.5.9/bitfount/models/nn.py`

 * *Files identical despite different names*

### Comparing `bitfount-0.5.86/bitfount/storage.py` & `bitfount-0.5.9/bitfount/storage.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,148 +1,56 @@
 """Classes and functions for handling interaction with online storage services."""
 import logging
 import os
 from pathlib import Path
 from typing import Any, BinaryIO, Final, List, Optional, Tuple, Union
 
-import httpx
-from httpx import HTTPError as HTTPXError, Response as HTTPXResponse
 import msgpack
 import requests
 from requests import HTTPError, RequestException, Response
 
 from bitfount.types import _S3PresignedPOSTFields, _S3PresignedPOSTURL, _S3PresignedURL
-from bitfount.utils import web_utils
 
 __all__: List[str] = []
 
 logger = logging.getLogger(__name__)
 
-# This forces `requests` to make IPv4 connections
-# TODO: [BIT-1443] Remove this once Hub/AM support IPv6
-requests.packages.urllib3.util.connection.HAS_IPV6 = False  # type: ignore[attr-defined] # Reason: see above # noqa: B950
-
-
 # Status codes that we consider as OK responses for upload
 _OK_UPLOAD_STATUS_CODES: Tuple[int, ...] = (200, 201, 203, 204)
 # Status codes that we consider as OK responses for download
 _OK_DOWNLOAD_STATUS_CODES: Tuple[int, ...] = (200,)
 
 _DEFAULT_FILE_NAME: Final[str] = "bitfount_file"
 
 
 def _handle_request_exception(
-    exc: Union[RequestException, HTTPXError],
-    response: Optional[Union[Response, HTTPXResponse]],
-    err_message: str,
+    exc: RequestException, response: Optional[Response], err_message: str
 ) -> None:
-    """Handles request exceptions in storage functions.
-
-    Converts httpx to requests-style exceptions.
-    """
+    """Handles request exceptions in storage functions."""
     # Explicit None check needed due to truthyness of Response objects
     if response is not None:
         err_message += f": ({response.status_code})"
         if response.text:
             err_message += f" {response.text}"
-    # If no response object, use exception info instead if present
-    elif exc_str := str(exc):
-        err_message += f": ({exc_str})"
-
-    # If the exception is a HTTPXError subclass, we need to handle it specifically
-    # to convert to a requests Exception.
-    if isinstance(exc, httpx.HTTPStatusError):
-        raise HTTPError(
-            err_message, request=exc.request, response=exc.response
-        ) from exc
-    if isinstance(exc, httpx.ConnectError):
-        raise requests.ConnectionError(err_message, request=exc.request) from exc
-    if isinstance(exc, HTTPXError):
-        # If it's a different HTTPXError subclass, handle it by a RequestException
-        raise RequestException(err_message) from exc
-
-    # Otherwise raise same type as what was passed in
+    # Raise same type as what was passed in
     raise type(exc)(err_message) from exc
 
 
-async def _async_upload_to_s3(
-    upload_url: _S3PresignedPOSTURL,
-    presigned_fields: _S3PresignedPOSTFields,
-    to_upload: Union[bytes, BinaryIO],
-    file_name: str = _DEFAULT_FILE_NAME,
-) -> None:
-    """Async Base s3 upload function.
-
-    Handles the upload and response.
-
-    Raises:
-        requests.HTTPError: If the response code is not ok for any reason.
-        requests.ConnectionError: If there is an error in the connection.
-        requests.RequestException: For any other request-related issues.
-    """
-    try:
-        response = await web_utils.async_post(
-            url=upload_url,
-            data=presigned_fields,
-            files={"file": (file_name, to_upload)},
-            timeout=None,
-        )
-        response.raise_for_status()
-        if response.status_code not in _OK_UPLOAD_STATUS_CODES:
-            raise HTTPError(f"Response not OK; not one of {_OK_UPLOAD_STATUS_CODES}")
-    except (HTTPError, HTTPXError) as ex:
-        try:
-            # noinspection PyUnboundLocalVariable
-            err_response: Optional[HTTPXResponse] = response
-        except NameError:
-            # response variable wasn't created yet
-            err_response = None
-        _handle_request_exception(ex, err_response, "Issue uploading object to S3")
-
-
-async def _async_upload_data_to_s3(
-    upload_url: _S3PresignedPOSTURL,
-    presigned_fields: _S3PresignedPOSTFields,
-    data: Any,
-) -> None:
-    """Asynchronously uploads an object to an s3 presigned POST url.
-
-    Data will be packed via msgpack automatically before upload.
-
-    Args:
-        upload_url: The s3 presigned URL to upload to.
-        presigned_fields: The additional fields needed to upload to the presigned
-            URL. These are provided when the presigned URL is created.
-        data: The data to upload.
-
-    Raises:
-        requests.HTTPError: If the response code is not ok for any reason.
-        requests.ConnectionError: If there is an error in the connection.
-        requests.RequestException: For any other request-related issues.
-    """
-    packed_data: bytes = msgpack.dumps(data)
-    await _async_upload_to_s3(upload_url, presigned_fields, packed_data)
-
-
 def _upload_to_s3(
     upload_url: _S3PresignedPOSTURL,
     presigned_fields: _S3PresignedPOSTFields,
     to_upload: Union[bytes, BinaryIO],
     file_name: str = _DEFAULT_FILE_NAME,
 ) -> None:
     """Base s3 upload function.
 
     Handles the upload and response.
-
-    Raises:
-        requests.HTTPError: If the response code is not 200 or 201.
-        requests.RequestException: If the response is not OK for another reason.
     """
     try:
-        response = web_utils.post(
+        response = requests.post(
             url=upload_url,
             data=presigned_fields,
             files={"file": (file_name, to_upload)},
         )
 
         response.raise_for_status()
 
@@ -166,20 +74,20 @@
     """Uploads an object to an s3 presigned POST url.
 
     Data will be packed via msgpack automatically before upload.
 
     Args:
         upload_url: The s3 presigned URL to upload to.
         presigned_fields: The additional fields needed to upload to the presigned
-            URL. These are provided when the presigned URL is created.
+                          URL. These are provided when the presigned URL is created.
         data: The data to upload.
 
     Raises:
-        requests.HTTPError: If the response code is not 200 or 201.
-        requests.RequestException: If the response is not OK for another reason.
+        HTTPError: If the response code is not 200 or 201.
+        RequestException: If the response is not OK for another reason.
     """
     packed_data: bytes = msgpack.dumps(data)
     _upload_to_s3(upload_url, presigned_fields, packed_data)
 
 
 def _upload_file_to_s3(
     upload_url: _S3PresignedPOSTURL,
@@ -193,27 +101,27 @@
 
     The file is read and uploaded in binary mode as this is recommended for
     `requests.post()`.
 
     Args:
         upload_url: The s3 presigned URL to upload to.
         presigned_fields: The additional fields needed to upload to the presigned
-            URL. These are provided when the presigned URL is created.
+                          URL. These are provided when the presigned URL is created.
         file_path: The path to the file to be uploaded. Cannot be provided if
-            file_contents is.
+                   file_contents is.
         file_contents: The contents of a file to be uploaded. Cannot be provided
-            if file_path is.
+                       if file_path is.
         file_encoding: The encoding to use to convert string file contents to bytes.
         file_name: The name to upload the file with. Optional, default will be extracted
-            automatically if using file_path, and use a default filename if using
-            file_contents.
+                  automatically if using file_path, and use a default filename
+                  if using file_contents.
 
     Raises:
-        requests.HTTPError: If the response code is not 200 or 201.
-        requests.RequestException: If the response is not OK for another reason.
+        HTTPError: If the response code is not 200 or 201.
+        RequestException: If the response is not OK for another reason.
     """
     if bool(file_path) == bool(file_contents):
         raise ValueError(
             "One of file_path and file_contents must be provided, but not both."
         )
 
     # Handle if file_path provided
@@ -236,71 +144,26 @@
         # Convert to bytes as this is what's supported in `requests.post()`
         if isinstance(file_contents, str):
             file_contents = file_contents.encode(file_encoding)
 
         _upload_to_s3(upload_url, presigned_fields, file_contents, file_name)
 
 
-async def _async_download_from_s3(download_url: _S3PresignedURL) -> bytes:
-    """Async Base s3 download function.
-
-    Handles the download and response parsing.
-
-    Raises:
-        httpx.HTTPError: If the response code is not ok for any reason.
-    """
-    try:
-        response = await web_utils.async_get(url=download_url, timeout=None)
-        response.raise_for_status()
-        if response.status_code not in _OK_DOWNLOAD_STATUS_CODES:
-            raise HTTPError(f"Response not OK; not one of {_OK_DOWNLOAD_STATUS_CODES}")
-    except (HTTPError, HTTPXError) as ex:
-        try:
-            # noinspection PyUnboundLocalVariable
-            err_response: Optional[HTTPXResponse] = response
-        except NameError:
-            # response variable wasn't created yet
-            err_response = None
-        _handle_request_exception(
-            ex, err_response, "Issue whilst retrieving data from S3"
-        )
-    return response.content
-
-
-async def _async_download_data_from_s3(download_url: _S3PresignedURL) -> Any:
-    """Downloads data from the specified s3 URL.
-
-    Will unpack the data using msgpack.
-
-    Args:
-        download_url: The s3 URL to download the data from.
-
-    Raises:
-        httpx.HTTPError: If the response code is not ok for any reason.
-    """
-    data: bytes = await _async_download_from_s3(download_url)
-    return msgpack.loads(data)
-
-
 def _download_from_s3(download_url: _S3PresignedURL) -> bytes:
     """Base s3 download function.
 
     Handles the download and response parsing.
-
-    Raises:
-        requests.HTTPError: If the response code is not 200.
-        requests.RequestException: If the response is not OK.
     """
     try:
-        response = web_utils.get(url=download_url)
+        response = requests.get(url=download_url)
 
         response.raise_for_status()
 
         if response.status_code not in _OK_DOWNLOAD_STATUS_CODES:
-            raise HTTPError(f"Response not OK; not one of {_OK_DOWNLOAD_STATUS_CODES}")
+            raise HTTPError(f"Response not OK; not one of {_OK_UPLOAD_STATUS_CODES}")
     except RequestException as ex:
         try:
             # noinspection PyUnboundLocalVariable
             err_response: Optional[Response] = response
         except NameError:
             # response variable wasn't created yet
             err_response = None
@@ -315,16 +178,16 @@
 
     Will unpack the data using msgpack.
 
     Args:
         download_url: The s3 URL to download the data from.
 
     Raises:
-        requests.HTTPError: If the response code is not 200.
-        requests.RequestException: If the response is not OK.
+        HTTPError: If the response code is not 200.
+        RequestException: If the response is not OK.
     """
     data: bytes = _download_from_s3(download_url)
     return msgpack.loads(data)
 
 
 def _download_file_from_s3(
     download_url: _S3PresignedURL, encoding: Optional[str] = None
@@ -335,16 +198,16 @@
         download_url: The s3 URL to download the data from.
         encoding: Optional. A string encoding to use to decode the data.
 
     Returns:
         The file contents, as a string if `encoding` provided otherwise as bytes.
 
     Raises:
-        requests.HTTPError: If the response code is not 200.
-        requests.RequestException: If the response is not OK.
+        HTTPError: If the response code is not 200.
+        RequestException: If the response is not OK.
     """
     data: bytes = _download_from_s3(download_url)
     if encoding:
         return data.decode(encoding)
     else:
         return data
```

### Comparing `bitfount-0.5.86/bitfount/transformations/__init__.py` & `bitfount-0.5.9/bitfount/transformations/__init__.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,18 +1,17 @@
 """Modules related to the transformations framework.
 
 All transformations should be imported here so that they are automatically added to the
 transformations registry.
 """
 from typing import List
 
-from bitfount.transformations.base_transformation import TRANSFORMATION_REGISTRY
 from bitfount.transformations.batch_operations import (
     IMAGE_TRANSFORMATIONS,
-    AlbumentationsImageTransformation,
+    ImageTransformation,
 )
 from bitfount.transformations.binary_operations import (
     AdditionTransformation,
     ComparisonTransformation,
     DivisionTransformation,
     MultiplicationTransformation,
     SubtractionTransformation,
@@ -30,32 +29,26 @@
     TransformationApplicationError,
     TransformationParsingError,
     TransformationProcessorError,
     TransformationRegistryError,
 )
 from bitfount.transformations.parser import TransformationsParser
 from bitfount.transformations.processor import TransformationProcessor
-from bitfount.transformations.torchio_batch_operations import (
-    TORCHIO_IMAGE_TRANSFORMATIONS,
-    TorchIOImageTransformation,
-)
 from bitfount.transformations.unary_operations import (
     InclusionTransformation,
     OneHotEncodingTransformation,
 )
 
 __all__: List[str] = [
     "AdditionTransformation",
-    "AlbumentationsImageTransformation",
-    "TorchIOImageTransformation",
     "CleanDataTransformation",
     "ComparisonTransformation",
     "DivisionTransformation",
     "IMAGE_TRANSFORMATIONS",
-    "TORCHIO_IMAGE_TRANSFORMATIONS",
+    "ImageTransformation",
     "InclusionTransformation",
     "IncorrectReferenceError",
     "InvalidBatchTransformationError",
     "MissingColumnReferenceError",
     "MultiplicationTransformation",
     "NormalizeDataTransformation",
     "NotColumnReferenceError",
@@ -64,14 +57,13 @@
     "SubtractionTransformation",
     "TransformationApplicationError",
     "TransformationsParser",
     "TransformationParsingError",
     "TransformationProcessor",
     "TransformationProcessorError",
     "TransformationRegistryError",
-    "TRANSFORMATION_REGISTRY",
 ]
 
 # See top level `__init__.py` for an explanation
 __pdoc__ = {}
 for _obj in __all__:
     __pdoc__[_obj] = False
```

### Comparing `bitfount-0.5.86/bitfount/transformations/base_transformation.py` & `bitfount-0.5.9/bitfount/transformations/base_transformation.py`

 * *Files 4% similar despite different names*

```diff
@@ -9,15 +9,14 @@
 import desert
 from marshmallow import Schema
 
 from bitfount.transformations.exceptions import TransformationRegistryError
 from bitfount.transformations.utils import _MarshmallowYamlShim
 
 TRANSFORMATION_REGISTRY: Dict[str, Type[Transformation]] = {}
-"""Dictionary of all registered transformations."""
 
 
 # Keyword-only dataclasses allow us to get around the issue of subclasses having
 # non-default args following default args (i.e. name and output). Every subclass
 # should be marked using the same.
 @attr.dataclass(kw_only=True)
 class Transformation:
@@ -40,15 +39,15 @@
             and the transformation is not registered.
     """
 
     # Non-serializable variables
     _registry_name: ClassVar[Optional[str]] = None
 
     # Serializable variables
-    name: str = None  # type: ignore[assignment] # Reason: set in __post_init__
+    name: Optional[str] = None
     output: bool = False
 
     def __attrs_post_init__(self) -> None:
         if not self.name:
             self.name = self._gen_name()
 
     @classmethod
@@ -70,16 +69,16 @@
 
         Raises:
             TypeError: If the transformation doesn't have a `TransformationSchema` as
                 the schema.
         """
         # If the transformation has a custom schema, use it
         if hasattr(cls, "_Schema"):
-            if issubclass(cls._Schema, _TransformationSchema):
-                return cast(_TransformationSchema, cls._Schema())
+            if issubclass(cls._Schema, _TransformationSchema):  # type: ignore[attr-defined] # Reason: hasattr check above # noqa: B950
+                return cast(_TransformationSchema, cls._Schema())  # type: ignore[attr-defined] # Reason: hasattr check above # noqa: B950
             else:
                 raise TypeError(
                     f"Schema attribute for class {cls.__name__} must be a "
                     f"TransformationSchema instance."
                 )
         # Otherwise generate one from desert
         else:
```

### Comparing `bitfount-0.5.86/bitfount/transformations/batch_operations.py` & `bitfount-0.5.9/bitfount/transformations/batch_operations.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,31 +1,26 @@
 """Operations done at batch time defined here."""
 from __future__ import annotations
 
 import inspect
-from typing import Any, Dict, List, Protocol, Type, Union, cast
+from typing import Dict, List, Protocol, Type, Union, cast
 
 import albumentations as A
-import albumentations.augmentations as albumentations_augmentations  # type: ignore[import] # Reason: only used for vars extraction, don't need type hints # noqa: B950
-import albumentations.pytorch as albumentations_pytorch  # type: ignore[import] # Reason: only used for vars extraction, don't need type hints # noqa: B950
+import albumentations.augmentations as albumentations_augmentations
+import albumentations.pytorch as albumentations_pytorch
 import attr
-from marshmallow import fields, post_load
-from marshmallow_union import Union as M_Union
 import numpy as np
 
 from bitfount.config import _PYTORCH_ENGINE, BITFOUNT_ENGINE
-from bitfount.data.types import DataSplit
-from bitfount.transformations.base_transformation import _TransformationSchema
+from bitfount.transformations.base_transformation import Transformation
 from bitfount.transformations.exceptions import TransformationParsingError
 from bitfount.transformations.unary_operations import UnaryOperation
 from bitfount.types import _JSONDict
 
 #: Dictionary of available image transformations and their corresponding classes.
-from bitfount.utils import delegates
-
 IMAGE_TRANSFORMATIONS: Dict[str, Type[A.BasicTransform]] = {
     name: class_
     for name, class_ in vars(albumentations_augmentations).items()
     if inspect.isclass(class_)
     and issubclass(class_, A.BasicTransform)
     and not inspect.isabstract(class_)
 }
@@ -37,49 +32,56 @@
             if inspect.isclass(class_)
             and issubclass(class_, A.BasicTransform)
             and not inspect.isabstract(class_)
         }
     )
 
 
-@delegates()
 @attr.dataclass(kw_only=True)
-class BatchTimeOperation(UnaryOperation):
+class BatchTimeOperation(Transformation):
     """Class just to denote that transformation will happen at batch time.
 
-    All batch time operations must be unary operations.
-
     Args:
-        step: Denotes whether transformations should be performed at training,
-            validation or test time.
+        step: Either "train" or "validation". Denotes whether transformations should be
+            performed at training time or validation time.
 
+    Raises:
+        ValueError: If step is not one of "train" or "validation".
     """
 
-    step: DataSplit
+    # Can't be a Literal because marshmallow then can't infer the type.
+    step: str
+
+    def _validate_args(self) -> None:
+        """This method should be called in subclasses to validate the arguments."""
+        if self.step not in ["train", "validation"]:
+            raise ValueError(
+                f"step must be one of 'train' or 'validation'. Got '{self.step}'"
+            )
 
 
-@delegates()
 @attr.dataclass(kw_only=True)
-class AlbumentationsImageTransformation(BatchTimeOperation):
+class ImageTransformation(UnaryOperation, BatchTimeOperation):
     """Represents image transformations done on a single column at batch time.
 
     Args:
         transformations: List of transformations to be performed in order as one
             transformation.
 
     Raises:
         ValueError: If the `output` is set to False.
     """
 
-    _registry_name = "albumentations"
+    _registry_name = "image"
 
     transformations: List[Union[str, Dict[str, _JSONDict]]]
 
     def __attrs_post_init__(self) -> None:
         super().__attrs_post_init__()
+        self._validate_args()
         if not self.output:
             raise ValueError("`output` cannot be False for a BatchTimeOperation")
 
     def _load_transformation(
         self, tfm: Union[str, Dict[str, _JSONDict]]
     ) -> A.BasicTransform:
         """Loads and returns transformation in albumentations.
@@ -122,60 +124,17 @@
         for tfm in self.transformations:
             a_tfm = self._load_transformation(tfm)
             list_of_transformations.append(a_tfm)
 
         tfm_callable = A.Compose(list_of_transformations)
         return cast(_AlbumentationsAugmentation, tfm_callable)
 
-    class _Schema(_TransformationSchema):
-        """Marshmallow schema for ImageTransformation."""
-
-        # From Transformation
-        name = fields.String(default=None)
-        output = fields.Boolean(default=True)
-        # From UnaryOperation
-        arg = fields.String(required=True)
-        # From BatchTimeOperation
-        step = fields.Method(
-            serialize="_serialize_step", deserialize="_deserialize_step"
-        )
-        # From ImageTransformation
-        transformations = fields.List(
-            M_Union(
-                [
-                    fields.String(),
-                    fields.Dict(
-                        keys=fields.String(),
-                        values=fields.Dict(keys=fields.String(), values=fields.Raw()),
-                    ),
-                ],
-            ),
-            required=True,
-        )
-
-        @staticmethod
-        def _serialize_step(transformation: AlbumentationsImageTransformation) -> str:
-            """Serializes the step of the transformation from Enum to string."""
-            return transformation.step.value
-
-        @staticmethod
-        def _deserialize_step(value: str) -> DataSplit:
-            """Deserializes the step of the transformation from string to Enum."""
-            return DataSplit(value)
-
-        @post_load
-        def make_transformation(
-            self, data: _JSONDict, **_kwargs: Any
-        ) -> AlbumentationsImageTransformation:
-            """Creates the ImageTransformation object from the marshmallow data."""
-            return AlbumentationsImageTransformation(**data)
-
 
 class _AlbumentationsAugmentation(Protocol):
     """Protocol for the signature of an albumentations augmentation function."""
 
     def __call__(self, *, image: np.ndarray) -> Dict[str, np.ndarray]:
-        """Calls the function.
+        """Call the function.
 
         `image` must be passed as a kwarg.
         """
         ...
```

### Comparing `bitfount-0.5.86/bitfount/transformations/binary_operations.py` & `bitfount-0.5.9/bitfount/transformations/binary_operations.py`

 * *Files 12% similar despite different names*

```diff
@@ -6,18 +6,16 @@
 from __future__ import annotations
 
 from typing import Any, Union
 
 import attr
 
 from bitfount.transformations.base_transformation import Transformation
-from bitfount.utils import delegates
 
 
-@delegates()
 @attr.dataclass(kw_only=True)
 class BinaryOperation(Transformation):
     """Base two-arg transformation.
 
     The base abstract class for all Binary Operation Transformations.
 
     Args:
@@ -25,15 +23,14 @@
         arg2: The second argument
     """
 
     arg1: Any
     arg2: Any
 
 
-@delegates()
 @attr.dataclass(kw_only=True)
 class NumericBinaryOperation(BinaryOperation):
     """Base two-arg operation involving numbers.
 
     This class represents any BinaryOperation where arg2 can be numeric or a
     second column name such as addition, multiplication, etc.
 
@@ -42,58 +39,53 @@
         arg2: The second argument (column name or numeric value).
     """
 
     arg1: str
     arg2: Union[float, str]
 
 
-@delegates()
 @attr.dataclass(kw_only=True)
 class AdditionTransformation(NumericBinaryOperation):
     """Represents the addition of two columns or of a constant to a column."""
 
     _registry_name = "add"
 
 
-@delegates()
 @attr.dataclass(kw_only=True)
 class SubtractionTransformation(NumericBinaryOperation):
     """Column subtracting transformation.
 
     Represents the subtraction of one column from another or of a constant from
     a column.
     """
 
     _registry_name = "subtract"
 
 
-@delegates()
 @attr.dataclass(kw_only=True)
 class MultiplicationTransformation(NumericBinaryOperation):
     """Represents the multiplication of two columns or of a column and a constant."""
 
     _registry_name = "multiply"
 
 
-@delegates()
 @attr.dataclass(kw_only=True)
 class DivisionTransformation(NumericBinaryOperation):
     """Column division transformation.
 
     Represents the division of one column by another or of one column by a constant.
     """
 
     _registry_name = "divide"
 
 
-@delegates()
 @attr.dataclass(kw_only=True)
 class ComparisonTransformation(NumericBinaryOperation):
     """Represents the comparison between two columns or of one column and a constant.
 
     The resulting output should be:
-        - -1 if arg1 < arg2
-        - 0 if arg1 == arg2
-        - +1 if arg1 > arg2.
+    - -1 if arg1 < arg2
+    - 0 if arg1 == arg2
+    - +1 if arg1 > arg2
     """
 
     _registry_name = "compare"
```

### Comparing `bitfount-0.5.86/bitfount/transformations/exceptions.py` & `bitfount-0.5.9/bitfount/transformations/exceptions.py`

 * *Files identical despite different names*

### Comparing `bitfount-0.5.86/bitfount/transformations/parser.py` & `bitfount-0.5.9/bitfount/transformations/parser.py`

 * *Files 1% similar despite different names*

```diff
@@ -2,15 +2,15 @@
 
 This module contains the transformations parser that will parse a set of
 Transformation specifications from a YAML file and create a set of
 Transformation instances.
 """
 from collections import defaultdict
 from os import PathLike
-from typing import DefaultDict, Dict, Iterable, List, Mapping, Set, Tuple, Union
+from typing import DefaultDict, Dict, Iterable, List, Mapping, Set, Tuple, Union, cast
 
 import yaml
 
 from bitfount.transformations.base_transformation import (
     TRANSFORMATION_REGISTRY,
     MultiColumnOutputTransformation,
     Transformation,
@@ -150,18 +150,18 @@
             )
 
         # Retrieve transformation schema from registry and deserialize
         t_name, t_details = list(t_spec.items())[0]  # as only one long
         t_name = t_name.lower()
         try:
             schema = TRANSFORMATION_REGISTRY[t_name].schema()
-        except KeyError as e:
+        except KeyError:
             raise TransformationParsingError(
                 f"No transformation registered with name {t_name}."
-            ) from e
+            )
         transformation: Transformation = schema.load(t_details)
         return transformation
 
     def _check_names_unique(self, transformations: Iterable[Transformation]) -> None:
         """Checks that each transformation has a unique name.
 
         Args:
@@ -193,20 +193,21 @@
             errors.extend(
                 [f"Duplicate transformation name: {dupe}." for dupe in duplicated]
             )
 
         # Check multi-column output transformations for clashes
         mco_clashes: DefaultDict[str, List[str]] = defaultdict(list)
         for t in multi_column_outputs:
+            name = cast(str, t.name)
             cols = set(t.columns)
 
             # Find clashes against `seen` set.
             clashes = cols.intersection(seen)
             if clashes:
-                mco_clashes[t.name].extend(clashes)
+                mco_clashes[name].extend(clashes)
             # Regardless, need to update `seen` with the new cols
             seen.update(cols)
         if mco_clashes:
             errors.extend(
                 [
                     f"Multi-column output clash: {clash} "
                     f"(from output column of {name})."
@@ -238,14 +239,15 @@
         """
         errors = []
         transforms_map: Dict[str, Transformation] = dict()
 
         # In order, check that any references to previous transformations are
         # correct and exist.
         for t in transformations:
+            t.name = cast(str, t.name)
             transforms_map[t.name] = t
 
             # Check if any attributes reference transformations
             for attr, value in vars(t).items():
                 try:
                     match = _TRANSFORMATION_REFERENCE.fullmatch(value)
                     # If referencing a transformation, replace the attribute with
```

### Comparing `bitfount-0.5.86/bitfount/transformations/processor.py` & `bitfount-0.5.9/bitfount/transformations/processor.py`

 * *Files 13% similar despite different names*

```diff
@@ -4,157 +4,156 @@
 import logging
 from typing import (
     TYPE_CHECKING,
     Any,
     Callable,
     Dict,
     List,
+    Literal,
     Optional,
     Set,
     Tuple,
     Type,
     Union,
     cast,
 )
 
 import numpy as np
-import pandas as pd
 from pandas.core.dtypes.common import is_numeric_dtype
 
-from bitfount.data.types import DataSplit, SemanticType
+from bitfount.data.types import SemanticType
 from bitfount.transformations.base_transformation import (
     MultiColumnOutputTransformation,
     Transformation,
 )
 from bitfount.transformations.batch_operations import (
-    AlbumentationsImageTransformation,
     BatchTimeOperation,
+    ImageTransformation,
 )
 from bitfount.transformations.binary_operations import (
     AdditionTransformation,
     ComparisonTransformation,
     DivisionTransformation,
     MultiplicationTransformation,
     SubtractionTransformation,
 )
 from bitfount.transformations.dataset_operations import (
     CleanDataTransformation,
     NormalizeDataTransformation,
-    ScalarAdditionDataTransformation,
-    ScalarMultiplicationDataTransformation,
 )
 from bitfount.transformations.exceptions import (
     InvalidBatchTransformationError,
     MissingColumnReferenceError,
     NotColumnReferenceError,
     TransformationApplicationError,
 )
 from bitfount.transformations.references import _extract_col_ref
-from bitfount.transformations.torchio_batch_operations import TorchIOImageTransformation
 from bitfount.transformations.unary_operations import (
     InclusionTransformation,
     OneHotEncodingTransformation,
 )
+from bitfount.types import _DataFrameLib, _DataFrameType, _SeriesType
+from bitfount.utils import _get_df_library, _get_df_library_type
 
 if TYPE_CHECKING:
-    from bitfount.data.schema import TableSchema
+    from bitfount.data.schema import BitfountSchema
 
 
 logger = logging.getLogger(__name__)
 
 
 class TransformationProcessor:
     """Processes Transformations on a given dataframe.
 
-    :::caution
-
-    The Transformation processor does not add any of the newly created columns
-    to the Schema. This must be done separately after processing the transformations.
-
-    :::
-
     Args:
         transformations: The list of transformations to apply.
         schema: The schema of the data to be transformed.
         col_refs: The set of columns referenced in those transformations.
 
     Attributes:
         transformations: The list of transformations to apply.
         schema: The schema of the data to be transformed.
         col_refs: The set of columns referenced in those transformations.
+
+    ::: warning
+
+    The Transformation processor does not add any of the newly created columns
+    to the Schema. This must be done separately after processing the transformations.
+
+    :::
     """
 
     def __init__(
         self,
         transformations: List[Transformation],
-        schema: Optional[TableSchema] = None,
+        schema: Optional[BitfountSchema] = None,
         col_refs: Optional[Set[str]] = None,
     ):
         self.transformations = transformations
         self.col_refs = set() if col_refs is None else col_refs
 
         self.schema = schema
         if self.schema is not None:
-            self._schema_cont_cols = self.schema.get_feature_names(
-                SemanticType.CONTINUOUS
-            )
-            self._schema_cat_cols = self.schema.get_feature_names(
-                SemanticType.CATEGORICAL
-            )
+            self._schema_cont_cols = self.schema.feature_names(SemanticType.CONTINUOUS)
+            self._schema_cat_cols = self.schema.feature_names(SemanticType.CATEGORICAL)
         else:
             self._schema_cont_cols = []
             self._schema_cat_cols = []
 
         self._operators: Dict[Type[Transformation], Callable] = {
             AdditionTransformation: self._do_addition,
             CleanDataTransformation: self._do_clean_data,
             ComparisonTransformation: self._do_comparison,
             DivisionTransformation: self._do_division,
             InclusionTransformation: self._do_inclusion,
             MultiplicationTransformation: self._do_multiplication,
             NormalizeDataTransformation: self._do_normalize_data,
             OneHotEncodingTransformation: self._do_one_hot_encoding,
             SubtractionTransformation: self._do_subtraction,
-            AlbumentationsImageTransformation: self._do_image_transformation,
-            TorchIOImageTransformation: self._do_torchio_image_transformation,
-            ScalarMultiplicationDataTransformation: self._do_scalar_multiplication,
-            ScalarAdditionDataTransformation: self._do_scalar_addition,
+            ImageTransformation: self._do_image_transformation,
         }
 
-    def transform(self, data: pd.DataFrame) -> pd.DataFrame:
+    def transform(self, data: _DataFrameType) -> _DataFrameType:
         """Performs `self.transformations` on `data` sequentially.
 
         Arguments to an operation are extracted by first checking if they are
         referencing another transformed column by checking for the name attribute.
         If not, we then check if they are referencing a non-transformed column by
         using a regular expression. Finally, if the regex comes back empty we take
         the argument 'as is' e.g. a string, integer, etc. After the transformations
         are complete, finally removes any columns that shouldn't be part of the final
         output.
 
         Args:
-            data: The `pandas` dataframe to be transformed.
+            data: The `pandas` or `koalas` dataframe to be transformed.
 
-        Raises:
-            MissingColumnReferenceError: If there is a reference to a non-existing
-                column.
-            TypeError: if there are clashes between column names or if unable
-                to apply transformation.
+        :::warning
+
+        Differing behaviour between `pandas` and `koalas` in this method.
+        Pandas **will not** create columns where there is a type mismatch for the
+        operation but koalas **will** create a column of `NaNs`/`None`s etc.
+
+        :::
         """
         data_columns = set(data.columns)
+
         # Check that all referenced columns are present in `data`
         missing_cols = sorted(self.col_refs.difference(data_columns))
         if missing_cols:
             raise MissingColumnReferenceError(
                 [f"Reference to non-existent column: {c}" for c in missing_cols]
             )
+
         # Loop through transformations and perform them sequentially
         application_errors = []
         cols_to_drop = []
         for transformation in self.transformations:
+            # Transformation name is always set
+            transformation.name = cast(str, transformation.name)
+
             # Check transformation output doesn't clash with existing column
             if isinstance(transformation, MultiColumnOutputTransformation):
                 clashes = data_columns.intersection(transformation.columns)
             else:
                 clashes = data_columns.intersection([transformation.name])
             if clashes:
                 application_errors.extend(
@@ -175,15 +174,15 @@
                 if isinstance(transformation, MultiColumnOutputTransformation):
                     cols_to_drop.extend(transformation.columns)
                 else:  # normal transformation type
                     cols_to_drop.append(transformation.name)
 
             # Attempt to perform transformation. If there is a type mismatch
             # pandas will throw a type error which we catch and skip the
-            # transformation.
+            # transformation. Koalas does not throw TypeErrors
             try:
                 data = operation(data, transformation)
             except TypeError as e:
                 application_errors.append(
                     f"Unable to apply transformation, skipping: "
                     f"{transformation.name}: {e}"
                 )
@@ -194,20 +193,21 @@
 
         # Drop any columns that should not be part of the final dataframe
         if cols_to_drop:
             return data.drop(cols_to_drop, axis=1)
 
         return data
 
-    def batch_transform(self, data: np.ndarray, step: DataSplit) -> np.ndarray:
+    def batch_transform(
+        self, data: np.ndarray, step: Literal["train", "validation"]
+    ) -> np.ndarray:
         """Performs batch transformations.
 
         Args:
             data: The data to be transformed at batch time as a numpy array.
-            step: The step at which the data should be transformed.
 
         Raises:
             InvalidBatchTransformationError: If one of the specified transformations
                 does not inherit from `BatchTimeOperation`.
 
         Returns:
             np.ndarray: The transformed data as a numpy array.
@@ -229,31 +229,31 @@
             operation = self._operators[type(transformation)]
             data = operation(data, transformation)
 
         return data
 
     @staticmethod
     def _apply_arg_conversion(
-        data: pd.DataFrame, *args: Union[Transformation, str, Any]
-    ) -> Union[Union[pd.Series, Any], List[Union[pd.Series, Any]]]:
+        data: _DataFrameType, *args: Union[Any, str, Transformation]
+    ) -> Union[_SeriesType, List[_SeriesType]]:
         """Applies argument conversion to each of the supplied *args.
 
         Check, in order, if it is a:
             - Transformation instance (use transformation output column from data)
             - A column reference (use original column from data)
             - Other (use arg as is)
 
         Args:
             data: Data in a dataframe.
             *args: The args to convert.
 
         Returns: A single converted arg if only one was supplied, or a list of converted
             args in the same order they were supplied.
         """
-        converted: List[Union[pd.Series, Any]] = []
+        converted = []
         for arg in args:
             # See if it is a transformation
             if isinstance(arg, Transformation):
                 converted.append(data[arg.name])
                 continue
 
             # See if it is a column reference
@@ -271,15 +271,15 @@
         if len(converted) == 1:
             return converted[0]
         # Otherwise return list
         return converted
 
     @staticmethod
     def _get_list_of_col_refs(
-        data: pd.DataFrame, cols: Union[str, List[str]], schema_cols: List[str]
+        data: _DataFrameType, cols: Union[str, List[str]], schema_cols: List[str]
     ) -> Tuple[List[str], bool]:
         """Gets the list of actual column references from a list of potentials.
 
         Generates a list of col names from a potential column list argument
         which could be:
             - a list of column references
             - "all": in which case use schema and data to generate list
@@ -296,44 +296,33 @@
         Returns:
             A tuple of the list of column references and a boolean indicating whether
             these have been "pre-extracted" (i.e. don't need to be compared to
             COLUMN_REFERENCE).
         """
         pre_extracted = False
         out_cols = cols
-
-        if isinstance(out_cols, list):
-            # If columns are not pre-extracted, they will have start
-            # with "c:" or "col:"
-            if not any(":" in col_name for col_name in out_cols):
-                pre_extracted = True
-            out_cols = out_cols
+        if out_cols == "all":
+            # Extract all target columns from schema
+            pre_extracted = True
+            out_cols = [col for col in schema_cols if col in data.columns]
         elif isinstance(out_cols, str):
-            if out_cols == "float":
-                pre_extracted = True
-                out_cols = data.select_dtypes(include=["float"]).columns.to_list()
-            elif out_cols == "all":
-                # Extract all target columns from schema
-                pre_extracted = True
-                out_cols = [col for col in schema_cols if col in data.columns]
-            else:
-                # Wrap single column in iterable
-                out_cols = [out_cols]
+            # Wrap single column in iterable
+            out_cols = [out_cols]
         return out_cols, pre_extracted
 
     @staticmethod
-    def _do_addition(data: pd.DataFrame, t: AdditionTransformation) -> pd.DataFrame:
+    def _do_addition(data: _DataFrameType, t: AdditionTransformation) -> _DataFrameType:
         """Performs addition transformation on `data` and returns it."""
         arg1, arg2 = TransformationProcessor._apply_arg_conversion(data, t.arg1, t.arg2)
         data[t.name] = arg1 + arg2
         return data
 
     def _do_clean_data(
-        self, data: pd.DataFrame, t: CleanDataTransformation
-    ) -> pd.DataFrame:
+        self, data: _DataFrameType, t: CleanDataTransformation
+    ) -> _DataFrameType:
         """Cleans categorical and continuous columns in the data.
 
         Replaces infinities and NAs.
         """
         # Get columns
         cols, pre_extracted = self._get_list_of_col_refs(
             data, t.cols, self._schema_cat_cols + self._schema_cont_cols
@@ -352,15 +341,17 @@
                 data[col_ref] = data[col_ref].fillna(value=0.0)
             else:
                 logger.warning(f"{col_ref} not found in Schema. Skipping cleaning.")
 
         return data
 
     @staticmethod
-    def _do_comparison(data: pd.DataFrame, t: ComparisonTransformation) -> pd.DataFrame:
+    def _do_comparison(
+        data: _DataFrameType, t: ComparisonTransformation
+    ) -> _DataFrameType:
         """Performs comparison between arg1 and arg2 of comparison transformation."""
         arg1, arg2 = TransformationProcessor._apply_arg_conversion(data, t.arg1, t.arg2)
 
         arg1 = arg1.to_numpy()
         try:
             # If arg2 is a series-like
             arg2 = arg2.to_numpy()
@@ -371,198 +362,139 @@
         choices = [-1, 0, 1]
 
         data[t.name] = np.select(conditions, choices, default=np.nan).tolist()
 
         return data
 
     @staticmethod
-    def _do_division(data: pd.DataFrame, t: DivisionTransformation) -> pd.DataFrame:
+    def _do_division(data: _DataFrameType, t: DivisionTransformation) -> _DataFrameType:
         """Performs division transformation on `data` and returns it."""
         arg1, arg2 = TransformationProcessor._apply_arg_conversion(data, t.arg1, t.arg2)
         data[t.name] = arg1 / arg2
         return data
 
     @staticmethod
-    def _do_inclusion(data: pd.DataFrame, t: InclusionTransformation) -> pd.DataFrame:
+    def _do_inclusion(
+        data: _DataFrameType, t: InclusionTransformation
+    ) -> _DataFrameType:
         # Only arg should be a column name in this transformation
         """Performs inclusion transformation on `data` and returns it."""
         arg = TransformationProcessor._apply_arg_conversion(data, t.arg)
-        arg = cast(pd.Series, arg)
+        arg = cast(_SeriesType, arg)
         data[t.name] = arg.str.contains(t.in_str)
         return data
 
     @staticmethod
     def _do_multiplication(
-        data: pd.DataFrame, t: MultiplicationTransformation
-    ) -> pd.DataFrame:
+        data: _DataFrameType, t: MultiplicationTransformation
+    ) -> _DataFrameType:
         """Performs multiplication transformation on `data` and returns it."""
         arg1, arg2 = TransformationProcessor._apply_arg_conversion(data, t.arg1, t.arg2)
         data[t.name] = arg1 * arg2
         return data
 
     def _do_normalize_data(
-        self,
-        data: pd.DataFrame,
-        t: NormalizeDataTransformation,
-    ) -> pd.DataFrame:
+        self, data: _DataFrameType, t: NormalizeDataTransformation
+    ) -> _DataFrameType:
         """Normalizes numeric columns using their mean and stddev.
 
-        Note: The default value of the transformation.cols for this
-        transformation is "float". Results in mean of 0 and stddev of 1.
+        Results in mean of 0 and stddev of 1.
         """
         cols, pre_extracted = self._get_list_of_col_refs(
-            data,
-            t.cols,
-            self._schema_cont_cols,
+            data, t.cols, self._schema_cont_cols
         )
         if not pre_extracted:
             cols = [_extract_col_ref(col) for col in cols]
+
         for col_ref in cols:
             if not is_numeric_dtype(data[col_ref]):
                 raise TypeError(
                     f'Cannot normalize column "{col_ref}" as it is not numeric.'
                 )
             mean = data[col_ref].mean()
             stddev = data[col_ref].std()
             data[col_ref] = (data[col_ref] - mean) / (1e-7 + stddev)
-        if self.schema is not None:
-            for col in cols:
-                self.schema.features["continuous"][col].dtype = data[col].dtype
 
-        return data
+        for col in self._schema_cont_cols:
+            # If self._schema_cont_cols is not empty then self.schema cannot be None
+            assert self.schema is not None  # nosec
+            self.schema.features["continuous"][col].dtype = data[col].dtype
 
-    def _do_scalar_multiplication(
-        self, data: pd.DataFrame, t: ScalarMultiplicationDataTransformation
-    ) -> pd.DataFrame:
-        """Performs column multiplication with scalar.
-
-        Only applicable to numeric columns.
-
-        Raises:
-            TypeError: if one of the referenced columns is not numeric.
-            MissingColumnReferenceError: if one of the referenced columns is missing.
-        """
-        # If the scalar is a numerical value, we apply the scalar
-        # multiplication to all numerical columns.
-        if isinstance(t.scalar, (int, float)):
-            cols, pre_extracted = self._get_list_of_col_refs(
-                data, t.cols, self._schema_cont_cols
-            )
-            if not pre_extracted:
-                cols = [_extract_col_ref(col) for col in cols]
-
-            data[cols] = data[cols] * t.scalar
-            return data
-        # Otherwise, it is a dictionary mapping column names to scalar,
-        # so we multiply each column with the scalar it is mapped to.
-        else:
-            missing_cols = set(t.scalar.keys()) - set(data.columns)
-            if missing_cols:
-                raise MissingColumnReferenceError(
-                    [f"Reference to non-existent column: {c}" for c in missing_cols]
-                )
-            for col_ref, scalar in t.scalar.items():
-                if not is_numeric_dtype(data[col_ref]):
-                    raise TypeError(
-                        f'Cannot normalize column "{col_ref}" as it is not numeric.'
-                    )
-                data[col_ref] = data[col_ref] * scalar
-            return data
-
-    def _do_scalar_addition(
-        self, data: pd.DataFrame, t: ScalarAdditionDataTransformation
-    ) -> pd.DataFrame:
-        """Performs column addition with scalar.
-
-        Only applicable to numeric columns.
-
-        Raises:
-            TypeError: if one of the referenced columns is not numeric.
-            MissingColumnReferenceError: if one of the referenced columns is missing.
-        """
-        # If the scalar is a numerical value, we apply the scalar
-        # addition to all numerical columns.
-        if isinstance(t.scalar, (int, float)):
-            cols, pre_extracted = self._get_list_of_col_refs(
-                data, t.cols, self._schema_cont_cols
-            )
-            if not pre_extracted:
-                cols = [_extract_col_ref(col) for col in cols]
-
-            data[cols] = data[cols] + t.scalar
-            return data
-        # Otherwise, it is a dictionary mapping column names to scalar,
-        # so we add the scalar to the column it is mapped to.
-        else:
-            missing_cols = set(t.scalar.keys()) - set(data.columns)
-            if missing_cols:
-                raise MissingColumnReferenceError(
-                    [f"Reference to non-existent column: {c}" for c in missing_cols]
-                )
-            for col_ref, scalar in t.scalar.items():
-                if not is_numeric_dtype(data[col_ref]):
-                    raise TypeError(
-                        f'Cannot normalize column "{col_ref}" as it is not numeric.'
-                    )
-                data[col_ref] = data[col_ref] + scalar
-            return data
+        return data
 
     @staticmethod
     def _do_one_hot_encoding(
-        data: pd.DataFrame, t: OneHotEncodingTransformation
-    ) -> pd.DataFrame:
+        data: _DataFrameType, t: OneHotEncodingTransformation
+    ) -> _DataFrameType:
         """Performs one hot encoding transformation on `data` and returns it."""
         # Get unencoded data and columns
-        # Only one arg supplied so therefore is single series that's returned
-        arg: pd.Series = cast(
-            pd.Series, TransformationProcessor._apply_arg_conversion(data, t.arg)
-        )
+        arg: _SeriesType = TransformationProcessor._apply_arg_conversion(data, t.arg)
         ohe_cols: List[str] = sorted(t.columns)
         n_rows: int = len(arg)
         n_cols: int = len(ohe_cols)
 
         # Create an appropriately sized dataframe, filled with zeros
-        ohe_df: pd.DataFrame = pd.DataFrame(
+        df_lib_type = _get_df_library_type(data)
+        df_lib = _get_df_library(data)
+        ohe_df: _DataFrameType = df_lib.DataFrame(
             data=np.zeros(shape=(n_rows, n_cols), dtype=np.int8), columns=ohe_cols
         )
 
         # First, set all values in the unknown column to 1 which correspond to
         # non-null values in the arg Series. These will be marked into the correct
         # column as we find matches, and if no match is found, they should be in
         # this column anyway.
-        not_null_idxs = arg.index[arg.notnull()]
+        if df_lib_type == _DataFrameLib.KOALAS:
+            # koalas indexes aren't subscriptable so must use a BooleanIndex instead
+            not_null_idxs = arg[arg.notnull()].index
+            # koalas indexes don't support __iter__ so will break the .loc
+            # setting below; instead we must convert it to a numpy array (which
+            # still won't work as it doesn't match a protocol pyspark expects) and
+            # then to a list. ks.Index.to_list() isn't implemented either.
+            #
+            # This approach is relatively slow compared to the native pandas
+            # implementation and should probably be replaced with something
+            # more performant.
+            # TODO: [BIT-1042] Increase performance, avoid koalas multi-conversion.
+            not_null_idxs = not_null_idxs.to_numpy().tolist()
+        else:
+            not_null_idxs = arg.index[arg.notnull()]
         ohe_df.loc[not_null_idxs, t.unknown_col] = 1
 
         # Iterate through the (value, target_col) pairs and set the correct column
         # to 1 for all locations the value is found. Sets the unknown column in
         # the same locations to 0.
         for val, target_col in t.values.items():
-            match_idxs = arg.index[arg == val]
+            if df_lib_type == _DataFrameLib.KOALAS:
+                # Same issues as above
+                match_idxs = arg[arg == val].index
+                match_idxs = match_idxs.to_numpy().tolist()
+            else:
+                match_idxs = arg.index[arg == val]
             ohe_df.loc[match_idxs, target_col] = 1
             ohe_df.loc[match_idxs, t.unknown_col] = 0
 
-        return pd.concat([data, ohe_df], axis=1)
+        if df_lib_type == _DataFrameLib.KOALAS:
+            with df_lib.option_context("compute.ops_on_diff_frames", True):  # type: ignore[attr-defined] # Reason: koalas has this function # noqa: B950
+                return df_lib.concat(
+                    [data, ohe_df], axis=1, sort=True
+                )  # sort=True is needed for koalas
+        else:
+            return df_lib.concat([data, ohe_df], axis=1)
 
     @staticmethod
     def _do_subtraction(
-        data: pd.DataFrame, t: SubtractionTransformation
-    ) -> pd.DataFrame:
+        data: _DataFrameType, t: SubtractionTransformation
+    ) -> _DataFrameType:
         """Performs subtraction transformation on `data` and returns it."""
         arg1, arg2 = TransformationProcessor._apply_arg_conversion(data, t.arg1, t.arg2)
         data[t.name] = arg1 - arg2
         return data
 
     @staticmethod
     def _do_image_transformation(
-        data: np.ndarray, t: AlbumentationsImageTransformation
+        data: np.ndarray, t: ImageTransformation
     ) -> np.ndarray:
         """Performs image transformation on `data` and returns it."""
+        logger.debug(f"Applying image transformations: {t.transformations}")
         tfm = t.get_callable()
         return tfm(image=data)["image"]
-
-    @staticmethod
-    def _do_torchio_image_transformation(
-        data: np.ndarray, t: TorchIOImageTransformation
-    ) -> np.ndarray:
-        """Performs image transformation on `data` and returns it."""
-        tfm = t.get_callable()
-        return tfm(data)
```

### Comparing `bitfount-0.5.86/bitfount/transformations/references.py` & `bitfount-0.5.9/bitfount/transformations/references.py`

 * *Files 4% similar despite different names*

```diff
@@ -51,24 +51,24 @@
     Raises:
         NotTransformationReferenceError:
             Raised if this is not a transformation reference by type or
             content.
     """
     try:
         return _extract_ref_regex(t_str, _TRANSFORMATION_REFERENCE)
-    except IncorrectReferenceError as e:
+    except IncorrectReferenceError:
         raise NotTransformationReferenceError(
             "Incorrect format for transformation reference; transformation references"
             'should start with "t:", "tran:" or "transformation:"'
-        ) from e
-    except TypeError as e:
+        )
+    except TypeError:
         raise NotTransformationReferenceError(
             f"Incorrect type for transformation reference; "
             f"expected str, got {type(t_str)}"
-        ) from e
+        )
 
 
 def _extract_col_ref(col_str: str) -> str:
     """Extracts the column name being referenced if possible.
 
     Args:
         col_str: The potential column reference.
@@ -78,23 +78,23 @@
 
     Raises:
         NotColumnReferenceError:
             Raised if this is not a column reference by type or content.
     """
     try:
         return _extract_ref_regex(col_str, _COLUMN_REFERENCE)
-    except IncorrectReferenceError as e:
+    except IncorrectReferenceError:
         raise NotColumnReferenceError(
             "Incorrect format for column reference; column references should "
             'start with "c:", "col:" or "column:"'
-        ) from e
-    except TypeError as e:
+        )
+    except TypeError:
         raise NotColumnReferenceError(
             f"Incorrect type for column reference; expected str, got {type(col_str)}"
-        ) from e
+        )
 
 
 def _extract_ref(r_str: str) -> str:
     """Extracts the column or transformation name being referenced if possible.
 
     Args:
         r_str: The potential column or transformation reference.
```

### Comparing `bitfount-0.5.86/bitfount/transformations/unary_operations.py` & `bitfount-0.5.9/bitfount/transformations/unary_operations.py`

 * *Files 2% similar despite different names*

```diff
@@ -18,52 +18,48 @@
     MultiColumnOutputTransformation,
     Transformation,
     _TransformationSchema,
 )
 from bitfount.transformations.exceptions import IncorrectReferenceError
 from bitfount.transformations.references import _extract_ref
 from bitfount.types import _JSONDict
-from bitfount.utils import delegates
 
 
-@delegates()
 @attr.dataclass(kw_only=True)
 class UnaryOperation(Transformation):
     """The base abstract class for all Unary Operation Transformations.
 
     Args:
         arg: The argument to the transformation.
     """
 
     arg: Any
 
 
-@delegates()
 @attr.dataclass(kw_only=True)
 class StringUnaryOperation(UnaryOperation):
     """This class represents any UnaryOperation where arg can only be a string.
 
     Args:
         arg: The argument to the transformation as a string.
     """
 
     arg: str
 
 
-@delegates()
 @attr.dataclass(kw_only=True)
 class OneHotEncodingTransformation(
     StringUnaryOperation, MultiColumnOutputTransformation
 ):
     """One hot encoding transformation.
 
     Represents the transformation of a column into a series of one-hot encoded
     columns.
 
-    Args:
+    Attributes:
         arg: Column or transformation reference to one-hot encode.
         values: Column values that should be one-hot encoded. This can either be
             a list of values, in which case the one-hot encoding will produce
             columns named `{name}_{value}`, or a dictionary of values to desired
             column suffixes, in which case the encoding will use those suffixes
             (if an entry in the dictionary maps to None, the column name will be
             generated in the same way as described above).
@@ -97,15 +93,14 @@
     # This is used to mark if name was set or not
     _no_name_provided: bool = attr.ib(init=False, default=False)
 
     def __attrs_post_init__(self) -> None:
         # Set this first as the super() call will randomly init name if not set
         if not self.name:
             self._no_name_provided = True
-
         super().__attrs_post_init__()
 
         # Check at least one value provided
         if len(self._raw_values) == 0:
             raise ValueError("At least one value must be provided to one-hot encode.")
 
         self._produce_full_col_map()
@@ -125,15 +120,15 @@
                 return _extract_ref(self.arg)
             except IncorrectReferenceError as ire:
                 raise ValueError(
                     "No name provided and no reference found in arg."
                 ) from ire
         # If name is provided, use it
         else:
-            return self.name
+            return cast(str, self.name)
 
     @property
     def unknown_col(self) -> str:
         """Returns the name of the column that unknown values are encoded to."""
         return f"{self.prefix}_{self.unknown_suffix}"
 
     def _produce_full_col_map(self) -> None:
@@ -213,15 +208,14 @@
             # Need to move schema "values" to "raw_values" for __init__ call.
             # It's not "_raw_values" because of how attrs handles private variables.
             raw_values = data.pop("values")
             data["raw_values"] = raw_values
             return OneHotEncodingTransformation(**data)
 
 
-@delegates()
 @attr.dataclass(kw_only=True)
 class InclusionTransformation(StringUnaryOperation):
     """Represents the test for substring inclusion in a column's entries.
 
     Check whether `in_str` (the test string) is in the elements of `arg` (the column).
 
     Args:
```

### Comparing `bitfount-0.5.86/bitfount/transformations/utils.py` & `bitfount-0.5.9/bitfount/transformations/utils.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 """Utility classes and functions related to Transformations.
 
 This module contains useful utility classes and functions for working
 with Transformations.
 """
-from typing import Any
+from typing import Any, cast
 
 import yaml
 
 
 class _MarshmallowYamlShim:
     """Shim to allow Marshmallow to load/save to YAML.
 
@@ -19,8 +19,8 @@
     def loads(s: str) -> Any:
         """Loads the supplied YAML string as an object."""
         return yaml.safe_load(s)
 
     @staticmethod
     def dumps(obj: Any) -> str:
         """Dumps the supplied object to a yaml string."""
-        return yaml.dump(obj)
+        return cast(str, yaml.dump(obj))
```

### Comparing `bitfount-0.5.86/bitfount/types.py` & `bitfount-0.5.9/bitfount/types.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,244 +1,232 @@
 """Type hints, enums and protocols for the Bitfount libraries."""
 from __future__ import annotations
 
+from enum import Enum, auto
 import os
+from pathlib import Path
 from typing import (
     TYPE_CHECKING,
     Any,
-    ClassVar,
     Dict,
+    Iterable,
     List,
     Mapping,
-    MutableMapping,
     NewType,
     Optional,
     Protocol,
     Sequence,
     Tuple,
+    Type,
     TypeVar,
     Union,
     runtime_checkable,
 )
 
-import marshmallow
-from marshmallow import fields
+from cryptography.hazmat.primitives.asymmetric.rsa import RSAPrivateKey
+from databricks import koalas as ks
 import numpy as np
-from pandas._typing import Dtype
+import pandas as pd
 
 if TYPE_CHECKING:
-    from bitfount.data.dataloaders import BitfountDataLoader
-    from bitfount.data.datasources.base_source import BaseSource
+    from bitfount.data.dataloader import _BitfountDataLoader
+    from bitfount.data.datasource import DataSource
     from bitfount.data.datastructure import DataStructure
-    from bitfount.data.schema import BitfountSchema
-    from bitfount.federated.helper import TaskContext
     from bitfount.federated.model_reference import BitfountModelReference
+    from bitfount.federated.shim import BackendTensorShim
     from bitfount.metrics import Metric
+    from bitfount.models.base_models import ModelContext, _BaseModel
 
-__all__: List[str] = ["BaseDistributedModelProtocol", "DistributedModelProtocol"]
+__all__: List[str] = ["DistributedModelProtocol"]
 
 # Tensor dtype type variable
 T_DTYPE = TypeVar("T_DTYPE", covariant=True)
 
 
 # TensorLike protocol and TensorLike composite types
 class _TensorLike(Protocol):
     """Protocol defining what methods and operations a Generic Tensor can perform."""
 
     def __mul__(self: _TensorLike, other: Any) -> _TensorLike:
-        """Multiplies the self tensor with the other tensor and returns the result."""
         ...
 
     def __sub__(self: _TensorLike, other: Any) -> _TensorLike:
-        """Subtracts the other tensor from the self tensor and returns the result."""
         ...
 
     def squeeze(self: _TensorLike, axis: Optional[Any] = None) -> _TensorLike:
         """Returns a tensor with all the dimensions of input of size 1 removed."""
         ...
 
 
 # Weight update types
-_SerializedWeights = Mapping[str, List[float]]
-_Residuals = Mapping[str, _TensorLike]
-_Weights = Mapping[str, _TensorLike]
+_SerializedWeights = Dict[str, Union[List[float], _TensorLike]]
+_WeightDict = Dict[str, _TensorLike]
+_WeightMapping = Mapping[str, _TensorLike]
 
 
-# Schema dtypes
-_DtypesValues = Union[Dtype, np.dtype]
-_Dtypes = Dict[str, _DtypesValues]
+# Pandas/Koalas related types
+_DataFrameType = Union[pd.DataFrame, ks.DataFrame]
+_SeriesType = Union[pd.Series, ks.Series]
 
 
+class _PandasLikeModule(Protocol):
+    """A protocol that matches the needed functions/classes from the pandas module."""
+
+    # These are the actual classes in the module; i.e. they can be instantiated, etc.
+    DataFrame: Type[_DataFrameType]
+    Series: Type[_SeriesType]
+
+    # pandas is not fully type-hinted. This signature was extracted and modified
+    # from inspect.signature(pd.read_csv). It is not the full signature, but only
+    # the elements we need and care about. Add more details if needed.
+    def read_csv(
+        self,
+        filepath_or_buffer: Union[str, os.PathLike],
+        *args: Any,
+        **kwargs: Any,
+    ) -> _DataFrameType:
+        """See pandas.read_csv() for details."""  # noqa: B950, D402; wrapper around other method
+        ...
+
+    # pandas is not fully type-hinted. This signature was extracted and modified
+    # from inspect.signature(pd.read_sql_table). It is not the full signature, but
+    # only the elements we need and care about. Add more details if needed.
+    def read_sql_table(
+        self,
+        table_name: str,
+        con: str,
+        schema: Optional[str] = None,
+        index_col: Optional[Union[str, Sequence[str]]] = None,
+        *args: Any,
+        columns: Optional[Sequence[str]] = None,
+        **kwargs: Any,
+    ) -> _DataFrameType:
+        """See pandas.read_sql_table() for details."""  # noqa: B950, D402; wrapper around other method
+        ...
+
+    # pandas is not fully type-hinted. This signature was extracted and modified
+    # from inspect.signature(pd.to_datetime). It is not the full signature, but
+    # only the elements we need and care about. Add more details if needed.
+    def to_datetime(
+        self,
+        arg: _SeriesType,
+        *args: Any,
+        infer_datetime_format: bool = False,
+        **kwargs: Any,
+    ) -> _SeriesType:
+        """See pandas.to_datetime() for details."""  # noqa: B950, D402; wrapper around other method
+        ...
+
+    # pandas is not fully type-hinted. This signature was extracted and modified
+    # from inspect.signature(pd.to_datetime). It is not the full signature, but
+    # only the elements we need and care about. Add more details if needed.
+    def concat(
+        self, objs: Iterable[_DataFrameType], axis: int = 0, *args: Any, **kwargs: Any
+    ) -> _DataFrameType:
+        """See pandas.concat() for details."""  # noqa: B950, D402; wrapper around other method
+        ...
+
+
+class _DataFrameLib(Enum):
+    """An enum representing dataframe libraries."""
+
+    PANDAS = auto()
+    KOALAS = auto()
+
+
+# DistributedModel protocol and types
 @runtime_checkable
-class BaseDistributedModelProtocol(Protocol[T_DTYPE]):
-    """Federated Model structural type that only specifies the methods.
+class DistributedModelProtocol(Protocol):
+    """Federated Model structural type.
 
-    The reason for this protocol is that `issubclass` checks with Protocols can only
-    be performed if the Protocol only specifies methods and not attributes. We still
-    want to specify the attributes in another protocol though for greater type safety,
-    (both statically and dynamically) so we have this protocol that only specifies
-    methods and another protocol that specifies the attributes.
+    This protocol should be implemented by classes that inherit from either
+    `BitfountModel`, or both of `_BaseModel` and `DistributedModelMixIn`.
     """
 
-    def tensor_precision(self) -> T_DTYPE:
-        """Defined in DistributedModelMixIn."""
+    name: str
+    datastructure: DataStructure
+    # Type hints below indicate that one of either `epochs` or `steps` needs to be
+    # supplied by the mixed-in class or other classes in the inheritance hierarchy
+    epochs: Optional[int]
+    steps: Optional[int]
+    _total_num_batches_trained: int
 
-    def get_param_states(self) -> _Weights:
+    @staticmethod
+    def backend_tensor_shim() -> BackendTensorShim:
         """Defined in DistributedModelMixIn."""
 
-    def apply_weight_updates(self, weight_updates: Sequence[_Weights]) -> _Weights:
+    def tensor_precision(self) -> T_DTYPE:
         """Defined in DistributedModelMixIn."""
 
-    def update_params(self, new_model_params: _Weights) -> None:
+    def get_param_states(self) -> _WeightDict:
         """Defined in DistributedModelMixIn."""
 
-    def serialize_params(self, weights: _Weights) -> _SerializedWeights:
+    def apply_weight_updates(
+        self, weight_updates: Sequence[_WeightMapping]
+    ) -> _WeightDict:
         """Defined in DistributedModelMixIn."""
 
-    def deserialize_params(self, serialized_weights: _SerializedWeights) -> _Weights:
+    def update_params(self, new_model_params: _WeightMapping) -> None:
         """Defined in DistributedModelMixIn."""
 
-    def diff_params(self, old_params: _Weights, new_params: _Weights) -> _Residuals:
+    def diff_params(
+        self, old_params: _WeightMapping, new_params: _WeightMapping
+    ) -> _WeightDict:
         """Defined in DistributedModelMixIn."""
 
     def set_model_training_iterations(self, iterations: int) -> None:
         """Defined in DistributedModelMixIn."""
 
     def reset_trainer(self) -> None:
         """Defined in DistributedModelMixIn."""
 
-    def set_datastructure_identifier(self, datastructure_identifier: str) -> None:
-        """Defined in DistributedModelMixIn."""
-
     def fit(
         self,
-        data: Optional[BaseSource] = None,
+        data: Optional[DataSource] = None,
         metrics: Optional[Dict[str, Metric]] = None,
         pod_identifiers: Optional[List[str]] = None,
+        private_key_or_file: Optional[Union[RSAPrivateKey, Path]] = None,
         **kwargs: Any,
     ) -> Optional[Dict[str, str]]:
         """Defined in DistributedModelMixIn."""
 
     def log_(self, name: str, value: Any, **kwargs: Any) -> Any:
         """Defined in DistributedModelMixIn."""
 
     def initialise_model(
         self,
-        data: Optional[BaseSource] = None,
-        context: Optional[TaskContext] = None,
+        data: Optional[DataSource] = None,
+        context: Optional[ModelContext] = None,
     ) -> None:
         """Defined in _BaseModel."""
 
     def evaluate(
-        self,
-        test_dl: Optional[BitfountDataLoader] = None,
-        pod_identifiers: Optional[List[str]] = None,
-        **kwargs: Any,
-    ) -> Union[Tuple[np.ndarray, np.ndarray], Dict[str, float]]:
-        """Defined in _BaseModel."""
-
-    def predict(
-        self,
-        data: Optional[BaseSource] = None,
-        pod_identifiers: Optional[List[str]] = None,
-        **kwargs: Any,
-    ) -> Union[List[np.ndarray], Dict[str, List[np.ndarray]]]:
+        self, test_dl: Optional[_BitfountDataLoader] = None, **kwargs: Any
+    ) -> Tuple[np.ndarray, np.ndarray]:
         """Defined in _BaseModel."""
 
     def serialize(self, filename: Union[str, os.PathLike]) -> None:
         """Defined in _BaseModel."""
 
     def deserialize(self, filename: Union[str, os.PathLike]) -> None:
         """Defined in _BaseModel."""
 
-
-# DistributedModel protocol and types
-@runtime_checkable
-class DistributedModelProtocol(BaseDistributedModelProtocol, Protocol[T_DTYPE]):
-    """Federated Model structural type.
-
-    This protocol should be implemented by classes that inherit from either
-    `BitfountModel`, or both of `_BaseModel` and `DistributedModelMixIn`.
-    """
-
-    class_name: str
-    datastructure: DataStructure
-    schema: BitfountSchema  # TODO: [NO_TICKET: To discuss about the schema being here.] # noqa: B950
-    # Type hints below indicate that one of either `epochs` or `steps` needs to be
-    # supplied by the mixed-in class or other classes in the inheritance hierarchy
-    epochs: Optional[int]
-    steps: Optional[int]
-    _total_num_batches_trained: int
-    # Used to identify the relevant section of the datastructure that applies to
-    # the pod/logical pod/datasource that this model is being run against (if any)
-    _datastructure_identifier: Optional[str] = None
-    # Denotes if param_clipping params are given to the model.
-    param_clipping: Optional[Dict[str, int]]
-    metrics: Optional[MutableMapping[str, Metric]]
-    fields_dict: ClassVar[T_FIELDS_DICT]
-    nested_fields: ClassVar[T_NESTED_FIELDS]
-
-    @property
-    def initialised(self) -> bool:
-        """Defined in _BaseModel."""
-
-
-T_FIELDS_DICT = Dict[str, marshmallow.fields.Field]
-T_NESTED_FIELDS = Dict[str, Mapping[str, Any]]
-
-
-class _BaseSerializableObjectMixIn:
-    """The base class from which all serializable objects should inherit from.
-
-    Attributes:
-        fields_dict: A dictionary mapping all attributes that will be serialized
-            in the class to their marshamllow field type. (e.g.
-            fields_dict = {"class_name": fields.Str()}).
-        nested_fields: A dictionary mapping all nested attributes to a registry
-            that contains class names mapped to the respective classes.
-            (e.g. nested_fields = {"datastructure": datastructure.registry})
-    """
-
-    fields_dict: ClassVar[T_FIELDS_DICT] = {"class_name": fields.Str()}
-    nested_fields: ClassVar[T_NESTED_FIELDS] = {}
-
-
-class BinaryFile(fields.Field):
-    """A marshmallow field for binary files."""
-
-    def _serialize(
-        self, value: str, attr: Optional[str], obj: Any, **kwargs: Any
-    ) -> str:
-        """Reads the file and returns the contents as a hex string."""
-        with open(value, "rb") as f:
-            return f.read().hex()
-
-    def _deserialize(
-        self,
-        value: str,
-        attr: Optional[str],
-        data: Optional[Mapping[str, Any]],
-        **kwargs: Any,
-    ) -> str:
-        """Simply returns the hex string."""
-        return value
+    @classmethod
+    def get_schema(cls) -> Type[_BaseModel._Schema]:
+        """Defined in the pytorch models."""
 
 
 if TYPE_CHECKING:
     _DistributedModelTypeOrReference = Union[
         DistributedModelProtocol, BitfountModelReference
     ]
 
 # Serialization Types
-_JSONDict = Dict[str, Any]  # A JSON-esque dictionary that is serializable
-
-# Common Types
-_StrAnyDict = Dict[str, Any]  # A dictionary with string keys and any value types
+_JSONDict = Dict[str, Any]
 
-# S3 Interaction Types
+# s3 interaction types
 _S3PresignedPOSTURL = NewType("_S3PresignedPOSTURL", str)
-# HTTPX explicitly expects a `dict` rather than a `mapping`
-_S3PresignedPOSTFields = NewType("_S3PresignedPOSTFields", Dict[str, str])
+_S3PresignedPOSTFields = NewType("_S3PresignedPOSTFields", Mapping[str, str])
 _S3PresignedURL = NewType("_S3PresignedURL", str)  # for GET requests
 
 # SAML Types
 _SAMLResponse = Mapping[str, Any]
```

### Comparing `bitfount-0.5.86/bitfount.egg-info/requires.txt` & `bitfount-0.5.9/bitfount.egg-info/requires.txt`

 * *Files 16% similar despite different names*

```diff
@@ -1,401 +1,371 @@
-GPUtil>=1.4.0
-Pillow<9.4.0,>=8.3.2
-PyYAML==6.0.1
+aiohttp>=3.7.4
+bleach>=3.3.0
+cryptography>=3.3.2
+lxml>=4.6.5
+nltk>=3.6.4
+notebook>=6.4.1
+Pillow>=8.3.2
+PyYAML>=5.4
+urllib3>=1.26.5
 aiohttp>=3.8.0
 albumentations>=1.0.0
-async-timeout~=4.0
 attrs>=19.3.0
-bitfount-apispec>=6.3.0
-bleach>=3.3.0
 cryptography>=3.4.4
-decorator>=5.0.0
-desert>=2022.9.22
-docstring_parser>=0.14.1
-environs>=9.0
-envyaml>=1.10.211231
+desert>=2020.1.3
 fire>=0.1.0
-gmpy2>=2.1.0
-grpcio>=1.48.0
-httpx>=0.23.0
-ipython>=8.8.0
-isort>=5.0.1
-lightgbm<4.0.0,>=3.0.0
-lxml>=4.6.5
+GPUtil>=1.4.0
+grpcio>=1.38.1
+koalas>=1.8.1
+marshmallow>=3.13.0
 marshmallow-enum>=1.5.1
-marshmallow-polyfield>=5.10
 marshmallow-union>=0.1.15
-marshmallow>=3.13.0
-methodtools>=0.4.5
+matplotlib>=3.4.3
 msgpack>=1.0.0
-numpy>=1.22
-packaging>=22.0
-pandas<2,>=1.3.0
-pandasql>=0.7.3
-protobuf<5,>=4
+numpy<=1.21.5,>=1.20.0
+pandas>=1.3.0
+Pillow>=8.3.2
 psutil>=5.0.0
-pyarrow~=7.0.0
 pydantic>=1.0
-pyjwt>=2.4.0
-pytest>=7.2.0
-pytorch-lightning<2,>=1.6.0
-pytorch-tabnet>=3.0.0
+pyjwt>=2.0.0
+pyspark>=3.1.1
+PyYAML~=5.4
 requests>=2.26.0
 scikit-image>=0.18.0
 scikit-learn>=1.0
-selenium<4.3
-sqlalchemy<2,>=1.4.0
-sqlparse>=0.4.2
-sqlvalidator>=0.0.18
 statsmodels>=0.11.0
-tensorboard>=2.2.0
+lightgbm>=3.0.0
+opacus<1.0.0,>=0.11.0
+pytorch-lightning~=1.3.8
+pytorch-tabnet>=3.0.0
+torch>=1.7.0
 torch-optimizer>=0.1.0
-torch<2,>=1.8.1
-torchio>=0.18.91
-torchmetrics<=0.10.2,>=0.6.0
-torchvision<0.15,>=0.9.1
-transformers>=4.23.0
-types-decorator>=5.1.4
-typing_extensions>=4.0.0
-urllib3<2,>=1.26.5
+torchmetrics<=0.5.1,>=0.2.0
+torchvision>=0.8.0
 
-[:python_version < "3.9"]
-ipython<8.13
-
-[dp]
-opacus<1.2,>=1
-pytorch-lightning!=1.6.4,!=1.6.5,<1.7,<2,>=1.6.0
-smartnoise-sql<1,>=0.2.3
-torch<2,==1.8.1,>=1.8.1
-torchcsprng>=0.2.1
-torchvision<0.15,==0.9.1,>=0.9.1
+[dev]
+altgraph==0.17.2
+attrs==21.4.0
+bandit==1.7.2
+black==22.1.0
+certifi==2021.10.8
+cfgv==3.3.1
+click==8.0.3
+colorama==0.4.4
+distlib==0.3.4
+filelock==3.4.2
+flake8==4.0.1
+flake8-bugbear==22.1.11
+flake8-docstrings==1.6.0
+gitdb==4.0.9
+gitpython==3.1.26
+grpc-stubs==1.24.7
+grpcio==1.43.0
+grpcio-tools==1.43.0
+identify==2.4.7
+idna==3.3
+importlib-metadata==4.10.1
+interrogate==1.5.0
+ipython-genutils==0.2.0
+isort==5.10.1
+jsonpointer==2.2
+jsonschema[format]==3.2.0
+jupyter-core==4.9.1
+jupytext==1.13.6
+macholib==1.15.2
+mako==1.1.6
+markdown==3.3.6
+markdown-it-py==1.1.0
+markupsafe==2.0.1
+mccabe==0.6.1
+mdit-py-plugins==0.3.0
+modulegraph==0.19.2
+msgpack-types==0.2.0
+mypy==0.931
+mypy-extensions==0.4.3
+nbformat==5.1.3
+nodeenv==1.6.0
+pandas-stubs==1.2.0.47
+pathspec==0.9.0
+pbr==5.8.0
+pdoc3==0.10.0
+pep517==0.12.0
+pip-tools==6.4.0
+platformdirs==2.4.1
+pre-commit==2.17.0
+protobuf==3.19.4
+py==1.11.0
+py2app==0.26.1
+pycodestyle==2.8.0
+pydocstyle==6.1.1
+pyflakes==2.4.0
+pyinstaller==4.8
+pyinstaller-hooks-contrib==2022.0
+pyrsistent==0.18.1
+pyyaml==5.4.1
+rfc3987==1.3.8
+rope==0.22.0
+six==1.16.0
+smmap==5.0.0
+snowballstemmer==2.2.0
+sphobjinv==2.2
+stevedore==3.5.0
+strict-rfc3339==0.7
+tabulate==0.8.9
+toml==0.10.2
+tomli==1.2.3
+traitlets==5.1.1
+types-click==7.1.8
+types-flask==1.1.6
+types-jinja2==2.11.9
+types-markupsafe==1.1.10
+types-pillow==9.0.5
+types-psutil==5.8.20
+types-pytest-lazy-fixture==0.6.2
+types-python-dateutil==2.8.9
+types-pyyaml==6.0.4
+types-requests==2.27.8
+types-selenium==3.141.7
+types-setuptools==57.4.8
+types-urllib3==1.26.8
+types-werkzeug==1.0.9
+typing-extensions==4.0.1
+virtualenv==20.13.0
+webcolors==1.11.1
+wheel==0.37.1
+zipp==3.7.0
 
 [tests]
-aiohttp==3.8.4
-aiosignal==1.3.1
-alembic==1.11.1
-anyio==3.7.0
-appdirs==1.4.4
-appnope==0.1.3
-argon2-cffi==21.3.0
-argon2-cffi-bindings==21.2.0
-arrow==1.2.3
-asttokens==2.2.1
+absl-py==1.0.0
+aiohttp==3.8.1
+aiosignal==1.2.0
+alembic==1.7.5
 async-generator==1.10
 async-timeout==4.0.2
-attrs==23.1.0
-backcall==0.2.0
-backports-zoneinfo==0.2.1
-beautifulsoup4==4.12.2
-bleach==6.0.0
-blinker==1.6.2
-boto3==1.26.153
-botocore==1.29.153
+attrs==21.4.0
+boto3==1.20.46
+botocore==1.23.46
 bravado==11.0.3
-bravado-core==5.17.1
-cachetools==5.3.1
-certifi==2023.5.7
-cffi==1.15.1
-chardet==5.1.0
-charset-normalizer==3.1.0
-chromedriver-autoinstaller==0.4.0
-click==8.1.3
-cloudpickle==2.2.1
-colorama==0.4.6
-comm==0.1.3
-contourpy==1.1.0
-coverage[toml]==7.2.7
-cryptography==41.0.1
-cycler==0.11.0
-databricks-cli==0.17.7
-debugpy==1.6.7
-decorator==5.1.1
-defusedxml==0.7.1
-distlib==0.3.6
-docker==6.1.3
+bravado-core==5.17.0
+cachetools==5.0.0
+certifi==2021.10.8
+cffi==1.15.0
+charset-normalizer==2.0.11
+chromedriver-autoinstaller==0.3.1
+click==8.0.3
+cloudpickle==2.0.0
+configparser==5.2.0
+coverage[toml]==6.3
+cryptography==36.0.1
+databricks-cli==0.16.2
+distlib==0.3.4
+docker==5.0.3
 docker-pycreds==0.4.0
-entrypoints==0.4
-et-xmlfile==1.1.0
-exceptiongroup==1.1.1
+entrypoints==0.3
 execnet==1.9.0
-executing==1.2.0
-fastjsonschema==2.17.1
-filelock==3.12.2
-flask==2.3.2
-fonttools==4.40.0
-fqdn==1.5.1
-frozenlist==1.3.3
-future==0.18.3
-gitdb==4.0.10
-gitpython==3.1.31
-greenlet==2.0.2
+filelock==3.4.2
+flask==2.0.2
+frozenlist==1.3.0
+future==0.18.2
+gitdb==4.0.9
+gitpython==3.1.26
+google-auth==2.6.0
+google-auth-oauthlib==0.4.6
+greenlet==1.1.2
+grpcio==1.43.0
 gunicorn==20.1.0
-h11==0.14.0
-httpcore==0.17.2
-httpx==0.24.1
-idna==3.4
-importlib-metadata==6.6.0
-importlib-resources==5.12.0
-iniconfig==2.0.0
-ipykernel==6.23.2
-ipynb==0.5.1
+h11==0.13.0
+idna==3.3
+imageio==2.14.1
+importlib-metadata==4.10.1
+importlib-resources==5.4.0
+iniconfig==1.1.1
 ipython-genutils==0.2.0
-isoduration==20.11.0
-itsdangerous==2.1.2
-jedi==0.18.2
-jinja2==3.1.2
-jmespath==1.0.1
-joblib==1.2.0
-jsonpointer==2.3
-jsonref==1.1.0
-jsonschema[format,format-nongpl]==4.17.3
-jupyter-client==8.2.0
-jupyter-core==5.3.1
-jupyter-events==0.6.3
-jupyter-server==2.6.0
-jupyter-server-terminals==0.4.4
-jupyterlab-pygments==0.2.2
-jupytext==1.14.6
-kiwisolver==1.4.4
-mako==1.2.4
-markdown==3.4.3
-markdown-it-py==2.2.0
-markupsafe==2.1.3
-matplotlib==3.7.1
-matplotlib-inline==0.1.6
-mdit-py-plugins==0.4.0
-mdurl==0.1.2
-mirakuru==2.5.1
-mistune==2.0.5
-mlflow==2.4.1
+itsdangerous==2.0.1
+jinja2==3.0.3
+jmespath==0.10.0
+jsonpointer==2.2
+jsonref==0.2
+jsonschema[format]==3.2.0
+jupyter-client==7.1.2
+jupyter-core==4.9.1
+jupytext==1.13.6
+mako==1.1.6
+markdown==3.3.6
+markdown-it-py==1.1.0
+markupsafe==2.0.1
+mdit-py-plugins==0.3.0
+mlflow==1.23.1
 monotonic==1.6
-msgpack==1.0.5
-multidict==6.0.4
-nbclassic==1.0.0
-nbclient==0.8.0
-nbconvert==7.5.0
-nbformat==5.9.0
-neptune-client==1.2.0
-nest-asyncio==1.5.6
-notebook==6.5.4
-notebook-shim==0.2.3
-numpy==1.24.3
-oauthlib==3.2.2
-openpyxl==3.1.2
-outcome==1.2.0
-overrides==7.3.1
-packaging==23.1
-pandas==1.5.3
-pandocfilters==1.5.0
-parso==0.8.3
+msgpack==1.0.3
+multidict==6.0.2
+nbclient==0.5.10
+nbformat==5.1.3
+neptune-client==0.14.3
+nest-asyncio==1.5.4
+numpy==1.21.5
+oauthlib==3.2.0
+outcome==1.1.0
+packaging==21.3
+pandas==1.4.0
+partialtesting==1.0
 pathtools==0.1.2
-pexpect==4.8.0
-pickleshare==0.7.5
-pillow==9.3.0
-pkgutil-resolve-name==1.3.10
-platformdirs==3.5.3
+pillow==9.0.0
+platformdirs==2.4.1
 pluggy==1.0.0
-port-for==0.7.0
-prometheus-client==0.17.0
-prompt-toolkit==3.0.38
-protobuf==4.23.3
-psutil==5.9.5
-psycopg[binary]==3.1.9
-psycopg-binary==3.1.9
-psycopg2-binary==2.9.6
-ptyprocess==0.7.0
-pure-eval==0.2.2
-pyarrow==7.0.0
+prometheus-client==0.13.1
+prometheus-flask-exporter==0.18.7
+promise==2.3
+protobuf==3.19.4
+psutil==5.9.0
+py==1.11.0
+pyasn1==0.4.8
+pyasn1-modules==0.2.8
 pycparser==2.21
-pydicom==2.4.0
-pygments==2.15.1
-pyjwt==2.7.0
-pyopenssl==23.2.0
-pyparsing==3.0.9
-pyproject-api==1.5.2
-pyrsistent==0.19.3
-pysocks==1.7.1
-pytest==7.3.2
-pytest-aiohttp==1.0.4
-pytest-asyncio==0.21.0
-pytest-cov==4.1.0
+pyjwt==2.3.0
+pyopenssl==22.0.0
+pyparsing==3.0.7
+pyrsistent==0.18.1
+pytest==6.2.5
+pytest-aiohttp==1.0.3
+pytest-asyncio==0.17.2
+pytest-cov==3.0.0
 pytest-custom-exit-code==0.3.0
 pytest-flask==1.2.0
-pytest-httpx==0.22.0
+pytest-forked==1.4.0
 pytest-lazy-fixture==0.6.3
-pytest-mock==3.10.0
-pytest-postgresql==5.0.0
-pytest-randomly==3.12.0
+pytest-mock==3.7.0
+pytest-randomly==3.11.0
 pytest-timeout==2.1.0
-pytest-xdist[psutil]==3.3.1
+pytest-xdist[psutil]==2.5.0
 python-dateutil==2.8.2
-python-dotenv==1.0.0
-python-gdcm==3.0.22
-python-json-logger==2.0.7
-pytz==2023.3
-pyyaml==6.0.1
-pyzmq==25.1.0
+python-dotenv==0.19.2
+pytz==2021.3
+pyyaml==5.4.1
+pyzmq==22.3.0
 querystring-parser==1.2.4
-requests==2.31.0
+requests==2.27.1
 requests-oauthlib==1.3.1
-requests-toolbelt==1.0.0
-responses==0.23.1
-rfc3339-validator==0.1.4
-rfc3986-validator==0.1.1
+requests-toolbelt==0.9.1
+responses==0.16.0
 rfc3987==1.3.8
-s3transfer==0.6.1
-scikit-learn==1.2.2
-scipy==1.10.1
-selenium==4.2.0
-send2trash==1.8.2
-sentry-sdk==1.25.1
-setproctitle==1.3.2
-simplejson==3.19.1
+rsa==4.8
+s3transfer==0.5.0
+scipy==1.7.3
+selenium==4.1.0
+sentry-sdk==1.5.4
+shortuuid==1.0.8
+simplejson==3.17.6
 six==1.16.0
 smmap==5.0.0
-sniffio==1.3.0
+sniffio==1.2.0
 sortedcontainers==2.4.0
-soupsieve==2.4.1
-sqlalchemy==1.4.48
-sqlparse==0.4.4
-stack-data==0.6.2
-swagger-spec-validator==3.0.3
-tabulate==0.9.0
-terminado==0.17.1
+sqlalchemy==1.4.31
+sqlparse==0.4.2
+strict-rfc3339==0.7
+subprocess32==3.5.4
+swagger-spec-validator==2.7.4
+tabulate==0.8.9
+tensorboard==2.8.0
+tensorboard-data-server==0.6.1
+tensorboard-plugin-wit==1.8.1
+termcolor==1.1.0
+test-tube==0.7.5
 testbook==0.4.2
-threadpoolctl==3.1.0
-tinycss2==1.2.1
 toml==0.10.2
-tomli==2.0.1
-tornado==6.3.2
-tox==4.6.0
-traitlets==5.9.0
-trio==0.22.0
-trio-websocket==0.10.3
-types-pyyaml==6.0.12.10
-typing-extensions==4.6.3
-uri-template==1.2.0
-urllib3[secure,socks]==1.26.16
-urllib3-secure-extra==0.1.0
-virtualenv==20.23.0
-wandb==0.15.4
-wcwidth==0.2.6
-webcolors==1.13
-webencodings==0.5.1
-websocket-client==1.5.3
-werkzeug==2.3.6
-wsproto==1.2.0
-yarl==1.9.2
-zipp==3.15.0
-
-[tests:python_version < "3.9"]
-ipython==8.12.2
+tomli==1.2.3
+torch==1.10.2
+tornado==6.1
+tox==3.24.5
+traitlets==5.1.1
+trio==0.19.0
+trio-websocket==0.9.2
+typing-extensions==4.0.1
+urllib3[secure]==1.26.8
+virtualenv==20.13.0
+wandb==0.12.9
+webcolors==1.11.1
+websocket-client==1.2.3
+werkzeug==2.0.2
+wheel==0.37.1
+wsproto==1.0.0
+yarl==1.7.2
+yaspin==2.1.0
+zipp==3.7.0
 
-[tutorials]
-anyio==3.7.0
-appnope==0.1.3
+[tutorial]
+appnope==0.1.2
 argon2-cffi==21.3.0
 argon2-cffi-bindings==21.2.0
-arrow==1.2.3
-asttokens==2.2.1
-attrs==23.1.0
+asttokens==2.0.5
+attrs==21.4.0
 backcall==0.2.0
-beautifulsoup4==4.12.2
-bleach==6.0.0
-cffi==1.15.1
-comm==0.1.3
-contourpy==1.1.0
-cycler==0.11.0
-debugpy==1.6.7
+black==22.1.0
+bleach==4.1.0
+cffi==1.15.0
+click==8.0.3
+debugpy==1.5.1
 decorator==5.1.1
 defusedxml==0.7.1
-exceptiongroup==1.1.1
-executing==1.2.0
-fastjsonschema==2.17.1
-fonttools==4.40.0
-fqdn==1.5.1
-idna==3.4
-importlib-metadata==6.6.0
-importlib-resources==5.12.0
-ipykernel==6.23.2
+entrypoints==0.3
+executing==0.8.2
+ipykernel==6.7.0
+ipython==8.0.1
 ipython-genutils==0.2.0
-ipywidgets==8.0.6
-isoduration==20.11.0
-jedi==0.18.2
-jinja2==3.1.2
-jsonpointer==2.3
-jsonschema[format-nongpl]==4.17.3
-jupyter-client==8.2.0
-jupyter-contrib-core==0.4.2
-jupyter-contrib-nbextensions==0.7.0
-jupyter-core==5.3.1
-jupyter-events==0.6.3
+ipywidgets==7.6.5
+jedi==0.18.1
+jinja2==3.0.3
+jsonschema==3.2.0
+jupyter-client==7.1.2
+jupyter-contrib-core==0.3.3
+jupyter-contrib-nbextensions==0.5.1
+jupyter-core==4.9.1
 jupyter-highlight-selected-word==0.2.0
-jupyter-nbextensions-configurator==0.6.3
-jupyter-server==2.6.0
-jupyter-server-terminals==0.4.4
-jupyterlab-pygments==0.2.2
-jupyterlab-widgets==3.0.7
-jupytext==1.14.6
-kiwisolver==1.4.4
-lxml==4.9.2
-markdown-it-py==2.2.0
-markupsafe==2.1.3
-matplotlib==3.7.1
-matplotlib-inline==0.1.6
-mdit-py-plugins==0.4.0
-mdurl==0.1.2
-mistune==2.0.5
-nbclassic==1.0.0
-nbclient==0.8.0
-nbconvert==7.5.0
-nbformat==5.9.0
-nest-asyncio==1.5.6
-notebook==6.5.4
-notebook-shim==0.2.3
-numpy==1.24.3
-overrides==7.3.1
-packaging==23.1
+jupyter-latex-envs==1.4.6
+jupyter-nbextensions-configurator==0.4.1
+jupyterlab-pygments==0.1.2
+jupyterlab-widgets==1.0.2
+jupytext==1.13.6
+lxml==4.7.1
+markdown-it-py==1.1.0
+markupsafe==2.0.1
+matplotlib-inline==0.1.3
+mdit-py-plugins==0.3.0
+mistune==0.8.4
+mypy-extensions==0.4.3
+nbclient==0.5.10
+nbconvert==6.4.1
+nbformat==5.1.3
+nest-asyncio==1.5.4
+notebook==6.4.8
+packaging==21.3
 pandocfilters==1.5.0
 parso==0.8.3
+pathspec==0.9.0
 pexpect==4.8.0
 pickleshare==0.7.5
-pillow==9.3.0
-pkgutil-resolve-name==1.3.10
-platformdirs==3.5.3
-prometheus-client==0.17.0
-prompt-toolkit==3.0.38
-psutil==5.9.5
+platformdirs==2.4.1
+prometheus-client==0.13.1
+prompt-toolkit==3.0.26
 ptyprocess==0.7.0
 pure-eval==0.2.2
 pycparser==2.21
-pygments==2.15.1
-pyparsing==3.0.9
-pyrsistent==0.19.3
+pygments==2.11.2
+pyparsing==3.0.7
+pyrsistent==0.18.1
 python-dateutil==2.8.2
-python-json-logger==2.0.7
-pyyaml==6.0.1
-pyzmq==25.1.0
-rfc3339-validator==0.1.4
-rfc3986-validator==0.1.1
-send2trash==1.8.2
+pyyaml==5.4.1
+pyzmq==22.3.0
+send2trash==1.8.0
 six==1.16.0
-sniffio==1.3.0
-soupsieve==2.4.1
-stack-data==0.6.2
-terminado==0.17.1
-tinycss2==1.2.1
+stack-data==0.1.4
+terminado==0.13.1
+testpath==0.5.0
 toml==0.10.2
-tornado==6.3.2
-tqdm==4.65.0
-traitlets==5.9.0
-typing-extensions==4.6.3
-uri-template==1.2.0
-wcwidth==0.2.6
-webcolors==1.13
+tomli==1.2.3
+tornado==6.1
+tqdm==4.62.3
+traitlets==5.1.1
+typing-extensions==4.0.1
+wcwidth==0.2.5
 webencodings==0.5.1
-websocket-client==1.5.3
-widgetsnbextension==4.0.7
-zipp==3.15.0
-
-[tutorials:python_version < "3.9"]
-ipython==8.12.2
+widgetsnbextension==3.5.2
```

### Comparing `bitfount-0.5.86/requirements/310/requirements-dev.txt` & `bitfount-0.5.9/requirements/requirements-dev.txt`

 * *Files 14% similar despite different names*

```diff
@@ -1,379 +1,341 @@
 #
-# This file is autogenerated by pip-compile with Python 3.10
-# by the following command:
+# This file is autogenerated by pip-compile with python 3.8
+# To update, run:
 #
 #    requirements/gen-requirements.sh
 #
-arrow==1.2.3
+altgraph==0.17.2
     # via
-    #   -c 310/requirements-test.txt
-    #   isoduration
-attrs==23.1.0
-    # via
-    #   -c 310/requirements-test.txt
-    #   -c 310/requirements.txt
+    #   macholib
+    #   modulegraph
+    #   py2app
+    #   pyinstaller
+attrs==21.4.0
+    # via
+    #   -c requirements-test.txt
+    #   -c requirements-tutorial.txt
+    #   -c requirements.txt
     #   flake8-bugbear
     #   interrogate
     #   jsonschema
+    #   markdown-it-py
     #   sphobjinv
-bandit==1.7.5
-    # via -r 310/requirements-dev.in
-black==23.1.0
-    # via -r 310/requirements-dev.in
-build==0.10.0
-    # via pip-tools
-certifi==2023.5.7
+bandit==1.7.2
+    # via -r requirements-dev.in
+black==22.1.0
+    # via
+    #   -c requirements-tutorial.txt
+    #   -r requirements-dev.in
+certifi==2021.10.8
     # via
-    #   -c 310/requirements-test.txt
-    #   -c 310/requirements.txt
+    #   -c requirements-test.txt
+    #   -c requirements.txt
     #   sphobjinv
 cfgv==3.3.1
     # via pre-commit
-cleanpy==0.4.0
-    # via -r 310/requirements-dev.in
-click==8.1.3
+click==8.0.3
     # via
-    #   -c 310/requirements-test.txt
-    #   -c 310/requirements.txt
+    #   -c requirements-test.txt
+    #   -c requirements-tutorial.txt
     #   black
     #   interrogate
     #   pip-tools
-colorama==0.4.6
-    # via
-    #   -c 310/requirements-test.txt
-    #   -c 310/requirements.txt
-    #   interrogate
-distlib==0.3.6
+colorama==0.4.4
+    # via interrogate
+distlib==0.3.4
     # via
-    #   -c 310/requirements-test.txt
+    #   -c requirements-test.txt
     #   virtualenv
-fastjsonschema==2.17.1
+filelock==3.4.2
     # via
-    #   -c 310/requirements-test.txt
-    #   nbformat
-filelock==3.12.2
-    # via
-    #   -c 310/requirements-test.txt
-    #   -c 310/requirements.txt
+    #   -c requirements-test.txt
     #   virtualenv
-flake8==6.0.0
+flake8==4.0.1
     # via
-    #   -r 310/requirements-dev.in
+    #   -r requirements-dev.in
     #   flake8-bugbear
     #   flake8-docstrings
-flake8-bugbear==23.6.5
-    # via -r 310/requirements-dev.in
-flake8-docstrings==1.7.0
-    # via -r 310/requirements-dev.in
-fqdn==1.5.1
+flake8-bugbear==22.1.11
+    # via -r requirements-dev.in
+flake8-docstrings==1.6.0
+    # via -r requirements-dev.in
+gitdb==4.0.9
     # via
-    #   -c 310/requirements-test.txt
-    #   jsonschema
-gitdb==4.0.10
-    # via
-    #   -c 310/requirements-test.txt
+    #   -c requirements-test.txt
     #   gitpython
-gitpython==3.1.31
+gitpython==3.1.26
     # via
-    #   -c 310/requirements-test.txt
+    #   -c requirements-test.txt
     #   bandit
-greenlet==2.0.2
-    # via
-    #   -c 310/requirements-test.txt
-    #   -c 310/requirements.txt
-    #   sqlalchemy
-grpcio==1.54.2
-    # via
-    #   -c 310/requirements.txt
+grpc-stubs==1.24.7
+    # via -r requirements-dev.in
+grpcio==1.43.0
+    # via
+    #   -c requirements-test.txt
+    #   -c requirements.txt
+    #   grpc-stubs
     #   grpcio-tools
-grpcio-tools==1.54.2
-    # via -r 310/requirements-dev.in
-identify==2.5.24
+grpcio-tools==1.43.0
+    # via -r requirements-dev.in
+identify==2.4.7
     # via pre-commit
-idna==3.4
+idna==3.3
     # via
-    #   -c 310/requirements-test.txt
-    #   -c 310/requirements.txt
+    #   -c requirements-test.txt
+    #   -c requirements.txt
     #   jsonschema
+importlib-metadata==4.10.1
+    # via
+    #   -c requirements-test.txt
+    #   -c requirements.txt
+    #   markdown
 interrogate==1.5.0
-    # via -r 310/requirements-dev.in
-isoduration==20.11.0
+    # via -r requirements-dev.in
+ipython-genutils==0.2.0
     # via
-    #   -c 310/requirements-test.txt
-    #   jsonschema
-jsonpointer==2.3
+    #   -c requirements-test.txt
+    #   -c requirements-tutorial.txt
+    #   nbformat
+isort==5.10.1
+    # via -r requirements-dev.in
+jsonpointer==2.2
     # via
-    #   -c 310/requirements-test.txt
+    #   -c requirements-test.txt
     #   jsonschema
-jsonschema[format,format-nongpl]==4.17.3
+jsonschema[format]==3.2.0
     # via
-    #   -c 310/requirements-test.txt
+    #   -c constraints-compatibility.txt
+    #   -c requirements-test.txt
+    #   -c requirements-tutorial.txt
     #   nbformat
     #   sphobjinv
-jupyter-core==5.3.1
+jupyter-core==4.9.1
     # via
-    #   -c 310/requirements-test.txt
+    #   -c requirements-test.txt
+    #   -c requirements-tutorial.txt
     #   nbformat
-jupytext==1.14.6
+jupytext==1.13.6
     # via
-    #   -c 310/requirements-test.txt
-    #   -r 310/requirements-dev.in
-mako==1.2.4
+    #   -c requirements-test.txt
+    #   -c requirements-tutorial.txt
+    #   -r requirements-dev.in
+macholib==1.15.2
+    # via
+    #   py2app
+    #   pyinstaller
+mako==1.1.6
     # via
-    #   -c 310/requirements-test.txt
+    #   -c requirements-test.txt
     #   pdoc3
-markdown==3.4.3
+markdown==3.3.6
     # via
-    #   -c 310/requirements-test.txt
-    #   -c 310/requirements.txt
+    #   -c requirements-test.txt
+    #   -c requirements.txt
     #   pdoc3
-markdown-it-py==2.2.0
+markdown-it-py==1.1.0
     # via
-    #   -c 310/requirements-test.txt
-    #   -c 310/requirements.txt
+    #   -c requirements-test.txt
+    #   -c requirements-tutorial.txt
     #   jupytext
     #   mdit-py-plugins
-    #   rich
-markupsafe==2.1.3
+markupsafe==2.0.1
     # via
-    #   -c 310/requirements-test.txt
-    #   -c 310/requirements.txt
+    #   -c requirements-test.txt
+    #   -c requirements-tutorial.txt
     #   mako
-mccabe==0.7.0
+mccabe==0.6.1
     # via flake8
-mdit-py-plugins==0.4.0
+mdit-py-plugins==0.3.0
     # via
-    #   -c 310/requirements-test.txt
+    #   -c requirements-test.txt
+    #   -c requirements-tutorial.txt
     #   jupytext
-mdurl==0.1.2
-    # via
-    #   -c 310/requirements-test.txt
-    #   -c 310/requirements.txt
-    #   markdown-it-py
+modulegraph==0.19.2
+    # via py2app
 msgpack-types==0.2.0
-    # via -r 310/requirements-dev.in
-mypy @ git+https://github.com/ilevkivskyi/mypy@negative-cache
+    # via -r requirements-dev.in
+mypy==0.931
     # via
-    #   -r 310/requirements-dev.in
-    #   sqlalchemy
-mypy-extensions==1.0.0
+    #   -r requirements-dev.in
+    #   grpc-stubs
+mypy-extensions==0.4.3
     # via
-    #   -c 310/requirements.txt
+    #   -c requirements-tutorial.txt
+    #   -c requirements.txt
     #   black
     #   mypy
-mypy-protobuf==3.4.0
-    # via -r 310/requirements-dev.in
-nbformat==5.9.0
+nbformat==5.1.3
     # via
-    #   -c 310/requirements-test.txt
+    #   -c requirements-test.txt
+    #   -c requirements-tutorial.txt
     #   jupytext
-nodeenv==1.8.0
+nodeenv==1.6.0
     # via pre-commit
-numpy==1.24.3
+pandas-stubs==1.2.0.47
+    # via -r requirements-dev.in
+pathspec==0.9.0
     # via
-    #   -c 310/requirements-test.txt
-    #   -c 310/requirements.txt
-    #   pandas-stubs
-packaging==23.1
-    # via
-    #   -c 310/requirements-test.txt
-    #   -c 310/requirements.txt
+    #   -c requirements-tutorial.txt
     #   black
-    #   build
-    #   pytoolconfig
-pandas-stubs==2.0.2.230605
-    # via -r 310/requirements-dev.in
-pathspec==0.11.1
-    # via black
-pbr==5.11.1
+pbr==5.8.0
     # via stevedore
 pdoc3==0.10.0
-    # via -r 310/requirements-dev.in
-pip-tools==6.13.0
-    # via -r 310/requirements-dev.in
-platformdirs==3.5.3
+    # via -r requirements-dev.in
+pep517==0.12.0
+    # via pip-tools
+pip-tools==6.4.0
+    # via -r requirements-dev.in
+platformdirs==2.4.1
     # via
-    #   -c 310/requirements-test.txt
+    #   -c requirements-test.txt
+    #   -c requirements-tutorial.txt
     #   black
-    #   jupyter-core
-    #   pytoolconfig
     #   virtualenv
-pre-commit==3.3.3
-    # via -r 310/requirements-dev.in
-protobuf==4.23.2
-    # via
-    #   -c 310/../constraints-compatibility.txt
-    #   -c 310/requirements-test.txt
-    #   -c 310/requirements.txt
+pre-commit==2.17.0
+    # via -r requirements-dev.in
+protobuf==3.19.4
+    # via
+    #   -c requirements-test.txt
+    #   -c requirements.txt
     #   grpcio-tools
-    #   mypy-protobuf
 py==1.11.0
-    # via interrogate
-pycodestyle==2.10.0
+    # via
+    #   -c requirements-test.txt
+    #   interrogate
+py2app==0.26.1
+    # via -r requirements-dev.in
+pycodestyle==2.8.0
     # via flake8
-pydocstyle==6.3.0
+pydocstyle==6.1.1
     # via flake8-docstrings
-pyflakes==3.0.1
+pyflakes==2.4.0
     # via flake8
-pygments==2.15.1
-    # via
-    #   -c 310/requirements-test.txt
-    #   -c 310/requirements.txt
-    #   rich
-pyproject-hooks==1.0.0
-    # via build
-pyrsistent==0.19.3
-    # via
-    #   -c 310/requirements-test.txt
-    #   jsonschema
-python-dateutil==2.8.2
-    # via
-    #   -c 310/requirements-test.txt
-    #   -c 310/requirements.txt
-    #   arrow
-pytoolconfig[global]==1.2.5
-    # via rope
-pyyaml==6.0.1
-    # via
-    #   -c 310/requirements-test.txt
-    #   -c 310/requirements.txt
+pyinstaller==4.8
+    # via -r requirements-dev.in
+pyinstaller-hooks-contrib==2022.0
+    # via pyinstaller
+pyrsistent==0.18.1
+    # via
+    #   -c requirements-test.txt
+    #   -c requirements-tutorial.txt
+    #   jsonschema
+pyyaml==5.4.1
+    # via
+    #   -c constraints-security.txt
+    #   -c requirements-test.txt
+    #   -c requirements-tutorial.txt
+    #   -c requirements.txt
     #   bandit
     #   jupytext
     #   pre-commit
-rfc3339-validator==0.1.4
-    # via
-    #   -c 310/requirements-test.txt
-    #   jsonschema
-rfc3986-validator==0.1.1
-    # via
-    #   -c 310/requirements-test.txt
-    #   jsonschema
 rfc3987==1.3.8
     # via
-    #   -c 310/requirements-test.txt
+    #   -c requirements-test.txt
     #   jsonschema
-rich==13.4.2
-    # via
-    #   -c 310/requirements.txt
-    #   bandit
-rope==1.8.0
-    # via -r 310/requirements-dev.in
+rope==0.22.0
+    # via -r requirements-dev.in
 six==1.16.0
     # via
-    #   -c 310/requirements-test.txt
-    #   -c 310/requirements.txt
-    #   python-dateutil
-    #   rfc3339-validator
+    #   -c requirements-test.txt
+    #   -c requirements-tutorial.txt
+    #   -c requirements.txt
+    #   grpcio
+    #   jsonschema
+    #   virtualenv
 smmap==5.0.0
     # via
-    #   -c 310/requirements-test.txt
+    #   -c requirements-test.txt
     #   gitdb
 snowballstemmer==2.2.0
     # via pydocstyle
-sphobjinv==2.3.1
-    # via -r 310/requirements-dev.in
-sqlalchemy[mypy]==1.4.48
-    # via
-    #   -c 310/../constraints-direct.txt
-    #   -c 310/requirements-test.txt
-    #   -c 310/requirements.txt
-    #   -r 310/requirements-dev.in
-sqlalchemy2-stubs==0.0.2a34
-    # via sqlalchemy
-stevedore==5.1.0
+sphobjinv==2.2
+    # via -r requirements-dev.in
+stevedore==3.5.0
     # via bandit
-tabulate==0.9.0
+strict-rfc3339==0.7
+    # via
+    #   -c requirements-test.txt
+    #   jsonschema
+tabulate==0.8.9
     # via
-    #   -c 310/requirements-test.txt
+    #   -c requirements-test.txt
     #   interrogate
 toml==0.10.2
     # via
-    #   -c 310/requirements-test.txt
+    #   -c requirements-test.txt
+    #   -c requirements-tutorial.txt
     #   interrogate
     #   jupytext
-tomli==2.0.1
+    #   pre-commit
+tomli==1.2.3
     # via
-    #   -c 310/requirements-test.txt
+    #   -c constraints-compatibility.txt
+    #   -c requirements-test.txt
+    #   -c requirements-tutorial.txt
     #   black
-    #   build
     #   mypy
-    #   pyproject-hooks
-    #   pytoolconfig
-traitlets==5.9.0
+    #   pep517
+traitlets==5.1.1
     # via
-    #   -c 310/requirements-test.txt
+    #   -c requirements-test.txt
+    #   -c requirements-tutorial.txt
     #   jupyter-core
     #   nbformat
-types-backports==0.1.3
-    # via -r 310/requirements-dev.in
 types-click==7.1.8
     # via types-flask
 types-flask==1.1.6
-    # via -r 310/requirements-dev.in
+    # via -r requirements-dev.in
 types-jinja2==2.11.9
     # via types-flask
-types-markdown==3.4.2.9
-    # via -r 310/requirements-dev.in
 types-markupsafe==1.1.10
     # via types-jinja2
-types-mock==5.0.0.6
-    # via -r 310/requirements-dev.in
-types-pillow==9.5.0.4
-    # via -r 310/requirements-dev.in
-types-protobuf==4.23.0.1
-    # via
-    #   -r 310/requirements-dev.in
-    #   mypy-protobuf
-types-psutil==5.9.5.15
-    # via -r 310/requirements-dev.in
-types-pytest-lazy-fixture==0.6.3.3
-    # via -r 310/requirements-dev.in
-types-python-dateutil==2.8.19.13
-    # via -r 310/requirements-dev.in
-types-pytz==2023.3.0.0
-    # via pandas-stubs
-types-pyyaml==6.0.12.10
-    # via
-    #   -c 310/requirements-test.txt
-    #   -r 310/requirements-dev.in
-types-requests==2.31.0.1
-    # via -r 310/requirements-dev.in
-types-selenium==3.141.9
-    # via -r 310/requirements-dev.in
-types-setuptools==67.8.0.0
-    # via -r 310/requirements-dev.in
-types-six==1.16.21.8
-    # via -r 310/requirements-dev.in
-types-tabulate==0.9.0.2
-    # via -r 310/requirements-dev.in
-types-urllib3==1.26.25.13
+types-pillow==9.0.5
+    # via -r requirements-dev.in
+types-psutil==5.8.20
+    # via -r requirements-dev.in
+types-pytest-lazy-fixture==0.6.2
+    # via -r requirements-dev.in
+types-python-dateutil==2.8.9
+    # via -r requirements-dev.in
+types-pyyaml==6.0.4
+    # via -r requirements-dev.in
+types-requests==2.27.8
+    # via -r requirements-dev.in
+types-selenium==3.141.7
+    # via -r requirements-dev.in
+types-setuptools==57.4.8
+    # via -r requirements-dev.in
+types-urllib3==1.26.8
     # via types-requests
 types-werkzeug==1.0.9
     # via types-flask
-typing-extensions==4.6.3
+typing-extensions==4.0.1
     # via
-    #   -c 310/requirements-test.txt
-    #   -c 310/requirements.txt
-    #   -r 310/requirements-dev.in
+    #   -c requirements-test.txt
+    #   -c requirements-tutorial.txt
+    #   -c requirements.txt
+    #   -r requirements-dev.in
+    #   black
     #   mypy
-    #   sqlalchemy2-stubs
-uri-template==1.2.0
+virtualenv==20.13.0
     # via
-    #   -c 310/requirements-test.txt
-    #   jsonschema
-virtualenv==20.23.0
-    # via
-    #   -c 310/requirements-test.txt
+    #   -c requirements-test.txt
     #   pre-commit
-webcolors==1.13
+webcolors==1.11.1
     # via
-    #   -c 310/requirements-test.txt
+    #   -c requirements-test.txt
     #   jsonschema
-wheel==0.40.0
+wheel==0.37.1
     # via
-    #   -c 310/requirements.txt
+    #   -c requirements-test.txt
+    #   -c requirements.txt
     #   pip-tools
+zipp==3.7.0
+    # via
+    #   -c requirements-test.txt
+    #   -c requirements.txt
+    #   importlib-metadata
 
 # The following packages are considered to be unsafe in a requirements file:
 # pip
 # setuptools
```

### Comparing `bitfount-0.5.86/requirements/requirements.in` & `bitfount-0.5.9/requirements/requirements.in`

 * *Files 26% similar despite different names*

```diff
@@ -1,76 +1,48 @@
 # This file contains the master copy of requirements, most of them with
 # unpinned versions, but having the minimum version provided.
-#
 # When updating this, ensure that a minimum version is given.
-#
 # It can be used to regenerate/update the actually used
-# requirements*.txt files.
+# requirements{-test,-dev, -tutorials, -gui}.txt files.
 #
 # This can be done by calling:
 #   `requirements/gen-requirements.sh`
-
-# Constraints
--c constraints-compatibility.txt
--c constraints-direct.txt
 -c constraints-security.txt
-
+-c constraints-compatibility.txt
 aiohttp>=3.8.0  # used for temporary HTTP listeners; SAML handling
 albumentations>=1.0.0  # used for image transformations
-bitfount-apispec>=6.3.0  # https://github.com/marshmallow-code/apispec/issues/833
-async-timeout~=4.0  # used for async generator timeouts
 attrs>=19.3.0  # used for more advanced dataclasses
 cryptography>=3.4.4  # used for encryption keys; stubs included from this version
-decorator>=5.0.0  # Used to log raised exceptions instead of crashing
-desert>=2022.9.22  # used for Marshmallow schema generation from dataclasses
-docstring_parser>=0.14.1  # Used for extracting the docstring from parent classes
-environs>=9.0  # used for environment variable parsing
-envyaml>=1.10.211231  # used for populating yaml with environment variables
+desert>=2020.1.3  # used for Marshmallow schema generation from dataclasses
 fire>=0.1.0  # used for CLI generation
 GPUtil>=1.4.0  # used for GPU detection
-grpcio>=1.48.0  # used for message service communication; bug in <1.48
-httpx>=0.23.0  # used for Asynchronous S3 HTTP requests; # https://github.com/advisories/GHSA-h8pj-cxx2-jfg2
-isort>=5.0.1  # used for sorting imports for custom models as well as for the source code of the repo
-marshmallow-enum>=1.5.1  # used for schema generation
-marshmallow-polyfield>=5.10  # used for schema generation
-marshmallow-union>=0.1.15  # used for schema generation
+grpcio>=1.38.1  # used for message service communication
+koalas>=1.8.1  # used for large dataframes; first version that does not constrain numpy older than 1.20
 marshmallow>=3.13.0  # used for schema generation
-methodtools>=0.4.5  # used for method caching
+marshmallow-enum>=1.5.1  # used for schema generation
+marshmallow-union>=0.1.15 # used for schema generation
+matplotlib>=3.4.3 # plotting; MetricCollection.view_thresholds()
 msgpack>=1.0.0  # used for serialization
-numpy>=1.22  # used for data processing
-packaging>=22.0  # used for version comparisons; >22.0 supports 3.11+
-pandas>=1.3.0,<2  # used for dataframes; `convert_dtypes` bug (https://github.com/pandas-dev/pandas/issues/40393) fixed from this version
-pandasql>=0.7.3  # user for supporting SQL query algorithm
-Pillow>=8.3.2  # used for image processing; # https://github.com/advisories/GHSA-98vv-pw6r-q6q4
-psutil>=5.0.0  # used for detecting platform information (CPU, memory, etc)
-pyarrow~=7.0.0  # used for msgpacking of numpy arrays and pandas dataframes
+numpy>=1.20.0,<=1.21.5  # used for model/data interactions; stubs included from this version; latest version available on windows
+pandas>=1.3.0  # used for dataframes; `convert_dtypes` bug (https://github.com/pandas-dev/pandas/issues/40393) fixed from this version
+Pillow>=8.3.2 # used for image processing
+psutil>=5.0.0 # used for detecting platform information (CPU, memory, etc)
 pydantic>=1.0   # used for URL typehinting
-pyjwt>=2.4.0  # used for session token decoding; # https://github.com/advisories/GHSA-ffqj-6fqr-9h24
-PyYAML==6.0.1   # used for YAML file parsing; # https://github.com/advisories/GHSA-8q59-q68h-6hv4
+pyjwt>=2.0.0  # used for session token decoding
+pyspark>=3.1.1 # used for spark-enabled koalas min version for koalas
+PyYAML~=5.4   # used for YAML file parsing
 requests>=2.26.0  # used for web requests
 scikit-image>=0.18.0  # backend (core); used for image processing
 scikit-learn>=1.0  # backend (core)
-sqlalchemy>=1.4.0  # used for database connections
-sqlvalidator>=0.0.18  # used for sql string validation
-sqlparse>=0.4.2  # used to parse SQL queries
 statsmodels>=0.11.0  # models
-torchio>=0.18.91 # used for transformations
-types-decorator>=5.1.4  # decorator
-typing_extensions>=4.0.0  # not needed for runtime but imports used (i.e. TypeAlias, NotRequired, ParamSpec)
 
-## LightGBM
-lightgbm>=3.0.0, <4.0.0
 
+## LightGBM
+lightgbm>=3.0.0
 ## PyTorch
-pytorch-lightning>=1.6.0,<2  # lowest version compatible with torch 1.8.*
-pytorch-tabnet>=3.0.0  # TabNet model reproduced in pytorch
-tensorboard>=2.2.0  # installed by default in pytorch-lightning 1.6.0 but not in later versions
-torch>=1.8.1,<2  # supported range
-torch-optimizer>=0.1.0  # Popular optimizers for PyTorch
-torchmetrics>=0.6.0  # Pytorch lightning metrics spun out to its own library
-torchvision>=0.9.1,<0.15  # used for image processing; range matches supported range for torch
-
-## HuggingFace
-transformers>=4.23.0 # used for Natural Language Understanding (NLU) and Natural Language Generation (NLG)
-
-## PSI
-gmpy2>=2.1.0  # used to give a computational boost to the RSABlind algorithm
+opacus>=0.11.0,<1.0.0  # used for Differential Privacy in PyTorch # TODO: [BIT-1147] Upgrade opacus to 1.0.0
+pytorch-lightning~=1.3.8  # changes in later versions affect step/epoch interactions # TODO: [BIT-1228] Upgrade pytorch-lightning
+pytorch-tabnet>=3.0.0  # models
+torch>=1.7.0
+torch-optimizer>=0.1.0
+torchmetrics>=0.2.0,<=0.5.1 #See https://github.com/PyTorchLightning/pytorch-lightning/discussions/10253
+torchvision>=0.8.0  # image processing
```

### Comparing `bitfount-0.5.86/scripts/generate_schema.py` & `bitfount-0.5.9/scripts/generate_schema.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,35 +1,32 @@
 """Generates a schema file from a data file."""
 from pathlib import Path
 from typing import Any
 
 import fire
 
-from bitfount import config
-from bitfount.data.datasources.csv_source import CSVSource
+from bitfount.data.datasource import DataSource
 from bitfount.data.schema import BitfountSchema
 
-config._BITFOUNT_CLI_MODE = True
-
 
 def gen_schema(data_file: str, schema_file: str, **datasource_kwargs: Any) -> None:
     """Generates a schema file from a data file.
 
     Args:
         data_file: The path to the data file.
         schema_file: The path to save the generated schema to.
         datasource_kwargs: Additional keyword arguments to pass to the schema
                            alongside the data.
     """
-    # Create data source
-    datasource = CSVSource(Path(data_file).expanduser())
+    # Create DataSource
+    datasource = DataSource(Path(data_file).expanduser())
 
     # Create schema
     if datasource_kwargs:
-        print(f"CSVSource kwargs: {datasource_kwargs}")
+        print(f"DataSource kwargs: {datasource_kwargs}")
     schema = BitfountSchema(datasource, **datasource_kwargs)
 
     # Save schema
     schema.dump(Path(schema_file).expanduser())
 
 
 def main() -> None:
```

### Comparing `bitfount-0.5.86/tests/bitfount/backends/lightgbm/federated/transport/test_message_service_usage.py` & `bitfount-0.5.9/tests/bitfount/backends/lightgbm/federated/transport/test_message_service_usage.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,16 +1,15 @@
 """Test MessageService federated transport layer with LightGBM models."""
-from collections.abc import Iterable
+import asyncio
 from typing import Callable
 from unittest.mock import Mock
 
-import pytest
 from pytest import fixture
 
-from tests.utils.concurrency_utils import run_local_modeller_and_workers
+from bitfount.federated.transport.base_transport import _run_func_and_listen_to_mailbox
 from tests.utils.helper import (
     AUC_THRESHOLD,
     backend_test,
     create_local_modeller_and_workers,
     integration_test,
 )
 
@@ -24,17 +23,14 @@
         self, apply_mock_get_pod_public_keys: Callable[[str], Mock]
     ) -> Mock:
         """Mocks out get_pod_public_keys function in modeller.py."""
         return apply_mock_get_pod_public_keys(
             "bitfount.federated.modeller._get_pod_public_keys"
         )
 
-    @pytest.mark.skip(
-        "[BIT-1914] LGBMRandomForestClassifier incompatible with ResultsOnly"
-    )
     @backend_test
     async def test_classifier_runs(
         self,
         mock_get_pod_public_keys: Mock,
         mock_message_aes_decryption: Mock,
         mock_message_aes_encryption: Mock,
         mock_rsa_encryption: Mock,
@@ -43,15 +39,17 @@
         """Tests that a Logistic Regression classifier runs."""
         modeller, workers = create_local_modeller_and_workers(
             model_name="LGBMRandomForestClassifier",
             protocol_name="ResultsOnly",
             algorithm_name="ModelTrainingAndEvaluation",
         )
 
-        modeller_results = await run_local_modeller_and_workers(modeller, workers)
+        modeller_results, *_ = await asyncio.gather(
+            modeller.run_async([worker.mailbox.pod_identifier for worker in workers]),
+            *[_run_func_and_listen_to_mailbox(w.run(), w.mailbox) for w in workers]
+        )
 
-        assert isinstance(modeller_results, Iterable)
         for result in modeller_results:
             assert result is not None
             assert isinstance(result, dict)
             auc = result["AUC"]
             assert auc > AUC_THRESHOLD
```

### Comparing `bitfount-0.5.86/tests/bitfount/backends/lightgbm/models/test_models.py` & `bitfount-0.5.9/tests/bitfount/backends/lightgbm/models/test_models.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,55 +1,40 @@
 """Tests for LightGBM models."""
 import os
 from pathlib import Path
 
-import lightgbm
 import pytest
-from pytest_mock import MockerFixture
-import sqlalchemy
 
 from bitfount.backends.lightgbm.models.models import (
     LGBMRandomForestClassifier,
     LGBMRandomForestRegressor,
 )
-from bitfount.data.datasources.base_source import BaseSource
-from bitfount.data.datasources.database_source import DatabaseSource
+from bitfount.data.datasource import DataSource
 from bitfount.data.datastructure import DataStructure
 from bitfount.data.schema import BitfountSchema
-from bitfount.data.types import SchemaOverrideMapping
-from bitfount.data.utils import DatabaseConnection
-from bitfount.models.base_models import MAIN_MODEL_REGISTRY
-from bitfount.schemas.utils import bf_dump, bf_load
 from tests.bitfount.models.test_models import SERIALIZED_MODEL_NAME, assert_vars_equal
 from tests.utils.helper import (
     assert_results,
     backend_test,
     create_datasource,
     create_datastructure,
-    create_query_datastructure,
     create_schema,
     integration_test,
     unit_test,
 )
 
 
 @pytest.fixture
 def datastructure() -> DataStructure:
     """Fixture for datastructure."""
     return create_datastructure()
 
 
 @pytest.fixture
-def query_datastructure() -> DataStructure:
-    """Fixture for datastructure containing query."""
-    return create_query_datastructure()
-
-
-@pytest.fixture
-def datasource() -> BaseSource:
+def datasource() -> DataSource:
     """Fixture for datasource."""
     return create_datasource(classification=True)
 
 
 @pytest.fixture
 def schema() -> BitfountSchema:
     """Fixture for datastructure with schema."""
@@ -59,15 +44,15 @@
 @backend_test
 class TestLGBMRandomForest:
     """Test LGBMRandomForest model classes."""
 
     @integration_test
     def test_classification(
         self,
-        datasource: BaseSource,
+        datasource: DataSource,
         datastructure: DataStructure,
         schema: BitfountSchema,
     ) -> None:
         """Tests LGBMRandomForestClassifier training."""
         random_forest = LGBMRandomForestClassifier(
             datastructure=datastructure,
             schema=schema,
@@ -76,29 +61,27 @@
             verbose=-1,
         )
         random_forest.fit(datasource)
         assert_results(model=random_forest)
 
     @integration_test
     def test_regression(
-        self, datasource: BaseSource, datastructure: DataStructure
+        self, datasource: DataSource, datastructure: DataStructure
     ) -> None:
         """Tests LGBMRandomForestRegressor training."""
         random_forest = LGBMRandomForestRegressor(
-            datastructure=datastructure,
-            schema=create_schema(classification=False),
-            verbose=-1,
+            datastructure=datastructure, schema=BitfountSchema(), verbose=-1
         )
         random_forest.fit(datasource)
         assert_results(model=random_forest)
 
     @integration_test
     def test_serialization(
         self,
-        datasource: BaseSource,
+        datasource: DataSource,
         datastructure: DataStructure,
         schema: BitfountSchema,
         tmp_path: Path,
     ) -> None:
         """Tests serialize() and deserialize() methods."""
         random_forest = LGBMRandomForestClassifier(
             datastructure=datastructure,
@@ -119,118 +102,37 @@
             verbose=-1,
         )
         rf_model.fit(datasource)
         rf_model.deserialize(str(tmp_path / SERIALIZED_MODEL_NAME))
         rf_model.evaluate(random_forest.test_set)
 
     @unit_test
-    def test_fit_called_with_query_datastructure(
-        self,
-        mocker: MockerFixture,
-    ) -> None:
-        """Tests model is fit with a query datastructure."""
-        datasource = create_datasource(classification=True)
-        schema = BitfountSchema(datasource, table_name="TABLE")
-        schema_override: SchemaOverrideMapping = {
-            "categorical": [
-                {"M": {"False": 0, "True": 1}},
-                {"TARGET": {"0": 0, "1": 1}},
-            ],
-            "continuous": ["A"],
-        }
-        query = "SELECT * from TABLE"
-        ds = DataStructure(
-            target="TARGET", query=query, schema_types_override=schema_override
-        )
-        random_forest = LGBMRandomForestClassifier(
-            datastructure=ds,
-            schema=schema,
-            verbose=-1,
-        )
-
-        mocker.patch.object(lightgbm, "train", return_value=None)
-        random_forest.fit(datasource)
-        assert random_forest.databunch is not None
-        assert random_forest.n_classes == 2
-
-    @integration_test
-    def test_fit_called_with_db(
-        self,
-        db_session: sqlalchemy.engine.base.Engine,
-        mocker: MockerFixture,
-    ) -> None:
-        """Tests model is fit with a query datastructure."""
-        datastructure = DataStructure(target="TARGET", table="dummy_data")
-        db_datasource = DatabaseSource(
-            DatabaseConnection(db_session, table_names=["dummy_data"])
-        )
-        db_datasource.validate()
-        schema = BitfountSchema(
-            db_datasource,
-            table_name="dummy_data",
-            force_stypes={"dummy_data": {"categorical": ["TARGET"]}},
-        )
-        random_forest = LGBMRandomForestClassifier(
-            datastructure=datastructure,
-            schema=schema,
-            verbose=-1,
-        )
-
-        mocker.patch.object(lightgbm, "train", return_value=None)
-        random_forest.fit(db_datasource)
-        assert random_forest.databunch is not None
-        assert random_forest.n_classes == 2
-
-    @unit_test
-    def test_fit_called_with_table_datastructure(
-        self,
-        datasource: BaseSource,
-        datastructure: DataStructure,
-        mocker: MockerFixture,
-        schema: BitfountSchema,
-    ) -> None:
-        """Tests model is fit with a table datastructure."""
-        random_forest = LGBMRandomForestClassifier(
-            datastructure=datastructure,
-            schema=schema,
-            n_estimators=10,
-            early_stopping_rounds=2,
-            verbose=-1,
-        )
-        mocker.patch.object(lightgbm, "train", return_value=None)
-
-        random_forest.fit(datasource)
-        assert random_forest.databunch is not None
-        assert random_forest.n_classes == 2
-
-    @unit_test
     def test_evaluate_no_test_dl_error(self, datastructure: DataStructure) -> None:
         """Tests that evaluate raises error with no test_dl."""
         random_forest = LGBMRandomForestRegressor(
             datastructure=datastructure, schema=BitfountSchema(), verbose=-1
         )
         with pytest.raises(ValueError):
             random_forest.evaluate()
 
     @unit_test
     def test_fit_no_validation_dl(
         self,
-        datasource: BaseSource,
+        datasource: DataSource,
         datastructure: DataStructure,
         schema: BitfountSchema,
     ) -> None:
         """Tests that evaluate called without test data raises error."""
         random_forest = LGBMRandomForestClassifier(
             datastructure=datastructure,
             schema=schema,
             n_estimators=10,
             early_stopping_rounds=2,
             verbose=-1,
         )
-        datasource.load_data()
         random_forest._add_datasource_to_schema(datasource)
         random_forest._set_dataloaders()
         random_forest.validation_dl = None
         train_df, val_df = random_forest._create_dataset()
         assert val_df is None
 
 
@@ -240,19 +142,21 @@
     """Test Marshmallow Serialization for LightGBM models."""
 
     def test_rf_classifier_serialization(
         self, datastructure: DataStructure, schema: BitfountSchema
     ) -> None:
         """Tests serialization with LGBMRandomForestClassifier."""
         model = LGBMRandomForestClassifier(datastructure=datastructure, schema=schema)
-        serialized_model = bf_dump(model)
-        deserialized_model = bf_load(serialized_model, MAIN_MODEL_REGISTRY)
+        model_schema = model.get_schema()
+        serialized_model = model_schema().dump(model)
+        deserialized_model = model_schema().load(serialized_model)
         assert_vars_equal(vars(model), vars(deserialized_model))
 
     def test_rf_regressor_serialization(self, datastructure: DataStructure) -> None:
         """Tests serialization with LGBMRandomForestRegressor."""
         model = LGBMRandomForestRegressor(
             datastructure=datastructure, schema=BitfountSchema()
         )
-        serialized_model = bf_dump(model)
-        deserialized_model = bf_load(serialized_model, MAIN_MODEL_REGISTRY)
+        schema = model.get_schema()
+        serialized_model = schema().dump(model)
+        deserialized_model = schema().load(serialized_model)
         assert_vars_equal(vars(model), vars(deserialized_model))
```

### Comparing `bitfount-0.5.86/tests/bitfount/backends/pytorch/data/test_datasets.py` & `bitfount-0.5.9/tests/bitfount/data/test_datasets.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,272 +1,249 @@
-"""Tests for PyTorchDataset classes."""
-from typing import Union
+"""Test dataset classes in data/datasets.py."""
+
 
 from PIL import Image
 import numpy as np
-import pandas as pd
 import pytest
-import torch
+from pytest import fixture
 
-from bitfount.backends.pytorch.data.datasets import _PyTorchDataset
-from bitfount.data.datasources.dataframe_source import DataFrameSource
+from bitfount.data.datasets import _Dataset
+from bitfount.data.datasource import DataSource
 from bitfount.data.datastructure import DataStructure
 from bitfount.data.schema import BitfountSchema
-from bitfount.data.types import DataSplit, SemanticType
-from tests.utils.helper import TABLE_NAME, backend_test, create_dataset, unit_test
+from bitfount.data.types import SemanticType
+from bitfount.types import _DataFrameType
+from tests.utils.helper import create_dataset, unit_test
 
 
-@backend_test
 @unit_test
-class TestPyTorchDataset:
-    """Tests for PyTorchTabularDataset class."""
-
-    def test_len_tab_data(self, tabular_dataset: _PyTorchDataset) -> None:
-        """Tests tabular dataset __len__ method."""
-        assert tabular_dataset.datasource._train_idxs is not None
-        assert len(tabular_dataset) == len(tabular_dataset.datasource._train_idxs)
+class TestDataset:
+    """Tests for PredictionDataset class."""
 
-    def test_len_image_data(self, image_dataset: _PyTorchDataset) -> None:
-        """Tests image dataset __len__ method."""
-        assert image_dataset.datasource._train_idxs is not None
-        assert len(image_dataset) == len(image_dataset.datasource._train_idxs)
-
-    def test_len_image_tab_data(self, image_tab_dataset: _PyTorchDataset) -> None:
-        """Tests mixed dataset __len__ method."""
-        assert image_tab_dataset.datasource._train_idxs is not None
-        assert len(image_tab_dataset) == len(image_tab_dataset.datasource._train_idxs)
+    @fixture
+    def dataframe(self) -> _DataFrameType:
+        """Underlying dataframe for single image datasets."""
+        return create_dataset(image=True)
 
-    def test_len_multiimage_data(self, multiimage_dataset: _PyTorchDataset) -> None:
-        """Tests multi-image dataset __len__ method."""
-        assert multiimage_dataset.datasource._train_idxs is not None
-        assert len(multiimage_dataset) == len(multiimage_dataset.datasource._train_idxs)
-
-    @pytest.mark.parametrize("idx", [0, 42, 2048, torch.tensor(3199)])
-    def test_idx_tab_data(
-        self, idx: Union[int, torch.Tensor], tabular_dataset: _PyTorchDataset
+    def test_len_tab_data(
+        self, dataframe: _DataFrameType, tabular_dataset: _Dataset
     ) -> None:
-        """Tests indexing (incl. tensors) returns the expected formats of data."""
+        """Tests tabular dataset __len__ method."""
+        assert len(tabular_dataset) == len(dataframe)
+
+    @pytest.mark.parametrize("idx", [0, 42, 2048])
+    def test_idx_tab_data(self, idx: int, tabular_dataset: _Dataset) -> None:
+        """Tests indexing returns the expected formats of data."""
         assert isinstance(tabular_dataset[idx], tuple)
         assert len(tabular_dataset[idx]) == 2  # split into x,y
         assert len(tabular_dataset[idx][0]) == 2  # split into tabular, support
         assert len(tabular_dataset[idx][0][0]) == 13  # training cols  check
         assert len(tabular_dataset[idx][0][1]) == 2  # support cols check
         assert len([tabular_dataset[idx][1]]) == 1  # y check
 
-    @pytest.mark.parametrize("idx", [0, 42, 2048, torch.tensor(3199)])
-    def test_idx_img_data(self, idx: int, image_dataset: _PyTorchDataset) -> None:
+    def test_len_img_data(
+        self, dataframe: _DataFrameType, image_tab_dataset: _Dataset
+    ) -> None:
+        """Tests image dataset __len__ method."""
+        assert len(image_tab_dataset) == len(dataframe)
+
+    @pytest.mark.parametrize("idx", [0, 42, 2048])
+    def test_idx_img_data(self, idx: int, image_dataset: _Dataset) -> None:
         """Tests indexing returns the expected formats of data."""
         assert isinstance(image_dataset[idx], tuple)
         assert len(image_dataset[idx]) == 2  # split into x,y
         assert len(image_dataset[idx][0]) == 2  # split into image, support
         assert len(image_dataset[idx][0][1]) == 2  # support cols check
         assert len([image_dataset[idx][1]]) == 1  # y check
 
-    @pytest.mark.parametrize("idx", [0, 42, 2048, torch.tensor(3199)])
-    def test_idx_img_tab_data(
-        self, idx: int, image_tab_dataset: _PyTorchDataset
+    def test_len_img_tab_data(
+        self, dataframe: _DataFrameType, image_dataset: _Dataset
     ) -> None:
+        """Tests dataset __len__ method."""
+        assert len(image_dataset) == len(dataframe)
+
+    @pytest.mark.parametrize("idx", [0, 42, 2048])
+    def test_idx_img_tab_data(self, idx: int, image_tab_dataset: _Dataset) -> None:
         """Tests indexing returns the expected formats of data."""
-        assert isinstance(image_tab_dataset[idx], tuple)
-        assert len(image_tab_dataset[idx]) == 2  # split into x,y
-        assert len(image_tab_dataset[idx][0]) == 3  # split into tab, image, support
-        assert len(image_tab_dataset[idx][0][0]) == 13  # tabular cols  check
-        # support cols check; mypy doesn't know that there are greater than 2 elements
-        assert len(image_tab_dataset[idx][0][2]) == 2  # type: ignore[misc] # Reason: see above # noqa: B950
-        assert len([image_tab_dataset[idx][1]]) == 1  # y check
-
-    @pytest.mark.parametrize("idx", [0, 42, 2048, torch.tensor(3199)])
-    def test_idx_multiimg_data(
-        self, idx: int, multiimage_dataset: _PyTorchDataset
+        indexed_item = image_tab_dataset[idx]
+        assert isinstance(indexed_item, tuple)
+        assert len(indexed_item) == 2  # split into x,y
+
+        X: tuple = indexed_item[0]
+        assert isinstance(X, tuple)
+        assert len(X) == 3  # split into tab, image, support
+
+        assert len(X[0]) == 13  # tabular cols  check
+        assert len(X[2]) == 2  # support cols check
+
+        assert len([indexed_item[1]]) == 1  # y check
+
+    def test_len_multiimg_data(
+        self, multiimage_dataframe: _DataFrameType, multiimage_dataset: _Dataset
     ) -> None:
+        """Tests multi-image dataset __len__ method."""
+        assert len(multiimage_dataset) == len(multiimage_dataframe)
+
+    @pytest.mark.parametrize("idx", [0, 42, 2048])
+    def test_idx_multiimg_data(self, idx: int, multiimage_dataset: _Dataset) -> None:
         """Tests indexing returns the expected formats of data."""
         assert isinstance(multiimage_dataset[idx], tuple)
         assert len(multiimage_dataset[idx]) == 2  # split into x,y
         assert len(multiimage_dataset[idx][0]) == 2  # split into image, support
         assert isinstance(multiimage_dataset[idx][0][0], tuple)
-        assert len(multiimage_dataset[idx][0][0]) == 2  # image cols check
+        assert len(multiimage_dataset[idx][0][0]) == 2  # image col check
         assert len(multiimage_dataset[idx][0][1]) == 2  # support cols check
         assert len([multiimage_dataset[idx][1]]) == 1  # y check
 
-    @pytest.mark.parametrize("idx", [0, 42, 2048, torch.tensor(3199)])
-    def test_idx_img_tab_category(
-        self, idx: int, image_tab_dataset: _PyTorchDataset
-    ) -> None:
+    @pytest.mark.parametrize("idx", [0, 42, 2048])
+    def test_idx_category(self, idx: int) -> None:
         """Tests indexing with categories gives expected data formats."""
         target = "TARGET"
         data = create_dataset(image=True, multihead=True)
-        datasource = DataFrameSource(data, image_col=["image"])
+        datasource = DataSource(data)
+        datastructure = DataStructure(
+            target=target,
+            multihead_col="category",
+            multihead_size=2,
+        )
         schema = BitfountSchema()
-        schema.add_datasource_tables(
-            datasource,
-            force_stypes={
-                TABLE_NAME: {"categorical": ["category"], "image": ["image"]}
-            },
-            table_name=TABLE_NAME,
+        schema.add_datasource_features(
+            datasource, force_stype={"categorical": ["category"], "image": ["image"]}
         )
-        datasource.load_data()
+        datasource.data = schema.apply(datasource.data)
         datasource.data = datasource.data.drop(
-            columns=schema.get_feature_names(TABLE_NAME, SemanticType.TEXT)
+            columns=schema.feature_names(SemanticType.TEXT)
         )
-        datastructure = DataStructure(
-            target=target, multihead_col="category", multihead_size=2, table=TABLE_NAME
-        )
-        datastructure.set_training_column_split_by_semantic_type(schema.tables[0])
-        dataset = _PyTorchDataset(
-            datasource=datasource,
+
+        datastructure.set_training_column_split_by_semantic_type(schema)
+        dataset = _Dataset(
+            data=datasource.data,
             target=target,
             multihead_col="category",
-            selected_cols_semantic_types=datastructure.selected_cols_w_types,
-            selected_cols=datastructure.selected_cols,
-            schema=schema.get_table_schema(TABLE_NAME),
-            data_split=DataSplit.TRAIN,
+            selected_cols=datastructure.selected_cols_w_types,
         )
-        assert isinstance(dataset[idx], tuple)
-        assert len(dataset[idx]) == 2  # split into x,y
-        assert len(dataset[idx][0]) == 3  # split into tab, image, support
-        assert (
-            len(dataset[idx][0][0]) == 14
-        )  # tabular cols check (multihead_col included)
-        # support cols check; mypy doesn't know that there are greater than 2 elements
-        assert len(dataset[idx][0][2]) == 3  # type: ignore[misc] # Reason: see above # noqa: B950
-        assert len([dataset[idx][1]]) == 1  # y check
+
+        indexed_item: tuple = dataset[idx]
+        assert isinstance(indexed_item, tuple)
+        assert len(indexed_item) == 2  # split into x,y
+
+        X: tuple = indexed_item[0]
+        assert isinstance(X, tuple)
+        assert len(X) == 3  # split into tab, image, support
+        assert len(X[0]) == 14  # tabular cols check (multihead_col included)
+        assert len(X[2]) == 3  # support cols check
+
+        assert len([indexed_item[1]]) == 1  # y check
 
     def test_dataset_works_only_with_continuous_features(
-        self, dataframe: pd.DataFrame
+        self, dataframe: _DataFrameType
     ) -> None:
         """Test no errors are raised if the dataset only has continuous features."""
-        datasource = DataFrameSource(dataframe.loc[:, ["A", "B", "TARGET"]])
-        datasource.load_data()
+        datasource = DataSource(dataframe.loc[:, ["A", "B", "TARGET"]])
         schema = BitfountSchema()
-        schema.add_datasource_tables(datasource, table_name=TABLE_NAME)
-        datastructure = DataStructure(
-            target=["TARGET"],
-            table=TABLE_NAME,
-        )
-        datastructure.set_training_column_split_by_semantic_type(schema.tables[0])
-        dataset = _PyTorchDataset(
-            datasource=datasource,
+        schema.add_datasource_features(datasource)
+        datastructure = DataStructure(target=["TARGET"])
+        datastructure.set_training_column_split_by_semantic_type(schema)
+        dataset = _Dataset(
+            data=datasource.data,
             target=["TARGET"],
-            selected_cols_semantic_types=datastructure.selected_cols_w_types,
-            selected_cols=datastructure.selected_cols,
-            schema=schema.get_table_schema(TABLE_NAME),
-            data_split=DataSplit.TRAIN,
+            selected_cols=datastructure.selected_cols_w_types,
         )
-        assert "categorical" not in schema.tables[0].features
+        assert "categorical" not in schema.features
         idx = 10
         assert isinstance(dataset[idx], tuple)
         assert len(dataset[idx]) == 2  # split into x,y
         assert len(dataset[idx][0]) == 2  # split into tabular, support
         assert len(dataset[idx][0][0]) == 2  # training cols  check
         assert len(dataset[idx][0][1]) == 2  # support cols check
         assert len([dataset[idx][1]]) == 1  # y check
 
-    def test_dataset_works_without_target(self, dataframe: pd.DataFrame) -> None:
+    def test_dataset_works_without_target(self, dataframe: _DataFrameType) -> None:
         """Test no errors are raised if the dataset has no target."""
-        datasource = DataFrameSource(dataframe.loc[:, ["A", "B"]])
-        datasource.load_data()
+        datasource = DataSource(dataframe.loc[:, ["A", "B"]])
         schema = BitfountSchema()
-        schema.add_datasource_tables(datasource, table_name=TABLE_NAME)
-        datastructure = DataStructure(table=TABLE_NAME)
-        datastructure.set_training_column_split_by_semantic_type(schema.tables[0])
-        dataset = _PyTorchDataset(
-            datasource=datasource,
-            selected_cols_semantic_types=datastructure.selected_cols_w_types,
-            selected_cols=datastructure.selected_cols,
-            schema=schema.get_table_schema(TABLE_NAME),
-            data_split=DataSplit.TRAIN,
+        schema.add_datasource_features(datasource)
+        datastructure = DataStructure()
+        datastructure.set_training_column_split_by_semantic_type(schema)
+        dataset = _Dataset(
+            data=datasource.data,
+            selected_cols=datastructure.selected_cols_w_types,
         )
         idx = 10
         assert isinstance(dataset[idx], tuple)
         assert len(dataset[idx]) == 2  # split into x,y
         assert len(dataset[idx][0]) == 2  # split into tabular, support
         assert len(dataset[idx][0][0]) == 2  # training cols  check
         assert len(dataset[idx][0][1]) == 2  # support cols check
         assert len([dataset[idx][1]]) == 1  # y check
         for i in range(0, len(dataset)):
             assert dataset[i][1] == 0  # all target values default to 0.
 
     def test_dataset_works_only_with_categorical_features(
-        self, dataframe: pd.DataFrame
+        self, dataframe: _DataFrameType
     ) -> None:
         """Test no errors are raised if the dataset only has categorical features."""
-        datasource = DataFrameSource(dataframe.loc[:, ["M", "N", "TARGET"]])
-        datasource.load_data()
+        datasource = DataSource(dataframe.loc[:, ["M", "N", "TARGET"]])
         schema = BitfountSchema()
-        schema.add_datasource_tables(
-            datasource,
-            force_stypes={TABLE_NAME: {"categorical": ["TARGET"]}},
-            table_name=TABLE_NAME,
+        schema.add_datasource_features(
+            datasource, force_stype={"categorical": ["TARGET"]}
         )
-        datastructure = DataStructure(target=["TARGET"], table=TABLE_NAME)
-        datastructure.set_training_column_split_by_semantic_type(schema.tables[0])
-        dataset = _PyTorchDataset(
-            datasource=datasource,
+        datastructure = DataStructure(target=["TARGET"])
+        datastructure.set_training_column_split_by_semantic_type(schema)
+        dataset = _Dataset(
+            data=datasource.data,
             target=["TARGET"],
-            selected_cols_semantic_types=datastructure.selected_cols_w_types,
-            selected_cols=datastructure.selected_cols,
-            schema=schema.get_table_schema(TABLE_NAME),
-            data_split=DataSplit.TRAIN,
+            selected_cols=datastructure.selected_cols_w_types,
         )
-        assert "continuous" not in schema.tables[0].features
+        assert "continuous" not in schema.features
         idx = 10
         assert isinstance(dataset[idx], tuple)
         assert len(dataset[idx]) == 2  # split into x,y
         assert len(dataset[idx][0]) == 2  # split into tabular, support
         assert len(dataset[idx][0][0]) == 2  # training cols  check
         assert len(dataset[idx][0][1]) == 2  # support cols check
         assert len([dataset[idx][1]]) == 1  # y check
 
-    def test_transform_image_with_default_batch_transformations(
-        self, dataframe: pd.DataFrame
+    def test_batch_transformation_step_missing_raises_value_error(
+        self, dataframe: _DataFrameType
     ) -> None:
-        """Test that the default transformations are applied to the image correctly.
-
-        This is done for every split.
-        """
+        """Tests that a ValueError is raised if batch transformation step is missing."""
         target = "TARGET"
-        datasource = DataFrameSource(dataframe)
-        datasource.load_data()
-        schema = BitfountSchema(
-            datasource,
-            force_stypes={TABLE_NAME: {"image": ["image"]}},
-            table_name=TABLE_NAME,
-        )
-
+        datasource = DataSource(dataframe)
+        schema = BitfountSchema()
+        schema.add_datasource_features(datasource, force_stype={"image": ["image"]})
+        datasource.data = schema.apply(datasource.data)
         datasource.data = datasource.data.drop(
-            columns=schema.get_feature_names(TABLE_NAME, SemanticType.TEXT)
+            columns=schema.feature_names(SemanticType.TEXT)
         )
         datastructure = DataStructure(
-            target=target,
-            table=TABLE_NAME,
-            selected_cols=["image", target],
-            image_cols=["image"],
+            target=target, selected_cols=["image", target], image_cols=["image"]
         )
-        datastructure.set_training_column_split_by_semantic_type(schema.tables[0])
-
-        # Iterate over all data splits
-        for split in DataSplit:
-            image_dataset = _PyTorchDataset(
-                datasource=datasource,
+        datastructure.set_training_column_split_by_semantic_type(schema)
+        with pytest.raises(ValueError):
+            _Dataset(
+                data=datasource.data,
                 target=target,
-                schema=schema.get_table_schema(TABLE_NAME),
-                selected_cols=datastructure.selected_cols,
-                selected_cols_semantic_types=datastructure.selected_cols_w_types,
+                selected_cols=datastructure.selected_cols_w_types,
                 batch_transforms=datastructure.get_batch_transformations(),
-                data_split=split,
             )
 
-            # Make assertions
-            assert image_dataset.batch_transforms is not None
-            img_array = np.array(Image.new("RGB", size=(224, 224), color=(55, 100, 2)))
-            transformed_image = image_dataset._transform_image(img_array.copy(), 0)
-            assert isinstance(transformed_image, torch.Tensor)
-
-            # Torch transformation resizes the image so that the channels are first
-            assert transformed_image.shape == (3, 224, 224)
-            torch_img_array = torch.from_numpy(img_array)
-            assert torch_img_array.shape == (224, 224, 3)
-
-            # Check that the image dtypes are different because the transformed image
-            # has been normalized. The actual tensors cannot be compared with eachother
-            # because the dtypes are different.
-            assert torch_img_array.dtype != transformed_image.dtype
+    def test_transform_image(self, image_dataset: _Dataset) -> None:
+        """Test transform_image method."""
+        assert image_dataset.batch_transforms is not None
+        img_array = np.array(Image.new("RGB", size=(224, 224), color=(55, 100, 2)))
+        transformed_image = image_dataset._transform_image(img_array, 0)
+        assert isinstance(transformed_image, np.ndarray)
+        assert transformed_image.shape == (224, 224, 3)
+
+        # Assert that the transformed image is not the same as the original
+        with pytest.raises(AssertionError):
+            np.testing.assert_array_equal(img_array, transformed_image)
+
+    def test_load_image(self, image_dataset: _Dataset) -> None:
+        """Test transform_image method."""
+        loaded_transformed_image = image_dataset._load_images(0)
+        assert isinstance(loaded_transformed_image, np.ndarray)
+        assert loaded_transformed_image.shape == (224, 224, 3)
+
+    # The below comment can be removed unless anyone thinks we need more tests.
+    # TODO: [BIT-983] Add non-backend dataset tests
```

### Comparing `bitfount-0.5.86/tests/bitfount/backends/pytorch/federated/test_distributed_model.py` & `bitfount-0.5.9/tests/bitfount/backends/pytorch/federated/test_distributed_model.py`

 * *Files 4% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 
 import pytest
 import pytorch_lightning as pl
 import torch
 
 from bitfount.backends.pytorch.models.models import PyTorchTabularClassifier
 from bitfount.backends.pytorch.types import _AdaptorForPyTorchTensor
-from bitfount.data.datasources.base_source import BaseSource
+from bitfount.data.datasource import DataSource
 from bitfount.data.datastructure import DataStructure
 from bitfount.data.schema import BitfountSchema
 from tests.bitfount.backends.pytorch.helper import get_params_mean
 from tests.utils.helper import (
     backend_test,
     create_datasource,
     create_datastructure,
@@ -23,15 +23,15 @@
 @pytest.fixture
 def datastructure() -> DataStructure:
     """Fixture for datastructure."""
     return create_datastructure()
 
 
 @pytest.fixture
-def datasource(datastructure: DataStructure) -> BaseSource:
+def datasource(datastructure: DataStructure) -> DataSource:
     """Fixture for datasource."""
     return create_datasource(classification=True)
 
 
 @backend_test
 class TestDistributedModel:
     """Test distributed model methods with PyTorch models."""
@@ -58,15 +58,15 @@
         )
         model.set_model_training_iterations(7)
         assert model.steps == 7
         assert model.epochs is None
 
     @unit_test
     def test_model_update_params(
-        self, datasource: BaseSource, datastructure: DataStructure
+        self, datasource: DataSource, datastructure: DataStructure
     ) -> None:
         """Check that updating model params works properly."""
         model1 = PyTorchTabularClassifier(
             datastructure=datastructure, schema=BitfountSchema(), seed=42, epochs=2
         )
         model1.initialise_model(datasource)
         model1_original_params = model1.get_param_states()
@@ -112,29 +112,18 @@
             datastructure=create_datastructure(),
             schema=BitfountSchema(),
             epochs=epochs,
             steps=steps,
         )
         model.initialise_model(create_datasource(classification=True))
         model._pl_trainer.random_attribute = 5  # type: ignore[attr-defined] # Reason: integral to test # noqa: B950
-        model._total_num_batches_trained = 5
         model.reset_trainer()
         assert isinstance(model._pl_trainer, pl.Trainer)
         # Check that new trainer does not have the random attribute to ensure it is new
         assert not hasattr(model._pl_trainer, "random_attribute")
-        # Check that the number of steps to be trained is updated correctly
-        if steps:
-            assert model.steps == steps
-            assert (
-                model._pl_trainer.max_steps == model._total_num_batches_trained + steps
-            )
-        # Check that the number of epochs to be trained is updated correctly
-        elif epochs:
-            assert model.epochs == epochs
-            assert model._pl_trainer.max_epochs == epochs
 
     @unit_test
     @pytest.mark.parametrize(
         "steps,total_num_batches_trained", [(1, 0), (3, 3), (5, 10)]
     )
     def test_reset_trainer_steps(
         self, steps: int, total_num_batches_trained: int
@@ -148,10 +137,11 @@
             schema=BitfountSchema(),
             steps=steps,
         )
         model.initialise_model(create_datasource(classification=True))
         model._total_num_batches_trained = total_num_batches_trained
         model.reset_trainer()
 
-        assert model._pl_trainer.max_steps == steps + (
-            total_num_batches_trained % len(model.train_dl)
+        assert (
+            model._pl_trainer.max_steps  # type: ignore[attr-defined] # Reason: Mypy can't detect attribute # noqa: B950
+            == steps + (total_num_batches_trained % len(model.train_dl))
         )
```

### Comparing `bitfount-0.5.86/tests/bitfount/backends/pytorch/federated/test_protocol.py` & `bitfount-0.5.9/tests/bitfount/backends/pytorch/federated/test_protocol.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,67 +1,57 @@
 """Test protocol.py with pytorch backend."""
 import copy
-from typing import List, Optional, Union, cast
+from typing import Any, Dict, List, Optional, Union, cast
 from unittest.mock import AsyncMock, Mock, create_autospec
 
+from _pytest.logging import LogCaptureFixture
 import pytest
-from pytest import LogCaptureFixture, fixture
+from pytest import fixture
 from pytest_mock import MockerFixture
 
-from bitfount.backends.pytorch.federated.shim import PyTorchBackendTensorShim
+from bitfount.backends.pytorch.federated.models import PyTorchBackendTensorShim
 from bitfount.backends.pytorch.models.models import PyTorchTabularClassifier
-from bitfount.data.datasources.dataframe_source import DataFrameSource
+from bitfount.data.datasource import DataSource
 from bitfount.data.datastructure import DataStructure
 from bitfount.data.schema import BitfountSchema
 from bitfount.federated.aggregators.aggregator import (
     Aggregator,
     _ModellerSide,
     _WorkerSide,
 )
 from bitfount.federated.aggregators.secure import SecureAggregator
 from bitfount.federated.algorithms.model_algorithms import federated_training
 from bitfount.federated.algorithms.model_algorithms.federated_training import (
     FederatedModelTraining,
     _BaseModelTrainingMixIn,
 )
-from bitfount.federated.protocols.model_protocols.federated_averaging import (
+from bitfount.federated.protocols.fed_avg import (
     FederatedAveraging,
     _FederatedAveragingCompatibleAlgoFactory,
 )
 from bitfount.federated.secure import SecureShare
 from bitfount.federated.transport.modeller_transport import _ModellerMailbox
 from bitfount.federated.transport.worker_transport import _WorkerMailbox
-from bitfount.types import DistributedModelProtocol, _SerializedWeights, _StrAnyDict
+from bitfount.types import DistributedModelProtocol, _SerializedWeights
 from tests.bitfount import TEST_SECURITY_FILES
 from tests.utils import PytestRequest
 from tests.utils.helper import (
     backend_test,
     create_dataset,
     create_datastructure,
     create_schema,
     unit_test,
 )
 
 
 @fixture
-def datasource() -> DataFrameSource:
+def datasource() -> DataSource:
     """Returns datasource."""
     dataset = create_dataset()
-    return DataFrameSource(dataset)
-
-
-@fixture
-def datastructure_mult_pods() -> DataStructure:
-    """Fixture for datastructure."""
-    ds = create_datastructure()
-    ds.table = {
-        "bitfount/census-income": "census-income",
-        "bitfount/census-income-2": "census-income-2",
-    }
-    return ds
+    return DataSource(data=dataset)
 
 
 @fixture
 def datastructure() -> DataStructure:
     """Fixture for datastructure."""
     return create_datastructure()
 
@@ -71,20 +61,22 @@
     """Fixture for schema."""
     return create_schema(classification=True)
 
 
 @fixture
 def aggregator(request: PytestRequest) -> Union[Aggregator, SecureAggregator]:
     """Returns mock aggregator."""
+    tensor_shim = PyTorchBackendTensorShim()
     # request.param denotes whether we should use secure aggregator
     if request.param:
         return SecureAggregator(
-            secure_share=SecureShare(),
+            tensor_shim=tensor_shim,
+            secure_share=SecureShare(tensor_shim=tensor_shim),
         )
-    return Aggregator()
+    return Aggregator(tensor_shim=tensor_shim)
 
 
 @fixture
 def mock_aggregator() -> Mock:
     """A mock that implements the aggregator (factory) interface."""
     mock_aggregator: Mock = create_autospec(spec=Aggregator, instance=True)
 
@@ -98,24 +90,24 @@
 
     return mock_aggregator
 
 
 @fixture
 def mock_modeller_mailbox() -> Mock:
     """Returns mock mailbox."""
+    # Returning "Any" since that is the given return type for create_autospec
     mock_mailbox: Mock = create_autospec(_ModellerMailbox, instance=True)
-    mock_mailbox._task_id = "task_id"
     return mock_mailbox
 
 
 @fixture
-def model(datastructure_mult_pods: DataStructure) -> PyTorchTabularClassifier:
+def model(datastructure: DataStructure) -> PyTorchTabularClassifier:
     """Returns distributed model."""
     return PyTorchTabularClassifier(
-        datastructure=datastructure_mult_pods, schema=BitfountSchema(), epochs=1
+        datastructure=datastructure, schema=BitfountSchema(), epochs=1
     )
 
 
 @fixture
 def federated_algorithm_with_model(
     model: PyTorchTabularClassifier,
 ) -> FederatedModelTraining:
@@ -125,19 +117,19 @@
 
 @fixture
 def federated_algorithm(
     datastructure: DataStructure, request: PytestRequest
 ) -> FederatedModelTraining:
     """Returns federated algorithm with configurable iterations."""
     epochs, steps = request.param
-    schema = create_schema(classification=True)
+
     return FederatedModelTraining(
         model=PyTorchTabularClassifier(
             datastructure=datastructure,
-            schema=schema,
+            schema=BitfountSchema(),
             epochs=epochs,
             steps=steps,
         )
     )
 
 
 @fixture
@@ -167,23 +159,23 @@
     return mock_algo_factory
 
 
 @fixture
 def mock_modeller_run_method(mocker: MockerFixture) -> AsyncMock:
     """Mocks out the Modeller.run() method in protocol.py."""
     mock_run_method: AsyncMock = mocker.patch(
-        "bitfount.federated.protocols.base._Modeller.run"
+        "bitfount.federated.protocols.base.Modeller.run"
     )
     return mock_run_method
 
 
 @fixture
 def pod_identifiers() -> List[str]:
     """A list of pod identifiers."""
-    return ["bitfount/census-income", "bitfount/census-income-2"]
+    return ["bitfount/prosper", "bitfount/prosper2"]
 
 
 @backend_test
 class TestFederatedAveraging:
     """Tests FederatedAveraging protocol with pytorch models."""
 
     # TODO: [BIT-983] Should these tests mostly be on the main protocol tests
@@ -264,30 +256,30 @@
             ((None, 10), None, 2),
             ((None, 20), None, 7),
         ],
         indirect=["federated_algorithm"],
     )
     async def test_worker_model_training_for_appropriate_amount(
         self,
-        datasource: DataFrameSource,
+        datasource: DataSource,
         epochs_between_parameter_updates: Optional[int],
         federated_algorithm: FederatedModelTraining,
         mock_aggregator: Mock,
         mocker: MockerFixture,
         schema: BitfountSchema,
         steps_between_parameter_updates: Optional[int],
     ) -> None:
         """Test underlying worker model iterations updated correctly during training."""
         # Create mailbox and mock out relevant functions
         mock_worker_mailbox = create_autospec(_WorkerMailbox, instance=True)
         mock_get_model_updates = mocker.patch(
-            "bitfount.federated.protocols.model_protocols.federated_averaging._get_model_parameters"
+            "bitfount.federated.protocols.fed_avg._get_model_parameters"
         )
         mock_send_model_updates = mocker.patch(
-            "bitfount.federated.protocols.model_protocols.federated_averaging._send_parameter_update"
+            "bitfount.federated.protocols.fed_avg._send_parameter_update"
         )
         # Mock out so training complete never returns truthy value prematurely
         mock_worker_mailbox.get_training_iteration_complete_update.return_value = False
 
         # Create protocol and initialise model
         protocol_factory = FederatedAveraging(
             algorithm=federated_algorithm,
@@ -302,20 +294,20 @@
             int, steps_between_parameter_updates or epochs_between_parameter_updates
         )
 
         async def mock_get_model_updates_coroutine(
             _mailbox: _WorkerMailbox,
         ) -> _SerializedWeights:
             """Mock out get_model_parameters."""
-            params = cast(
-                _BaseModelTrainingMixIn, protocol.algorithm
-            ).get_param_states()
-            model_params = copy.deepcopy(cast(_StrAnyDict, params))
+            model_params: Dict[str, Any] = copy.deepcopy(
+                cast(_BaseModelTrainingMixIn, protocol.algorithm).get_param_states()
+            )
             for name, param in model_params.items():
                 model_params[name] = PyTorchBackendTensorShim.to_list(param)
+
             return model_params
 
         mock_get_model_updates.side_effect = mock_get_model_updates_coroutine
 
         await protocol.run(datasource=datasource)
 
         # We expect to send and receive updates in each federated iteration (and
@@ -327,19 +319,17 @@
 
         # Check that the iterations are correct, whichever form we're using.
         # If we are using epochs, we can only check that the epoch number is right on
         # the final iteration. If we are using steps, instead we check that the total
         # number of iterations is correct.
         assert isinstance(federated_algorithm.model, DistributedModelProtocol)
         if epochs_between_parameter_updates:
-            # The `current_epoch` attribute denotes the epoch number of the epoch
-            # about to be trained on the next iteration (starting from 0)
             assert (
                 federated_algorithm.model._pl_trainer.current_epoch  # type: ignore[attr-defined] # Reason: Model has _pl_trainer # noqa: B950
-                == epochs_between_parameter_updates
+                == epochs_between_parameter_updates - 1
             )
         else:
             assert (
                 expected_total_num_iterations
                 == federated_algorithm.model._total_num_batches_trained
             )
 
@@ -348,53 +338,38 @@
         self,
         federated_algorithm_with_model: FederatedModelTraining,
         mock_bitfount_session: Mock,
         mock_modeller_run_method: Mock,
         pod_identifiers: List[str],
     ) -> None:
         """Tests protocol helper run method with algorithm."""
-        protocol = FederatedAveraging(algorithm=federated_algorithm_with_model)
-
-        protocol.run(
+        FederatedAveraging.run(
             pod_identifiers=pod_identifiers,
+            algorithm=federated_algorithm_with_model,
             private_key_or_file=TEST_SECURITY_FILES / "test_private.testkey",
         )
 
-        mock_modeller_run_method.assert_called_once_with(
-            pod_identifiers,
-            require_all_pods=False,
-            run_on_new_data_only=False,
-            model_out=None,
-            project_id=None,
-            batched_execution=False,
-        )
+        mock_modeller_run_method.assert_called_once_with(pod_identifiers)
 
     @unit_test
     def test_helper_run_method_with_model(
         self,
         mock_bitfount_session: Mock,
         mock_modeller_run_method: Mock,
         model: PyTorchTabularClassifier,
         pod_identifiers: List[str],
     ) -> None:
         """Tests helper run method with a model."""
-        protocol = FederatedAveraging(algorithm=FederatedModelTraining(model=model))
-        protocol.run(
+        FederatedAveraging.run(
             pod_identifiers=pod_identifiers,
+            model=model,
             private_key_or_file=TEST_SECURITY_FILES / "test_private.testkey",
         )
 
-        mock_modeller_run_method.assert_called_once_with(
-            pod_identifiers,
-            require_all_pods=False,
-            run_on_new_data_only=False,
-            model_out=None,
-            project_id=None,
-            batched_execution=False,
-        )
+        mock_modeller_run_method.assert_called_once_with(pod_identifiers)
 
     @unit_test
     def test_helper_run_method_with_model_and_algorithm(
         self,
         caplog: LogCaptureFixture,
         federated_algorithm_with_model: FederatedModelTraining,
         mock_bitfount_session: Mock,
@@ -403,26 +378,19 @@
     ) -> None:
         """Tests helper run method with an algorithm and a model.
 
         This tests that the run method will still run but that it just issues a warning
         regarding the extra model argument.
         """
         mock_model = Mock()
-        protocol = FederatedAveraging(
-            model=mock_model, algorithm=federated_algorithm_with_model
-        )
-        protocol.run(
+
+        FederatedAveraging.run(
             pod_identifiers=pod_identifiers,
+            model=mock_model,
+            algorithm=federated_algorithm_with_model,
             private_key_or_file=TEST_SECURITY_FILES / "test_private.testkey",
         )
 
-        mock_modeller_run_method.assert_called_once_with(
-            pod_identifiers,
-            require_all_pods=False,
-            run_on_new_data_only=False,
-            model_out=None,
-            project_id=None,
-            batched_execution=False,
-        )
+        mock_modeller_run_method.assert_called_once_with(pod_identifiers)
         mock_model.assert_not_called()
         mock_model.backend_tensor_shim.assert_not_called()
         assert "Ignoring provided model. Algorithm already has a model." in caplog.text
```

### Comparing `bitfount-0.5.86/tests/bitfount/backends/pytorch/federated/test_secure.py` & `bitfount-0.5.9/tests/bitfount/backends/pytorch/federated/test_secure.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,31 +1,31 @@
 """Test secure multi party computation with pytorch backend."""
 import copy
 from functools import partial
 from typing import Any, Callable, Dict, List, Tuple, cast
-from unittest.mock import AsyncMock, MagicMock, create_autospec
+from unittest.mock import AsyncMock, MagicMock, Mock, create_autospec
 
 import numpy as np
-import pandas as pd
 import pytest
-from pytest import LogCaptureFixture, fixture
+from pytest import fixture
 from pytest_mock import MockerFixture
 import torch
 
+from bitfount.backends.pytorch.federated.models import PyTorchBackendTensorShim
 from bitfount.backends.pytorch.models.models import PyTorchTabularClassifier
 from bitfount.backends.pytorch.types import _AdaptorForPyTorchTensor
-from bitfount.data.datasources.base_source import BaseSource
+from bitfount.data.datasource import DataSource
 from bitfount.data.datastructure import DataStructure
 from bitfount.data.schema import BitfountSchema
+from bitfount.federated.exceptions import SecureShareError
 from bitfount.federated.secure import SecureShare
 from bitfount.federated.transport.worker_transport import _WorkerMailbox
-from bitfount.types import _TensorLike, _Weights
+from bitfount.types import _TensorLike, _WeightDict
 from tests.utils.helper import (
     backend_test,
-    create_dataset,
     create_datasource,
     create_datastructure,
     unit_test,
 )
 
 
 @backend_test
@@ -35,15 +35,15 @@
 
     @fixture
     def datastructure(self) -> DataStructure:
         """Fixture for datastructure."""
         return create_datastructure()
 
     @fixture
-    def datasource(self) -> BaseSource:
+    def datasource(self) -> DataSource:
         """Fixture for datasource."""
         return create_datasource(classification=True)
 
     @fixture
     def mock_secure_transport_methods(self) -> Tuple[Callable, Callable]:
         """Returns tuple of mocked SecureShare methods which need a mailbox."""
 
@@ -74,15 +74,17 @@
             (1913, [0.1, -0.4, 0.4], 1000),
         ],
     )
     def test_encode_and_decode_finite_field(
         self, precision: int, prime_q: int, tensor_list: List[float]
     ) -> None:
         """Ensures output matches input after encoding and decoding."""
-        sec = SecureShare(prime_q=prime_q, precision=precision)
+        sec = SecureShare(
+            tensor_shim=PyTorchBackendTensorShim(), prime_q=prime_q, precision=precision
+        )
         tensor = _AdaptorForPyTorchTensor(
             torch.tensor(tensor_list, dtype=torch.float64)
         )
         encoded_tensor = sec._encode_finite_field(tensor)
         assert isinstance(encoded_tensor, np.ndarray)
         decoded_tensor = sec._decode_finite_field(encoded_tensor)
         assert isinstance(decoded_tensor, np.ndarray)
@@ -98,15 +100,15 @@
             ([-123, 50], [-1, 17]),
             ([-123.23, 50.1], [-1.01, 16.99]),
             ([0.1234001, 0.23451], [0.13245, 0.674532]),
         ],
     )
     def test_addition_finite_field(self, a: List[float], b: List[float]) -> None:
         """Tests addition of encoded tensors, compares result with regular addition."""
-        sec = SecureShare()
+        sec = SecureShare(tensor_shim=PyTorchBackendTensorShim())
         encoded_a = sec._encode_finite_field(
             _AdaptorForPyTorchTensor(torch.tensor(a, dtype=torch.float32))
         )
         encoded_b = sec._encode_finite_field(
             _AdaptorForPyTorchTensor(torch.tensor(b, dtype=torch.float32))
         )
         encoded_result = sec._add([encoded_a, encoded_b])
@@ -127,47 +129,41 @@
             (7, [0.1, -0.39, 0.4], True, 0),
             (13, [0.1, -0.2, -0.3], False, 1),
             (13, [0.1, 0.2, 0.3], True, 2),
             (13, [0.1, -0.39, 0.39], True, 3),
         ],
     )
     def test_encode_finite_field_secure_share_error(
-        self,
-        caplog: LogCaptureFixture,
-        error: bool,
-        num_other_workers: int,
-        prime_q: int,
-        tensor: List[float],
+        self, error: bool, num_other_workers: int, prime_q: int, tensor: List[float]
     ) -> None:
-        """Tests that clamping is applied during encoding.
+        """Ensures SecureShareError is raised appropriately during encoding.
 
         When there are not enough positive integers in prime_q to map every positive
         and negative integer to an integer in the finite field without multiple values
-        corresponding to the same value, we clamp the parameters before encoding.
-        This is applied *only* when steps between parameter updates in the protocol
-        is set to 1.
+        corresponding to the same value, a SecureShareError should be raised. When the
+        precision is low and we still have floats after multiplying by `precision`, the
+        number is truncated. This is why 0.39 does not raise a SecureShareError when
+        `prime_q` is 7 but 0.4 does.
         """
-        sec = SecureShare(prime_q=prime_q, precision=10)
+        sec = SecureShare(
+            tensor_shim=PyTorchBackendTensorShim(), prime_q=prime_q, precision=10
+        )
         sec._own_shares = [1]
         sec._own_shares = sec._own_shares * num_other_workers
         if error:
-            sec._encode_finite_field(_AdaptorForPyTorchTensor(torch.tensor(tensor)))
-            assert (
-                caplog.records[0].msg
-                == "Parameter weights have been clipped. If you want to avoid this, "
-                "choose a larger `prime_q` or a smaller `precision` for "
-                "the `SecureShare` or normalize continuous features prior "
-                "to training."
-            )
+            with pytest.raises(SecureShareError):
+                sec._encode_finite_field(_AdaptorForPyTorchTensor(torch.tensor(tensor)))
         else:
             sec._encode_finite_field(_AdaptorForPyTorchTensor(torch.tensor(tensor)))
 
     def test_get_random_number(self) -> None:
         """Tests _get_random_number method."""
-        sec = SecureShare(prime_q=101, precision=10)
+        sec = SecureShare(
+            tensor_shim=PyTorchBackendTensorShim(), prime_q=101, precision=10
+        )
         assert sec._own_shares == []
         for i in range(1, 10):
             sec._get_random_number()
             assert len(sec._own_shares) == i
             assert sec._own_shares[-1] < sec.prime_q
 
     @pytest.mark.parametrize(
@@ -176,50 +172,54 @@
             ([0.6, 0.7, 0.8], [1, 2, 3]),
             ([-1, -4, -3], [994, 964, 974]),
             ([-2, -3, 4.1], [984, 974, 36]),
         ],
     )
     def test_encode_secret(self, input: List[float], output: List[int]) -> None:
         """Tests _encode_secret method with fixed shares and simple tensors."""
-        sec = SecureShare(prime_q=1009, precision=10)
+        sec = SecureShare(
+            tensor_shim=PyTorchBackendTensorShim(), prime_q=1009, precision=10
+        )
         sec._own_shares = [-2, 3, 4]
         encoded_secret = sec._encode_secret(
             _AdaptorForPyTorchTensor(torch.tensor(input))
         )
         assert isinstance(encoded_secret, np.ndarray)
         np.testing.assert_array_equal(encoded_secret, np.array(output))
 
     def test_reconstruct_secret(self) -> None:
         """Tests _reconstruct_secret with simple tensors."""
-        sec = SecureShare(prime_q=101, precision=10)
+        sec = SecureShare(
+            tensor_shim=PyTorchBackendTensorShim(), prime_q=101, precision=10
+        )
         reconstructed = sec._reconstruct_secret([np.array([1, 2, 3]), 4, 5, 6])
         assert isinstance(reconstructed, np.ndarray)
         assert len(reconstructed) == 3
         np.testing.assert_array_equal(reconstructed, np.array([16, 17, 18]))
 
-    def test_encode_and_reconstruct_state_dict(self) -> None:
-        """Tests _encode_and_reconstruct_state_dict with simple tensors."""
-        sec = SecureShare()
-        state_dict: _Weights = {
+    def test_encode_and_reconstruct_update(self) -> None:
+        """Tests _encode_and_reconstruct_update with simple tensors."""
+        sec = SecureShare(tensor_shim=PyTorchBackendTensorShim())
+        state_dict: _WeightDict = {
             "tensor_a": _AdaptorForPyTorchTensor(torch.tensor([-0.1, 0.9, 1.9])),
             "tensor_b": _AdaptorForPyTorchTensor(torch.tensor([-1.0, 0.0, 1.0])),
         }
-        encoded_state_dict = sec._encode_and_reconstruct_state_dict(state_dict)
+        encoded_state_dict = sec._encode_and_reconstruct_update(state_dict)
         assert isinstance(encoded_state_dict, dict)
         assert len(encoded_state_dict) == 2
         for param in encoded_state_dict.values():
             assert isinstance(param, np.ndarray)
             assert len(param) == 3
 
     async def test_do_secure_aggregation(self, mocker: MockerFixture) -> None:
         """Tests do_secure_aggregation method.
 
         Mocks out transport layer and makes the relevant assertions on calls/awaits.
         """
-        sec = SecureShare()
+        sec = SecureShare(tensor_shim=PyTorchBackendTensorShim())
         state_dict: Dict[str, _TensorLike] = {
             "tensor_a": _AdaptorForPyTorchTensor(torch.tensor([-0.1, 0.9, 1.9])),
             "tensor_b": _AdaptorForPyTorchTensor(torch.tensor([-1.0, 0.0, 1.0])),
         }
         num_other_pods = 3
         mock_mailbox = create_autospec(_WorkerMailbox, instance=True)
 
@@ -257,38 +257,42 @@
             "tensor_a": torch.tensor([-0.9, 0.1, 1.1]),
             "tensor_b": torch.tensor([-2.0, -1.0, 0.0]),
         }
         state_dict_b = {
             "tensor_a": torch.tensor([-0.1, 0.9, 1.9]),
             "tensor_b": torch.tensor([-1.0, 0.0, 1.0]),
         }
-        sec_a = SecureShare(prime_q=1009, precision=10)
-        sec_b = SecureShare(prime_q=1009, precision=10)
+        sec_a = SecureShare(
+            tensor_shim=PyTorchBackendTensorShim(), prime_q=1009, precision=10
+        )
+        sec_b = SecureShare(
+            tensor_shim=PyTorchBackendTensorShim(), prime_q=1009, precision=10
+        )
         rand_a = sec_a._get_random_number()
         rand_b = sec_b._get_random_number()
         sec_a._other_worker_shares.append(rand_b)
         sec_b._other_worker_shares.append(rand_a)
-        encoded_state_dict_a = sec_a._encode_and_reconstruct_state_dict(
-            cast(_Weights, state_dict_a)
+        encoded_state_dict_a = sec_a._encode_and_reconstruct_update(
+            cast(_WeightDict, state_dict_a)
         )
-        encoded_state_dict_b = sec_b._encode_and_reconstruct_state_dict(
-            cast(_Weights, state_dict_b)
+        encoded_state_dict_b = sec_b._encode_and_reconstruct_update(
+            cast(_WeightDict, state_dict_b)
         )
 
         average_state_dict = sec_a.average_and_decode_state_dicts(
-            [encoded_state_dict_a, encoded_state_dict_b],
+            [encoded_state_dict_a, encoded_state_dict_b], dtype=torch.float32
         )
 
         expected_average = [
             torch.tensor([-0.5, 0.5, 1.5]),
             torch.tensor([-1.5, -0.5, 0.5]),
         ]
 
         for x, y in zip(average_state_dict.values(), expected_average):
-            np.testing.assert_array_equal(x, y)
+            assert torch.equal(x.torchtensor, y)  # type: ignore[attr-defined] # reason: x will always be a AdaptorForPyTorchTensor in this test # noqa: B950
 
     @pytest.mark.parametrize(
         "a, b",
         [
             ([13, 10], [29, 1]),
             ([13, 50], [49, 7]),
             ([-13, -50], [-49, -7]),
@@ -301,51 +305,47 @@
     def test_secure_averaging_with_simple_tensors_without_mailbox(
         self, a: List[float], b: List[float]
     ) -> None:
         """Tests secure averaging with simple tensors.
 
         The methods concerning the sharing and receiving of shares have been bypassed.
         """
-        sec_a = SecureShare()
-        sec_b = SecureShare()
+        sec_a = SecureShare(tensor_shim=PyTorchBackendTensorShim())
+        sec_b = SecureShare(tensor_shim=PyTorchBackendTensorShim())
         tensor_a = _AdaptorForPyTorchTensor(torch.tensor(a, dtype=torch.float64))
         tensor_b = _AdaptorForPyTorchTensor(torch.tensor(b, dtype=torch.float64))
         rand_a = sec_a._get_random_number()
         rand_b = sec_b._get_random_number()
         sec_a._other_worker_shares.append(rand_b)
         sec_b._other_worker_shares.append(rand_a)
-        encoded_tensor_a = sec_a._encode_and_reconstruct_state_dict(
-            {"tensor": tensor_a}
-        )
-        encoded_tensor_b = sec_b._encode_and_reconstruct_state_dict(
-            {"tensor": tensor_b}
-        )
+        encoded_tensor_a = sec_a._encode_and_reconstruct_update({"tensor": tensor_a})
+        encoded_tensor_b = sec_b._encode_and_reconstruct_update({"tensor": tensor_b})
 
         encoded_result = sec_a._add(
             [encoded_tensor_a["tensor"], encoded_tensor_b["tensor"]]
         )
         decoded_result = sec_a._decode_finite_field(encoded_result)
 
         np.testing.assert_almost_equal(
             np.array(a) + np.array(b), decoded_result, decimal=4
         )
 
     @pytest.mark.parametrize("num_workers", [2, 3, 5])
     async def test_secure_averaging_with_simple_tensors(
         self,
-        mock_secure_transport_methods: Tuple[Callable, Callable],
+        mock_secure_transport_methods: Mock,
         mocker: MockerFixture,
         num_workers: int,
     ) -> None:
         """Tests secure averaging with simple tensors.
 
         The methods which rely on the transport layer have been mocked.
         """
         worker_shares: Dict[str, List[SecureShare]] = {}
-        nn_params: List[_Weights] = []
+        nn_params: List[_WeightDict] = []
         for i in range(num_workers):
             params = {
                 "tensor": torch.tensor([-5.1, -3.2, -1.3], dtype=torch.float64) + i
             }
             nn_params.append(cast(Dict[str, _TensorLike], params))
             worker_shares[f"worker_{i}"] = []
 
@@ -358,30 +358,30 @@
         mocker.patch.object(
             SecureShare,
             "_receive_worker_shares",
             autospec=True,
             side_effect=partial(mock_secure_transport_methods[1], worker_shares),
         )
 
-        secures: List[SecureShare] = []
+        secures = []
 
         for name in worker_shares:
-            sec = SecureShare()
+            sec = SecureShare(tensor_shim=PyTorchBackendTensorShim())
             sec.name = name  # type: ignore[attr-defined] # Reason: name attr is only used in our patched functions above # noqa: B950
             await sec._share_own_shares(None)  # type: ignore[arg-type] # Reason: the function is mocked # noqa: B950
             secures.append(sec)
 
         for sec in secures:
             await sec._receive_worker_shares(None)  # type: ignore[arg-type] # Reason: the function is mocked # noqa: B950
 
         encoded_weights = []
 
         for i, sec in enumerate(secures):
             reconstructed_secret = list(
-                sec._encode_and_reconstruct_state_dict(nn_params[i]).values()
+                sec._encode_and_reconstruct_update(nn_params[i]).values()
             )
             encoded_weights.append(reconstructed_secret[0])
 
         summed_weights = encoded_weights[0]
         for i in range(1, len(encoded_weights)):
             summed_weights = sec._add([summed_weights, encoded_weights[i]])
 
@@ -389,20 +389,23 @@
         insecure_sum: torch.Tensor = torch.sum(
             torch.stack([cast(torch.Tensor, list(p.values())[0]) for p in nn_params]),
             dim=0,
         )
         for decoded_item, insecure_item in zip(decoded, insecure_sum):
             assert torch.isclose(decoded_item, insecure_item, atol=1e-4)
 
-    @pytest.mark.parametrize("num_workers", [2, 3, 10])
+    @pytest.mark.parametrize(
+        "num_workers",
+        [2, 3, 10],
+    )
     async def test_secure_averaging_with_neural_network_parameters(
         self,
-        datasource: BaseSource,
+        datasource: DataSource,
         datastructure: DataStructure,
-        mock_secure_transport_methods: Tuple[Callable, Callable],
+        mock_secure_transport_methods: Mock,
         mocker: MockerFixture,
         num_workers: int,
     ) -> None:
         """Tests secure averaging with neural network parameters.
 
         The methods which rely on a transport layer have been mocked.
         """
@@ -433,109 +436,45 @@
         mocker.patch.object(
             SecureShare,
             "_receive_worker_shares",
             autospec=True,
             side_effect=partial(mock_secure_transport_methods[1], worker_shares),
         )
 
-        secures: List[SecureShare] = []
+        secures = []
 
         for name in worker_shares:
-            sec = SecureShare()
+            sec = SecureShare(tensor_shim=PyTorchBackendTensorShim())
             sec.name = name  # type: ignore[attr-defined] # Reason: name attr is only used in our patched functions above # noqa: B950
             await sec._share_own_shares(None)  # type: ignore[arg-type] # Reason: the function is mocked # noqa: B950
             secures.append(sec)
 
         for sec in secures:
             await sec._receive_worker_shares(None)  # type: ignore[arg-type] # Reason: the function is mocked # noqa: B950
 
         encoded_weights: List[Dict[str, np.ndarray]] = []
 
         for i, sec in enumerate(secures):
             reconstructed_state_dict: Dict[
                 str, np.ndarray
-            ] = sec._encode_and_reconstruct_state_dict(
+            ] = sec._encode_and_reconstruct_update(
                 nn._get_torch_adapter_states(nn_params[i])
             )
             encoded_weights.append(reconstructed_state_dict)
 
-        # It doesn't matter which SecureShare we use to average and decode the
-        # state dictionaries so we just use the last one
-        secure_decoded_average = sec.average_and_decode_state_dicts(encoded_weights)
-
+        secure_decoded_average = sec.average_and_decode_state_dicts(
+            encoded_weights, dtype=torch.float32
+        )
+        secure_decoded_average_tensors = {}
+        for k, v in secure_decoded_average.items():
+            if isinstance(v, _AdaptorForPyTorchTensor):
+                secure_decoded_average_tensors[k] = v.torchtensor
         for param in nn_params[0]:
             insecure_average = nn_params[0][param]
             for i in range(1, len(nn_params)):
                 insecure_average += nn_params[i][param]
             insecure_average /= len(nn_params)
-
-            np.testing.assert_array_almost_equal(
-                torch.mean(insecure_average).numpy(),
-                np.mean(secure_decoded_average[param]),
-                decimal=4,
-            )
-
-    @pytest.mark.parametrize("num_workers", [2, 3, 10])
-    async def test_secure_averaging_with_pandas_dataframes(
-        self,
-        mock_secure_transport_methods: Tuple[Callable, Callable],
-        mocker: MockerFixture,
-        num_workers: int,
-    ) -> None:
-        """Tests secure averaging with neural network parameters.
-
-        The methods which rely on a transport layer have been mocked.
-        """
-        worker_shares: Dict[str, List[SecureShare]] = {}
-        dataframes: List[pd.DataFrame] = []
-        for i in range(num_workers):
-            # Using the index of each worker as the seed to ensure each dataset
-            # is different. The dataset is also subsetted for simplicity.
-            dataframes.append(create_dataset(seed=i)[["A", "B", "C", "D"]])
-            worker_shares[f"worker_{i}"] = []
-
-        mocker.patch.object(
-            SecureShare,
-            "_share_own_shares",
-            autospec=True,
-            side_effect=partial(mock_secure_transport_methods[0], worker_shares),
-        )
-        mocker.patch.object(
-            SecureShare,
-            "_receive_worker_shares",
-            autospec=True,
-            side_effect=partial(mock_secure_transport_methods[1], worker_shares),
-        )
-
-        secures: List[SecureShare] = []
-
-        for name in worker_shares:
-            sec = SecureShare()
-            sec.name = name  # type: ignore[attr-defined] # Reason: name attr is only used in our patched functions above # noqa: B950
-            await sec._share_own_shares(None)  # type: ignore[arg-type] # Reason: the function is mocked # noqa: B950
-            secures.append(sec)
-
-        for sec in secures:
-            await sec._receive_worker_shares(None)  # type: ignore[arg-type] # Reason: the function is mocked # noqa: B950
-
-        encoded_dataframes: List[Dict[str, np.ndarray]] = []
-
-        for i, sec in enumerate(secures):
-            reconstructed_state_dict = sec._encode_and_reconstruct_state_dict(
-                {col: dataframes[i][col].to_numpy() for col in dataframes[i].columns}
-            )
-            encoded_dataframes.append(reconstructed_state_dict)
-
-        sec = SecureShare()
-        secure_decoded_average = sec.average_and_decode_state_dicts(encoded_dataframes)
-
-        for field in dataframes[0].columns:
-            insecure_average = dataframes[0][field]
-            for i in range(1, len(dataframes)):
-                insecure_average += dataframes[i][field]
-            insecure_average /= len(dataframes)
-
-            np.testing.assert_array_almost_equal(
-                np.mean(insecure_average),
-                np.mean(secure_decoded_average[field]),
-                decimal=4,
+            assert torch.isclose(
+                torch.mean(insecure_average),
+                torch.mean(secure_decoded_average_tensors[param]),
+                atol=1e-4,
             )
```

### Comparing `bitfount-0.5.86/tests/bitfount/backends/pytorch/federated/transport/test_message_service_usage.py` & `bitfount-0.5.9/tests/bitfount/backends/pytorch/federated/transport/test_message_service_usage.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,17 +1,15 @@
 """Tests for MessageService federated transport layer with PyTorch models."""
-from collections.abc import Sequence
-from pathlib import Path
+import asyncio
 from typing import Callable
 from unittest.mock import Mock
 
-from pytest import MonkeyPatch, fixture
-import pytest as pytest
+from pytest import fixture
 
-from tests.utils.concurrency_utils import run_local_modeller_and_workers
+from bitfount.federated.transport.base_transport import _run_func_and_listen_to_mailbox
 from tests.utils.helper import (
     AUC_THRESHOLD,
     backend_test,
     create_local_modeller_and_workers,
     integration_test,
 )
 
@@ -26,44 +24,34 @@
         self, apply_mock_get_pod_public_keys: Callable[[str], Mock]
     ) -> Mock:
         """Mocks out get_pod_public_keys function in modeller.py."""
         return apply_mock_get_pod_public_keys(
             "bitfount.federated.modeller._get_pod_public_keys"
         )
 
-    @fixture(autouse=True)
-    def patch_get_modeller_path(self, monkeypatch: MonkeyPatch, tmp_path: Path) -> None:
-        """Patches modeller key path to a writeable temporary directory."""
-        from bitfount.federated.modeller import _Modeller
-
-        monkeypatch.setattr(
-            _Modeller,
-            "_get_modeller_key_storage_path",
-            lambda *args, **kwargs: Path(tmp_path),
-        )
-
     async def test_classification_results_only(
         self,
         mock_get_pod_public_keys: Mock,
         mock_message_aes_decryption: Mock,
         mock_message_aes_encryption: Mock,
         mock_rsa_encryption: Mock,
         mock_rsa_sign_message: Mock,
     ) -> None:
         """Tests PyTorchTabularClassifier with ResultsOnly protocol."""
         modeller, workers = create_local_modeller_and_workers(
             model_name="PyTorchTabularClassifier",
             protocol_name="ResultsOnly",
             algorithm_name="ModelTrainingAndEvaluation",
         )
+        modeller_results, *_ = await asyncio.gather(
+            modeller.run_async([worker.mailbox.pod_identifier for worker in workers]),
+            *[_run_func_and_listen_to_mailbox(w.run(), w.mailbox) for w in workers]
+        )
 
-        modeller_results = await run_local_modeller_and_workers(modeller, workers)
-
-        assert isinstance(modeller_results, dict)
-        for result in modeller_results.values():
+        for result in modeller_results:
             assert result is not None
             assert isinstance(result, dict)
             auc = result["AUC"]
             assert auc > AUC_THRESHOLD
 
     async def test_classification_federated_averaging_and_early_stopping_autoeval_true(
         self,
@@ -77,18 +65,20 @@
         modeller, workers = create_local_modeller_and_workers(
             model_name="PyTorchTabularClassifier",
             protocol_name="FederatedAveraging",
             algorithm_name="FederatedModelTraining",
             early_stopping=True,
         )
 
-        modeller_results = await run_local_modeller_and_workers(modeller, workers)
+        modeller_results, *_ = await asyncio.gather(
+            modeller.run_async([worker.mailbox.pod_identifier for worker in workers]),
+            *[_run_func_and_listen_to_mailbox(w.run(), w.mailbox) for w in workers]
+        )
 
         assert modeller_results is not None
-        assert isinstance(modeller_results, Sequence)
         auc = float(modeller_results[0]["AUC"])
         assert auc > AUC_THRESHOLD
 
     async def test_classification_federated_averaging_and_early_stopping_autoeval_false(
         self,
         mock_get_pod_public_keys: Mock,
         mock_message_aes_decryption: Mock,
@@ -101,35 +91,36 @@
             model_name="PyTorchTabularClassifier",
             protocol_name="FederatedAveraging",
             algorithm_name="FederatedModelTraining",
             early_stopping=True,
             auto_eval=False,
         )
 
-        modeller_results = await run_local_modeller_and_workers(modeller, workers)
-
+        modeller_results, *_ = await asyncio.gather(
+            modeller.run_async([worker.mailbox.pod_identifier for worker in workers]),
+            *[_run_func_and_listen_to_mailbox(w.run(), w.mailbox) for w in workers]
+        )
         assert modeller_results == []
 
-    @pytest.mark.skip(reason="Flaky test, to investigate.")
     async def test_classification_secure_aggregation(
         self,
         mock_get_pod_public_keys: Mock,
         mock_message_aes_decryption: Mock,
         mock_message_aes_encryption: Mock,
-        mock_rsa_decryption: Mock,
         mock_rsa_encryption: Mock,
         mock_rsa_sign_message: Mock,
     ) -> None:
         """Tests PyTorchTabularClassifier Federated Averaging and Secure Aggregation."""
         modeller, workers = create_local_modeller_and_workers(
             model_name="PyTorchTabularClassifier",
             protocol_name="FederatedAveraging",
             algorithm_name="FederatedModelTraining",
             secure_aggregation=True,
         )
-
-        modeller_results = await run_local_modeller_and_workers(modeller, workers)
+        modeller_results, *_ = await asyncio.gather(
+            modeller.run_async([worker.mailbox.pod_identifier for worker in workers]),
+            *[_run_func_and_listen_to_mailbox(w.run(), w.mailbox) for w in workers]
+        )
 
         assert modeller_results is not None
-        assert isinstance(modeller_results, Sequence)
         auc = float(modeller_results[0]["AUC"])
         assert auc > AUC_THRESHOLD
```

### Comparing `bitfount-0.5.86/tests/bitfount/backends/pytorch/helper.py` & `bitfount-0.5.9/tests/bitfount/backends/pytorch/helper.py`

 * *Files identical despite different names*

### Comparing `bitfount-0.5.86/tests/bitfount/data/conftest.py` & `bitfount-0.5.9/tests/bitfount/backends/pytorch/data/test_datasets.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,294 +1,243 @@
-"""Pytest fixtures for data tests."""
-from pathlib import Path
+"""Tests for PyTorchDataset classes."""
+from typing import Union
 
-import pandas as pd
+import pytest
 from pytest import fixture
-import sqlalchemy
+import torch
 
-from bitfount.data.datasets import _BitfountDataset, _IterableBitfountDataset
-from bitfount.data.datasources.database_source import DatabaseSource
-from bitfount.data.datasources.dataframe_source import DataFrameSource
+from bitfount.backends.pytorch.data.dataloaders import _PyTorchBitfountDataLoader
+from bitfount.backends.pytorch.data.datasets import _PyTorchDataset
+from bitfount.data.datasource import DataSource
 from bitfount.data.datastructure import DataStructure
 from bitfount.data.schema import BitfountSchema
-from bitfount.data.types import DataSplit, SemanticType
-from bitfount.data.utils import DatabaseConnection
-from tests.utils.helper import TABLE_NAME, create_dataset, create_segmentation_dataset
-
-
-@fixture
-def dataframe() -> pd.DataFrame:
-    """Underlying dataframe for single image datasets."""
-    return create_dataset(image=True)
-
-
-@fixture
-def grayscale_image_dataframe() -> pd.DataFrame:
-    """Underlying dataframe for grayscale image datasets."""
-    return create_dataset(image=True, grayscale_image=True)
-
-
-@fixture
-def tabular_dataset(dataframe: pd.DataFrame) -> _BitfountDataset:
-    """Basic tabular dataset for tests as fixture."""
-    target = "TARGET"
-    datasource = DataFrameSource(dataframe, ignore_cols=["image"])
-    datasource.load_data()
-
-    schema = BitfountSchema()
-    schema.add_datasource_tables(datasource, table_name=TABLE_NAME)
-    datasource.data = datasource.data.drop(
-        columns=schema.get_feature_names(TABLE_NAME, SemanticType.TEXT)
-    )
-    datastructure = DataStructure(
-        target=target, ignore_cols=["image"], table=TABLE_NAME
-    )
-    datastructure.set_training_column_split_by_semantic_type(schema.tables[0])
-    return _BitfountDataset(
-        datasource=datasource,
-        target=target,
-        selected_cols=datastructure.selected_cols,
-        data_split=DataSplit.TRAIN,
-        schema=schema.get_table_schema(TABLE_NAME),
-        selected_cols_semantic_types=datastructure.selected_cols_w_types,
-    )
-
-
-@fixture
-def db_dataset(db_session: sqlalchemy.engine.base.Engine) -> _BitfountDataset:
-    """Basic tabular dataset for tests as fixture."""
-    target = "TARGET"
-    db_conn = DatabaseConnection(db_session, table_names=["dummy_data", "dummy_data_2"])
-    datasource = DatabaseSource(db_conn, seed=420)
-    datasource.validate()
-    datasource.load_data(sql_query="SELECT * FROM dummy_data")
-
-    schema = BitfountSchema()
-    schema.add_datasource_tables(datasource, table_name="dummy_data")
-    schema.add_datasource_tables(datasource, table_name="dummy_data_2")
-
-    datastructure = DataStructure(
-        target=target, ignore_cols=["image"], table="dummy_data"
-    )
-    datastructure.set_training_column_split_by_semantic_type(schema.tables[0])
-    return _BitfountDataset(
-        datasource=datasource,
-        target=target,
-        selected_cols=datastructure.selected_cols,
-        data_split=DataSplit.TRAIN,
-        schema=schema.get_table_schema("dummy_data"),
-        selected_cols_semantic_types=datastructure.selected_cols_w_types,
-    )
-
-
-@fixture
-def iter_tabular_dataset(dataframe: pd.DataFrame) -> _IterableBitfountDataset:
-    """Basic tabular dataset for tests as fixture."""
-    target = "TARGET"
-    datasource = DataFrameSource(dataframe, ignore_cols=["image"])
-    datasource.load_data()
-
-    schema = BitfountSchema()
-    schema.add_datasource_tables(datasource, table_name=TABLE_NAME)
-    datasource.data = datasource.data.drop(
-        columns=schema.get_feature_names(TABLE_NAME, SemanticType.TEXT)
-    )
-    datastructure = DataStructure(
-        target=target, ignore_cols=["image"], table=TABLE_NAME
-    )
-    datastructure.set_training_column_split_by_semantic_type(schema.tables[0])
-    return _IterableBitfountDataset(
-        datasource=datasource,
-        target=target,
-        selected_cols=datastructure.selected_cols,
-        data_split=DataSplit.TRAIN,
-        schema=schema.get_table_schema(TABLE_NAME),
-        selected_cols_semantic_types=datastructure.selected_cols_w_types,
-    )
-
-
-@fixture
-def iter_db_dataset(
-    db_session: sqlalchemy.engine.base.Engine,
-) -> _IterableBitfountDataset:
-    """Basic tabular dataset for tests as fixture."""
-    target = "TARGET"
-    db_conn = DatabaseConnection(db_session, table_names=["dummy_data", "dummy_data_2"])
-    datasource = DatabaseSource(db_conn, seed=420)
-    datasource.validate()
-    datasource.load_data()
-
-    schema = BitfountSchema()
-    schema.add_datasource_tables(datasource, table_name="dummy_data")
-    schema.add_datasource_tables(datasource, table_name="dummy_data_2")
-
-    datastructure = DataStructure(
-        target=target, ignore_cols=["image"], table="dummy_data"
-    )
-    datastructure.set_training_column_split_by_semantic_type(schema.tables[0])
-    return _IterableBitfountDataset(
-        datasource=datasource,
-        target=target,
-        selected_cols=datastructure.selected_cols,
-        data_split=DataSplit.TRAIN,
-        schema=schema.get_table_schema("dummy_data"),
-        selected_cols_semantic_types=datastructure.selected_cols_w_types,
-    )
-
-
-@fixture
-def image_dataset(dataframe: pd.DataFrame) -> _BitfountDataset:
-    """Basic image dataset for tests as fixture."""
-    target = "TARGET"
-    datasource = DataFrameSource(dataframe)
-    datasource.load_data()
-    schema = BitfountSchema(
-        datasource,
-        force_stypes={TABLE_NAME: {"image": ["image"]}},
-        table_name=TABLE_NAME,
-    )
-
-    datasource.data = datasource.data.drop(
-        columns=schema.get_feature_names(TABLE_NAME, SemanticType.TEXT)
-    )
-    datastructure = DataStructure(
-        target=target,
-        table=TABLE_NAME,
-        selected_cols=["image", target],
-        image_cols=["image"],
-        batch_transforms=[
-            {
-                "albumentations": {
-                    "step": "train",
-                    "output": True,
-                    "arg": "image",
-                    "transformations": [
-                        {"Resize": {"height": 224, "width": 224}},
-                        "Normalize",
-                    ],
-                }
-            }
-        ],
-    )
-    datastructure.set_training_column_split_by_semantic_type(schema.tables[0])
-    return _BitfountDataset(
-        datasource=datasource,
-        target=target,
-        schema=schema.get_table_schema(TABLE_NAME),
-        selected_cols=datastructure.selected_cols,
-        selected_cols_semantic_types=datastructure.selected_cols_w_types,
-        batch_transforms=datastructure.get_batch_transformations(),
-        data_split=DataSplit.TRAIN,
-    )
-
-
-@fixture
-def seg_image_dataset(tmp_path: Path) -> _BitfountDataset:
-    """Basic segmentation image dataset for tests as fixture."""
-    data = create_segmentation_dataset(seg_dir=tmp_path, count=10)
-    dataframe = pd.DataFrame(data)
-    target = "masks"
-    datasource = DataFrameSource(dataframe)
-    datasource.load_data()
-    schema = BitfountSchema(
-        datasource,
-        force_stypes={TABLE_NAME: {"image": ["img", "masks"]}},
-        table_name=TABLE_NAME,
-    )
-
-    datasource.data = datasource.data.drop(
-        columns=schema.get_feature_names(TABLE_NAME, SemanticType.TEXT)
-    )
-    datastructure = DataStructure(
-        target=target,
-        table=TABLE_NAME,
-        selected_cols=["img", target],
-        image_cols=["img", target],
-        batch_transforms=[
-            {
-                "albumentations": {
-                    "step": "train",
-                    "output": True,
-                    "arg": "masks",
-                    "transformations": [
-                        {"Resize": {"height": 100, "width": 100}},
-                        "Normalize",
-                    ],
-                }
-            }
-        ],
-    )
-    datastructure.set_training_column_split_by_semantic_type(schema.tables[0])
-    return _BitfountDataset(
-        datasource=datasource,
-        target=target,
-        schema=schema.get_table_schema(TABLE_NAME),
-        selected_cols=datastructure.selected_cols,
-        selected_cols_semantic_types=datastructure.selected_cols_w_types,
-        batch_transforms=datastructure.get_batch_transformations(),
-        data_split=DataSplit.TRAIN,
-    )
-
-
-@fixture
-def image_tab_dataset(dataframe: pd.DataFrame) -> _BitfountDataset:
-    """Basic tabular and image dataset for tests as fixture."""
-    target = "TARGET"
-    datasource = DataFrameSource(dataframe)
-    datasource.load_data()
-    schema = BitfountSchema()
-    schema.add_datasource_tables(
-        datasource,
-        force_stypes={TABLE_NAME: {"image": ["image"]}},
-        table_name=TABLE_NAME,
-    )
-    datasource.data = datasource.data.drop(
-        columns=schema.get_feature_names(TABLE_NAME, SemanticType.TEXT)
-    )
-    datastructure = DataStructure(target=target, image_cols=["image"], table=TABLE_NAME)
-    datastructure.set_training_column_split_by_semantic_type(schema.tables[0])
-    return _BitfountDataset(
-        datasource=datasource,
-        target=target,
-        selected_cols=datastructure.selected_cols,
-        selected_cols_semantic_types=datastructure.selected_cols_w_types,
-        schema=schema.get_table_schema(TABLE_NAME),
-        data_split=DataSplit.TRAIN,
-    )
-
-
-@fixture
-def multiimage_dataframe() -> pd.DataFrame:
-    """Underlying dataframe for multi-image dataset."""
-    return create_dataset(multiimage=True)
-
-
-@fixture
-def multiimage_dataset(multiimage_dataframe: pd.DataFrame) -> _BitfountDataset:
-    """Basic multi-image dataset for tests as fixture."""
-    target = "TARGET"
-    datasource = DataFrameSource(multiimage_dataframe)
-    datasource.load_data()
-    schema = BitfountSchema()
-    schema.add_datasource_tables(
-        datasource,
-        force_stypes={TABLE_NAME: {"image": ["image1", "image2"]}},
-        table_name=TABLE_NAME,
-    )
-    datasource.data = datasource.data.drop(
-        columns=schema.get_feature_names(TABLE_NAME, SemanticType.TEXT)
-    )
-    datastructure = DataStructure(
-        target=target,
-        selected_cols=["image1", "image2", target],
-        table=TABLE_NAME,
-    )
-    datastructure.set_training_column_split_by_semantic_type(schema.tables[0])
-
-    return _BitfountDataset(
-        datasource=datasource,
-        target=target,
-        selected_cols=datastructure.selected_cols,
-        selected_cols_semantic_types=datastructure.selected_cols_w_types,
-        schema=schema.get_table_schema(TABLE_NAME),
-        data_split=DataSplit.TRAIN,
-    )
+from bitfount.data.types import SemanticType
+from bitfount.types import _DataFrameType
+from tests.utils.helper import backend_test, create_dataset, unit_test
+
+
+@backend_test
+@unit_test
+class TestPyTorchDataset:
+    """Tests for PyTorchTabularDataset class."""
+
+    @fixture
+    def dataframe(self) -> _DataFrameType:
+        """Underlying dataframe for dataset."""
+        return create_dataset(image=True)
+
+    @fixture
+    def tabular_dataset(self, dataframe: _DataFrameType) -> _PyTorchDataset:
+        """Basic PyTorch tabular dataset for tests as fixture."""
+        target = "TARGET"
+        datasource = DataSource(dataframe)
+        schema = BitfountSchema()
+        schema.add_datasource_features(datasource)
+        datasource.data = schema.apply(datasource.data)
+        datasource.data = datasource.data.drop(
+            columns=schema.feature_names(SemanticType.TEXT)
+        )
+        datastructure = DataStructure(target=target, ignore_cols=["image"])
+        datastructure.set_training_column_split_by_semantic_type(schema)
+        return _PyTorchDataset(
+            data=datasource.data,
+            target=target,
+            selected_cols=datastructure.selected_cols_w_types,
+        )
+
+    @fixture
+    def image_dataset(self, dataframe: _DataFrameType) -> _PyTorchDataset:
+        """Basic PyTorch image dataset for tests as fixture."""
+        target = "TARGET"
+        datasource = DataSource(dataframe)
+        schema = BitfountSchema()
+        schema.add_datasource_features(datasource, force_stype={"image": ["image"]})
+        datasource.data = schema.apply(datasource.data)
+        datasource.data = datasource.data.drop(
+            columns=schema.feature_names(SemanticType.TEXT)
+        )
+        datastructure = DataStructure(target=target, selected_cols=["image"])
+        datastructure.set_training_column_split_by_semantic_type(schema)
+        return _PyTorchDataset(
+            data=datasource.data,
+            target=target,
+            selected_cols=datastructure.selected_cols_w_types,
+        )
+
+    @fixture
+    def image_tab_dataset(self, dataframe: _DataFrameType) -> _PyTorchDataset:
+        """Basic PyTorchPredictionDataset dataset for tests as fixture."""
+        target = "TARGET"
+        datasource = DataSource(dataframe)
+        schema = BitfountSchema()
+        schema.add_datasource_features(datasource, force_stype={"image": ["image"]})
+        datasource.data = schema.apply(datasource.data)
+        datasource.data = datasource.data.drop(
+            columns=schema.feature_names(SemanticType.TEXT)
+        )
+        datastructure = DataStructure(target=target)
+        datastructure.set_training_column_split_by_semantic_type(schema)
+        return _PyTorchDataset(
+            data=datasource.data,
+            target=target,
+            selected_cols=datastructure.selected_cols_w_types,
+        )
+
+    @fixture
+    def multiimage_dataframe(self) -> _DataFrameType:
+        """Underlying dataframe for multi-image dataset."""
+        return create_dataset(multiimage=True)
+
+    @fixture
+    def multiimage_dataset(
+        self, multiimage_dataframe: _DataFrameType
+    ) -> _PyTorchDataset:
+        """Basic multi-image dataset for tests as fixture."""
+        target = "TARGET"
+        datasource = DataSource(multiimage_dataframe)
+        schema = BitfountSchema()
+        schema.add_datasource_features(
+            datasource, force_stype={"image": ["image1", "image2"]}
+        )
+        datasource.data = schema.apply(datasource.data)
+        datasource.data = datasource.data.drop(
+            columns=schema.feature_names(SemanticType.TEXT)
+        )
+        datastructure = DataStructure(
+            target=target, selected_cols=["image1", "image2", target]
+        )
+        datastructure.set_training_column_split_by_semantic_type(schema)
+        return _PyTorchDataset(
+            data=datasource.data,
+            target=target,
+            selected_cols=datastructure.selected_cols_w_types,
+        )
+
+    def test_len_tab_data(
+        self, dataframe: _DataFrameType, tabular_dataset: _PyTorchDataset
+    ) -> None:
+        """Tests tabular dataset __len__ method."""
+        assert len(tabular_dataset) == len(dataframe)
+
+    def test_len_image_data(
+        self, dataframe: _DataFrameType, image_dataset: _PyTorchDataset
+    ) -> None:
+        """Tests image dataset __len__ method."""
+        assert len(image_dataset) == len(dataframe)
+
+    def test_len_image_tab_data(
+        self, dataframe: _DataFrameType, image_tab_dataset: _PyTorchDataset
+    ) -> None:
+        """Tests mixed dataset __len__ method."""
+        assert len(image_tab_dataset) == len(dataframe)
+
+    def test_len_multiimage_data(
+        self, multiimage_dataframe: _DataFrameType, multiimage_dataset: _PyTorchDataset
+    ) -> None:
+        """Tests multi-image dataset __len__ method."""
+        assert len(multiimage_dataset) == len(multiimage_dataframe)
+
+    @pytest.mark.parametrize("idx", [0, 42, 2048, torch.tensor(3999)])
+    def test_idx_tab_data(
+        self, idx: Union[int, torch.Tensor], tabular_dataset: _PyTorchDataset
+    ) -> None:
+        """Tests indexing (incl. tensors) returns the expected formats of data."""
+        assert isinstance(tabular_dataset[idx], tuple)
+        assert len(tabular_dataset[idx]) == 2  # split into x,y
+        assert len(tabular_dataset[idx][0]) == 2  # split into tabular, support
+        assert len(tabular_dataset[idx][0][0]) == 13  # training cols  check
+        assert len(tabular_dataset[idx][0][1]) == 2  # support cols check
+        assert len([tabular_dataset[idx][1]]) == 1  # y check
+
+    @pytest.mark.parametrize("idx", [0, 42, 2048, torch.tensor(3999)])
+    def test_idx_img_data(self, idx: int, image_dataset: _PyTorchDataset) -> None:
+        """Tests indexing returns the expected formats of data."""
+        assert isinstance(image_dataset[idx], tuple)
+        assert len(image_dataset[idx]) == 2  # split into x,y
+        assert len(image_dataset[idx][0]) == 2  # split into image, support
+        assert len(image_dataset[idx][0][1]) == 2  # support cols check
+        assert len([image_dataset[idx][1]]) == 1  # y check
+
+    @pytest.mark.parametrize("idx", [0, 42, 2048, torch.tensor(3999)])
+    def test_idx_img_tab_data(
+        self, idx: int, image_tab_dataset: _PyTorchDataset
+    ) -> None:
+        """Tests indexing returns the expected formats of data."""
+        assert isinstance(image_tab_dataset[idx], tuple)
+        assert len(image_tab_dataset[idx]) == 2  # split into x,y
+        assert len(image_tab_dataset[idx][0]) == 3  # split into tab, image, support
+        assert len(image_tab_dataset[idx][0][0]) == 13  # tabular cols  check
+        assert len(image_tab_dataset[idx][0][2]) == 2  # support cols check
+        assert len([image_tab_dataset[idx][1]]) == 1  # y check
+
+    @pytest.mark.parametrize("idx", [0, 42, 2048, torch.tensor(3999)])
+    def test_idx_multiimg_data(
+        self, idx: int, multiimage_dataset: _PyTorchDataset
+    ) -> None:
+        """Tests indexing returns the expected formats of data."""
+        assert isinstance(multiimage_dataset[idx], tuple)
+        assert len(multiimage_dataset[idx]) == 2  # split into x,y
+        assert len(multiimage_dataset[idx][0]) == 2  # split into image, support
+        assert isinstance(multiimage_dataset[idx][0][0], tuple)
+        assert len(multiimage_dataset[idx][0][0]) == 2  # image cols check
+        assert len(multiimage_dataset[idx][0][1]) == 2  # support cols check
+        assert len([multiimage_dataset[idx][1]]) == 1  # y check
+
+    @pytest.mark.parametrize("idx", [0, 42, 2048, torch.tensor(3999)])
+    def test_idx_img_tab_category(
+        self, idx: int, image_tab_dataset: _PyTorchDataset
+    ) -> None:
+        """Tests indexing with categories gives expected data formats."""
+        target = "TARGET"
+        data = create_dataset(image=True, multihead=True)
+        datasource = DataSource(data, image_col=["image"])
+        schema = BitfountSchema()
+        schema.add_datasource_features(
+            datasource, force_stype={"categorical": ["category"], "image": ["image"]}
+        )
+        datasource.data = schema.apply(datasource.data)
+        datasource.data = datasource.data.drop(
+            columns=schema.feature_names(SemanticType.TEXT)
+        )
+        datastructure = DataStructure(
+            target=target, multihead_col="category", multihead_size=2
+        )
+        datastructure.set_training_column_split_by_semantic_type(schema)
+        dataset = _PyTorchDataset(
+            data=datasource.data,
+            target=target,
+            multihead_col="category",
+            selected_cols=datastructure.selected_cols_w_types,
+        )
+        assert isinstance(dataset[idx], tuple)
+        assert len(dataset[idx]) == 2  # split into x,y
+        assert len(dataset[idx][0]) == 3  # split into tab, image, support
+        assert (
+            len(dataset[idx][0][0]) == 14
+        )  # tabular cols check (multihead_col included)
+        assert len(dataset[idx][0][2]) == 3  # support cols check
+        assert len([dataset[idx][1]]) == 1  # y check
+
+    def test_dataloader_pytorch(self, image_tab_dataset: _PyTorchDataset) -> None:
+        """Tests iteration of dataloader."""
+        batch_size = 64
+        dl = _PyTorchBitfountDataLoader(image_tab_dataset, batch_size=batch_size)
+        iterator = iter(dl)
+        assert isinstance(next(iterator), list)
+        assert len(next(iterator)[0]) == 3  # the x part
+        assert len(next(iterator)[0][0]) == batch_size  # tabular part
+        assert len(next(iterator)[0][1]) == batch_size  # image part
+        assert len(next(iterator)[0][2]) == batch_size  # support part
+        assert len(next(iterator)[1]) == batch_size  # the y part
+
+    def test_dataloader_non_pytorch_img_tab(
+        self, image_tab_dataset: _PyTorchDataset
+    ) -> None:
+        """Tests x- and y-dataframe retrieval from dataloader."""
+        dl = _PyTorchBitfountDataLoader(image_tab_dataset)
+        tab, img = dl.get_x_dataframe()
+        y = dl.get_y_dataframe()
+        assert len(tab) == len(y)
+        assert len(img) == len(y)
+        assert len(y.columns) == 1
+        assert len(tab.columns) == 13
+        assert len(img.columns) == 1
```

### Comparing `bitfount-0.5.86/tests/bitfount/data/test_datafactory.py` & `bitfount-0.5.9/tests/bitfount/data/test_datafactory.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,30 +1,27 @@
 """Tests for datafactory.py."""
-from unittest.mock import create_autospec
 
+from _pytest.monkeypatch import MonkeyPatch
 import pytest
-from pytest import MonkeyPatch
 
 from bitfount.config import _BASIC_ENGINE, _PYTORCH_ENGINE
-from bitfount.data.datafactory import _BasicDataFactory, _get_default_data_factory
-from bitfount.data.dataloaders import BitfountDataLoader
-from bitfount.data.datasets import _BaseBitfountDataset
+from bitfount.data.datafactory import _BasicDataFactory, _load_default_data_factory
 from bitfount.exceptions import BitfountEngineError
 from tests.utils.helper import unit_test
 
 
 @unit_test
 class TestDefaultDataFactoryLoading:
     """Tests the default factory loading."""
 
     def test_load_default_data_factory(self, monkeypatch: MonkeyPatch) -> None:
         """Test that the default data factory can load."""
         # Set envvar value
         monkeypatch.setattr("bitfount.data.datafactory.BITFOUNT_ENGINE", _BASIC_ENGINE)
-        df = _get_default_data_factory()
+        df = _load_default_data_factory()
         assert isinstance(df, _BasicDataFactory)
 
     def test_load_default_data_factory_fail_on_import_error(
         self, monkeypatch: MonkeyPatch
     ) -> None:
         """Tests handling if the imported library throws an ImportError."""
         # Try with PyTorch backend, even if can't be imported here
@@ -35,28 +32,21 @@
             monkeypatch.delattr(
                 "bitfount.backends.pytorch.data.datafactory._PyTorchDataFactory"
             )
         except ImportError:
             pass
 
         with pytest.raises(BitfountEngineError, match="pytorch"):
-            _get_default_data_factory()
+            _load_default_data_factory()
 
     def test_load_default_data_factory_fails_unknown_engine(
         self, monkeypatch: MonkeyPatch
     ) -> None:
         """Tests that if the engine type is unknown, an error is raised."""
         monkeypatch.setattr(
             "bitfount.data.datafactory.BITFOUNT_ENGINE", "NOT_AN_ENGINE"
         )
 
         with pytest.raises(
             BitfountEngineError, match="Unable to load engine NOT_AN_ENGINE."
         ):
-            _get_default_data_factory()
-
-    def test_create_dataloader(self) -> None:
-        """Tests create_dataloader returns BitfountDataLoader."""
-        data_factory = _BasicDataFactory()
-        dataset = create_autospec(_BaseBitfountDataset)
-        dataloader = data_factory.create_dataloader(dataset)
-        assert isinstance(dataloader, BitfountDataLoader)
+            _load_default_data_factory()
```

### Comparing `bitfount-0.5.86/tests/bitfount/data/test_dataloaders.py` & `bitfount-0.5.9/tests/bitfount/data/test_dataloader.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,219 +1,230 @@
 """Tests dataloaders.py."""
 import numpy as np
 import pandas as pd
 import pytest
 from pytest import fixture
 
-from bitfount.data.dataloaders import BitfountDataLoader
-from bitfount.data.datasets import _BitfountDataset
-from bitfount.data.datasources.dataframe_source import DataFrameSource
-from bitfount.data.datasplitters import PercentageSplitter
+from bitfount.data.dataloader import _BitfountDataLoader
+from bitfount.data.datasets import _Dataset
+from bitfount.data.datasource import DataSource
 from bitfount.data.datastructure import DataStructure
 from bitfount.data.schema import BitfountSchema
-from bitfount.data.types import DataSplit, SemanticType
-from tests.utils.helper import TABLE_NAME, create_dataset, unit_test
+from bitfount.data.types import SemanticType
+from bitfount.types import _DataFrameType
+from tests.utils.helper import create_dataset, unit_test
 
 
 class TestDataLoaders:
     """Tests BitfountDataloader."""
 
     @fixture
-    def dataframe(self) -> pd.DataFrame:
+    def dataframe(self) -> _DataFrameType:
         """Dataframe fixture."""
         df = create_dataset(image=True)
         # Drop the date column until we support datetime.
         return df.drop(columns=["Date"])
 
     @unit_test
-    def test_get_x_dataframe_tabular_only(self, dataframe: pd.DataFrame) -> None:
+    def test_get_x_dataframe_tabular_only(self, dataframe: _DataFrameType) -> None:
         """Tests get_x_dataframe for tabular data."""
         df = dataframe.drop(columns=["image"])
-        datasource = DataFrameSource(df)
-        datasource.load_data()
-        datastucture = DataStructure(target="TARGET", table=TABLE_NAME)
-        schema = BitfountSchema()
-        schema.add_datasource_tables(datasource, table_name=TABLE_NAME)
-        datastucture.set_training_column_split_by_semantic_type(schema.tables[0])
-        dataset = _BitfountDataset(
-            datasource=datasource,
-            target="TARGET",
-            selected_cols_semantic_types=datastucture.selected_cols_w_types,
-            selected_cols=datastucture.selected_cols,
-            schema=schema.get_table_schema(TABLE_NAME),
-            data_split=DataSplit.TRAIN,
+        datasource = DataSource(df)
+        datastucture = DataStructure(target="TARGET")
+        schema = BitfountSchema()
+        schema.add_datasource_features(datasource)
+        datastucture.set_training_column_split_by_semantic_type(schema)
+        dataset = _Dataset(
+            df, target="TARGET", selected_cols=datastucture.selected_cols_w_types
         )
-        dl = BitfountDataLoader(dataset=dataset)
+        dl = _BitfountDataLoader(dataset=dataset)
         x_df = dl.get_x_dataframe()
         # Drop target and text columns
         new_df = df.drop(columns=["TARGET", "I", "J", "K", "L"])
         assert isinstance(x_df, pd.DataFrame)
         assert set(x_df.columns) == set(new_df.columns)
 
     @unit_test
-    def test_get_x_dataframe_image_only(self, dataframe: pd.DataFrame) -> None:
+    def test_get_x_dataframe_image_only(self, dataframe: _DataFrameType) -> None:
         """Tests get_x_dataframe for tabular data."""
         df = dataframe[["image", "TARGET"]]
-        datasource = DataFrameSource(df, data_splitter=PercentageSplitter(0, 0))
-        datasource.load_data()
-        datastucture = DataStructure(
-            target="TARGET", table=TABLE_NAME, selected_cols=["image", "TARGET"]
-        )
-        schema = BitfountSchema()
-        schema.add_datasource_tables(
-            datasource,
-            force_stypes={TABLE_NAME: {"image": ["image"]}},
-            table_name=TABLE_NAME,
-        )
-        datastucture.set_training_column_split_by_semantic_type(schema.tables[0])
-        dataset = _BitfountDataset(
-            datasource=datasource,
-            target="TARGET",
-            selected_cols_semantic_types=datastucture.selected_cols_w_types,
-            selected_cols=datastucture.selected_cols,
-            schema=schema.get_table_schema(TABLE_NAME),
-            data_split=DataSplit.TRAIN,
+        datasource = DataSource(df)
+        datastucture = DataStructure(target="TARGET")
+        schema = BitfountSchema()
+        schema.add_datasource_features(datasource, force_stype={"image": ["image"]})
+        datastucture.set_training_column_split_by_semantic_type(schema)
+        dataset = _Dataset(
+            df, target="TARGET", selected_cols=datastucture.selected_cols_w_types
         )
-        dl = BitfountDataLoader(dataset=dataset)
+        dl = _BitfountDataLoader(dataset=dataset)
         x_df = dl.get_x_dataframe()
         assert isinstance(x_df, pd.DataFrame)
-        assert x_df.shape == df[["image"]].shape
-        assert x_df.columns == df[["image"]].columns
+        assert x_df.equals(df[["image"]])
 
     @unit_test
-    def test_get_x_dataframe_image_and_tab(self, dataframe: pd.DataFrame) -> None:
+    def test_get_x_dataframe_image_and_tab(self, dataframe: _DataFrameType) -> None:
         """Tests get_x_dataframe for tabular data."""
-        datasource = DataFrameSource(dataframe, data_splitter=PercentageSplitter(0, 0))
-        datasource.load_data()
-        datastucture = DataStructure(target="TARGET", table=TABLE_NAME)
-        schema = BitfountSchema()
-        schema.add_datasource_tables(
-            datasource,
-            force_stypes={TABLE_NAME: {"image": ["image"]}},
-            table_name=TABLE_NAME,
-        )
-        datastucture.set_training_column_split_by_semantic_type(schema.tables[0])
-        dataset = _BitfountDataset(
-            datasource=datasource,
-            target="TARGET",
-            selected_cols_semantic_types=datastucture.selected_cols_w_types,
-            selected_cols=datastucture.selected_cols,
-            schema=schema.get_table_schema(TABLE_NAME),
-            data_split=DataSplit.TRAIN,
+        datasource = DataSource(dataframe)
+        datastucture = DataStructure(target="TARGET")
+        schema = BitfountSchema()
+        schema.add_datasource_features(datasource, force_stype={"image": ["image"]})
+        datastucture.set_training_column_split_by_semantic_type(schema)
+        dataset = _Dataset(
+            dataframe, target="TARGET", selected_cols=datastucture.selected_cols_w_types
         )
-        dl = BitfountDataLoader(dataset=dataset)
+        dl = _BitfountDataLoader(dataset=dataset)
         x_df = dl.get_x_dataframe()
         # Drop target, image, and text columns
         new_df = dataframe.drop(columns=["TARGET", "I", "J", "K", "L", "image"])
-        # x_df is a tuple of dataframes (tabular followed by image)
         assert isinstance(x_df, tuple)
         assert set(x_df[0].columns) == set(new_df.columns)
-        assert isinstance(x_df[1], pd.DataFrame)
-        assert x_df[1].shape == dataframe[["image"]].shape
-        assert x_df[1].columns == dataframe[["image"]].columns
+        assert x_df[1].equals(dataframe[["image"]])
+
+    @unit_test
+    def test_iterator_tab_data(self, dataframe: _DataFrameType) -> None:
+        """Tests iteration of dataloader for tabular data."""
+        df = dataframe.drop(columns=["image"])
+        datasource = DataSource(df)
+        datastucture = DataStructure(target="TARGET")
+        schema = BitfountSchema()
+        schema.add_datasource_features(datasource)
+        datastucture.set_training_column_split_by_semantic_type(schema)
+        dataset = _Dataset(
+            df, target="TARGET", selected_cols=datastucture.selected_cols_w_types
+        )
+        batch_size = 16
+        dl = _BitfountDataLoader(dataset=dataset, batch_size=batch_size)
+        dl_iterator = iter(dl)
+        assert isinstance(next(dl_iterator), tuple)
+        assert len(next(dl_iterator)[0]) == batch_size
+        assert len(next(dl_iterator)[1]) == batch_size
 
     @unit_test
-    def test_empty_dataframe_raises_valerror(self, dataframe: pd.DataFrame) -> None:
+    def test_iterator_image_data(self, dataframe: _DataFrameType) -> None:
+        """Tests iteration of dataloader for image data."""
+        df = dataframe[["image", "TARGET"]]
+        datasource = DataSource(df)
+        datastucture = DataStructure(target="TARGET")
+        schema = BitfountSchema()
+        schema.add_datasource_features(datasource, force_stype={"image": ["image"]})
+        datastucture.set_training_column_split_by_semantic_type(schema)
+        dataset = _Dataset(
+            df, target="TARGET", selected_cols=datastucture.selected_cols_w_types
+        )
+        batch_size = 16
+        dl = _BitfountDataLoader(dataset=dataset, batch_size=batch_size)
+        dl_iterator = iter(dl)
+        assert isinstance(next(dl_iterator), tuple)
+        assert len(next(dl_iterator)[0]) == batch_size
+        assert len(next(dl_iterator)[1]) == batch_size
+
+    @unit_test
+    def test_iterator_image_tab_data(self, dataframe: _DataFrameType) -> None:
+        """Tests iteration of dataloader for mixed image and tabular data."""
+        datasource = DataSource(dataframe)
+        datastucture = DataStructure(target="TARGET")
+        schema = BitfountSchema()
+        schema.add_datasource_features(datasource, force_stype={"image": ["image"]})
+        datastucture.set_training_column_split_by_semantic_type(schema)
+        dataset = _Dataset(
+            dataframe, target="TARGET", selected_cols=datastucture.selected_cols_w_types
+        )
+        batch_size = 16
+        dl = _BitfountDataLoader(dataset=dataset, batch_size=batch_size)
+        dl_iterator = iter(dl)
+        assert isinstance(next(dl_iterator), tuple)
+        assert isinstance(next(dl_iterator)[0], tuple)
+        assert len(next(dl_iterator)[0][0]) == batch_size
+        assert len(next(dl_iterator)[0][1]) == batch_size
+        assert len(next(dl_iterator)[1]) == batch_size
+
+    @unit_test
+    def test_empty_dataframe_raises_valerror(self, dataframe: _DataFrameType) -> None:
         """Tests get_x_dataframe with empty df raises error."""
         df = pd.DataFrame(dataframe[["TARGET"]])
-        datasource = DataFrameSource(df)
-        datasource.load_data()
-        datastucture = DataStructure(
-            target="TARGET", selected_cols=["TARGET"], table=TABLE_NAME
-        )
-        schema = BitfountSchema()
-        schema.add_datasource_tables(datasource, table_name=TABLE_NAME)
-        datastucture.set_training_column_split_by_semantic_type(schema.tables[0])
-        dataset = _BitfountDataset(
-            datasource=datasource,
-            target="TARGET",
-            selected_cols_semantic_types=datastucture.selected_cols_w_types,
-            selected_cols=datastucture.selected_cols,
-            schema=schema.get_table_schema(TABLE_NAME),
-            data_split=DataSplit.TRAIN,
+        datasource = DataSource(df)
+        datastucture = DataStructure(target="TARGET", selected_cols=["TARGET"])
+        schema = BitfountSchema()
+        schema.add_datasource_features(datasource)
+        datastucture.set_training_column_split_by_semantic_type(schema)
+        dataset = _Dataset(
+            dataframe, target="TARGET", selected_cols=datastucture.selected_cols_w_types
         )
-        dl = BitfountDataLoader(dataset=dataset)
+        dl = _BitfountDataLoader(dataset=dataset)
         with pytest.raises(ValueError):
             dl.get_x_dataframe()
 
     @unit_test
-    def test_dataloader_tab(self, tabular_dataset: _BitfountDataset) -> None:
+    def test_dataloader_tab(self, tabular_dataset: _Dataset) -> None:
         """Tests x- and y-dataframe retrieval from dataloader for tabular data."""
-        df = BitfountDataLoader(tabular_dataset)
+        df = _BitfountDataLoader(tabular_dataset)
         x = df.get_x_dataframe()
         y = df.get_y_dataframe()
         assert isinstance(x, pd.DataFrame)
         assert len(x) == len(y)
         assert len(x.columns) == 12
         assert len(y.columns) == 1
 
     @unit_test
-    def test_dataloader_img(self, image_dataset: _BitfountDataset) -> None:
+    def test_dataloader_img(self, image_dataset: _Dataset) -> None:
         """Tests x- and y-dataframe retrieval from dataloader for image data."""
-        df = BitfountDataLoader(image_dataset)
+        df = _BitfountDataLoader(image_dataset)
         x = df.get_x_dataframe()
         y = df.get_y_dataframe()
         assert isinstance(x, pd.DataFrame)
         assert len(x) == len(y)
         assert len(x.columns) == 1
         assert len(y.columns) == 1
 
     @unit_test
-    def test_dataloader_img_tab(self, image_tab_dataset: _BitfountDataset) -> None:
+    def test_dataloader_img_tab(self, image_tab_dataset: _Dataset) -> None:
         """Tests x- and y-dataframe retrieval from dataloader."""
-        df = BitfountDataLoader(image_tab_dataset)
+        df = _BitfountDataLoader(image_tab_dataset)
         x = df.get_x_dataframe()
         y = df.get_y_dataframe()
         assert isinstance(x, tuple)
         tab, img = x
         assert len(tab) == len(y)
         assert len(img) == len(y)
         assert len(y.columns) == 1
         assert len(tab.columns) == 12
         assert len(img.columns) == 1
 
     @unit_test
-    def test_dataloader_multiimage(self, multiimage_dataset: _BitfountDataset) -> None:
+    def test_dataloader_multiimage(self, multiimage_dataset: _Dataset) -> None:
         """Tests x- and y-dataframe retrieval from dataloader for multi-image."""
-        df = BitfountDataLoader(multiimage_dataset)
+        df = _BitfountDataLoader(multiimage_dataset)
         x = df.get_x_dataframe()
         y = df.get_y_dataframe()
         assert isinstance(x, pd.DataFrame)
         assert len(x) == len(y)
         assert len(x.columns) == 2
         assert len(y.columns) == 1
 
     @unit_test
     def test_dataloader_multilabel(self) -> None:
         """Tests x- and y-dataframe retrieval from dataloader for multilabel target."""
         data = create_dataset(image=True)
         data = data.assign(TARGET_2=np.zeros(len(data)))
         data.loc[(data.A < 700) & (data.F < 0.5) & (data.D % 2 == 1), "TARGET_2"] = 1
-        datasource = DataFrameSource(data)
-        datasource.load_data()
+        datasource = DataSource(data)
         schema = BitfountSchema()
-        schema.add_datasource_tables(
-            datasource,
-            force_stypes={TABLE_NAME: {"image": ["image"]}},
-            table_name=TABLE_NAME,
-        )
+        schema.add_datasource_features(datasource, force_stype={"image": ["image"]})
+        datasource.data = schema.apply(datasource.data)
         datasource.data = datasource.data.drop(
-            columns=schema.get_feature_names(TABLE_NAME, SemanticType.TEXT)
+            columns=schema.feature_names(SemanticType.TEXT)
         )
-        datastructure = DataStructure(target=["TARGET", "TARGET_2"], table=TABLE_NAME)
-        datastructure.set_training_column_split_by_semantic_type(schema.tables[0])
-        multilabel_dataset = _BitfountDataset(
-            datasource=datasource,
+        datastructure = DataStructure(target=["TARGET", "TARGET_2"])
+        datastructure.set_training_column_split_by_semantic_type(schema)
+        multilabel_dataset = _Dataset(
+            data=datasource.data,
             target=["TARGET", "TARGET_2"],
-            selected_cols_semantic_types=datastructure.selected_cols_w_types,
-            selected_cols=datastructure.selected_cols,
-            schema=schema.get_table_schema(TABLE_NAME),
-            data_split=DataSplit.TRAIN,
+            selected_cols=datastructure.selected_cols_w_types,
         )
-        df = BitfountDataLoader(multilabel_dataset)
+        df = _BitfountDataLoader(multilabel_dataset)
         x = df.get_x_dataframe()
         y = df.get_y_dataframe()
         assert isinstance(x, tuple)
         tab, img = x
         assert len(tab) == len(y)
         assert len(img) == len(y)
         assert len(y.columns) == 2
```

### Comparing `bitfount-0.5.86/tests/bitfount/data/test_datasplitters.py` & `bitfount-0.5.9/tests/bitfount/transformations/test_processor.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,533 +1,735 @@
-"""Tests datasplitters.py."""
-from typing import Optional, cast
-from unittest.mock import MagicMock
+"""Tests for transformation processing."""
+from typing import Tuple, Type, cast
 
+from PIL import Image
+import databricks.koalas as ks
 import numpy as np
 import pandas as pd
+from pandas._testing import assert_frame_equal
 import pytest
-from pytest import LogCaptureFixture, fixture
-from pytest_mock import MockerFixture
+from pytest import fixture
 
-from bitfount.data.datasources.database_source import DatabaseSource
-from bitfount.data.datasources.dicom_source import DICOMSource
-from bitfount.data.datasplitters import (
-    DatasetSplitter,
-    PercentageSplitter,
-    SplitterDefinedInData,
+from bitfount.data.datasource import DataSource
+from bitfount.data.schema import BitfountSchema
+from bitfount.transformations.exceptions import (
+    InvalidBatchTransformationError,
+    MissingColumnReferenceError,
+    TransformationApplicationError,
+    TransformationProcessorError,
 )
-from bitfount.data.types import DataSplit
-from tests.utils.helper import DATASET_ROW_COUNT, create_dataset, unit_test
+from bitfount.transformations.parser import TransformationsParser
+from bitfount.transformations.processor import TransformationProcessor
+from bitfount.transformations.unary_operations import OneHotEncodingTransformation
+from bitfount.types import _DataFrameLib, _DataFrameType
+from tests.utils import PytestRequest
+from tests.utils.helper import create_dataset, integration_test, unit_test
+
+
+def parse_and_process_data(
+    data: _DataFrameType, schema: BitfountSchema, transformations_yaml: str
+) -> Tuple[_DataFrameType, BitfountSchema]:
+    """Parses transformations YAML and applies them to data."""
+    parser = TransformationsParser()
+    transformations, cols = parser.parse(transformations_yaml)
+    processor = TransformationProcessor(transformations, schema, cols)
+    data = processor.transform(data)
+    return data, cast(BitfountSchema, processor.schema)
+
+
+IntegerTypes = Tuple[
+    Type[int], Type[np.int32], Type[np.int64], pd.Int32Dtype, pd.Int64Dtype
+]
+
+
+class TestTransformationProcessor:
+    """Tests for TransformationProcessor class."""
+
+    @fixture(scope="function")
+    def data_and_schema(
+        self, request: PytestRequest
+    ) -> Tuple[_DataFrameType, BitfountSchema]:
+        """Dataset and schema to be used for tests."""
+        data = create_dataset(koalas=request.param)
+        datasource = DataSource(data)
+        schema = BitfountSchema()
+        schema.add_datasource_features(datasource)
+        return datasource.data, schema
+
+    @fixture
+    def integer_types(self) -> IntegerTypes:
+        """Fixture determining what counts as an integer type."""
+        return (int, np.int32, np.int64, pd.Int32Dtype(), pd.Int64Dtype())
 
+    @pytest.mark.parametrize(
+        "data_and_schema",
+        [
+            pytest.param(True, marks=integration_test),
+            pytest.param(False, marks=unit_test),
+        ],
+        indirect=["data_and_schema"],
+    )
+    def test_addition_transformation(
+        self, data_and_schema: Tuple[_DataFrameType, BitfountSchema]
+    ) -> None:
+        """Tests addition transformation works correctly."""
+        data, schema = data_and_schema
+        test_yaml = """
+        Transformations:
+        - Add:
+            name: A_and_B_sum
+            output: true
+            arg1: col:A
+            arg2: c:B
+        """
+        data, _ = parse_and_process_data(data, schema, test_yaml)
+        assert "A_and_B_sum" in data.columns
+        assert data["A_and_B_sum"].sum() == data.A.sum() + data.B.sum()
 
-@fixture
-def data() -> pd.DataFrame:
-    """Returns dataset."""
-    return create_dataset()
-
+    @pytest.mark.parametrize(
+        "data_and_schema",
+        [
+            pytest.param(True, marks=integration_test),
+            pytest.param(False, marks=unit_test),
+        ],
+        indirect=["data_and_schema"],
+    )
+    def test_comparison_transformation(
+        self, data_and_schema: Tuple[_DataFrameType, BitfountSchema]
+    ) -> None:
+        """Tests comparison transformation works correctly."""
+        data, schema = data_and_schema
+        test_yaml = """
+        Transformations:
+        - Compare:
+            name: A_and_B_comp
+            output: true
+            arg1: col:A
+            arg2: c:B
+        """
+        data, _ = parse_and_process_data(data, schema, test_yaml)
+        assert "A_and_B_comp" in data.columns
+        comp = sum((data.A > data.B).to_list()) - sum((data.B > data.A).to_list())
+        assert data["A_and_B_comp"].sum() == comp
 
-@unit_test
-class TestDataSplitter:
-    """Tests abstract DataSplitter."""
+    @pytest.mark.parametrize(
+        "data_and_schema",
+        [
+            pytest.param(True, marks=integration_test),
+            pytest.param(False, marks=unit_test),
+        ],
+        indirect=["data_and_schema"],
+    )
+    def test_comparison_transformation_with_constant(
+        self, data_and_schema: Tuple[_DataFrameType, BitfountSchema]
+    ) -> None:
+        """Tests comparison transformation works correctly with a constant."""
+        data, schema = data_and_schema
+        test_yaml = """
+        Transformations:
+        - Compare:
+            name: A_and_2_comp
+            output: true
+            arg1: col:A
+            arg2: 2
+        """
+        data, _ = parse_and_process_data(data, schema, test_yaml)
+        assert "A_and_2_comp" in data.columns
+        comp = sum((data.A > 2).to_list()) - sum((data.A < 2).to_list())
+        assert data["A_and_2_comp"].sum() == comp
 
     @pytest.mark.parametrize(
-        "splitter_name",
+        "data_and_schema",
         [
-            PercentageSplitter.splitter_name(),
-            # This helps ensure that it can be mapped to
-            # from a string in the config file
-            # in case we accidentally change the return type
-            # Of the class property above
-            "percentage",
-        ],
-    )
-    def test_create_produces_percentage_splitter(self, splitter_name: str) -> None:
-        """Test Percentage splitter created."""
-        created_splitter = DatasetSplitter.create(splitter_name)
-        assert isinstance(created_splitter, PercentageSplitter)
-
-    def test_create_produces_percentage_splitter_with_arguments(self) -> None:
-        """Test created percentage splitter has expected values."""
-        time_series_sort = ["hello"]
-
-        created_splitter = DatasetSplitter.create(
-            PercentageSplitter.splitter_name(),
-            validation_percentage=1,
-            test_percentage=2,
-            time_series_sort_by=time_series_sort,
+            pytest.param(True, marks=integration_test),
+            pytest.param(False, marks=unit_test),
+        ],
+        indirect=["data_and_schema"],
+    )
+    def test_division_transformation(
+        self, data_and_schema: Tuple[_DataFrameType, BitfountSchema]
+    ) -> None:
+        """Tests division transformation works correctly."""
+        data, schema = data_and_schema
+        test_yaml = """
+        Transformations:
+        - Divide:
+            name: A_and_B_div
+            output: true
+            arg1: col:A
+            arg2: c:B
+        """
+        data, _ = parse_and_process_data(data, schema, test_yaml)
+        assert "A_and_B_div" in data.columns
+        assert int(data["A_and_B_div"].sum()) == int(
+            np.sum((data.A / data.B).to_numpy())
         )
 
-        assert isinstance(created_splitter, PercentageSplitter)
-        assert created_splitter.validation_percentage == 1
-        assert created_splitter.test_percentage == 2
-        assert created_splitter.time_series_sort_by == time_series_sort
-
-    @pytest.mark.parametrize(
-        "splitter_name",
-        [
-            SplitterDefinedInData.splitter_name(),
-            # This helps ensure that it can be mapped to
-            # from a string in the config file
-            # in case we accidentally change the return type
-            # Of the class property above
-            "predefined",
-        ],
-    )
-    def test_create_produces_predefined_splitter(self, splitter_name: str) -> None:
-        """Test create predefined splitter."""
-        created_splitter = DatasetSplitter.create(splitter_name)
-        assert isinstance(created_splitter, SplitterDefinedInData)
-
-    def test_create_produces_predefined_splitter_with_arguments(self) -> None:
-        """Test created predefined splitter has expected values."""
-        column_name = "someColumn"
-        training_set_label = "UNIT_TRAIN"
-        validation_set_label = "UNIT_VALIDATION"
-        test_set_label = "UNIT_TEST"
-
-        created_splitter = DatasetSplitter.create(
-            SplitterDefinedInData.splitter_name(),
-            column_name=column_name,
-            training_set_label=training_set_label,
-            validation_set_label=validation_set_label,
-            test_set_label=test_set_label,
-        )
+    @pytest.mark.parametrize(
+        "data_and_schema",
+        [
+            pytest.param(True, marks=integration_test),
+            pytest.param(False, marks=unit_test),
+        ],
+        indirect=["data_and_schema"],
+    )
+    def test_inclusion_transformation(
+        self, data_and_schema: Tuple[_DataFrameType, BitfountSchema]
+    ) -> None:
+        """Tests inclusion transformation works correctly."""
+        data, schema = data_and_schema
+        test_yaml = """
+        Transformations:
+        - In:
+            name: a_in_I
+            output: true
+            in_str: a
+            arg: c:I
+        """
+        data, _ = parse_and_process_data(data, schema, test_yaml)
+        assert "a_in_I" in data.columns
+        assert sum(data["a_in_I"].to_list()) == sum((data.I == "a").to_list())
 
-        assert isinstance(created_splitter, SplitterDefinedInData)
-        assert created_splitter.column_name == column_name
-        assert created_splitter.training_set_label == training_set_label
-        assert created_splitter.validation_set_label == validation_set_label
-        assert created_splitter.test_set_label == test_set_label
+    @pytest.mark.parametrize(
+        "data_and_schema",
+        [
+            pytest.param(True, marks=integration_test),
+            pytest.param(False, marks=unit_test),
+        ],
+        indirect=["data_and_schema"],
+    )
+    def test_multiplication_transformation(
+        self, data_and_schema: Tuple[_DataFrameType, BitfountSchema]
+    ) -> None:
+        """Tests multiplication transformation works correctly."""
+        data, schema = data_and_schema
+        test_yaml = """
+        Transformations:
+        - Multiply:
+            name: A_and_B_prod
+            output: true
+            arg1: col:A
+            arg2: c:B
+        """
+        data, _ = parse_and_process_data(data, schema, test_yaml)
+        assert "A_and_B_prod" in data.columns
+        assert data["A_and_B_prod"].sum() == data.A @ data.B
 
+    @pytest.mark.parametrize(
+        "data_and_schema",
+        [
+            pytest.param(True, marks=integration_test),
+            pytest.param(False, marks=unit_test),
+        ],
+        indirect=["data_and_schema"],
+    )
+    def test_one_hot_encoding_transformation(
+        self, data_and_schema: Tuple[_DataFrameType, BitfountSchema]
+    ) -> None:
+        """Tests one-hot encoding transformation works correctly."""
+        data, schema = data_and_schema
+        test_yaml = """
+        Transformations:
+        - OneHotEncode:
+            name: ohe_I
+            output: true
+            arg: col:I
+            values: [e,f,g]
+        """
+        original_data_len = len(data.index)
+        data, _ = parse_and_process_data(data, schema, test_yaml)
 
-@unit_test
-class TestPercentageSplitter:
-    """Tests the Percentage splitter."""
+        # Check all one hot columns are included
+        one_hot_cols = [f"ohe_I_{letter}" for letter in [*"efg", "UNKNOWN"]]
+        for col in one_hot_cols:
+            assert col in data.columns
+
+        # Check no others are
+        for col in data.columns:
+            if col.startswith("ohe"):
+                assert col in one_hot_cols
+
+        # Check additional columns have been correctly concatenated
+        assert len(data.index) == original_data_len
+
+    @fixture(params=[True, False])
+    def ohe_mini_dataset_and_results(
+        self, request: PytestRequest
+    ) -> Tuple[_DataFrameType, _DataFrameType, _DataFrameLib]:
+        """Provides a small dataset, results and dataframe type for tests.
+
+        Generates a (4,3) test dataframe for testing one-hot encoding and a
+        corresponding results dataframe (4,7) which encodes values [1, 2] from
+        "col1".
 
-    @pytest.mark.parametrize("validation_percentage", [0, 7])
-    @pytest.mark.parametrize("test_percentage", [0, 7])
-    def test_sets_split_by_percentage(
-        self, data: pd.DataFrame, test_percentage: int, validation_percentage: int
-    ) -> None:
-        """Test the percentages are as expected."""
-        untouched_copy_of_data = data.copy()
-        data_splitter = PercentageSplitter(
-            validation_percentage=validation_percentage, test_percentage=test_percentage
-        )
+        Also returns the type of the dataframes.
+        """
+        data_in = {
+            "col1": [1, 2, 3, None],
+            "col2": [4, 5, 6, None],
+            "col3": [7, 8, 9, None],
+        }
+        results = {
+            "col1": [1, 2, 3, None],
+            "col2": [4, 5, 6, None],
+            "col3": [7, 8, 9, None],
+            "col1_1": [1, 0, 0, 0],
+            "col1_2": [0, 1, 0, 0],
+            "col1_UNKNOWN": [0, 0, 1, 0],
+        }
+        if request.param:
+            return (
+                ks.DataFrame(data=data_in),
+                ks.DataFrame(data=results),
+                _DataFrameLib.KOALAS,
+            )
+        else:
+            return (
+                pd.DataFrame(data=data_in),
+                pd.DataFrame(data=results),
+                _DataFrameLib.PANDAS,
+            )
 
-        (
-            train_indices,
-            validation_indices,
-            test_indices,
-        ) = data_splitter.create_dataset_splits(data)
-
-        # Ensure we have the expected number of indices
-        assert (
-            len(train_indices) + len(validation_indices) + len(test_indices)
-            == DATASET_ROW_COUNT
-        )
-        # Ensure validation and test (and transitively training sets)
-        # are the expected sizes
-        assert len(validation_indices) == DATASET_ROW_COUNT * (
-            validation_percentage / 100
-        )
-        assert len(test_indices) == DATASET_ROW_COUNT * (test_percentage / 100)
-        # check shuffle was applied
-        assert sorted(train_indices.tolist()) != train_indices.tolist()
-
-        # Ensure indices are distinct
-        # (combines all 3 sets then filters out any duplicates)
-        recombined_indices = np.concatenate(
-            [train_indices, validation_indices, test_indices]
-        )
-        unique_indices_returned = np.unique(recombined_indices)
-        assert len(unique_indices_returned) == DATASET_ROW_COUNT
+    @unit_test
+    def test__do_one_hot_encoding(
+        self,
+        ohe_mini_dataset_and_results: Tuple[
+            _DataFrameType, _DataFrameType, _DataFrameLib
+        ],
+    ) -> None:
+        """Tests the one hot encoding method directly.
 
-        # Ensure we didn't do any sorting
-        pd.testing.assert_frame_equal(data, untouched_copy_of_data)
+        Checks that values are being encoded correctly.
+        """
+        data_in, expected_results, df_type = ohe_mini_dataset_and_results
 
-    @pytest.mark.parametrize("validation_percentage", [0, 7])
-    @pytest.mark.parametrize("test_percentage", [0, 7])
-    def test_sets_split_by_percentage_shuffle_false(
-        self, data: pd.DataFrame, test_percentage: int, validation_percentage: int
-    ) -> None:
-        """Test the percentages are as expected when shuffle is false."""
-        untouched_copy_of_data = data.copy()
-        data_splitter = PercentageSplitter(
-            validation_percentage=validation_percentage,
-            test_percentage=test_percentage,
-            shuffle=False,
-        )
+        ohe_transform = OneHotEncodingTransformation(arg="c:col1", raw_values=[1, 2])
+        results = TransformationProcessor._do_one_hot_encoding(data_in, ohe_transform)
 
-        (
-            train_indices,
-            validation_indices,
-            test_indices,
-        ) = data_splitter.create_dataset_splits(data)
-        # Ensure we have the expected number of indices
-        assert (
-            len(train_indices) + len(validation_indices) + len(test_indices)
-            == DATASET_ROW_COUNT
-        )
-        # Ensure validation and test (and transitively training sets)
-        # are the expected sizes
-        assert len(validation_indices) == DATASET_ROW_COUNT * (
-            validation_percentage / 100
-        )
-        assert len(test_indices) == DATASET_ROW_COUNT * (test_percentage / 100)
-        # check no shuffling on indices
-        assert sorted(train_indices.tolist()) == train_indices.tolist()
-        assert sorted(validation_indices.tolist()) == validation_indices.tolist()
-        assert sorted(test_indices.tolist()) == test_indices.tolist()
-
-        # Ensure indices are distinct
-        # (combines all 3 sets then filters out any duplicates)
-        recombined_indices = np.concatenate(
-            [train_indices, validation_indices, test_indices]
-        )
-        unique_indices_returned = np.unique(recombined_indices)
-        assert len(unique_indices_returned) == DATASET_ROW_COUNT
+        # The assertion method below only works on pandas DataFrames so need to convert
+        if df_type == _DataFrameLib.KOALAS:
+            results = results.to_pandas()
+            expected_results = expected_results.to_pandas()
+
+        assert_frame_equal(results, expected_results, check_dtype=False)
+
+    @pytest.mark.parametrize(
+        "data_and_schema",
+        [
+            pytest.param(True, marks=integration_test),
+            pytest.param(False, marks=unit_test),
+        ],
+        indirect=["data_and_schema"],
+    )
+    def test_subtraction_transformation(
+        self, data_and_schema: Tuple[_DataFrameType, BitfountSchema]
+    ) -> None:
+        """Tests subtraction transformation works correctly."""
+        data, schema = data_and_schema
+        test_yaml = """
+        Transformations:
+        - Subtract:
+            name: A_and_B_sub
+            output: true
+            arg1: col:A
+            arg2: c:B
+        """
+        data, _ = parse_and_process_data(data, schema, test_yaml)
+        assert "A_and_B_sub" in data.columns
+        assert data["A_and_B_sub"].sum() == data.A.sum() - data.B.sum()
+
+    @pytest.mark.parametrize(
+        "data_and_schema",
+        [
+            pytest.param(True, marks=integration_test),
+            pytest.param(False, marks=unit_test),
+        ],
+        indirect=["data_and_schema"],
+    )
+    def test_cleandata_transformation(
+        self, data_and_schema: Tuple[_DataFrameType, BitfountSchema]
+    ) -> None:
+        """Tests clean data transformation works correctly."""
+        data, schema = data_and_schema
+        test_yaml = """
+        Transformations:
+        - CleanData:
+            name: cleandata
+            cols: col:Date
+        """
+        data, _ = parse_and_process_data(data, schema, test_yaml)
+        assert "cleandata" not in data.columns
+        assert data["Date"].isnull().sum() == 0
 
-        # Ensure we didn't do any sorting
-        pd.testing.assert_frame_equal(data, untouched_copy_of_data)
+    @pytest.mark.parametrize(
+        "data_and_schema",
+        [
+            pytest.param(True, marks=integration_test),
+            pytest.param(False, marks=unit_test),
+        ],
+        indirect=["data_and_schema"],
+    )
+    def test_cleandata_transformation_no_args(
+        self, data_and_schema: Tuple[_DataFrameType, BitfountSchema]
+    ) -> None:
+        """Tests clean data transformation with no arguments (i.e. all cols)."""
+        data, schema = data_and_schema
+        test_yaml = """
+        Transformations:
+        - CleanData
+        """
+        data, _ = parse_and_process_data(data, schema, test_yaml)
+        assert data["Date"].isnull().sum() == 0
 
-    @pytest.mark.parametrize("validation_percentage", [0, 7])
-    @pytest.mark.parametrize("test_percentage", [0, 7])
-    def test_time_series(
+    @pytest.mark.parametrize(
+        "data_and_schema",
+        [
+            pytest.param(True, marks=integration_test),
+            pytest.param(False, marks=unit_test),
+        ],
+        indirect=["data_and_schema"],
+    )
+    def test_normalize_transformation(
         self,
-        data: pd.DataFrame,
-        mocker: MockerFixture,
-        test_percentage: int,
-        validation_percentage: int,
-    ) -> None:
-        """Checks the time series sorting works correctly."""
-        data["Date"] = pd.to_datetime(data["Date"], infer_datetime_format=True)
-        data = data.reindex(np.random.permutation(data.index.tolist()))
-
-        data_splitter = PercentageSplitter(
-            validation_percentage=validation_percentage,
-            test_percentage=test_percentage,
-            time_series_sort_by="Date",
-        )
-        random_shuffle_mock = mocker.patch("numpy.random.shuffle")
+        data_and_schema: Tuple[_DataFrameType, BitfountSchema],
+        integer_types: IntegerTypes,
+    ) -> None:
+        """Tests normalize transformation works."""
+        data, schema = data_and_schema
+        test_yaml = """
+        Transformations:
+        - NormalizeData:
+            name: blah
+            cols:
+                - col:A
+                - col:B
+        """
+        # Assert columns
+        for col in ["A", "B", "C", "D"]:
+            assert schema.features["continuous"][col].dtype in integer_types
+        data, schema = parse_and_process_data(data, schema, test_yaml)
+        assert round(data.A.mean()) == 0
+        assert round(data.A.std()) == 1
+        assert round(data.B.mean()) == 0
+        assert round(data.B.std()) == 1
+
+        # Assert that only columns A and B have been normalized
+        assert schema.features["continuous"]["A"].dtype == np.float64
+        assert schema.features["continuous"]["B"].dtype == np.float64
+        assert schema.features["continuous"]["C"].dtype in integer_types
+        assert schema.features["continuous"]["D"].dtype in integer_types
 
-        (
-            train_indices,
-            validation_indices,
-            test_indices,
-        ) = (x.tolist() for x in data_splitter.create_dataset_splits(data))
-
-        # Check that random.shuffle has been called with default shuffle=True
-        assert random_shuffle_mock.call_count == 3
-        np.testing.assert_array_equal(
-            random_shuffle_mock.call_args_list[0][0][0], np.array(train_indices)
-        )
-        np.testing.assert_array_equal(
-            random_shuffle_mock.call_args_list[1][0][0], np.array(validation_indices)
-        )
-        np.testing.assert_array_equal(
-            random_shuffle_mock.call_args_list[2][0][0], np.array(test_indices)
-        )
+    @pytest.mark.parametrize(
+        "data_and_schema",
+        [
+            pytest.param(True, marks=integration_test),
+            pytest.param(False, marks=unit_test),
+        ],
+        indirect=["data_and_schema"],
+    )
+    def test_normalize_transformation_on_categorical_column(
+        self, data_and_schema: Tuple[_DataFrameType, BitfountSchema]
+    ) -> None:
+        """Tests exception raised if normalize called on categorical column."""
+        data, schema = data_and_schema
+        test_yaml = """
+        Transformations:
+        - NormalizeData:
+            name: blah
+            cols:
+                - col:I
+        """
+        with pytest.raises(TransformationApplicationError):
+            parse_and_process_data(data, schema, test_yaml)
 
-        # Ensure no overlap of indices between all 3 datasets
-        assert not bool(set(train_indices).intersection(validation_indices))
-        assert not bool(set(validation_indices).intersection(test_indices))
-        assert not bool(set(train_indices).intersection(test_indices))
-
-        train_max = data.loc[train_indices, "Date"].max(skipna=True)
-        validation_min = data.loc[validation_indices, "Date"].min(skipna=True)
-        validation_max = data.loc[validation_indices, "Date"].max(skipna=True)
-        test_min = data.loc[test_indices, "Date"].min(skipna=True)
-
-        # Ensure no overlap of dates between all 3 datasets
-        if validation_percentage > 0:
-            assert validation_min > train_max
-
-        if test_percentage > 0:
-            assert test_min > train_max
-            if validation_percentage > 0:
-                assert test_min > validation_max
-
-    @pytest.mark.parametrize("validation_percentage", [0, 7])
-    @pytest.mark.parametrize("test_percentage", [0, 7])
-    def test_time_series_shuffle_false(
+    @pytest.mark.parametrize(
+        "data_and_schema",
+        [
+            pytest.param(True, marks=integration_test),
+            pytest.param(False, marks=unit_test),
+        ],
+        indirect=["data_and_schema"],
+    )
+    def test_normalize_transformation_no_args(
         self,
-        data: pd.DataFrame,
-        mocker: MockerFixture,
-        test_percentage: int,
-        validation_percentage: int,
-    ) -> None:
-        """Checks the time series sorting works correctly when shuffle is false."""
-        data["Date"] = pd.to_datetime(data["Date"], infer_datetime_format=True)
-        data = data.reindex(np.random.permutation(data.index.tolist()))
-        random_shuffle_mock = mocker.patch("numpy.random.shuffle")
-        data_splitter = PercentageSplitter(
-            validation_percentage=validation_percentage,
-            test_percentage=test_percentage,
-            time_series_sort_by="Date",
-            shuffle=False,
-        )
+        data_and_schema: Tuple[_DataFrameType, BitfountSchema],
+        integer_types: IntegerTypes,
+    ) -> None:
+        """Tests NormalizeData transformation when no args supplied (all cols)."""
+        data, schema = data_and_schema
+        test_yaml = """
+        Transformations:
+        - NormalizeData
+        """
+        for feature in ["A", "B", "C", "D"]:
+            assert schema.features["continuous"][feature].dtype in integer_types
 
-        (
-            train_indices,
-            validation_indices,
-            test_indices,
-        ) = (x.tolist() for x in data_splitter.create_dataset_splits(data))
-
-        # Ensure no overlap of indices between all 3 datasets
-        assert not bool(set(train_indices).intersection(validation_indices))
-        assert not bool(set(validation_indices).intersection(test_indices))
-        assert not bool(set(train_indices).intersection(test_indices))
-        random_shuffle_mock.assert_not_called()
-
-        train_max = data.loc[train_indices, "Date"].max(skipna=True)
-        validation_min = data.loc[validation_indices, "Date"].min(skipna=True)
-        validation_max = data.loc[validation_indices, "Date"].max(skipna=True)
-        test_min = data.loc[test_indices, "Date"].min(skipna=True)
-
-        # Ensure no overlap of dates between all 3 datasets
-        if validation_percentage > 0:
-            assert validation_min > train_max
-
-        if test_percentage > 0:
-            assert test_min > train_max
-            if validation_percentage > 0:
-                assert test_min > validation_max
-
-    def test_time_series_multiple_columns(self, data: pd.DataFrame) -> None:
-        """Tests time series ordering works when multiple columns are supplied."""
-        data["Date"] = pd.to_datetime(data["Date"], infer_datetime_format=True)
-        data["year"] = data.Date.dt.year
-        data["month"] = data.Date.dt.month
-
-        data = data.reindex(np.random.permutation(data.index.tolist()))
-        data_splitter = PercentageSplitter(time_series_sort_by=["year", "month"])
-
-        (
-            train_indices,
-            validation_indices,
-            test_indices,
-        ) = (x.tolist() for x in data_splitter.create_dataset_splits(data))
-
-        # Ensure no overlap of indices between all 3 datasets
-        assert not bool(set(train_indices).intersection(validation_indices))
-        assert not bool(set(validation_indices).intersection(test_indices))
-        assert not bool(set(train_indices).intersection(test_indices))
-
-        # Train set
-        train = data.loc[train_indices]
-        train_max_year, train_max_month = train.loc[train["Date"].idxmax()][
-            ["year", "month"]
-        ]
-        # Validation set
-        validation = data.loc[validation_indices]
-        validation_min_year, validation_min_month = validation.loc[
-            validation["Date"].idxmin()
-        ][["year", "month"]]
-        validation_max_year, validation_max_month = validation.loc[
-            validation["Date"].idxmax()
-        ][["year", "month"]]
-        # Test set
-        test = data.loc[test_indices]
-        test_min_year, test_min_month = test.loc[test["Date"].idxmin()][
-            ["year", "month"]
-        ]
+        data, schema = parse_and_process_data(data, schema, test_yaml)
+        assert round(data.A.mean()) == 0
+        assert round(data.A.std()) == 1
+        assert round(data.B.mean()) == 0
+        assert round(data.B.std()) == 1
 
-        # Ensure no overlap of dates between all 3 datasets
-        assert (
-            validation_min_year > train_max_year
-            or validation_min_month > train_max_month
-        )
-        assert (
-            test_min_year > validation_max_year or test_min_month > validation_max_month
-        )
+        for feature in ["A", "B", "C", "D"]:
+            assert schema.features["continuous"][feature].dtype == np.float64
 
     @pytest.mark.parametrize(
-        "split", [DataSplit.TRAIN, DataSplit.VALIDATION, DataSplit.TEST, None]
+        "data_and_schema",
+        [
+            pytest.param(True, marks=integration_test),
+            pytest.param(False, marks=unit_test),
+        ],
+        indirect=["data_and_schema"],
     )
-    def test_get_split_query(self, split: Optional[DataSplit]) -> None:
-        """Tests the get_split_query method."""
-        data_splitter = PercentageSplitter()
-        mock_datasource = MagicMock(spec=DatabaseSource, query="SELECT * FROM table")
-        mock_datasource.__len__.return_value = 1000
-
-        if split is None:
-            with pytest.raises(ValueError, match="Split not recognised"):
-                data_splitter.get_split_query(mock_datasource, split)  # type: ignore[arg-type] # Reason: purpose of test # noqa: B950
-        else:
-            query = data_splitter.get_split_query(mock_datasource, split)
-
-            if split == DataSplit.TRAIN:
-                assert (
-                    query == "SELECT * FROM (SELECT * FROM table) q LIMIT 800 OFFSET 0"
-                )  # first 80%
-            elif split == DataSplit.VALIDATION:
-                assert (
-                    query
-                    == "SELECT * FROM (SELECT * FROM table) q LIMIT 100 OFFSET 800"
-                )  # next 10%
-            elif split == DataSplit.TEST:
-                assert (
-                    query
-                    == "SELECT * FROM (SELECT * FROM table) q LIMIT 100 OFFSET 900"
-                )  # last 10%
-
-    def test_get_split_query_time_series(self, caplog: LogCaptureFixture) -> None:
-        """Tests that the get_split_query method logs a warning if time series used."""
-        data_splitter = PercentageSplitter(time_series_sort_by="Date")
-        mock_datasource = MagicMock(spec=DatabaseSource, query="SELECT * FROM table")
-        mock_datasource.__len__.return_value = 1000
-        data_splitter.get_split_query(mock_datasource, DataSplit.TRAIN)
-
-        assert (
-            "Time series sort by is not supported for Database percentage splits. "
-            "The sort by will be ignored. If you want to use time series sort by, "
-            "please sort the dataset as you want in the SQL query."
-        ) in caplog.text
-
-    @pytest.mark.parametrize(
-        "split", [DataSplit.TRAIN, DataSplit.VALIDATION, DataSplit.TEST, None]
-    )
-    def test_get_file_names(self, split: Optional[DataSplit]) -> None:
-        """Tests the get_file_names method."""
-        data_splitter = PercentageSplitter()
-        mock_datasource = MagicMock(spec=DICOMSource)
-        mock_datasource.file_names = [
-            "file1",
-            "file2",
-            "file3",
-            "file4",
-            "file5",
-            "file6",
-            "file7",
-            "file8",
-            "file9",
-            "file10",
+    def test_errors_if_transformation_same_name_as_col(
+        self, data_and_schema: Tuple[_DataFrameType, BitfountSchema]
+    ) -> None:
+        """Tests an error is raised when a transformation name matches a column."""
+        data, schema = data_and_schema
+        test_yaml = """
+        Transformations:
+        - add:
+            name: A  # this matches a column name
+            arg1: col:C
+            arg2: col:D
+        """
+        with pytest.raises(TransformationApplicationError) as tpe:
+            parse_and_process_data(data, schema, test_yaml)
+        assert tpe.value.errors == [
+            "Output column A, from transformation A, "
+            "clashes with an existing data column name."
         ]
-        mock_datasource.selected_file_names = mock_datasource.file_names
-        mock_datasource.__len__.return_value = 10
 
-        if split is None:
-            with pytest.raises(ValueError, match="Split not recognised"):
-                data_splitter.get_filenames(mock_datasource, split)  # type: ignore[arg-type] # Reason: purpose of test # noqa: B950
-        else:
-            file_names = data_splitter.get_filenames(mock_datasource, split)
+    @pytest.mark.parametrize(
+        "data_and_schema",
+        [
+            pytest.param(True, marks=integration_test),
+            pytest.param(False, marks=unit_test),
+        ],
+        indirect=["data_and_schema"],
+    )
+    def test_errors_if_multi_column_transformation_out_col_same_as_col(
+        self, data_and_schema: Tuple[_DataFrameType, BitfountSchema]
+    ) -> None:
+        """Tests an error is raised when an MCO output column name matches a column."""
+        data, schema = data_and_schema
+        test_yaml = """
+        Transformations:
+        - onehotencode:
+            name: A  # this matches a column name, but doesn't matter
+            arg: col:B
+            values: {1: "C", 2: "D"}
+        """
+        # Rename data column to be problematic
+        data = data.rename(columns={"C": "A_C", "D": "A_D"})
 
-            if split == DataSplit.TRAIN:
-                assert file_names == [
-                    "file1",
-                    "file2",
-                    "file3",
-                    "file4",
-                    "file5",
-                    "file6",
-                    "file7",
-                    "file8",
-                ]
-            elif split == DataSplit.VALIDATION:
-                assert file_names == ["file9"]
-            elif split == DataSplit.TEST:
-                assert file_names == ["file10"]
-
-    def test_get_file_names_with_selected_file_names(self) -> None:
-        """Tests the get_file_names method respects `selected_file_names`.
-
-        Filenames that aren't contained in `selected_file_names` shouldn't be returned.
-        """
-        data_splitter = PercentageSplitter(0, 100)
-        mock_datasource = MagicMock(spec=DICOMSource)
-        mock_datasource.file_names = [
-            "file1",
-            "file2",
-            "file3",
-            "file4",
-            "file5",
+        with pytest.raises(TransformationApplicationError) as tpe:
+            parse_and_process_data(data, schema, test_yaml)
+
+        assert tpe.value.errors == [
+            "Output column A_C, from transformation A, "
+            "clashes with an existing data column name.",
+            "Output column A_D, from transformation A, "
+            "clashes with an existing data column name.",
         ]
-        mock_datasource.selected_file_names = ["file1", "file3"]
-        mock_datasource.__len__.return_value = 10
 
-        file_names = data_splitter.get_filenames(mock_datasource, DataSplit.TEST)
-        assert file_names == ["file1", "file3"]
+    @pytest.mark.parametrize(
+        "data_and_schema",
+        [
+            pytest.param(True, marks=integration_test),
+            pytest.param(False, marks=unit_test),
+        ],
+        indirect=["data_and_schema"],
+    )
+    def test_multi_transformation(
+        self, data_and_schema: Tuple[_DataFrameType, BitfountSchema]
+    ) -> None:
+        """Tests the application of multiple transformations."""
+        data, schema = data_and_schema
+        test_yaml = """
+        Transformations:
+        - divide:
+            name: A_and_B_div
+            output: false
+            arg1: col:A
+            arg2: c:B
+        - multiply:
+            name: percentage
+            output: true
+            arg1: tran:A_and_B_div
+            arg2: 100
+        - divide:
+            name: reciprocal
+            output: true
+            arg2: 1
+            arg1: tran:A_and_B_div
+        - in:
+            in_str: "a"
+            arg: column:I
+            output: true
+        """
+        original_col_length = len(data.columns)
+        data, _ = parse_and_process_data(data, schema, test_yaml)
+        assert "reciprocal" in data.columns
+        assert "percentage" in data.columns
+        assert "A_and_B_div" not in data.columns
+        assert original_col_length + 3 == len(data.columns)
+        assert data.iloc[:, -1:].columns[0][:3] == "in_"
 
+    @pytest.mark.parametrize(
+        "data_and_schema",
+        [
+            pytest.param(True, marks=integration_test),
+            pytest.param(False, marks=unit_test),
+        ],
+        indirect=["data_and_schema"],
+    )
+    def test_multi_column_output_drop_cols(
+        self, data_and_schema: Tuple[_DataFrameType, BitfountSchema]
+    ) -> None:
+        """Tests that multicolumn outputs are all dropped correctly."""
+        data, schema = data_and_schema
+        test_yaml = """
+        Transformations:
+        - OneHotEncode:
+            name: ohe_I
+            output: false
+            arg: col:I
+            values: [e,f,g]
+        """
+        data, _ = parse_and_process_data(data, schema, test_yaml)
+        # Check no column with ohe_I in it exists
+        for col in data.columns:
+            assert not col.startswith("ohe_I")
+
+    @unit_test
+    def test_schema_invalid_columns(self) -> None:
+        """Tests exception raised if referenced column doesn't exist."""
+        test_yaml = """
+        Transformations:
+        - divide:
+            name: A_and_B_div
+            output: true
+            arg1: col:col_that_doesntexist
+            arg2: c:second_nonexistent_col
+        """
+        data = create_dataset()
+        schema = BitfountSchema(DataSource(data))
+        parser = TransformationsParser()
+        transformations, cols = parser.parse(test_yaml)
+        processor = TransformationProcessor(transformations, schema, cols)
+        with pytest.raises(MissingColumnReferenceError) as tpe:
+            processor.transform(data)
+        assert tpe.value.errors == [
+            f"Reference to non-existent column: {c}"
+            for c in ("col_that_doesntexist", "second_nonexistent_col")
+        ]
 
-@unit_test
-class TestSplitterDefinedInData:
-    """Test SplitterDefinedInData."""
+    @unit_test
+    def test_schema_invalid_operation(self) -> None:
+        """Tests exception raised if transformation operation invalid."""
+        test_yaml = """
+        Transformations:
+        - divide:
+            name: A_and_B_div
+            output: false
+            arg1: col:A
+            arg2: c:I
+        """
+        data = create_dataset()
+        schema = BitfountSchema(DataSource(data))
+        parser = TransformationsParser()
+        transformations, cols = parser.parse(test_yaml)
+        processor = TransformationProcessor(transformations, schema, cols)
+        original_col_length = len(data.columns)
+        with pytest.raises(TransformationApplicationError):
+            processor.transform(data)
+        assert "A_and_B_div" not in data.columns
+        assert original_col_length == len(data.columns)
+
+    @unit_test
+    def test_image_transformation(self) -> None:
+        """Tests image transformation works correctly."""
+        test_yaml = """
+        Transformations:
+        - Image:
+            arg: col:col1
+            output: True
+            step: train
+            transformations:
+                - HorizontalFlip:
+                    p: 0.5
+                - RandomBrightnessContrast
+        """
+        parser = TransformationsParser()
+        tfms, _ = parser.parse(test_yaml)
+        proc = TransformationProcessor(tfms)
+        image = np.array(Image.new("RGB", size=(50, 50), color=(100, 0, 255)))
+        transformed_image = proc.batch_transform(image, step="train")
+        assert isinstance(transformed_image, np.ndarray)
+        assert transformed_image.shape == image.shape
+
+    @unit_test
+    def test_image_transformation_step_is_respected(self) -> None:
+        """Tests that transformation is not applied if step does not match."""
+        test_yaml = """
+        Transformations:
+        - Image:
+            arg: col:col1
+            output: True
+            step: train
+            transformations:
+                - HorizontalFlip:
+                    p: 0.5
+                - RandomBrightnessContrast
+        """
+        parser = TransformationsParser()
+        tfms, _ = parser.parse(test_yaml)
+        proc = TransformationProcessor(tfms)
+        image = np.array(Image.new("RGB", size=(50, 50), color=(100, 0, 255)))
+        transformed_image = proc.batch_transform(image, step="validation")
 
-    def test_sets_split(self, data: pd.DataFrame) -> None:
-        """Test that the specification in the data is respected."""
-        # This is a string column that's present in the dataset
-        data_split_column_name = "L"
-        training_set_label = "BF_TRAIN"
-        validation_set_label = "BF_VALIDATION"
-        test_set_label = "BF_TEST"
-
-        data_split_column = np.random.choice(
-            [training_set_label, validation_set_label, test_set_label],
-            size=data.shape[0],
-        )
-        data.update(
-            # Because name is set, will be coerced into dataframe
-            cast(
-                pd.DataFrame,
-                pd.Series(data_split_column, name=data_split_column_name),
-            )
-        )
+        # Assert that the 'transformed' image is exactly the same
+        np.testing.assert_array_equal(transformed_image, image)
 
-        data = data.reindex(np.random.permutation(data.index.tolist()))
-        data_splitter = SplitterDefinedInData(
-            column_name=data_split_column_name,
-            training_set_label=training_set_label,
-            validation_set_label=validation_set_label,
-            test_set_label=test_set_label,
-        )
+    @unit_test
+    def test_batch_transform_raises_value_error_with_non_batch_transformation(
+        self,
+    ) -> None:
+        """Tests that `batch_tranform` can only be used with batch transformations."""
+        test_yaml = """
+        Transformations:
+        - divide:
+            name: A_and_B_div
+            output: false
+            arg1: col:A
+            arg2: c:I
+        """
+        parser = TransformationsParser()
+        tfms, _ = parser.parse(test_yaml)
+        proc = TransformationProcessor(tfms)
+        image = np.array(Image.new("RGB", size=(50, 50), color=(100, 0, 255)))
+        with pytest.raises(InvalidBatchTransformationError):
+            proc.batch_transform(image, step="train")
 
-        (
-            train_indices,
-            validation_indices,
-            test_indices,
-        ) = data_splitter.create_dataset_splits(data)
-
-        # Indices for all rows are included
-        assert (
-            len(train_indices) + len(validation_indices) + len(test_indices)
-            == data.shape[0]
-        )
 
-        # We use the indices to get the
-        # The training set indices refer to the relevant rows in the dataframe
-        assert np.all(data.loc[train_indices].L == training_set_label)
-        # The test set indices refer to the relevant rows in the dataframe
-        assert np.all(data.loc[test_indices].L == test_set_label)
-        # The validation set indices refer to the relevant rows in the dataframe
-        assert np.all(data.loc[validation_indices].L == validation_set_label)
-
-    @pytest.mark.parametrize(
-        "split", [DataSplit.TRAIN, DataSplit.VALIDATION, DataSplit.TEST, None]
-    )
-    def test_get_split_query(self, split: Optional[DataSplit]) -> None:
-        """Tests the get_split_query method."""
-        data_splitter = SplitterDefinedInData()
-        mock_datasource = MagicMock(spec=DatabaseSource, query="SELECT * FROM table")
-        mock_datasource.__len__.return_value = 1000
-
-        if split is None:
-            with pytest.raises(ValueError, match="Split not recognised"):
-                data_splitter.get_split_query(mock_datasource, split)  # type: ignore[arg-type] # Reason: purpose of test # noqa: B950
-        else:
-            query = data_splitter.get_split_query(mock_datasource, split)
+@unit_test
+class TestTransformationProcessorError:
+    """Tests for TransformationProcessorError exception."""
 
-            if split == DataSplit.TRAIN:
-                assert (
-                    query == "SELECT * FROM (SELECT * FROM table) q"
-                    " WHERE BITFOUNT_SPLIT_CATEGORY = 'TRAIN'"
-                )
-            elif split == DataSplit.VALIDATION:
-                assert (
-                    query == "SELECT * FROM (SELECT * FROM table) q"
-                    " WHERE BITFOUNT_SPLIT_CATEGORY = 'VALIDATE'"
-                )
-            elif split == DataSplit.TEST:
-                assert (
-                    query == "SELECT * FROM (SELECT * FROM table) q"
-                    " WHERE BITFOUNT_SPLIT_CATEGORY = 'TEST'"
-                )
+    def test_single_error_arg(self) -> None:
+        """Tests creation of TransformationProcessorError from single error."""
+        with pytest.raises(TransformationProcessorError) as tpe:
+            raise TransformationProcessorError("test")
+        assert tpe.value.errors == ["test"]
+
+    def test_multiple_error_arg(self) -> None:
+        """Tests creation of TransformationProcessorError with multiple errors."""
+        with pytest.raises(TransformationProcessorError) as tpe:
+            raise TransformationProcessorError(["test", "test2"])
+        assert tpe.value.errors == ["test", "test2"]
+
+    def test_str(self) -> None:
+        """Tests string representation of TransformationProcessorError."""
+        with pytest.raises(TransformationProcessorError) as tpe:
+            raise TransformationProcessorError(["test", "test2"])
+        assert str(tpe.value) == "Errors: \ntest\ntest2"
```

### Comparing `bitfount-0.5.86/tests/bitfount/data/test_utils.py` & `bitfount-0.5.9/tests/bitfount/models/test_models.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,553 +1,603 @@
-"""Tests for data utils classes and methods."""
-import datetime
-import hashlib
-import json
-from typing import Any, Dict, Generator, List, Mapping, Optional, Protocol, Union
-from unittest.mock import MagicMock, Mock
+"""Testcases for all classes in bitfount/models.py."""
+import os
+from pathlib import Path
+from typing import Any, Dict, cast
+from unittest.mock import Mock
 
 import numpy as np
 import pandas as pd
-from pandas._typing import Dtype
 import pytest
 from pytest import fixture
 from pytest_mock import MockerFixture
-import sqlalchemy
+from sklearn.linear_model import LinearRegression as sklearnLinearRegression
+from sklearn.neighbors import KNeighborsClassifier
 
-from bitfount import BitfountSchema, DataStructure
-from bitfount.data.datasources.dataframe_source import DataFrameSource
-from bitfount.data.exceptions import (
-    DatabaseMissingTableError,
-    DatabaseSchemaNotFoundError,
-    DatabaseUnsupportedQueryError,
-    DatabaseValueError,
+from bitfount.data.dataloader import _BitfountDataLoader
+from bitfount.data.datasource import DataSource
+from bitfount.data.datastructure import DataStructure
+from bitfount.data.schema import BitfountSchema
+from bitfount.metrics import MetricCollection
+from bitfount.models.base_models import CNNModelStructure, NeuralNetworkModelStructure
+from bitfount.models.models import LogisticRegressionClassifier, RegBoostRegressor
+from bitfount.types import _DataFrameType
+from tests.utils.helper import (
+    assert_results,
+    create_dataset,
+    create_datasource,
+    create_datastructure,
+    create_schema,
+    integration_test,
+    unit_test,
 )
-from bitfount.data.helper import convert_epochs_to_steps
-from bitfount.data.utils import (
-    DatabaseConnection,
-    DataStructureSchemaCompatibility,
-    _convert_python_dtypes_to_pandas_dtypes,
-    _generate_dtypes_hash,
-    _hash_str,
-    check_datastructure_schema_compatibility,
-)
-from bitfount.types import _Dtypes
-from tests.utils.helper import unit_test
 
+SERIALIZED_MODEL_NAME = "test_model.pickle"
 
-@unit_test
-def test_convert_epochs_to_steps() -> None:
-    """Test converting of epochs to steps is correct."""
-    dataloader = MagicMock()
-    dataloader.__len__.return_value = 100
-    steps = convert_epochs_to_steps(5, dataloader)
-    assert steps == 100 * 5
 
+def assert_vars_equal(vars_original: Dict[str, Any], vars_copy: Dict[str, Any]) -> None:
+    """Asserts both vars() are equal; handles FunctionalForm as well."""
+    for variable, value in vars_original.items():
+        if isinstance(value, DataStructure):
+            assert value.target == vars_copy[variable].target
+        elif isinstance(value, BitfountSchema):
+            assert isinstance(vars_copy[variable], BitfountSchema)
+        else:
+            assert value == vars_copy[variable]
 
-@unit_test
-class TestDatabaseConnection:
-    """Tests DatabaseConnection class."""
 
-    @fixture
-    def mock_engine(self) -> Mock:
-        """Returns mock sqlalchemy engine."""
-        return Mock(spec=sqlalchemy.engine.base.Engine)
-
-    @fixture(autouse=True)
-    def mock_inspector(self, mocker: MockerFixture) -> Generator[Mock, None, None]:
-        """Automatically mocks sqlalchemy inspector and yields mocked object."""
-        mock_inspector = Mock(
-            default_schema_name="public", spec=sqlalchemy.engine.Inspector
-        )
-        mock_inspector.get_schema_names.return_value = ["public"]
-        mock_inspector.get_table_names.return_value = ["test_data"]
-        mocker.patch("bitfount.data.utils.inspect", return_value=mock_inspector)
-        yield mock_inspector
-
-    def test_create_engine_from_connection_string(self, mocker: MockerFixture) -> None:
-        """Tests that a sqlalchemy object can be created from a database URI."""
-        mock_create_engine = mocker.patch(
-            "bitfount.data.utils.create_engine", autospec=True
-        )
-        db = DatabaseConnection(
-            "postgresql://localhost:5432/test", query="SELECT * FROM test_data"
-        )
-        db.validate()
-        mock_create_engine.assert_called_once_with("postgresql://localhost:5432/test")
-
-    def test_schema_not_found_in_database_raises_value_error(
-        self, mock_engine: Mock, mock_inspector: Mock
-    ) -> None:
-        """Tests that DatabaseSchemaNotFoundError raised if schema not in database."""
-        with pytest.raises(DatabaseSchemaNotFoundError):
-            db = DatabaseConnection(mock_engine, db_schema="nonexistent_schema")
-            db.validate()
-
-        mock_inspector.get_schema_names.assert_called_once()
-
-    def test_query_and_table_names_raises_value_error(self, mock_engine: Mock) -> None:
-        """Tests that query and table names can't both be specified."""
-        with pytest.raises(DatabaseValueError):
-            db = DatabaseConnection(
-                mock_engine, query="SELECT * FROM test_data", table_names=["test_data"]
-            )
-            db.validate()
-
-    def test_specified_table_not_found_in_schema_raises_value_error(
-        self,
-        mock_engine: Mock,
-    ) -> None:
-        """Tests that DatabaseMissingTableError raised if table not in schema."""
-        with pytest.raises(DatabaseMissingTableError):
-            db = DatabaseConnection(mock_engine, table_names=["nonexistent_table"])
-            db.validate()
-
-    def test_no_tables_found_in_schema_raises_value_error(
-        self, mock_engine: Mock, mock_inspector: Mock
-    ) -> None:
-        """Tests that DatabaseMissingTableError raised if no tables in schema."""
-        mock_inspector.get_table_names.return_value = []
-        with pytest.raises(DatabaseMissingTableError):
-            db = DatabaseConnection(mock_engine)
-            db.validate()
-
-    def test_get_default_schema(self, mock_engine: Mock, mock_inspector: Mock) -> None:
-        """Tests that default schema is used if none specified."""
-        db = DatabaseConnection(mock_engine)
-        db.validate()
-        mock_inspector.get_table_names.assert_called_once_with(schema="public")
-
-    def test_single_table_name(self, mock_engine: Mock) -> None:
-        """Tests that a single table name can be specified."""
-        db_conn = DatabaseConnection(mock_engine, table_names=["test_data"])
-        db_conn.validate()
-        assert not db_conn.multi_table
-
-    def test_multiple_table_names(
-        self, mock_engine: Mock, mock_inspector: Mock
-    ) -> None:
-        """Tests that multiple table names can be specified."""
-        mock_inspector.get_table_names.return_value = ["test_data", "test_data_2"]
-        db_conn = DatabaseConnection(
-            mock_engine, table_names=["test_data", "test_data_2"]
-        )
-        db_conn.validate()
-        assert db_conn.multi_table
-
-    def test_all_tables_in_schema(
-        self, mock_engine: Mock, mock_inspector: Mock
-    ) -> None:
-        """Tests that connection will default to all tables if none provided."""
-        mock_inspector.get_table_names.return_value = [
-            "test_data",
-            "test_data_2",
-            "test_data_3",
-        ]
-        db_conn = DatabaseConnection(mock_engine)
-        db_conn.validate()
-        assert db_conn.multi_table
-        assert db_conn.table_names == ["test_data", "test_data_2", "test_data_3"]
-
-    def test_query(self, mock_engine: Mock) -> None:
-        """Tests that query can be specified."""
-        db_conn = DatabaseConnection(mock_engine, query="SELECT * FROM test_data")
-        db_conn.validate()
-        assert not db_conn.multi_table
-        assert db_conn.query
-
-    def test_into_query_error(self, mock_engine: Mock) -> None:
-        """Tests that a query containing into raises error."""
-        with pytest.raises(DatabaseUnsupportedQueryError):
-            db = DatabaseConnection(mock_engine, query="SELECT * INTO df")
-            db.validate()
+@pytest.fixture
+def datastructure() -> DataStructure:
+    """Fixture for datastructure."""
+    return create_datastructure()
 
 
-@unit_test
-class TestDataFrameHashing:
-    """Tests for generate_dataframe_hash()."""
+@pytest.fixture
+def datasource() -> DataSource:
+    """Fixture for datasource."""
+    return create_datasource(classification=True)
 
-    @fixture
-    def dtypes(self) -> _Dtypes:
-        """A test dataframe with data."""
-        return {
-            "np_test": np.dtype(int),
-            "pd_test": pd.core.arrays.integer.Int64Dtype(),
-        }
 
-    @fixture
-    def dtypes_hash(self) -> str:
-        """The expected hash for the dataframe fixture."""
-        # The hash is on the DataFrame.dtypes (which returns a Series), so we
-        # manually construct the expected matching one.
-        dtypes = {
-            "np_test": str(np.dtype(int)),
-            "pd_test": str(pd.core.arrays.integer.Int64Dtype()),
-        }
-        str_rep = json.dumps(dtypes, sort_keys=True)
-        return hashlib.sha256(str_rep.encode("utf8")).hexdigest()
-
-    @fixture
-    def empty_dtypes(self) -> _Dtypes:
-        """A test dtype mapping with no data."""
-        return {}
-
-    @fixture
-    def empty_dtypes_hash(self) -> str:
-        """The expected hash of an empty dataframe."""
-        # The hash is on the DataFrame.dtypes (which returns a Series), so we
-        # manually construct the expected matching one.
-        empty_series: Dict = {}
-        str_rep: str = str(empty_series)
-        return hashlib.sha256(str_rep.encode("utf8")).hexdigest()
-
-    def test_generate_dtypes_hash(self, dtypes: _Dtypes, dtypes_hash: str) -> None:
-        """Tests generated hash is expected one for non-empty dataframe."""
-        assert _generate_dtypes_hash(dtypes) == dtypes_hash
-
-    def test_generate_dtypes_hash_empty_dataframe(
-        self, empty_dtypes: _Dtypes, empty_dtypes_hash: str
-    ) -> None:
-        """Tests generated hash is expected one for empty dataframe."""
-        assert _generate_dtypes_hash(empty_dtypes) == empty_dtypes_hash
-
-    def test_generate_dtypes_hash_same_for_same_dtypes(
-        self, dtypes: Dict[str, Union[Dtype, np.dtype]], dtypes_hash: str
-    ) -> None:
-        """Tests generated hash is consistent for two dataframes with same cols."""
-        dtypes_2 = dtypes.copy()
-
-        # Check they are different instances
-        assert dtypes is not dtypes_2
-        # Check hashes match
-        assert (
-            _generate_dtypes_hash(dtypes)
-            == _generate_dtypes_hash(dtypes_2)
-            == dtypes_hash
-        )
-
-    def test_generate_dtypes_hash_different_for_different_dtype_dataframes(
-        self, dtypes: _Dtypes, dtypes_hash: str
-    ) -> None:
-        """Tests hash is different for two dataframes with diff col dtypes."""
-        # Change the column dtype from int64 to string
-        dtypes_2 = {k: np.dtype(str) for k in dtypes.keys()}
-
-        # Check they are different instances
-        assert dtypes is not dtypes_2
-        # Check hashes differ
-        assert (
-            _generate_dtypes_hash(dtypes)
-            != _generate_dtypes_hash(dtypes_2)
-            != dtypes_hash
-        )
+@pytest.fixture
+def schema() -> BitfountSchema:
+    """Fixture for schema."""
+    return create_schema(classification=True)
 
 
 @unit_test
-def test_hash_str() -> None:
-    """Tests that hash_str() works."""
-    test_string = "Hello, world!"
-    expected_hash = "315f5bdb76d078c43b8ac0064e4a0164612b1fce77c869345bfc94c75894edd3"
-    assert _hash_str(test_string) == expected_hash
+class TestNeuralNetworkModelStructure:
+    """Test NeuralNetworkModelStructure classes."""
 
+    def test_nn_model_structure_dropout_layer_mismatch(self) -> None:
+        """Test ValueError is raised if dropout layers don't match linear layers."""
+        with pytest.raises(ValueError):
+            NeuralNetworkModelStructure(layers=[100, 50], dropout_probs=[0.1, 0.3, 0.5])
+
+    def test_cnn_model_structure_invalid_pooling_function(self) -> None:
+        """Test ValueError is raised if an invalid pooling function is used."""
+        with pytest.raises(ValueError):
+            CNNModelStructure(pooling_function="notarealpoolingfunction")
+
+    def test_cnn_model_structure_dropout_layer_mismatch(self) -> None:
+        """Test ValueError is raised if dropout layers don't match conv layers."""
+        with pytest.raises(ValueError):
+            CNNModelStructure(layers=[100, 50], dropout_probs=[0.1, 0.3, 0.5])
+
+    def test_cnn_model_structure_serialization(self) -> None:
+        """Test CNN Model is serialized correctly."""
+        model_structure = CNNModelStructure(pooling_function="avg", padding=5, stride=1)
+        schema = model_structure._Schema()
+        dumped_model_structure = schema.dumps(model_structure)
+        reloaded = schema.loads(dumped_model_structure)
+        assert reloaded.pooling_function == model_structure.pooling_function
+        assert reloaded.padding == model_structure.padding
+        assert reloaded.stride == model_structure.stride
+
+
+class TestLogisticRegression:
+    """Test LogisticRegression class from bitfount.models."""
+
+    @integration_test
+    def test_fit_and_results(
+        self, datasource: DataSource, datastructure: DataStructure
+    ) -> None:
+        """Test LogisticRegression fit() and get_results() methods."""
+        log_reg = LogisticRegressionClassifier(
+            datastructure=datastructure, schema=BitfountSchema(), verbose=0
+        )
+        log_reg.fit(datasource)
+        assert_results(model=log_reg)
+
+    @unit_test
+    def test_missing_image_col_raises_error(self) -> None:
+        """Tests that a value error when images are not in the datasource."""  # noqa: B950
+        datasource = create_datasource(classification=True, image=True)
+        datastructure = DataStructure(target=["TARGET"], image_cols=["img"])
+        model = LogisticRegressionClassifier(
+            datastructure=datastructure, schema=BitfountSchema(), verbose=0
+        )
+        with pytest.raises(ValueError):
+            model.fit(datasource)
+
+    @unit_test
+    def test_evaluate_no_test_dl_error(self, datastructure: DataStructure) -> None:
+        """Tests that evaluate raises error with no test_dl."""
+        model = LogisticRegressionClassifier(
+            datastructure=datastructure, schema=BitfountSchema(), verbose=-1
+        )
+        with pytest.raises(ValueError):
+            model.evaluate()
+
+    @unit_test
+    def test_serialization(
+        self, datasource: DataSource, datastructure: DataStructure, tmp_path: Path
+    ) -> None:
+        """Test serialize() and deserialize() methods."""
+        log_reg = LogisticRegressionClassifier(
+            datastructure=datastructure, schema=BitfountSchema(), verbose=0
+        )
+        log_reg.fit(datasource)
+        log_reg.serialize(tmp_path / SERIALIZED_MODEL_NAME)
+        assert os.path.exists(tmp_path / SERIALIZED_MODEL_NAME)
+
+        log_reg = LogisticRegressionClassifier(
+            datastructure=datastructure, schema=BitfountSchema()
+        )
+        log_reg.fit(datasource)
+        log_reg.deserialize(tmp_path / SERIALIZED_MODEL_NAME)
 
-@unit_test
-def test_convert_python_dtypes_to_panda_dtypes_error_unsupported_type() -> None:
-    """Tests error is raised with unsupported type."""
-    with pytest.raises(ValueError):
-        _convert_python_dtypes_to_pandas_dtypes(set, "col_name")
+        assert log_reg.test_dl is not None
+        log_reg.evaluate(log_reg.test_dl)
 
 
-@unit_test
-def test_convert_python_dtypes_to_panda_dtypes_error_date_type() -> None:
-    """Tests string is returned with date type."""
-    dtype = _convert_python_dtypes_to_pandas_dtypes(datetime.date, "col_name")
-    assert dtype == pd.StringDtype()
-
-
-@unit_test
-def test_convert_python_dtypes_to_panda_dtypes_error_datetime_type() -> None:
-    """Tests string is returned with datetime type."""
-    dtype = _convert_python_dtypes_to_pandas_dtypes(datetime.datetime, "col_name")
-    assert dtype == pd.StringDtype()
+class TestRegBoostRegressor:
+    """Test RegBoost model."""
 
+    @fixture
+    def mock_ols(self, mocker: MockerFixture) -> Mock:
+        """Mock OLS import."""
+        mock_ols: Mock = mocker.patch("statsmodels.api.OLS", autospec=True)
+        return mock_ols
+
+    @unit_test
+    def test_model_tree_structure(self) -> None:
+        """Test basic Model tree structure."""
+        data = create_dataset(classification=True)
+        modeltree = RegBoostRegressor._ModelTree(data.index.to_numpy(), 0)
+        assert modeltree.negatives is None
+        assert modeltree.positives is None
+        assert isinstance(modeltree.model, sklearnLinearRegression)
+        assert modeltree.depth == 0
+        assert len(modeltree.data_indices) == len(data)
+
+    @unit_test
+    def test_model_tree_structure_add_children(self) -> None:
+        """Test Model tree structure adding children."""
+        data = create_dataset(classification=True)
+        modeltree = RegBoostRegressor._ModelTree(data.index.to_numpy(), 0)
+        child = modeltree.add_child(
+            RegBoostRegressor._ModelTreeSide.NEGATIVE, np.array([1, 2, 3, 4, 5])
+        )
+        assert modeltree.negatives is not None
+        assert modeltree.negatives.depth == 1
+        assert child == modeltree.negatives
+        assert isinstance(modeltree.negatives.model, sklearnLinearRegression)
+        assert len(modeltree.negatives.data_indices) == 5
+        assert modeltree.positives is None
+
+        child2 = modeltree.add_child(
+            RegBoostRegressor._ModelTreeSide.POSITIVE, np.array([1, 2, 3, 4])
+        )
+
+        assert modeltree.positives is not None
+        assert modeltree.positives.depth == 1
+        assert child2 == modeltree.positives
+        assert child != child2
+        assert isinstance(modeltree.positives.model, sklearnLinearRegression)
+        assert len(modeltree.positives.data_indices) == 4
+        assert modeltree.negatives is not None
+
+    @unit_test
+    @pytest.mark.parametrize("max_depth", [1, 5, 10])
+    def test_model_tree_building(
+        self, datasource: DataSource, datastructure: DataStructure, max_depth: int
+    ) -> None:
+        """Test building of Model tree."""
+        reg = RegBoostRegressor(
+            datastructure=datastructure, schema=BitfountSchema(), max_depth=max_depth
+        )
+        reg.fit(datasource)
+
+        assert isinstance(reg._model, RegBoostRegressor._ModelTree)
+
+        def find_tree_max_depth(model: RegBoostRegressor._ModelTree) -> int:
+            depths = []
+            for model_ in [model.positives, model.negatives]:
+                if model_ is not None:
+                    # Assert data indices are a subset of parent and >= minimum required
+                    assert len(model_.data_indices) >= reg.min_data_points_per_node
+                    assert set(model_.data_indices).issubset(set(model.data_indices))
+                    # Assert features are a subset of parent
+                    assert len(model_.features) <= len(model.features)
+                    assert set(model_.features).issubset(set(model.features))
+                    depths.append(find_tree_max_depth(model_))
+                else:
+                    depths.append(model.depth)
+            return max(depths)
 
-class DataStructureBuilder(Protocol):
-    """Callback Protocol for the datastructure_builder fixture."""
-
-    def __call__(
-        self,
-        table: Optional[Union[str, Mapping[str, str]]] = "main",
-        target: Optional[Union[str, List[str]]] = "c",
-        ignore_cols: Optional[List[str]] = None,
-        selected_cols: Optional[List[str]] = None,
-        loss_weights_col: Optional[str] = None,
-        multihead_col: Optional[str] = None,
-        ignore_classes_col: Optional[str] = None,
-        image_cols: Optional[List[str]] = None,
-        query_based_ds: bool = False,
-        multihead_size: Optional[int] = None,
-    ) -> DataStructure:
-        """Call signature of the builder fixture."""
-        ...
-
-
-@unit_test
-class TestDataStructureSchemaCompatibilityChecking:
-    """Tests for the check_datastructure_schema_compatibility function."""
-
-    @fixture(scope="class")
-    def data(self) -> Dict[str, List[Any]]:
-        """Data to use for the schema."""
-        return {"a": [1, 2, 3], "b": [1.0, 2.0, 3.0], "c": ["hello", "world", "hello"]}
-
-    @fixture(scope="class")
-    def dataframe(self, data: Dict[str, List[Any]]) -> pd.DataFrame:
-        """The DataFrame representation of the data."""
-        return pd.DataFrame.from_dict(data)
-
-    @fixture(scope="class")
-    def schema(self, dataframe: pd.DataFrame) -> BitfountSchema:
-        """Pod Schema generated from the data.
-
-        Uses "main" as the name of the sole table.
-        """
-        return BitfountSchema(DataFrameSource(dataframe), table_name="main")
+        depth = find_tree_max_depth(reg._model)
+        assert depth <= reg.max_depth
 
-    @fixture(scope="class")
-    def datastructure_builder(self) -> DataStructureBuilder:
-        """Builder function for different forms of DataStructure."""
-
-        def _builder(
-            table: Optional[Union[str, Mapping[str, str]]] = "main",
-            target: Optional[Union[str, List[str]]] = "c",
-            ignore_cols: Optional[List[str]] = None,
-            selected_cols: Optional[List[str]] = None,
-            loss_weights_col: Optional[str] = None,
-            multihead_col: Optional[str] = None,
-            ignore_classes_col: Optional[str] = None,
-            image_cols: Optional[List[str]] = None,
-            query_based_ds: bool = False,
-            multihead_size: Optional[int] = None,
-        ) -> DataStructure:
-            # Query-based fail out fast in compat-check so can just mock out the
-            # details
-            if query_based_ds:
-                return DataStructure(query=Mock(), schema_types_override=Mock())
-
-            # Set defaults if not provided
-            if ignore_cols is None:
-                ignore_cols = []
-
-            # Set defaults if not provided
-            if selected_cols is None:
-                selected_cols = ["a", "b"]
-
-            return DataStructure(
-                table=table,
-                target=target,
-                ignore_cols=ignore_cols,
-                selected_cols=selected_cols,
-                loss_weights_col=loss_weights_col,
-                multihead_col=multihead_col,
-                ignore_classes_col=ignore_classes_col,
-                image_cols=image_cols,
-                multihead_size=multihead_size,
-            )
-
-        return _builder
-
-    def test_compat_check_works(
-        self, datastructure_builder: DataStructureBuilder, schema: BitfountSchema
-    ) -> None:
-        """Test that COMPATIBLE returned if ds/schema are compatible."""
-        compat, msgs = check_datastructure_schema_compatibility(
-            datastructure_builder(), schema
-        )
-
-        assert compat == DataStructureSchemaCompatibility.COMPATIBLE
-        assert not msgs
-
-    def test_query_compat_check_errors(
-        self, datastructure_builder: DataStructureBuilder, schema: BitfountSchema
-    ) -> None:
-        """Test that WARNING returned if ds is query-based."""
-        compat, msgs = check_datastructure_schema_compatibility(
-            datastructure_builder(query_based_ds=True), schema
-        )
-
-        assert compat == DataStructureSchemaCompatibility.WARNING
-        assert msgs == ["Warning: Cannot check query compatibility."]
-
-    @pytest.mark.parametrize(
-        "data_identifier", (None, "not_main"), ids=lambda x: f"data_id={x}"
-    )
-    def test_compat_check_missing_data_identifier_errors(
+    @unit_test
+    def test_fit_and_evaluate_model_tree(
         self,
-        data_identifier: Optional[str],
-        datastructure_builder: DataStructureBuilder,
+        datasource: DataSource,
+        datastructure: DataStructure,
+        mocker: MockerFixture,
         schema: BitfountSchema,
     ) -> None:
-        """Test that ERROR returned if ds for "multiple" pods but data_id missing."""
-        # DS set up as though multiple pod references (even though just single)
-        # and we pass through data_identifiers that aren't in the dict
-        compat, msgs = check_datastructure_schema_compatibility(
-            datastructure_builder(table={"main": "main"}),
-            schema,
-            data_identifier=data_identifier,
-        )
+        """Test fitting and evaluating of model tree with very basic dummy data.
 
-        assert compat == DataStructureSchemaCompatibility.ERROR
-        assert msgs == [
-            f"Error: Multiple pods are specified in the datastructure"
-            f' but pod "{data_identifier}" was not one of them.'
+        Dummy data consists of 6 data points which can be separated into two exactly
+        linear relationships of 3 data points each such that our predictions should be
+        precisely accurate after one splitting of the decision tree.
+        """
+        model = RegBoostRegressor(
+            datastructure=datastructure,
+            schema=schema,
+            max_depth=1,
+            learning_rate=0.1,
+            min_data_points_per_node=2,
+        )
+
+        # Patch out train dataloader
+        model.train_dl = Mock()
+        X = pd.DataFrame({"x": [3, 5, 7, 2, 4, 6]})
+        y = pd.DataFrame({"TARGET": [2, 3, 4, 5, 6, 7]})
+        positive_indices = [3, 4, 5]
+        negative_indices = [0, 1, 2]
+        model.train_dl.get_x_dataframe = Mock(return_value=X)
+        model.train_dl.get_y_dataframe = Mock(return_value=y)
+
+        mock_fit = mocker.patch.object(
+            RegBoostRegressor._ModelTree,
+            "fit",
+            autospec=True,
+            side_effect=lambda self_, X, y: self_.model.fit(X, y),
+        )
+        mock_predict = mocker.patch.object(
+            RegBoostRegressor._ModelTree,
+            "predict",
+            autospec=True,
+            side_effect=lambda self_, X: self_.model.predict(X),
+        )
+
+        modeltree = RegBoostRegressor._ModelTree(np.array(range(6)), 0)
+        model._fit_model_tree(modeltree)
+
+        assert modeltree.positives is not None
+        assert modeltree.negatives is not None
+        assert modeltree.positives.data_indices.tolist() == positive_indices
+        assert modeltree.negatives.data_indices.tolist() == negative_indices
+        assert modeltree.negatives.is_leaf
+        assert modeltree.positives.is_leaf
+        mock_predict.assert_called_once()
+        pd.testing.assert_frame_equal(mock_predict.call_args[0][1], X)
+
+        # One for each of root node, positive child and negative child
+        assert mock_fit.call_count == 3
+
+        # ROOT NODE
+        # First call, second positional arg (positional args include 'self')
+        pd.testing.assert_frame_equal(mock_fit.call_args_list[0][0][1], X)
+        # First call, third positional arg
+        np.testing.assert_array_equal(
+            mock_fit.call_args_list[0][0][2], y.TARGET.to_numpy()
+        )
+        # POSITIVE CHILD
+        # Second call, second positional arg
+        pd.testing.assert_frame_equal(
+            mock_fit.call_args_list[1][0][1], X.loc[positive_indices]
+        )
+        # Second call, third positional arg
+        np.testing.assert_array_equal(
+            mock_fit.call_args_list[1][0][2],
+            # new y values (target - (learning rate * parent predictions))
+            np.array([4.6, 5.56, 6.52], dtype=np.float32),
+        )
+        # NEGATIVE CHILD
+        # Third call, second positional arg
+        pd.testing.assert_frame_equal(
+            mock_fit.call_args_list[2][0][1], X.loc[negative_indices]
+        )
+        # Third call, third positional arg
+        np.testing.assert_array_equal(
+            mock_fit.call_args_list[2][0][2],
+            # new y values (target - (learning rate * parent predictions))
+            np.array([1.58, 2.54, 3.5], dtype=np.float32),
+        )
+
+        preds = model._evaluate_model_tree(modeltree, X, 1)
+        predictions: np.ndarray = model._aggregate_model_predictions(preds)
+        # Assert that our model predictions are precisely accurate
+        np.testing.assert_almost_equal(
+            predictions, y.TARGET.to_numpy(dtype=np.float32), decimal=4
+        )
+
+    @unit_test
+    def test_training_needed(
+        self, datasource: DataSource, datastructure: DataStructure
+    ) -> None:
+        """Tests `training_needed` property."""
+        model = RegBoostRegressor(
+            datastructure=datastructure, schema=BitfountSchema(), max_depth=3
+        )
+        assert model.training_needed
+        model.fit(datasource)
+        assert not model.training_needed
+
+    @unit_test
+    def test_serialization(
+        self, datasource: DataSource, datastructure: DataStructure, tmp_path: Path
+    ) -> None:
+        """Test serialize() and deserialize() methods."""
+        model = RegBoostRegressor(datastructure=datastructure, schema=BitfountSchema())
+        model.fit(datasource)
+        model.serialize(tmp_path / SERIALIZED_MODEL_NAME)
+        assert os.path.exists(tmp_path / SERIALIZED_MODEL_NAME)
+        model = RegBoostRegressor(datastructure=datastructure, schema=BitfountSchema())
+        model.fit(datasource)
+        model.deserialize(tmp_path / SERIALIZED_MODEL_NAME)
+        model.evaluate(k=5)
+
+    @unit_test
+    def test_append_model_predictions(self) -> None:
+        """Test appending of model predictions is in correct order."""
+        parent_preds = [
+            [0.1, 0.2, 0.3],
+            [0.4, 0.5, 0.6],
         ]
-
-    def test_compat_check_missing_table_schema_errors(
-        self, datastructure_builder: DataStructureBuilder, schema: BitfountSchema
-    ) -> None:
-        """Test that ERROR returned if ds requests non-existent table."""
-        # Setup DS so we can extract the table name for the target data_identifier
-        # but it's the WRONG table name
-        compat, msgs = check_datastructure_schema_compatibility(
-            datastructure_builder(table={"main": "not_the_right_table_name"}),
-            schema,
-            data_identifier="main",
-        )
-
-        assert compat == DataStructureSchemaCompatibility.ERROR
-        assert msgs == [
-            "Error: Unable to find the table schema for"
-            ' the table name "not_the_right_table_name".'
+        child_preds = [
+            [0.7],
+            [0.8],
+        ]
+        indices = [True, False]
+        parent_preds = RegBoostRegressor._append_model_predictions(
+            parent_preds, child_preds, indices
+        )
+        assert parent_preds == [
+            [0.1, 0.2, 0.3, 0.7],
+            [0.4, 0.5, 0.6],
         ]
 
-    @pytest.mark.parametrize(
-        "col_type_attr_map",
-        ({"ignore": "ignore_cols"},),
-        ids=lambda d: "attrs_referencing_missing_cols=" + ",".join(sorted(d.values())),
-    )
-    def test_compat_check_missing_ignore_cols_warns(
-        self,
-        col_type_attr_map: Dict[str, str],
-        datastructure_builder: DataStructureBuilder,
-        schema: BitfountSchema,
-    ) -> None:
-        """Test that WARNING returned if ds refs missing cols in "warn" attrs.
-
-        Checks that we only return warning level if the datastructure references
-        columns that don't exist but only for uses that are considered "warning"
-        level (e.g. ignore_cols) as the task might still be able to run.
-        """
-        # Set the builder args for the desired column types to reference non-existing
-        # columns
-        missing_cols = ["d", "e"]
-        kwargs: Dict[str, Any] = {v: missing_cols for _, v in col_type_attr_map.items()}
-
-        if "ignore_cols" in kwargs:
-            # as can't have both
-            kwargs["selected_cols"] = []
-
-        compat, msgs = check_datastructure_schema_compatibility(
-            datastructure_builder(**kwargs), schema
-        )
-
-        assert compat == DataStructureSchemaCompatibility.WARNING
-        assert msgs == [
-            f'Warning: Expected "{col_type}" column, "{i}",'
-            f" but it could not be found in the data schema."
-            for col_type in col_type_attr_map
-            for i in missing_cols
+    @unit_test
+    def test_aggregate_model_predictions(
+        self, datasource: DataSource, datastructure: DataStructure
+    ) -> None:
+        """Test models predictions are aggregated correctly."""
+        model = RegBoostRegressor(
+            datastructure=datastructure, schema=BitfountSchema(), learning_rate=0.1
+        )
+        preds = [
+            [0.1, 0.2, 0.3],
+            [0.4, 0.5, 0.6],
         ]
+        agg_preds = model._aggregate_model_predictions(preds)
+        # All bar last element in array are multiplied by learning rate and then summed
+        np.testing.assert_array_equal(agg_preds, np.array([0.33, 0.69]))
+
+    @unit_test
+    def test_model_classifier(
+        self, datasource: DataSource, datastructure: DataStructure
+    ) -> None:
+        """Test fitting and evaluating of model classifier."""
+        model = RegBoostRegressor(datastructure=datastructure, schema=BitfountSchema())
+        model.fit(datasource)
+        assert isinstance(model._model, RegBoostRegressor._ModelTree)
+        assert model._model.classifier is None
+        model._fit_model_tree_classifier(model._model, k=5)
+        assert isinstance(model._model.classifier, KNeighborsClassifier)
+        test_df: _DataFrameType = cast(
+            _BitfountDataLoader, model.validation_dl
+        ).get_x_dataframe()
+        preds = model._eval_model_tree_classifier(
+            model._model, test_df[model._model.features]
+        )
+        assert len(preds) == len(test_df)
+
+        # Set negatives to None and assert we get positive classes only
+        temp_model = model._model.negatives
+        model._model.negatives = None
+        model._model.classifier = None
+        preds = model._eval_model_tree_classifier(
+            model._model, test_df[model._model.features]
+        )
+        np.testing.assert_array_equal(preds, np.ones(len(test_df), dtype=int))
+
+        # Set positives to None and assert we get negative classes only
+        model._model.negatives = temp_model
+        model._model.positives = None
+        model._model.classifier = None
+        preds = model._eval_model_tree_classifier(
+            model._model, test_df[model._model.features]
+        )
+        np.testing.assert_array_equal(preds, np.zeros(len(test_df), dtype=int))
+
+    @unit_test
+    def test_missing_image_col_raises_error(self) -> None:
+        """Tests that a value error when images are not in the datasource."""  # noqa: B950
+        datasource = create_datasource(classification=True, image=True)
+        datastructure = DataStructure(target=["TARGET"], image_cols=["img"])
+        model = RegBoostRegressor(
+            datastructure=datastructure, schema=BitfountSchema(), verbose=0
+        )
+        with pytest.raises(ValueError):
+            model.fit(datasource)
 
-    @pytest.mark.parametrize(
-        "col_type_attr_map",
-        (
-            {"target": "target"},
-            {"select": "selected_cols"},
-            {"image": "image_cols"},
-            {"loss weights": "loss_weights_col"},
-            {"multihead": "multihead_col"},
-            {"ignore classes": "ignore_classes_col"},
-            {
-                "target": "target",
-                "select": "selected_cols",
-            },
-        ),
-        ids=lambda d: "attrs_referencing_missing_cols=" + ",".join(sorted(d.values())),
-    )
-    def test_compat_check_missing_error_cols_incompatible(
+    @unit_test
+    def test_perform_stepwise_regression(
         self,
-        col_type_attr_map: Dict[str, str],
-        datastructure_builder: DataStructureBuilder,
-        schema: BitfountSchema,
-    ) -> None:
-        """Test that INCOMPATIBLE returned if ds refs missing cols in "error" attrs.
+        datasource: DataSource,
+        datastructure: DataStructure,
+        mocker: MockerFixture,
+    ) -> None:
+        """Test stepwise regression function."""
+        model = RegBoostRegressor(
+            datastructure=datastructure,
+            schema=BitfountSchema(),
+            stepwise_regression="forward",
+        )
+
+        fwd = mocker.patch.object(
+            RegBoostRegressor,
+            "_forward_stepwise_regression",
+            autospec=True,
+            side_effect=lambda x, y, z: ["test"],
+        )
+        bwd = mocker.patch.object(
+            RegBoostRegressor,
+            "_backward_stepwise_regression",
+            autospec=True,
+            side_effect=lambda x, y, z: ["test"],
+        )
+
+        X = pd.DataFrame({"x": [3, 5, 7, 2, 4, 6]})
+        y = np.array([2, 3, 4, 5, 6, 7])
+
+        # Test forward stepwise regression
+        features = model._perform_stepwise_regression(X, y)
+        fwd.assert_called_once()
+        bwd.assert_not_called()
+        assert features == ["test"]
+
+        # Test backward stepwise regression
+        model.stepwise_regression = "backward"
+        fwd.reset_mock()
+        features = model._perform_stepwise_regression(X, y)
+        bwd.assert_called_once()
+        fwd.assert_not_called()
+        assert features == ["test"]
+
+        # Test stepwise regression not recognised
+        with pytest.raises(ValueError):
+            model.stepwise_regression = "blah"  # type: ignore[assignment] # Reason: purpose of test # noqa: B950
+            model._perform_stepwise_regression(X, y)
+
+    @unit_test
+    def test_forward_stepwise_regression(self, mock_ols: Mock) -> None:
+        """Test forward stepwise regression."""
+        ols_model = Mock()
+        mock_ols.return_value = ols_model
+        # Cycles through features, adds one and then cycles through remaining features
+        ols_model.fit.side_effect = [
+            Mock(pvalues={"b": 0.02}),
+            Mock(pvalues={"a": 0.11}),
+            Mock(pvalues={"b": 0.09, "a": 0.11}),
+        ]
 
-        Checks that we return incompatible level if the datastructure references
-        columns that don't exist for uses that are considered "error" level
-        (e.g. target) as the task will not be able to run.
-        """
-        # Set the builder args for the desired column types to reference non-existing
-        # columns
-        missing_cols = ["d", "e"]
-        kwargs: Dict[str, Any] = {v: missing_cols for _, v in col_type_attr_map.items()}
+        X = pd.DataFrame({"b": [3, 5, 7, 2, 4, 6], "a": [3, 5, 7, 2, 4, 6]})
+        y = np.array([2, 3, 4, 5, 6, 7])
+        features = RegBoostRegressor._forward_stepwise_regression(X, y, 0.1)
+        assert features == ["b"]
+        assert mock_ols.call_count == 3
+
+    @unit_test
+    def test_backward_stepwise_regression(self, mock_ols: Mock) -> None:
+        """Test backward stepwise regression."""
+        ols_model = Mock()
+        mock_ols.return_value = ols_model
+        # Cycles through features, adds one and then cycles through remaining features
+        ols_model.fit.side_effect = [
+            Mock(pvalues=pd.Series([0, 0.09, 0.11], index=["ignore-me", "b", "a"])),
+        ]
 
-        if "multihead_col" in kwargs:
-            # as needed if multihead_col is specified
-            kwargs["multihead_size"] = 1
+        X = pd.DataFrame({"b": [3, 5, 7, 2, 4, 6], "a": [3, 5, 7, 2, 4, 6]})
+        y = np.array([2, 3, 4, 5, 6, 7])
+        features = RegBoostRegressor._backward_stepwise_regression(X, y, 0.1)
+        assert features == ["b"]
+        assert mock_ols.call_count == 1
+
+    @pytest.mark.parametrize("max_depth", [1, 3])
+    @unit_test
+    def test_model_visualisation(
+        self, datasource: DataSource, datastructure: DataStructure, max_depth: int
+    ) -> None:
+        """Test model visualisation works."""
+        model = RegBoostRegressor(
+            datastructure=datastructure,
+            schema=BitfountSchema(),
+            max_depth=max_depth,
+        )
+        model.fit(datasource)
+        string_display = str(model._model)
+        num_nodes = sum([2**i for i in range(1, max_depth + 1)]) + 1
+        num_tabs = sum([i * (2**i) for i in range(1, max_depth + 1)])
+        assert string_display.count("\n") == num_nodes
+        assert string_display.count("\t") == num_tabs
+        assert string_display.count("Depth") == num_nodes
+        assert string_display.count("Features") == num_nodes
+        assert string_display.count("Data") == num_nodes
+
+    @unit_test
+    def test_regboost_evaluate_no_test_dl_error(
+        self, datastructure: DataStructure
+    ) -> None:
+        """Tests that evaluate raises error with no test_dl."""
+        model = RegBoostRegressor(
+            datastructure=datastructure,
+            schema=BitfountSchema(),
+            min_data_points_per_node=5,
+            learning_rate=0.5,
+            stepwise_regression_threshold=0.01,
+        )
+        with pytest.raises(ValueError):
+            model.evaluate()
+
+    @integration_test
+    def test_fit_and_results(
+        self, datasource: DataSource, datastructure: DataStructure
+    ) -> None:
+        """Test fit() and evaluate() methods as classifier outputs."""
+        model = RegBoostRegressor(
+            datastructure=datastructure,
+            schema=BitfountSchema(),
+            min_data_points_per_node=5,
+            learning_rate=0.5,
+            stepwise_regression_threshold=0.01,
+        )
+        model.fit(datasource)
+        test_preds, test_target = model.evaluate(k=3)
+        # normalize outputs to be between 0 and 1
+        test_preds = (test_preds - np.min(test_preds)) / (
+            np.max(test_preds) - np.min(test_preds)
+        )
+        metrics = MetricCollection.create_from_model(model)
+        results = metrics.compute(test_target, test_preds)
+        # This model does not perform very well, at least
+        # check that the relevant metrics are computed
+        assert "MAE" in results.keys()
 
-        compat, msgs = check_datastructure_schema_compatibility(
-            datastructure_builder(**kwargs), schema
-        )
 
-        assert compat == DataStructureSchemaCompatibility.INCOMPATIBLE
-        assert msgs == sorted(
-            [
-                f'Incompatible: Expected "{col_type}" column, "{col}",'
-                f" but it could not be found in the data schema."
-                for col_type in col_type_attr_map
-                for col in missing_cols
-            ]
-        )
+@unit_test
+class TestMarshmallowSerialization:
+    """Test Marshmallow Serialization for core Bitfount models."""
 
-    def test_compat_check_missing_warning_and_error_cols_incompatible(
-        self, datastructure_builder: DataStructureBuilder, schema: BitfountSchema
+    def test_logistic_regression_serialization(
+        self, datasource: DataSource, datastructure: DataStructure
     ) -> None:
-        """INCOMPATIBLE returned if ds refs missing cols in some "error" attrs.
-
-        Checks that we return incompatible level if the datastructure references
-        columns that don't exist for uses that are considered "error" level
-        (e.g. target) AND also generates some warnings.
-
-        Check that both "incompatible" and "warning" messages are returned.
-        """
-        # Set the builder args for the desired column types to reference non-existing
-        # columns
-        col_type_attr_map: Dict[str, str] = {
-            "target": "target",
-            "image": "image_cols",
-            "ignore": "ignore_cols",
-        }
-        missing_cols = ["d", "e"]
-        kwargs: Dict[str, Any] = {v: missing_cols for _, v in col_type_attr_map.items()}
-
-        compat, msgs = check_datastructure_schema_compatibility(
-            datastructure_builder(selected_cols=[], **kwargs), schema
-        )
-
-        assert compat == DataStructureSchemaCompatibility.INCOMPATIBLE
-        assert msgs == sorted(
-            [
-                f'Incompatible: Expected "{col_type}" column, "{col}",'
-                f" but it could not be found in the data schema."
-                for col_type in col_type_attr_map
-                for col in missing_cols
-                if col_type != "ignore"
-            ]
-        ) + sorted(
-            [
-                f'Warning: Expected "{col_type}" column, "{i}",'
-                f" but it could not be found in the data schema."
-                for col_type in col_type_attr_map
-                for i in missing_cols
-                if col_type == "ignore"
-            ]
-        )
+        """Test Logistic Regression serialization."""
+        model = LogisticRegressionClassifier(
+            datastructure=datastructure, schema=BitfountSchema()
+        )
+        schema = model.get_schema()
+        serialized_model = schema().dump(model)
+        deserialized_model = schema().load(serialized_model)
+        assert_vars_equal(vars(model), vars(deserialized_model))
+
+    def test_regboost_regressor_serialization(
+        self, datasource: DataSource, datastructure: DataStructure
+    ) -> None:
+        """Test RegBoost serialization."""
+        model = RegBoostRegressor(datastructure=datastructure, schema=BitfountSchema())
+        schema = model.get_schema()
+        serialized_model = schema().dump(model)
+        deserialized_model = schema().load(serialized_model)
+        assert_vars_equal(vars(model), vars(deserialized_model))
```

### Comparing `bitfount-0.5.86/tests/bitfount/federated/aggregators/util.py` & `bitfount-0.5.9/tests/bitfount/federated/aggregators/util.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,26 +1,22 @@
 """Utility functions for aggregator tests."""
 from typing import Mapping, Union
 
 import numpy as np
 from numpy.testing import assert_array_equal
 
-from bitfount.backends.pytorch.types import _AdaptorForPyTorchTensor
-from bitfount.types import _Weights
+from bitfount.types import _WeightMapping
 
 
 def assert_equal_weight_dicts(
-    a: Union[_Weights, Mapping[str, np.ndarray]],
-    b: Union[_Weights, Mapping[str, np.ndarray]],
+    a: Union[_WeightMapping, Mapping[str, np.ndarray]],
+    b: Union[_WeightMapping, Mapping[str, np.ndarray]],
 ) -> None:
     """Checks if two WeightDicts are equal.
 
     Check keys match and array values match.
     """
     # Check keys
     assert a.keys() == b.keys()
     # Check arrays
     for key, value in a.items():
-        if isinstance(value, _AdaptorForPyTorchTensor):
-            assert_array_equal(np.asarray(value.torchtensor), np.asarray(b[key]))
-        else:
-            assert_array_equal(np.asarray(value), np.asarray(b[key]))
+        assert_array_equal(np.asarray(value), np.asarray(b[key]))
```

### Comparing `bitfount-0.5.86/tests/bitfount/federated/algorithms/model_algorithms/test_train_and_evaluate.py` & `bitfount-0.5.9/tests/bitfount/federated/algorithms/model_algorithms/test_training.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,130 +1,103 @@
-"""Tests for the model training and evaluation algorithm."""
+"""Tests for the federated model training algorithm."""
 from typing import TYPE_CHECKING
 from unittest.mock import Mock, create_autospec
 
 from pytest_mock import MockerFixture
 
-from bitfount.backends.pytorch.models.models import PyTorchTabularClassifier
-from bitfount.data.schema import BitfountSchema
 from bitfount.federated.algorithms.base import (
-    BaseModellerAlgorithm,
-    BaseWorkerAlgorithm,
     _BaseAlgorithm,
+    _BaseModellerAlgorithm,
+    _BaseWorkerAlgorithm,
 )
-from bitfount.federated.algorithms.model_algorithms import train_and_evaluate
 from bitfount.federated.algorithms.model_algorithms.base import (
     _BaseModelAlgorithm,
     _BaseModellerModelAlgorithm,
     _BaseWorkerModelAlgorithm,
 )
 from bitfount.federated.algorithms.model_algorithms.federated_training import (
     FederatedModelTraining,
-)
-from bitfount.federated.algorithms.model_algorithms.train_and_evaluate import (
-    ModelTrainingAndEvaluation,
+    _BaseModelTrainingMixIn,
     _ModellerSide,
     _WorkerSide,
 )
-from bitfount.federated.utils import _ALGORITHMS
 from bitfount.hub import BitfountHub
-from bitfount.schemas.utils import bf_dump, bf_load
-from tests.bitfount.backends.pytorch.models.test_models import assert_vars_equal
-from tests.utils.helper import backend_test, create_datastructure, unit_test
+from tests.utils.helper import unit_test
 
 
-class TestModelTrainingAndEvaluation:
-    """Test Remote Model Training And Evaluation algorithm."""
+class TestFederatedModelTraining:
+    """Test Federated Model Training algorithm."""
 
     @unit_test
     def test_modeller(self, model: Mock) -> None:
         """Test modeller method."""
-        algorithm_factory = ModelTrainingAndEvaluation(model=model)
+        algorithm_factory = FederatedModelTraining(model=model)
         algorithm = algorithm_factory.modeller()
         for type_ in [
             _BaseAlgorithm,
-            BaseModellerAlgorithm,
+            _BaseModellerAlgorithm,
             _BaseModelAlgorithm,
             _BaseModellerModelAlgorithm,
         ]:
             assert isinstance(algorithm, type_)
-
-    @unit_test
-    def test_worker_run(self, mocker: MockerFixture, model: Mock) -> None:
-        """Tests that worker run does metric calculation."""
-        worker = train_and_evaluate._WorkerSide(model=model)
-        mock_metrics = Mock()
-        mocker.patch(
-            "bitfount.metrics.MetricCollection.create_from_model", mock_metrics
-        )
-        worker.run(data=Mock())
-        model.fit.assert_called_once()
-        model.evaluate.assert_called_once()
-        mock_metrics.assert_called_once()
-
-    @unit_test
-    def test_modeller_run(self, model: Mock) -> None:
-        """Tests that modeller run returns results."""
-        modeller = train_and_evaluate._ModellerSide(model=model)
-        results = {"pod1": {"AUC": 0.5}}
-        assert results == modeller.run(results=results)
+        assert algorithm.steps == 10
 
     @unit_test
     def test_worker(self, model: Mock) -> None:
         """Test worker method."""
-        algorithm_factory = ModelTrainingAndEvaluation(model=model)
+        algorithm_factory = FederatedModelTraining(model=model)
         algorithm = algorithm_factory.worker(
             hub=create_autospec(BitfountHub, instance=True)
         )
         for type_ in [
             _BaseAlgorithm,
-            BaseWorkerAlgorithm,
+            _BaseWorkerAlgorithm,
             _BaseModelAlgorithm,
             _BaseWorkerModelAlgorithm,
         ]:
             assert isinstance(algorithm, type_)
+        assert algorithm.steps == 10
 
-
-@backend_test
-@unit_test
-class TestMarshmallowSerialization:
-    """Test Marshmallow Serialization for FederatedModelTraining algorithm."""
-
-    def test_serialization(self) -> None:
-        """Test Marshmallow Serialization for FederatedModelTraining algorithm."""
-        model = PyTorchTabularClassifier(
-            datastructure=create_datastructure(),
-            schema=BitfountSchema(),
-            epochs=2,
+    @unit_test
+    def test_modeller_logs_validation_metrics(
+        self, mocker: MockerFixture, model: Mock
+    ) -> None:
+        """Test that the validation metrics are logged for the modeller."""
+        mocker.patch.object(
+            _BaseModelTrainingMixIn,
+            "get_param_states",
+            return_value={"layer1": [0, 1, 2, 3]},
         )
+        mock_model_log = Mock()
+        mocker.patch.object(model, "log_", mock_model_log)
+
         algorithm_factory = FederatedModelTraining(model=model)
-        dumped = bf_dump(algorithm_factory)
-        loaded = bf_load(dumped, _ALGORITHMS)
+        algorithm = algorithm_factory.modeller()
 
-        assert algorithm_factory.class_name == loaded.class_name
-        assert_vars_equal(vars(algorithm_factory.model), vars(loaded.model))
+        algorithm.run(validation_metrics={"AUC": 0.8})
+        mock_model_log.assert_called()
 
 
 # Static tests for algorithm-protocol compatibility
 if TYPE_CHECKING:
     from typing import cast
 
-    from bitfount.federated.protocols.results_only import (
-        _ResultsOnlyCompatibleModelAlgoFactory,
-        _ResultsOnlyCompatibleModellerAlgorithm,
-        _ResultsOnlyDataCompatibleWorkerAlgorithm,
+    from bitfount.federated.protocols.fed_avg import (
+        _FederatedAveragingCompatibleAlgoFactory,
+        _FederatedAveragingCompatibleModeller,
+        _FederatedAveragingCompatibleWorker,
     )
     from bitfount.types import (
         DistributedModelProtocol,
         _DistributedModelTypeOrReference,
     )
 
-    # Check compatible with ResultsOnly
-    _algo_factory: _ResultsOnlyCompatibleModelAlgoFactory = ModelTrainingAndEvaluation(
+    # Check compatible with FederatedAveraging
+    _algo_factory: _FederatedAveragingCompatibleAlgoFactory = FederatedModelTraining(
         model=cast(_DistributedModelTypeOrReference, object())
     )
-    _modeller_side: _ResultsOnlyCompatibleModellerAlgorithm = _ModellerSide(
+    _modeller_side: _FederatedAveragingCompatibleModeller = _ModellerSide(
         model=cast(DistributedModelProtocol, object())
     )
-    _worker_side: _ResultsOnlyDataCompatibleWorkerAlgorithm = _WorkerSide(
+    _worker_side: _FederatedAveragingCompatibleWorker = _WorkerSide(
         model=cast(DistributedModelProtocol, object())
     )
```

### Comparing `bitfount-0.5.86/tests/bitfount/federated/algorithms/model_algorithms/test_training.py` & `bitfount-0.5.9/tests/bitfount/federated/test_helper.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,217 +1,206 @@
-"""Tests for the federated model training algorithm."""
-import os
-import tempfile
-from typing import TYPE_CHECKING
+"""Tests federated helper.py."""
+from typing import Optional, cast
 from unittest.mock import Mock, create_autospec
 
-import numpy as np
+from _pytest.logging import LogCaptureFixture
+import pytest
+from pytest import fixture
+from pytest_lazyfixture import lazy_fixture
 from pytest_mock import MockerFixture
 
-from bitfount import BITFOUNT_LOGS_DIR, BitfountSchema
-from bitfount.backends.pytorch.models.models import PyTorchTabularClassifier
-from bitfount.federated.algorithms.base import (
-    BaseModellerAlgorithm,
-    BaseWorkerAlgorithm,
-    _BaseAlgorithm,
-)
-from bitfount.federated.algorithms.model_algorithms.base import (
-    _BaseModelAlgorithm,
-    _BaseModellerModelAlgorithm,
-    _BaseWorkerModelAlgorithm,
-)
+from bitfount.data.schema import BitfountSchema
+from bitfount.federated.aggregators.aggregator import Aggregator
+from bitfount.federated.aggregators.secure import SecureAggregator
 from bitfount.federated.algorithms.model_algorithms.federated_training import (
     FederatedModelTraining,
-    _BaseModelTrainingMixIn,
-    _ModellerSide,
-    _WorkerSide,
 )
-from bitfount.federated.model_reference import BitfountModelReference
-from bitfount.federated.utils import _ALGORITHMS
-from bitfount.hub import BitfountHub
-from bitfount.schemas.utils import bf_dump, bf_load
-from tests.bitfount.backends.pytorch.models.test_models import assert_vars_equal
-from tests.utils.helper import (
-    backend_test,
-    create_datastructure,
-    create_schema,
-    unit_test,
+from bitfount.federated.helper import (
+    _create_aggregator,
+    _create_and_connect_pod_mailbox,
+    _create_federated_averaging_protocol_factory,
+    _create_message_service,
 )
+from bitfount.federated.protocols.fed_avg import FederatedAveraging
+from bitfount.federated.transport.config import (
+    _DEV_MESSAGE_SERVICE_PORT,
+    _DEV_MESSAGE_SERVICE_TLS,
+    _DEV_MESSAGE_SERVICE_URL,
+    _STAGING_MESSAGE_SERVICE_URL,
+    PRODUCTION_MESSAGE_SERVICE_URL,
+    MessageServiceConfig,
+)
+from bitfount.federated.transport.message_service import _MessageService
+from bitfount.federated.transport.pod_transport import _PodMailbox
+from bitfount.hub.authentication_flow import BitfountSession
+from tests.utils.helper import create_schema, get_warning_logs, unit_test
 
 
-class TestFederatedModelTraining:
-    """Test Federated Model Training algorithm."""
+@unit_test
+class TestHelperFunctions:
+    """Test federated helper functions."""
 
-    @unit_test
-    def test_modeller(self, model: Mock) -> None:
-        """Test modeller method."""
-        algorithm_factory = FederatedModelTraining(model=model)
-        algorithm = algorithm_factory.modeller()
-        for type_ in [
-            _BaseAlgorithm,
-            BaseModellerAlgorithm,
-            _BaseModelAlgorithm,
-            _BaseModellerModelAlgorithm,
-        ]:
-            assert isinstance(algorithm, type_)
-        if not isinstance(model, BitfountModelReference):
-            assert algorithm.steps == 10
-
-    @unit_test
-    def test_worker(self, model: Mock) -> None:
-        """Test worker method."""
-        algorithm_factory = FederatedModelTraining(model=model)
-        algorithm = algorithm_factory.worker(
-            hub=create_autospec(BitfountHub, instance=True)
-        )
-        for type_ in [
-            _BaseAlgorithm,
-            BaseWorkerAlgorithm,
-            _BaseModelAlgorithm,
-            _BaseWorkerModelAlgorithm,
-        ]:
-            assert isinstance(algorithm, type_)
-        if not isinstance(model, BitfountModelReference):
-            assert algorithm.steps == 10
-
-    @unit_test
-    def test_modeller_logs_validation_metrics(
-        self, mocker: MockerFixture, model: Mock
+    @fixture
+    def test_url(self) -> str:
+        """Test URL."""
+        return "not.a.real.url.com"
+
+    @fixture
+    def schema(self) -> BitfountSchema:
+        """Creates a schema fixture."""
+        return create_schema(classification=True)
+
+    def test_create_insecure_aggregator_successfully(self) -> None:
+        """Tests create_aggregator function without secure aggregation."""
+        model = Mock()  # Mock() is inherently instance of DistributedModelProtocol
+        aggregator = _create_aggregator(model=model, secure_aggregation=False)
+        assert isinstance(aggregator, Aggregator)
+        assert not isinstance(aggregator, SecureAggregator)
+
+    def test_create_secure_aggregator_successfully(self) -> None:
+        """Tests create_aggregator function with secure aggregation."""
+        model = Mock()  # Mock() is inherently instance of DistributedModelProtocol
+        aggregator = _create_aggregator(model=model, secure_aggregation=True)
+        assert isinstance(aggregator, SecureAggregator)
+
+    @pytest.mark.parametrize(
+        "environment, expected_config, input_config",
+        [
+            (
+                "production",
+                MessageServiceConfig(url=PRODUCTION_MESSAGE_SERVICE_URL),
+                None,
+            ),
+            ("staging", MessageServiceConfig(url=_STAGING_MESSAGE_SERVICE_URL), None),
+            (
+                "dev",
+                MessageServiceConfig(
+                    url=_DEV_MESSAGE_SERVICE_URL,
+                    port=_DEV_MESSAGE_SERVICE_PORT,
+                    tls=_DEV_MESSAGE_SERVICE_TLS,
+                ),
+                None,
+            ),
+            (
+                None,
+                MessageServiceConfig(url=cast(str, lazy_fixture("test_url"))),
+                MessageServiceConfig(url=cast(str, lazy_fixture("test_url"))),
+            ),
+        ],
+        indirect=["environment"],
+    )
+    def test_create_message_service_successfully(
+        self,
+        environment: None,
+        expected_config: MessageServiceConfig,
+        input_config: Optional[MessageServiceConfig],
     ) -> None:
-        """Test that the validation metrics are logged for the modeller."""
-        mocker.patch.object(
-            _BaseModelTrainingMixIn,
-            "get_param_states",
-            return_value={"layer1": np.ndarray([0, 1, 2, 3])},
-        )
-        mock_model_log = Mock()
-        if not isinstance(model, BitfountModelReference):
-            mocker.patch.object(model, "log_", mock_model_log)
-
-        algorithm_factory = FederatedModelTraining(
-            model=model, modeller_checkpointing=False
-        )
-        algorithm = algorithm_factory.modeller()
-        algorithm.run(validation_metrics={"AUC": 0.8})
-        if not isinstance(model, BitfountModelReference):
-            mock_model_log.assert_called()
-
-    @unit_test
-    def test_algorithm_serialize(self, mocker: MockerFixture, model: Mock) -> None:
-        """Test that the validation metrics are logged for the modeller."""
-        mocker.patch.object(
-            _BaseModelTrainingMixIn,
-            "get_param_states",
-            return_value={"layer1": np.ndarray([0, 1, 2, 3])},
-        )
-        mock_model_log = Mock()
-        if not isinstance(model, BitfountModelReference):
-            mocker.patch.object(model, "log_", mock_model_log)
-
-        algorithm_factory = FederatedModelTraining(
-            model=model, checkpoint_filename=tempfile.mkstemp()  # type: ignore[arg-type] # Reason: using tempdir for testing # noqa: B950
-        )
-
-        algorithm = algorithm_factory.modeller()
-        algorithm.run(validation_metrics={"AUC": 0.8})
-        if not isinstance(model, BitfountModelReference):
-            mock_model_log.assert_called()
+        """Tests create_message_service method works.
 
-    @unit_test
-    def test_algorithm_removes_first_iteration_checkpoint_file(
-        self, mocker: MockerFixture
+        Ensures correct config is passed through to the created message service.
+        """
+        message_service = _create_message_service(
+            session=create_autospec(BitfountSession, instance=True),
+            ms_config=input_config,
+        )
+        assert isinstance(message_service, _MessageService)
+        assert message_service.config == expected_config
+
+    def test_create_message_service_issues_warning_for_local_storage(
+        self, caplog: LogCaptureFixture
     ) -> None:
-        """Test that the checkpoints are saved and removed correctly."""
-        mocker.patch.object(
-            _BaseModelTrainingMixIn,
-            "get_param_states",
-            return_value={"layer1": np.ndarray([0, 1, 2, 3])},
-        )
-        model = PyTorchTabularClassifier(
-            datastructure=create_datastructure(),
-            schema=create_schema(classification=True),
-            epochs=1,
-        )
-        mock_model_log = Mock()
-        mocker.patch.object(model, "log_", mock_model_log)
-        checkpoint_filename = "mock_checkpoint"
-        algorithm_factory = FederatedModelTraining(
-            model=model, checkpoint_filename=checkpoint_filename
-        )
-
-        algorithm = algorithm_factory.modeller()
-        algorithm.run(validation_metrics={"AUC": 0.8})
-        assert os.path.isfile(
-            f"{BITFOUNT_LOGS_DIR}/{checkpoint_filename}-iteration-0.pt"
-        )
-        algorithm.run(validation_metrics={"AUC": 0.8}, iteration=1)
-        assert not os.path.isfile(
-            f"{BITFOUNT_LOGS_DIR}/{checkpoint_filename}-iteration-0.pt"
-        )
-        assert os.path.isfile(
-            f"{BITFOUNT_LOGS_DIR}/{checkpoint_filename}-iteration-1.pt"
-        )
-        os.remove(f"{BITFOUNT_LOGS_DIR}/{checkpoint_filename}-iteration-1.pt")
-
-    @unit_test
-    def test_worker_run_calls_diff_params(
-        self, mocker: MockerFixture, model: Mock
+        """Tests create_message_service issues warning if using local storage."""
+        ms_config = MessageServiceConfig(use_local_storage=True)
+        message_service = _create_message_service(
+            session=create_autospec(BitfountSession, instance=True),
+            ms_config=ms_config,
+        )
+        assert isinstance(message_service, _MessageService)
+
+        # Check for warning message; should be thing logged
+        warning_logs = get_warning_logs(caplog)
+        assert (
+            "Messages will contain local file references. "
+            "Ensure all pods have access to your local file system. "
+            "Otherwise your task will hang." in warning_logs
+        )
+
+    async def test_create_and_connect_pod_mailbox_successfully(
+        self, mocker: MockerFixture
     ) -> None:
-        """Test worker run method calls diff_params.
+        """Tests create_and_connect_pod_mailbox method works."""
+        # We know that `create_message_service` is already tested so we mock out
+        # its response and check it is called correctly.
+        mock_create_message_service = mocker.patch(
+            "bitfount.federated.helper._create_message_service"
+        )
 
-        Test we return the difference between the old and new model parameters.
-        """
-        algorithm_factory = FederatedModelTraining(model=model)
-        algorithm = algorithm_factory.worker(
-            hub=create_autospec(BitfountHub, instance=True)
-        )
-        mock_diff_params = mocker.patch.object(algorithm, "diff_params")
-        data = Mock()
-        serialized_model_params = Mock()
-        algorithm.run(data, serialized_model_params, iterations=1)
-        mock_diff_params.assert_called_once()
+        # We also need to mock out the PodMailbox.connect_pod() class method.
+        mock_connect_pod = mocker.patch.object(
+            _PodMailbox, "connect_pod", autospec=True
+        )
+        mock_connect_pod.return_value = create_autospec(_PodMailbox, instance=True)
 
+        # Create mocks for args we don't care about
+        pod_name = "not-a-real-pod"
+        mock_session = create_autospec(BitfountSession, instance=True)
+        mock_ms_config = create_autospec(MessageServiceConfig, instance=True)
 
-@backend_test
-@unit_test
-class TestMarshmallowSerialization:
-    """Test Marshmallow Serialization for column average algorithm."""
+        mailbox = await _create_and_connect_pod_mailbox(
+            pod_name=pod_name,
+            session=mock_session,
+            ms_config=mock_ms_config,
+        )
 
-    def test_serialization(self) -> None:
-        """Test Marshmallow Serialization for column average algorithm."""
-        model = PyTorchTabularClassifier(
-            datastructure=create_datastructure(),
-            schema=BitfountSchema(),
-            epochs=2,
-        )
-        algorithm_factory = FederatedModelTraining(model=model)
-        dumped = bf_dump(algorithm_factory)
-        loaded = bf_load(dumped, _ALGORITHMS)
-
-        assert algorithm_factory.class_name == loaded.class_name
-        assert_vars_equal(vars(algorithm_factory.model), vars(loaded.model))
-
-
-# Static tests for algorithm-protocol compatibility
-if TYPE_CHECKING:
-    from typing import cast
-
-    from bitfount.federated.protocols.model_protocols.federated_averaging import (
-        _FederatedAveragingCompatibleAlgoFactory,
-        _FederatedAveragingCompatibleModeller,
-        _FederatedAveragingCompatibleWorker,
-    )
-    from bitfount.types import (
-        DistributedModelProtocol,
-        _DistributedModelTypeOrReference,
-    )
+        assert isinstance(mailbox, _PodMailbox)
+        mock_connect_pod.assert_called_once_with(
+            pod_name=pod_name, message_service=mock_create_message_service()
+        )
 
-    # Check compatible with FederatedAveraging
-    _algo_factory: _FederatedAveragingCompatibleAlgoFactory = FederatedModelTraining(
-        model=cast(_DistributedModelTypeOrReference, object())
-    )
-    _modeller_side: _FederatedAveragingCompatibleModeller = _ModellerSide(
-        model=cast(DistributedModelProtocol, object())
-    )
-    _worker_side: _FederatedAveragingCompatibleWorker = _WorkerSide(
-        model=cast(DistributedModelProtocol, object())
-    )
+    @pytest.mark.parametrize("secure_aggregation", [True, False])
+    def test_create_federated_averaging_protocol_factory_successfully(
+        self, schema: BitfountSchema, secure_aggregation: bool
+    ) -> None:
+        """Tests that create_federated_averaging_protocol_factory works as expected."""
+        model = Mock(epochs=1)
+
+        algorithm = FederatedModelTraining(model=model)
+
+        aggregator = _create_aggregator(
+            model=algorithm.model, secure_aggregation=secure_aggregation
+        )
+        protocol = _create_federated_averaging_protocol_factory(
+            protocol_cls=FederatedAveraging,
+            algorithm=algorithm,
+            aggregator=aggregator,
+            steps_between_parameter_updates=None,
+            epochs_between_parameter_updates=2,
+            auto_eval=False,
+        )
+        assert isinstance(protocol, FederatedAveraging)
+        assert protocol.epochs_between_parameter_updates == 2
+        assert protocol.steps_between_parameter_updates is None
+
+    @pytest.mark.parametrize("secure_aggregation", [True, False])
+    def test_create_federated_averaging_protocol_factory_iterations_not_provided(
+        self, schema: BitfountSchema, secure_aggregation: bool
+    ) -> None:
+        """Tests create_federated_averaging_protocol_factory with no iteration provided.
+
+        Tests that iterations is set to a default value if no value is provided.
+        """
+        model = Mock(epochs=1)
+        algorithm = FederatedModelTraining(model=model)
+        aggregator = _create_aggregator(
+            model=algorithm.model, secure_aggregation=secure_aggregation
+        )
+        protocol = _create_federated_averaging_protocol_factory(
+            protocol_cls=FederatedAveraging,
+            algorithm=algorithm,
+            aggregator=aggregator,
+            steps_between_parameter_updates=None,
+            epochs_between_parameter_updates=None,
+            auto_eval=False,
+        )
+        assert isinstance(protocol, FederatedAveraging)
+        assert protocol.steps_between_parameter_updates == 1
+        assert protocol.epochs_between_parameter_updates is None
```

### Comparing `bitfount-0.5.86/tests/bitfount/federated/privacy/test_differential.py` & `bitfount-0.5.9/tests/bitfount/federated/privacy/test_differential.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,107 +1,97 @@
 """Tests for differential privacy module."""
 import copy
 import logging
+from typing import Any, Dict
 
+from _pytest.logging import LogCaptureFixture
 import desert
 import marshmallow
-import numpy as np
 import pytest
-from pytest import LogCaptureFixture, fixture
+from pytest import fixture
 
-from bitfount.federated.exceptions import DPParameterError
 from bitfount.federated.privacy.differential import (
     DPModellerConfig,
     DPPodConfig,
     _DifferentiallyPrivate,
 )
-from bitfount.types import _StrAnyDict
-from tests.utils.helper import dp_test
+from tests.utils.helper import unit_test
 
 
 @fixture
-def config_dict() -> _StrAnyDict:
+def config_dict() -> Dict[str, Any]:
     """Configuration dictionary for Differential Privacy settings."""
     return {
         "max_grad_norm": 1.0,
         "noise_multiplier": 0.0,
-        "alphas": [0.0, 1.1],
-        "delta": 0.01,
+        "alphas": [0.0, 1.0],
+        "target_delta": 0.01,
         "loss_reduction": "sum",
-        "epsilon": 10.0,
+        "max_epsilon": 10.0,
         "auto_fix": False,
     }
 
 
-@dp_test
+@unit_test
 class TestDPModellerConfig:
     """Tests for differential privacy config dataclass."""
 
     def test_loss_reduction_validation(self) -> None:
         """Tests config rejects invalid loss_reduction values."""
         with pytest.raises(ValueError, match="loss_reduction must be one of"):
             # noinspection PyTypeChecker
-            DPModellerConfig(epsilon=1.0, loss_reduction="test")  # type: ignore[arg-type] # Reason: purpose of test # noqa: B950
+            DPModellerConfig(max_epsilon=1.0, loss_reduction="test")  # type: ignore[arg-type] # Reason: purpose of test # noqa: B950
 
-    def test_schema_loads(self, config_dict: _StrAnyDict) -> None:
+    def test_schema_loads(self, config_dict: Dict[str, Any]) -> None:
         """Tests config loading from data."""
         schema = desert.schema(DPModellerConfig)
         test_config = schema.load(config_dict)
 
         # Assertions for correct conversion
         assert test_config.noise_multiplier == config_dict["noise_multiplier"]
         assert test_config.max_grad_norm == config_dict["max_grad_norm"]
         assert test_config.alphas == config_dict["alphas"]
-        assert test_config.delta == config_dict["delta"]
+        assert test_config.target_delta == config_dict["target_delta"]
         assert test_config.loss_reduction == config_dict["loss_reduction"]
-        assert test_config.epsilon == config_dict["epsilon"]
+        assert test_config.max_epsilon == config_dict["max_epsilon"]
         assert test_config.auto_fix == config_dict["auto_fix"]
 
     def test_schema_validation_failures(self) -> None:
         """Tests that schema validation catches incorrect options."""
         schema = desert.schema(DPModellerConfig)
         with pytest.raises(marshmallow.exceptions.ValidationError):
             schema.load({"loss_reduction": "test"})
 
-    def test_error_alpha_1(self) -> None:
-        """Tests error is raised when one alpha is 1."""
-        with pytest.raises(DPParameterError):
-            DPModellerConfig(
-                epsilon=1,
-                delta=1e-6,
-                alphas=list(np.linspace(1.0, 440.0, 440)),
-            )
 
-
-@dp_test
+@unit_test
 class TestDifferentiallyPrivate:
     """Tests related to the DifferentiallyPrivate class."""
 
     @fixture
     def dp_config(self) -> DPModellerConfig:
         """Creates a DPConfig instance."""
-        return DPModellerConfig(epsilon=10.0, delta=1.0)
+        return DPModellerConfig(max_epsilon=10.0, target_delta=1.0)
 
     @fixture
     def pod_dp_config(self) -> DPPodConfig:
         """Creates a DPPodConfig instance."""
-        return DPPodConfig(epsilon=1.0, delta=0.5)
+        return DPPodConfig(max_epsilon=1.0, max_target_delta=0.5)
 
-    def test__convert_to_dpconfig(self, config_dict: _StrAnyDict) -> None:
+    def test__convert_to_dpconfig(self, config_dict: Dict[str, Any]) -> None:
         """Tests the conversion from dict to DPConfig."""
         test_config = _DifferentiallyPrivate._convert_to_dpconfig(config_dict)
 
         # Assertions for correct conversion
         assert test_config is not None
         assert test_config.noise_multiplier == config_dict["noise_multiplier"]
         assert test_config.max_grad_norm == config_dict["max_grad_norm"]
         assert test_config.alphas == config_dict["alphas"]
-        assert test_config.delta == config_dict["delta"]
+        assert test_config.target_delta == config_dict["target_delta"]
         assert test_config.loss_reduction == config_dict["loss_reduction"]
-        assert test_config.epsilon == config_dict["epsilon"]
+        assert test_config.max_epsilon == config_dict["max_epsilon"]
         assert test_config.auto_fix == config_dict["auto_fix"]
 
     def test__convert_to_dpconfig_with_none(self) -> None:
         """Tests that None is returned if None supplied."""
         assert None is _DifferentiallyPrivate._convert_to_dpconfig(None)
 
     def test_config_settings_logged(
@@ -129,15 +119,15 @@
     ) -> None:
         """Tests apply_pod_dp causes no changes if no pod dp."""
         caplog.set_level(logging.INFO)
         orig_dp_config = copy.deepcopy(dp_config)
         dp = _DifferentiallyPrivate(dp_config)
         dp.apply_pod_dp(None)
 
-        assert vars(dp._dp_config) == vars(orig_dp_config)  # hasn't changed
+        assert dp._dp_config == orig_dp_config  # hasn't changed
         assert "No pod DP preferences, using modeller preferences." in caplog.text
 
     def test_apply_pod_dp_caps_values(
         self,
         caplog: LogCaptureFixture,
         dp_config: DPModellerConfig,
         pod_dp_config: DPPodConfig,
@@ -147,25 +137,25 @@
         dp = _DifferentiallyPrivate(dp_config)
         dp.apply_pod_dp(pod_dp_config)
 
         # Check config applied
         assert dp._dp_config is not None
 
         # Check caps are applied
-        assert dp._dp_config.epsilon == pod_dp_config.epsilon
-        assert dp._dp_config.delta == pod_dp_config.delta
+        assert dp._dp_config.max_epsilon == pod_dp_config.max_epsilon
+        assert dp._dp_config.target_delta == pod_dp_config.max_target_delta
 
         # Check these are different from the supplied ones
-        assert dp._dp_config.epsilon != orig_dp_config.epsilon
-        assert dp._dp_config.delta != orig_dp_config.delta
+        assert dp._dp_config.max_epsilon != orig_dp_config.max_epsilon
+        assert dp._dp_config.target_delta != orig_dp_config.target_delta
 
         # Check these changes are logged
         assert (
-            f"Requested DP max epsilon ({orig_dp_config.epsilon}) exceeds "
+            f"Requested DP max epsilon ({orig_dp_config.max_epsilon}) exceeds "
             f"maximum value allowed by pod. Using pod max of "
-            f"{pod_dp_config.epsilon}." in caplog.text
+            f"{pod_dp_config.max_epsilon}." in caplog.text
         )
         assert (
-            f"Requested DP target delta ({orig_dp_config.delta}) exceeds "
+            f"Requested DP target delta ({orig_dp_config.target_delta}) exceeds "
             f"maximum value allowed by pod. Using pod max of "
-            f"{pod_dp_config.delta}." in caplog.text
+            f"{pod_dp_config.max_target_delta}." in caplog.text
         )
```

### Comparing `bitfount-0.5.86/tests/bitfount/federated/protocols/conftest.py` & `bitfount-0.5.9/tests/bitfount/federated/protocols/conftest.py`

 * *Files identical despite different names*

### Comparing `bitfount-0.5.86/tests/bitfount/federated/protocols/test_fed_avg.py` & `bitfount-0.5.9/tests/bitfount/federated/protocols/test_fed_avg.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,175 +1,160 @@
 """Tests for the federated averaging protocol."""
 from pathlib import Path
 import re
-from typing import Dict, Tuple, cast
+from typing import Tuple
 from unittest import mock
 from unittest.mock import ANY, AsyncMock, MagicMock, Mock, create_autospec
 
+from _pytest.logging import LogCaptureFixture
 import pytest
-from pytest import LogCaptureFixture, fixture
+from pytest import fixture
 from pytest_mock import MockerFixture
 
-from bitfount import BitfountModelReference, BitfountSchema
-from bitfount.backends.pytorch import PyTorchTabularClassifier
-from bitfount.data.datasources.base_source import BaseSource
+from bitfount.data import DataSource
 import bitfount.federated.aggregators
 import bitfount.federated.aggregators.aggregator
 from bitfount.federated.aggregators.aggregator import Aggregator
 from bitfount.federated.aggregators.base import _AggregatorWorkerFactory
 from bitfount.federated.aggregators.secure import _InterPodAggregatorWorkerFactory
 from bitfount.federated.algorithms.model_algorithms import federated_training
 from bitfount.federated.algorithms.model_algorithms.federated_training import (
     FederatedModelTraining,
 )
-from bitfount.federated.helper import _create_aggregator
-from bitfount.federated.modeller import _Modeller
+from bitfount.federated.protocols import fed_avg
 from bitfount.federated.protocols.base import (
-    BaseModellerProtocol,
-    BaseWorkerProtocol,
+    _BaseModellerProtocol,
     _BaseProtocol,
+    _BaseWorkerProtocol,
 )
-import bitfount.federated.protocols.base as protocols
-from bitfount.federated.protocols.model_protocols import federated_averaging
-from bitfount.federated.protocols.model_protocols.federated_averaging import (
+from bitfount.federated.protocols.fed_avg import (
     FederatedAveraging,
     _FederatedAveragingCompatibleAlgoFactory,
-    _ModellerSide,
-    _WorkerSide,
 )
 from bitfount.federated.transport.modeller_transport import _ModellerMailbox
-from bitfount.federated.transport.worker_transport import (
-    _InterPodWorkerMailbox,
-    _WorkerMailbox,
-)
-from bitfount.hooks import _HOOK_DECORATED_ATTRIBUTE
-from bitfount.schemas.utils import bf_dump, bf_load
-from bitfount.types import _JSONDict
-from tests.bitfount.backends.pytorch.models.test_models import assert_vars_equal
-from tests.utils import PytestRequest
-from tests.utils.helper import backend_test, create_datastructure, unit_test
+from bitfount.federated.transport.worker_transport import _WorkerMailbox
+from tests.bitfount import TEST_SECURITY_FILES
+from tests.utils.helper import unit_test
 
 
 def mocked_modeller_runner_functions(
     mocker: MockerFixture,
 ) -> Tuple[Aggregator, AsyncMock, Mock]:
     """Mock functions used in protocol.modeller.run."""
-    mocker.patch.object(federated_averaging._ModellerSide, "perform_iterations_checks")
+    mocker.patch.object(fed_avg._ModellerSide, "perform_iterations_checks")
 
     mock_algorithm_run = Mock()
     mocker.patch.object(
         federated_training._ModellerSide,
         "run",
         mock_algorithm_run,
     )
 
     mocker.patch.object(
-        federated_averaging._ModellerSide,
+        fed_avg._ModellerSide,
         "_send_parameters",
     )
 
     mocker.patch.object(
-        federated_averaging._ModellerSide,
+        fed_avg._ModellerSide,
         "_receive_parameter_updates",
         return_value=["some params"],
     )
 
     mocker.patch.object(
-        federated_averaging._ModellerSide,
+        fed_avg._ModellerSide,
         "get_num_federated_iterations",
         return_value=2,
     )
 
     mock_get_training_metrics = AsyncMock()
     mocker.patch.object(
-        federated_averaging._ModellerSide,
+        fed_avg._ModellerSide,
         "_get_training_metrics_updates",
         mock_get_training_metrics,
     )
     mock_get_training_metrics.return_value = {"AUC": "0.7"}
 
     mock_aggregator_run = Mock()
 
     mocker.patch.object(
         bitfount.federated.aggregators.aggregator._ModellerSide,
         "run",
         mock_aggregator_run,
     )
     mock_aggregator_run.return_value = {"some_weight": "some_value"}
-    mocked_aggregator_factory = Aggregator()
+    mocked_aggregator_factory = Aggregator(tensor_shim=AsyncMock())
 
     mock_task_complete = MagicMock()
     mock_task_complete.return_value.set_result(True)
     mocker.patch.object(
         _ModellerMailbox, "send_training_iteration_complete_update", mock_task_complete
     )
     mocker.patch.object(_ModellerMailbox, "send_task_complete_message")
     return mocked_aggregator_factory, mock_get_training_metrics, mock_algorithm_run
 
 
 def mocked_worker_runner_functions(
     mocker: MockerFixture,
 ) -> Tuple[Aggregator, AsyncMock]:
     """Mock functions used in protocol.worker.run."""
-    mocker.patch.object(federated_averaging._WorkerSide, "perform_iterations_checks")
+    mocker.patch.object(fed_avg._WorkerSide, "perform_iterations_checks")
 
     mocker.patch.object(
         federated_training._WorkerSide,
         "run",
         return_value=([1, 2, 3], [{"AUC": "0.7"}]),
     )
 
     mocker.patch.object(
         federated_training._WorkerSide,
         "save_final_parameters",
     )
 
     mocker.patch.object(
-        federated_averaging._WorkerSide,
+        fed_avg._WorkerSide,
         "get_num_federated_iterations",
         return_value=2,
     )
 
     mocker.patch.object(
-        federated_averaging._WorkerSide,
+        fed_avg._WorkerSide,
         "_send_parameter_update",
     )
     mock_send_training_metrics = AsyncMock()
     mocker.patch.object(
-        federated_averaging._WorkerSide,
+        fed_avg._WorkerSide,
         "_send_training_metrics",
         mock_send_training_metrics,
     )
 
     mock_aggregator_run = AsyncMock()
 
     mocker.patch.object(
         bitfount.federated.aggregators.aggregator._WorkerSide,
         "run",
         mock_aggregator_run,
     )
-    mocked_aggregator_factory = Aggregator()
+    mocked_aggregator_factory = Aggregator(tensor_shim=AsyncMock())
 
     mock_task_complete = AsyncMock(return_value=True)
     mocker.patch.object(
         _WorkerMailbox, "get_training_iteration_complete_update", mock_task_complete
     )
     mocker.patch.object(_WorkerMailbox, "get_task_complete_update", mock_task_complete)
     return mocked_aggregator_factory, mock_send_training_metrics
 
 
 class TestFederatedAveraging:
     """Test Federated Averaging protocol."""
 
-    @fixture(scope="function", params=[None, "/mock/file"])
-    def federated_algorithm(self, request: PytestRequest) -> FederatedModelTraining:
+    @fixture
+    def federated_algorithm(self) -> FederatedModelTraining:
         """Returns federated algorithm."""
-        return FederatedModelTraining(
-            model=Mock(), pretrained_file=request.param, modeller_checkpointing=False
-        )
+        return FederatedModelTraining(model=Mock())
 
     @fixture
     def mock_federated_algorithm(self) -> Mock:
         """Returns a mock algorithm compatible with FederatedAveraging."""
         mock_algorithm: Mock = create_autospec(
             _FederatedAveragingCompatibleAlgoFactory, instance=True
         )
@@ -193,53 +178,14 @@
     def mocked_worker_runner_fixture(
         self, mocker: MockerFixture
     ) -> Tuple[Aggregator, AsyncMock]:
         """Fixture for getting the necessary mocks for worker protocol runner."""
         return mocked_worker_runner_functions(mocker)
 
     @unit_test
-    def test_fed_avg_methods_are_decorated_appropriately(self) -> None:
-        """Tests that protocol methods are decorated.
-
-        The `__init__` and `run` methods should be auto-decorated with a function which
-        calls the relevant hooks before and after.
-        """
-        protocol_factory = FederatedAveraging(algorithm=Mock())
-        worker_protocol = protocol_factory.worker(mailbox=Mock(), hub=Mock())
-        modeller_protocol = protocol_factory.modeller(mailbox=Mock())
-        for protocol in (worker_protocol, modeller_protocol):
-            assert isinstance(protocol, (_WorkerSide, _ModellerSide))
-            assert getattr(protocol.__init__, _HOOK_DECORATED_ATTRIBUTE)  # type: ignore[misc] # Reason: This is a test. # noqa: B950
-            assert getattr(protocol.run, _HOOK_DECORATED_ATTRIBUTE)
-
-        # Other methods should not be decorated
-        assert not getattr(
-            worker_protocol._receive_parameters, _HOOK_DECORATED_ATTRIBUTE, False
-        )
-        assert not getattr(
-            worker_protocol._send_parameter_update, _HOOK_DECORATED_ATTRIBUTE, False
-        )
-        assert not getattr(
-            worker_protocol._send_training_metrics, _HOOK_DECORATED_ATTRIBUTE, False
-        )
-        assert not getattr(
-            modeller_protocol._get_training_metrics_updates,
-            _HOOK_DECORATED_ATTRIBUTE,
-            False,
-        )
-        assert not getattr(
-            modeller_protocol._receive_parameter_updates,
-            _HOOK_DECORATED_ATTRIBUTE,
-            False,
-        )
-        assert not getattr(
-            modeller_protocol._send_parameters, _HOOK_DECORATED_ATTRIBUTE, False
-        )
-
-    @unit_test
     def test_algorithm_not_compatible_raises_type_error(
         self,
         mock_aggregator: Mock,
     ) -> None:
         """Check that TypeError is raised if algorithm is not compatible."""
         mock_algorithm: Mock = Mock(spec_set=["__name__"])
         with pytest.raises(
@@ -268,16 +214,16 @@
             aggregator=mock_aggregator,
             steps_between_parameter_updates=2,
         )
         protocol = protocol_factory.modeller(mailbox=mock_modeller_mailbox)
 
         for type_ in [
             _BaseProtocol,
-            BaseModellerProtocol,
-            federated_averaging._ModellerSide,
+            _BaseModellerProtocol,
+            fed_avg._ModellerSide,
         ]:
             assert isinstance(protocol, type_)
 
     @unit_test
     def test_worker(
         self,
         federated_algorithm: FederatedModelTraining,
@@ -291,32 +237,31 @@
             aggregator=mock_aggregator,
             steps_between_parameter_updates=2,
         )
         protocol = protocol_factory.worker(mailbox=mock_worker_mailbox, hub=mock_hub)
 
         for type_ in [
             _BaseProtocol,
-            BaseWorkerProtocol,
-            federated_averaging._WorkerSide,
+            _BaseWorkerProtocol,
+            fed_avg._WorkerSide,
         ]:
             assert isinstance(protocol, type_)
 
     @unit_test
     def test_worker_with_different_aggregator_types(
         self,
         mock_federated_algorithm: Mock,
         mock_hub: Mock,
         mock_worker_mailbox: Mock,
         mocker: MockerFixture,
     ) -> None:
         """Test worker method with different aggregator types."""
         # Mock out WorkerSide constructor
         mock_worker_side_cls = mocker.patch(
-            "bitfount.federated.protocols.model_protocols.federated_averaging._WorkerSide",
-            autospec=True,
+            "bitfount.federated.protocols.fed_avg._WorkerSide", autospec=True
         )
 
         # Test with an instance of AggregatorWorkerFactory
         mock_aggregator: Mock = create_autospec(_AggregatorWorkerFactory, instance=True)
         protocol_factory = FederatedAveraging(
             algorithm=mock_federated_algorithm,
             aggregator=mock_aggregator,
@@ -337,132 +282,142 @@
         mock_aggregator.worker.assert_called_once_with()
 
         # Test with an instance of InterPodAggregatorWorkerFactory
         mock_worker_side_cls.reset_mock()
         mock_aggregator = create_autospec(
             _InterPodAggregatorWorkerFactory, instance=True
         )
-        mock_interpod_worker_mailbox = create_autospec(
-            _InterPodWorkerMailbox, instance=True
-        )
         protocol_factory = FederatedAveraging(
             algorithm=mock_federated_algorithm,
             aggregator=mock_aggregator,
             steps_between_parameter_updates=2,
         )
-        protocol = protocol_factory.worker(
-            mailbox=mock_interpod_worker_mailbox, hub=mock_hub
-        )
+        protocol = protocol_factory.worker(mailbox=mock_worker_mailbox, hub=mock_hub)
         # Check WorkerSide constructed as expected
         assert protocol == mock_worker_side_cls.return_value
         mock_worker_side_cls.assert_called_once_with(
             algorithm=mock_federated_algorithm.worker.return_value,
             aggregator=mock_aggregator.worker.return_value,
             steps_between_parameter_updates=2,
             epochs_between_parameter_updates=None,
             auto_eval=ANY,
-            mailbox=mock_interpod_worker_mailbox,
+            mailbox=mock_worker_mailbox,
         )
         # Check aggregator.worker() called as expected
-        mock_aggregator.worker.assert_called_once_with(
-            mailbox=mock_interpod_worker_mailbox
-        )
+        mock_aggregator.worker.assert_called_once_with(mailbox=mock_worker_mailbox)
 
         # Test with an unknown type of aggregator instance
         mock_aggregator = Mock()
         protocol_factory = FederatedAveraging(
             algorithm=mock_federated_algorithm,
             aggregator=mock_aggregator,
             steps_between_parameter_updates=2,
         )
         with pytest.raises(TypeError, match="Unrecognised aggregator factory"):
             protocol_factory.worker(mailbox=mock_worker_mailbox, hub=mock_hub)
 
     @unit_test
-    def test_helper_run_method_without_algorithm_raises_type_error(
+    def test_helper_run_method_without_model_or_algorithm_raises_value_error(
         self,
     ) -> None:
-        """Tests helper run method without algorithm raises TypeError."""
-        with pytest.raises(TypeError):
-            FederatedAveraging()  # type: ignore[call-arg] # Reason: this is what we are testing for # noqa: B950
+        """Tests helper run method without algorithm or model raises ValueError."""
+        with pytest.raises(ValueError):
+            FederatedAveraging.run(
+                pod_identifiers=["bitfount/prosper", "bitfount/prosper2"],
+                private_key_or_file=TEST_SECURITY_FILES / "test_private.testkey",
+            )
 
     @unit_test
     def test_helper_run_method_with_algorithm(
         self, mock_hub: Mock, mocker: MockerFixture
     ) -> None:
         """Tests helper run method with algorithm."""
         federated_algorithm_with_model = FederatedModelTraining(model=Mock())
-        mock_run_protocol = mocker.patch.object(FederatedAveraging, "run")
+        mock_run_protocol = mocker.patch(
+            "bitfount.federated.protocols.fed_avg._run_protocol"
+        )
         mock_create_aggregator = mocker.patch(
-            "bitfount.federated.protocols.model_protocols.federated_averaging._create_aggregator"
+            "bitfount.federated.protocols.fed_avg._create_aggregator"
+        )
+        mock_algorithm_constructor = mocker.patch(
+            "bitfount.federated.protocols.fed_avg.FederatedModelTraining",
         )
 
-        protocol = FederatedAveraging(algorithm=federated_algorithm_with_model)
-
-        protocol.run(
+        FederatedAveraging.run(
             pod_identifiers=["bitfount/fake", "bitfount/fake2"],
+            algorithm=federated_algorithm_with_model,
             hub=mock_hub,
             private_key_or_file=Path("fake.testkey"),
         )
 
+        mock_algorithm_constructor.assert_not_called()
         mock_run_protocol.assert_called_once()
         mock_create_aggregator.assert_called()
 
     @unit_test
     def test_helper_run_method_with_model(
         self, mock_hub: Mock, mocker: MockerFixture
     ) -> None:
         """Tests helper run method with a model."""
         federated_model = Mock()
-        mock_run_protocol = mocker.patch.object(FederatedAveraging, "run")
+
+        mock_run_protocol = mocker.patch(
+            "bitfount.federated.protocols.fed_avg._run_protocol"
+        )
         mock_create_aggregator = mocker.patch(
-            "bitfount.federated.protocols.model_protocols.federated_averaging._create_aggregator"
+            "bitfount.federated.protocols.fed_avg._create_aggregator"
         )
-        protocol = FederatedAveraging(
-            algorithm=FederatedModelTraining(model=federated_model)
+        mock_algorithm_constructor = mocker.patch(
+            "bitfount.federated.protocols.fed_avg.FederatedModelTraining",
+            return_value=FederatedModelTraining(model=Mock()),
         )
 
-        protocol.run(
+        FederatedAveraging.run(
             pod_identifiers=["bitfount/fake", "bitfount/fake2"],
+            model=federated_model,
             hub=mock_hub,
             private_key_or_file=Path("fake.testkey"),
         )
 
+        mock_algorithm_constructor.assert_called()
         mock_run_protocol.assert_called_once()
         mock_create_aggregator.assert_called()
 
     @unit_test
     def test_helper_run_method_with_model_and_algorithm(
         self, caplog: LogCaptureFixture, mock_hub: Mock, mocker: MockerFixture
     ) -> None:
         """Tests helper run method with an algorithm and a model.
 
         This tests that the run method will still run but that it just issues a warning
         regarding the extra model argument.
         """
         model = Mock()
-        mocker.patch.object(
-            model.datastructure, "get_pod_identifiers", return_value=None
-        )
         federated_algorithm_with_model = FederatedModelTraining(model=Mock())
 
-        mock_run_protocol = mocker.patch.object(FederatedAveraging, "run")
+        mock_run_protocol = mocker.patch(
+            "bitfount.federated.protocols.fed_avg._run_protocol"
+        )
         mock_create_aggregator = mocker.patch(
-            "bitfount.federated.protocols.model_protocols.federated_averaging._create_aggregator",
+            "bitfount.federated.protocols.fed_avg._create_aggregator",
         )
-
-        protocol = FederatedAveraging(
-            model=model, algorithm=federated_algorithm_with_model
+        mock_algorithm_constructor = mocker.patch(
+            "bitfount.federated.protocols.fed_avg.FederatedModelTraining",
         )
-        protocol.run(
+
+        FederatedAveraging.run(
             pod_identifiers=["bitfount/fake", "bitfount/fake2"],
+            model=model,
+            algorithm=federated_algorithm_with_model,
             hub=mock_hub,
+            auto_eval=True,
             private_key_or_file=Path("fake.testkey"),
         )
 
+        mock_algorithm_constructor.assert_not_called()
         mock_run_protocol.assert_called_once()
         mock_create_aggregator.assert_called()
 
         model.assert_not_called()
         model.backend_tensor_shim.assert_not_called()
         assert (
             caplog.records[0].msg
@@ -487,15 +442,15 @@
         protocol_factory = FederatedAveraging(
             algorithm=federated_algorithm,
             aggregator=mocked_aggregator_factory,
             steps_between_parameter_updates=2,
             auto_eval=True,
         )
         protocol = protocol_factory.worker(mailbox=mock_worker_mailbox, hub=mock_hub)
-        await protocol.run(datasource=create_autospec(BaseSource, instance=True))
+        await protocol.run(datasource=create_autospec(DataSource, instance=True))
         mock_send_training_metrics.assert_called_once()
 
     @unit_test
     async def test_worker_auto_eval_false(
         self,
         federated_algorithm: FederatedModelTraining,
         mock_aggregator: Mock,
@@ -512,38 +467,18 @@
         protocol_factory = FederatedAveraging(
             algorithm=federated_algorithm,
             aggregator=mocked_aggregator_factory,
             steps_between_parameter_updates=2,
             auto_eval=False,
         )
         protocol = protocol_factory.worker(mailbox=mock_worker_mailbox, hub=mock_hub)
-        await protocol.run(datasource=create_autospec(BaseSource, instance=True))
+        await protocol.run(datasource=create_autospec(DataSource, instance=True))
         mock_send_training_metrics.assert_not_called()
 
     @unit_test
-    def test_worker_raises_exception_interpod_communication_incorrect(
-        self,
-        mock_hub: Mock,
-        mock_worker_mailbox: Mock,
-    ) -> None:
-        """Tests exception raised if interpod mailbox needed but not provided."""
-        protocol_factory = FederatedAveraging(
-            algorithm=Mock(),
-            aggregator=create_autospec(_InterPodAggregatorWorkerFactory, instance=True),
-        )
-
-        with pytest.raises(
-            TypeError,
-            match=re.escape(
-                "Inter-pod aggregators require an inter-pod worker mailbox."
-            ),
-        ):
-            protocol_factory.worker(mock_worker_mailbox, mock_hub)
-
-    @unit_test
     async def test_modeller_auto_eval_true(
         self,
         federated_algorithm: FederatedModelTraining,
         mock_aggregator: Mock,
         mock_hub: Mock,
         mock_modeller_mailbox: Mock,
         mocked_modeller_runner_fixture: Tuple[Aggregator, AsyncMock, Mock],
@@ -556,33 +491,27 @@
         ) = mocked_modeller_runner_fixture
 
         protocol_factory = FederatedAveraging(
             algorithm=federated_algorithm,
             aggregator=mocked_aggregator_factory,
             steps_between_parameter_updates=1,
         )
-        mock_modeller_mailbox._task_id = "task_id"
         protocol = protocol_factory.modeller(
             mailbox=mock_modeller_mailbox, hub=mock_hub
         )
-        protocol.mailbox._task_id = "task_id"
         await protocol.run()
         mock_get_training_metrics.assert_called()
         assert protocol.validation_results == [{"AUC": "0.7"}, {"AUC": "0.7"}]
         algorithm_mock_calls = [
             mock.call(update=None),
             mock.call(
-                update={"some_weight": "some_value"},
-                validation_metrics={"AUC": "0.7"},
-                iteration=1,
+                update={"some_weight": "some_value"}, validation_metrics={"AUC": "0.7"}
             ),
             mock.call(
-                update={"some_weight": "some_value"},
-                validation_metrics={"AUC": "0.7"},
-                iteration=2,
+                update={"some_weight": "some_value"}, validation_metrics={"AUC": "0.7"}
             ),
         ]
         assert mock_algorithm_run.mock_calls == algorithm_mock_calls
 
     @unit_test
     async def test_modeller_auto_eval_false(
         self,
@@ -603,176 +532,16 @@
             aggregator=mocked_aggregator_factory,
             steps_between_parameter_updates=2,
             auto_eval=False,
         )
         protocol = protocol_factory.modeller(
             mailbox=mock_modeller_mailbox, hub=mock_hub
         )
-        protocol.mailbox._task_id = "task_id"
         await protocol.run()
         mock_get_training_metrics.assert_not_called()
         assert protocol.validation_results == []
         algorithm_mock_calls = [
             mock.call(update=None),
             mock.call(update={"some_weight": "some_value"}),
             mock.call(update={"some_weight": "some_value"}),
         ]
         assert mock_algorithm_run.mock_calls == algorithm_mock_calls
-
-    @unit_test
-    @pytest.mark.parametrize(
-        "iterations, model_reference",
-        [
-            ({"epochs": 1, "steps": None}, True),
-            ({"epochs": None, "steps": 1}, True),
-            ({"epochs": 1, "steps": None}, False),
-            ({"epochs": None, "steps": 1}, False),
-        ],
-    )
-    async def test_federated_averaging_protocol_no_iter(
-        self, iterations: Dict[str, int], mock_aggregator: Mock, model_reference: bool
-    ) -> None:
-        """Tests FederatedAveraging with no iteration provided.
-
-        Tests that iterations is set to a default value if no value
-        is provided.
-        """
-        if model_reference:
-            model = create_autospec(BitfountModelReference)
-            model.hyperparameters = iterations
-        else:
-            model = Mock(**iterations)
-        algorithm = FederatedModelTraining(model=model)
-
-        protocol = FederatedAveraging(
-            algorithm=algorithm,
-            aggregator=mock_aggregator,
-            steps_between_parameter_updates=None,
-            epochs_between_parameter_updates=None,
-            auto_eval=False,
-        )
-
-        if iterations.get("steps"):
-            assert protocol.steps_between_parameter_updates == 1
-            assert protocol.epochs_between_parameter_updates is None
-        else:
-            assert protocol.epochs_between_parameter_updates == 1
-            assert protocol.steps_between_parameter_updates is None
-
-    @unit_test
-    @pytest.mark.parametrize("secure_aggregation", [True, False])
-    async def test_federated_averaging_protocol_successfully(
-        self, secure_aggregation: bool
-    ) -> None:
-        """Tests that FederatedAveraging works as expected."""
-        model = Mock(epochs=1)
-
-        algorithm = FederatedModelTraining(model=model)
-
-        aggregator = _create_aggregator(secure_aggregation=secure_aggregation)
-        protocol = FederatedAveraging(
-            algorithm=algorithm,
-            aggregator=aggregator,
-            steps_between_parameter_updates=None,
-            epochs_between_parameter_updates=2,
-            auto_eval=False,
-        )
-        assert isinstance(protocol, FederatedAveraging)
-        assert protocol.epochs_between_parameter_updates == 2
-        assert protocol.steps_between_parameter_updates is None
-
-    @unit_test
-    def test_protocol_datastructure_error(self, mock_hub: Mock) -> None:
-        """Tests that protocol raises `DataStructure` error."""
-        model = Mock(epochs=1, datastructure=create_datastructure())
-
-        algorithm = FederatedModelTraining(model=model)
-
-        protocol = FederatedAveraging(
-            algorithm=algorithm,
-            steps_between_parameter_updates=None,
-            epochs_between_parameter_updates=2,
-            auto_eval=False,
-        )
-        with pytest.raises(ValueError):
-            protocol.run(pod_identifiers=["pod1", "pod2"], hub=mock_hub)
-
-    @unit_test
-    def test_run_sugar_method_batched_execution_doesnt_work_for_multi_pod(
-        self, caplog: LogCaptureFixture, mock_hub: Mock, mocker: MockerFixture
-    ) -> None:
-        """Tests run method with batched execution.
-
-        Tests that batched execution doesn't work for multi-pod settings.
-        """
-        mock_modeller_run_method = mocker.patch.object(_Modeller, "run")
-
-        protocol = FederatedAveraging(
-            algorithm=[],  # type: ignore[arg-type] # Reason: Not necessary for this test # noqa: B950
-            steps_between_parameter_updates=None,
-            epochs_between_parameter_updates=2,
-            auto_eval=False,
-        )
-        pod_identifiers = ["pod1", "pod2"]
-        protocol.run(
-            pod_identifiers=pod_identifiers, hub=mock_hub, batched_execution=True
-        )
-        assert (
-            "Batched execution is only supported for single pod tasks. "
-            "Resuming task without batched execution."
-        ) in caplog.text
-        mock_modeller_run_method.assert_called_once_with(
-            pod_identifiers,
-            require_all_pods=False,
-            model_out=None,
-            project_id=None,
-            run_on_new_data_only=False,
-            batched_execution=False,
-        )
-
-
-@backend_test
-@unit_test
-class TestMarshmallowSerialization:
-    """Test Marshmallow Serialization for FederatedAveraging protocol."""
-
-    def test_serialization_model_algorithm(self) -> None:
-        """Test Marshmallow Serialization for FederatedAveraging protocol."""
-        model = PyTorchTabularClassifier(
-            datastructure=create_datastructure(),
-            schema=BitfountSchema(),
-            epochs=2,
-        )
-        algorithm_factory = FederatedModelTraining(model=model)
-        fed_avg = FederatedAveraging(
-            algorithm=algorithm_factory, aggregator=Aggregator()
-        )
-
-        dumped = bf_dump(fed_avg)
-        loaded = bf_load(dumped, protocols.registry)
-
-        assert vars(fed_avg).keys() == vars(loaded).keys()
-        assert fed_avg.algorithm.class_name == loaded.algorithm.class_name
-        assert fed_avg.aggregator.class_name == loaded.aggregator.class_name
-
-        assert_vars_equal(vars(fed_avg.algorithm.model), vars(loaded.algorithm.model))
-
-    def test_serialization_model_algorithm_from_protocol(self) -> None:
-        """Test Marshmallow Serialization for FederatedAveraging protocol."""
-        model = PyTorchTabularClassifier(
-            datastructure=create_datastructure(),
-            schema=BitfountSchema(),
-            epochs=2,
-        )
-        algorithm_factory = FederatedModelTraining(model=model)
-        fed_avg = FederatedAveraging(
-            algorithm=algorithm_factory, aggregator=Aggregator()
-        )
-
-        dumped = fed_avg.dump()
-        loaded = bf_load(cast(_JSONDict, dumped), protocols.registry)
-
-        assert vars(fed_avg).keys() == vars(loaded).keys()
-        assert fed_avg.algorithm.class_name == loaded.algorithm.class_name
-        assert fed_avg.aggregator.class_name == loaded.aggregator.class_name
-
-        assert_vars_equal(vars(fed_avg.algorithm.model), vars(loaded.algorithm.model))
```

### Comparing `bitfount-0.5.86/tests/bitfount/federated/test_authorisation_checkers.py` & `bitfount-0.5.9/tests/bitfount/federated/test_authorisation_checkers.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,64 +1,70 @@
 """Test Authorisation Checkers."""
 import asyncio
+import base64
 from datetime import datetime, timedelta, timezone
 import functools
+import json
 import logging
 import re
 from types import MethodType
-from typing import Callable, Dict, List, Literal, Optional, Type, Union, cast
+from typing import Callable, Dict, List, Literal, Type, Union, cast
 from unittest.mock import AsyncMock, Mock, NonCallableMock, call, create_autospec
 
+from _pytest.logging import LogCaptureFixture
 from cryptography.hazmat.primitives.asymmetric.rsa import RSAPrivateKey, RSAPublicKey
 import pytest
-from pytest import LogCaptureFixture, fixture
+from pytest import fixture
 from pytest_mock import MockerFixture
 from requests import HTTPError, RequestException
 import responses
 
+from bitfount.federated.algorithms.model_algorithms.base import (
+    _BaseModelAlgorithmFactory,
+)
 from bitfount.federated.authorisation_checkers import (
     IdentityVerificationMethod,
+    _AuthorisationChecker,
+    _CheckAccessRequests,
     _LocalAuthorisation,
     _OIDCAuthorisationCode,
     _OIDCDeviceCode,
     _SAMLAuthorisation,
     _SignatureBasedAuthorisation,
     check_identity_verification_method,
 )
+from bitfount.federated.encryption import _RSAEncryption
+from bitfount.federated.model_reference import BitfountModelReference
 from bitfount.federated.pod_response_message import _PodResponseMessage
 from bitfount.federated.task_requests import (
     _EncryptedTaskRequest,
+    _ProtocolDetails,
     _SignedEncryptedTaskRequest,
     _TaskRequest,
     _TaskRequestMessage,
 )
 from bitfount.federated.transport.types import (
     _OIDCAuthFlowResponse,
     _PodDeviceCodeDetails,
 )
 from bitfount.federated.transport.worker_transport import _WorkerMailbox
-from bitfount.federated.types import (
-    SerializedAlgorithm,
-    SerializedProtocol,
-    _PodResponseType,
-    _TaskRequestMessageGenerator,
-)
-from bitfount.hub.api import BitfountAM, BitfountHub
-from bitfount.hub.authentication_flow import _AuthEnv
-from bitfount.hub.authentication_handlers import (
+from bitfount.federated.types import _PodResponseType, _TaskRequestMessageGenerator
+from bitfount.hub.api import AccessRequest, BitfountAM, BitfountHub
+from bitfount.hub.authentication_flow import (
     _AUTHORIZATION_PENDING_ERROR,
     _DEVICE_CODE_GRANT_TYPE,
     _SLOW_DOWN_ERROR,
+    _AuthEnv,
 )
 from bitfount.hub.types import (
     _DeviceAccessTokenRequestDict,
     _DeviceAccessTokenResponseJSON,
     _PKCEAccessTokenResponseJSON,
 )
-from bitfount.utils import web_utils
+from tests.bitfount import TEST_SECURITY_FILES
 from tests.utils import PytestRequest
 from tests.utils.helper import get_info_logs, get_warning_logs, unit_test
 
 
 @unit_test
 class TestIdentityVerificationMethodChecker:
     """Tests for check_identity_verification_method()."""
@@ -80,33 +86,227 @@
         fake_method = "fake_method"
         with pytest.raises(
             ValueError, match=f"Unsupported identity verification method: {fake_method}"
         ):
             check_identity_verification_method(fake_method)
 
 
+@unit_test
+class TestCheckAccessRequests:
+    """Tests for CheckAccessRequests."""
+
+    @fixture
+    def modeller_protocol_details(self) -> _ProtocolDetails:
+        """The loaded task."""
+        return _ProtocolDetails(
+            "bitfount.FederatedAveraging",
+            "bitfount.FederatedModelTraining",
+            aggregator="bitfount.Aggregator",
+        )
+
+    @fixture
+    def modeller_task_packed(
+        self, modeller_protocol_details: _ProtocolDetails
+    ) -> bytes:
+        """Returns modeller job request message body."""
+        return modeller_protocol_details.serialize()
+
+    @fixture
+    def modeller_signature(
+        self, modeller_private_key: RSAPrivateKey, modeller_task_packed: bytes
+    ) -> bytes:
+        """Correct modeller signature for message."""
+        return _RSAEncryption.sign_message(modeller_private_key, modeller_task_packed)
+
+    @fixture
+    def check_access_requests(
+        self, modeller_public_key_string: str, request: PytestRequest
+    ) -> _CheckAccessRequests:
+        """Returns CheckAccessRequests object."""
+        mock_hub = create_autospec(BitfountHub, instance=True)
+        access_requests = []
+        authoriser_public_key = _RSAEncryption.load_public_key(
+            TEST_SECURITY_FILES / "test_public.testkey"
+        )
+        if request.param:
+            access_json = json.dumps(
+                {
+                    "protocol": "FederatedAveraging",
+                    "modellerPublicKey": modeller_public_key_string,
+                    "status": request.param,
+                }
+            )
+            authoriser_private_key = _RSAEncryption.load_private_key(
+                TEST_SECURITY_FILES / "test_private.testkey"
+            )
+            signed_permission: bytes = _RSAEncryption.sign_message(
+                authoriser_private_key, access_json.encode()
+            )
+            signed_permission_str: str = base64.b64encode(signed_permission).decode(
+                "ascii"
+            )
+            access_requests = [
+                AccessRequest(
+                    {
+                        "accessJSON": access_json,
+                        "signedPermission": signed_permission_str,
+                        "modellerPublicKeyPem": "",
+                        "status": "",
+                    }
+                )
+            ]
+
+        mock_hub.get_access_requests.side_effect = lambda x, y: access_requests
+        return _CheckAccessRequests(
+            hub=mock_hub, access_manager_public_key=authoriser_public_key
+        )
+
+    @pytest.mark.parametrize("check_access_requests", ["REJECTED"], indirect=True)
+    def test_check_rejected_status(
+        self,
+        check_access_requests: _CheckAccessRequests,
+        modeller_protocol_details: _ProtocolDetails,
+        modeller_signature: bytes,
+        modeller_task_packed: bytes,
+    ) -> None:
+        """Tests rejected requests are unauthorised."""
+        assert _PodResponseType.PROTOCOL_NOT_APPROVED == check_access_requests.check(
+            modeller_task_packed,
+            modeller_protocol_details,
+            modeller_signature,
+            "someModeller",
+            "some/pod_id",
+        )
+
+    @pytest.mark.parametrize("check_access_requests", ["PENDING"], indirect=True)
+    def test_check_pending_status(
+        self,
+        check_access_requests: _CheckAccessRequests,
+        modeller_protocol_details: _ProtocolDetails,
+        modeller_signature: bytes,
+        modeller_task_packed: bytes,
+    ) -> None:
+        """Tests pending requests are unauthorised."""
+        assert _PodResponseType.PROTOCOL_NOT_APPROVED == check_access_requests.check(
+            modeller_task_packed,
+            modeller_protocol_details,
+            modeller_signature,
+            "someModeller",
+            "some/pod_id",
+        )
+
+    @pytest.mark.parametrize("check_access_requests", ["WEIRD"], indirect=True)
+    def test_check_strange_status_errors(
+        self,
+        check_access_requests: _CheckAccessRequests,
+        modeller_protocol_details: _ProtocolDetails,
+        modeller_signature: bytes,
+        modeller_task_packed: bytes,
+    ) -> None:
+        """Tests unexpected request status are marked as unauthorised."""
+        assert _PodResponseType.PROTOCOL_NOT_APPROVED == check_access_requests.check(
+            modeller_task_packed,
+            modeller_protocol_details,
+            modeller_signature,
+            "someModeller",
+            "some/pod_id",
+        )
+
+    @pytest.mark.parametrize("check_access_requests", ["APPROVED"], indirect=True)
+    def test_check_access_successful(
+        self,
+        check_access_requests: _CheckAccessRequests,
+        modeller_protocol_details: _ProtocolDetails,
+        modeller_signature: bytes,
+        modeller_task_packed: bytes,
+    ) -> None:
+        """Tests approved and signed access requests are authorised."""
+        assert _PodResponseType.ACCEPT == check_access_requests.check(
+            modeller_task_packed,
+            modeller_protocol_details,
+            modeller_signature,
+            "someModeller",
+            "some/pod_id",
+        )
+
+    @pytest.mark.parametrize("check_access_requests", [False], indirect=True)
+    def test_check_access_no_requests(
+        self,
+        check_access_requests: _CheckAccessRequests,
+        modeller_protocol_details: _ProtocolDetails,
+        modeller_signature: bytes,
+        modeller_task_packed: bytes,
+    ) -> None:
+        """Tests correct response when algorithm has not been approved for modeller."""
+        assert _PodResponseType.PROTOCOL_NOT_APPROVED == check_access_requests.check(
+            modeller_task_packed,
+            modeller_protocol_details,
+            modeller_signature,
+            "someModeller",
+            "some/pod_id",
+        )
+
+    @pytest.mark.parametrize("check_access_requests", ["APPROVED"], indirect=True)
+    def test_check_access_with_bad_modeller(
+        self,
+        check_access_requests: _CheckAccessRequests,
+        modeller_protocol_details: _ProtocolDetails,
+        modeller_task_packed: bytes,
+    ) -> None:
+        """Tests correct response when modeller signature incorrect."""
+        assert (
+            _PodResponseType.MODELLER_SIGNATURE_DOES_NOT_MATCH
+            == check_access_requests.check(
+                modeller_task_packed,
+                modeller_protocol_details,
+                b"BAD SIGNATURE",
+                "someModeller",
+                "some/pod_id",
+            )
+        )
+
+    @pytest.mark.parametrize("check_access_requests", ["APPROVED"], indirect=True)
+    def test_check_bad_authoriser(
+        self,
+        check_access_requests: _CheckAccessRequests,
+        modeller_protocol_details: _ProtocolDetails,
+        modeller_public_key: RSAPublicKey,
+        modeller_signature: bytes,
+        modeller_task_packed: bytes,
+    ) -> None:
+        """Tests correct response recv when AM not authorised."""
+        check_access_requests.access_manager_public_key = modeller_public_key
+        assert (
+            _PodResponseType.ACCESS_MANAGER_NOT_AUTHORISED
+            == check_access_requests.check(
+                modeller_task_packed,
+                modeller_protocol_details,
+                modeller_signature,
+                "someModeller",
+                "some/pod_id",
+            )
+        )
+
+
 @fixture
 def pod_identifier() -> str:
     """Single pod identifier."""
     return "someUser/somePod"
 
 
 @fixture
 def pod_identifiers() -> List[str]:
     """List of pod identifiers."""
     return ["pod_1/identifier_1", "pod_1/identifier_2"]
 
 
 @fixture
-def serialized_protocol() -> SerializedProtocol:
+def protocol_details() -> _ProtocolDetails:
     """Protocol details."""
-    return SerializedProtocol(
-        class_name="some protocol",
-        algorithm=SerializedAlgorithm(class_name="some algorithm"),
-    )
+    return _ProtocolDetails("some protocol", "some algorithm")
 
 
 @fixture
 def modeller_name() -> str:
     """Name of modeller."""
     return "someModeller"
 
@@ -257,18 +457,18 @@
         mock_task_request_message
     )
     return mock_task_request_message_import
 
 
 @fixture
 def task_request(
-    aes_key: bytes, pod_identifiers: List[str], serialized_protocol: SerializedProtocol
+    aes_key: bytes, pod_identifiers: List[str], protocol_details: _ProtocolDetails
 ) -> _TaskRequest:
     """Constructed TaskRequest instance."""
-    return _TaskRequest(serialized_protocol, pod_identifiers, aes_key)
+    return _TaskRequest(protocol_details, pod_identifiers, aes_key)
 
 
 @fixture
 def encrypted_task_request(task_request: _TaskRequest) -> _EncryptedTaskRequest:
     """Constructed EncryptedTaskRequest instance.
 
     The `encrypted_request` parameter is not actually encrypted, but just serialized
@@ -291,66 +491,185 @@
     return _SignedEncryptedTaskRequest(
         encrypted_request=task_request.serialize(), signature=b"fake_signature"
     )
 
 
 @fixture
 def get_task_request_message(
-    encrypted_task_request: _EncryptedTaskRequest,
-    serialized_protocol: SerializedProtocol,
+    encrypted_task_request: _EncryptedTaskRequest, protocol_details: _ProtocolDetails
 ) -> Callable[[str], _TaskRequestMessage]:
     """Return function that will create TaskRequestMessage instances from auth_type."""
 
     def _make_task_request_message(auth_type: str) -> _TaskRequestMessage:
         return _TaskRequestMessage(
-            serialized_protocol=serialized_protocol,
+            protocol_details=protocol_details,
             auth_type=auth_type,
             request=encrypted_task_request.serialize(),
         )
 
     return _make_task_request_message
 
 
 @fixture
 def get_signed_task_request_message(
-    serialized_protocol: SerializedProtocol,
+    protocol_details: _ProtocolDetails,
     signed_encrypted_task_request: _EncryptedTaskRequest,
 ) -> Callable[[str], _TaskRequestMessage]:
     """Return function that will create TaskRequestMessage instances from auth_type.
 
     Inner message type is a signed and encrypted task request.
     """
 
     def _make_task_request_message(auth_type: str) -> _TaskRequestMessage:
         return _TaskRequestMessage(
-            serialized_protocol=serialized_protocol,
+            protocol_details=protocol_details,
             auth_type=auth_type,
             request=signed_encrypted_task_request.serialize(),
         )
 
     return _make_task_request_message
 
 
 @unit_test
+class TestAuthorisationChecker:
+    """Tests base class AuthorisationChecker."""
+
+    @fixture
+    def authoriser(
+        self, modeller_name: str, pod_identifier: str
+    ) -> _AuthorisationChecker:
+        """An AuthorisationChecker object.
+
+        An instance of LocalAuthorisation is returned because AuthorisationChecker
+        cannot itself be instantiated.
+        """
+        expected_pod_response = _PodResponseMessage(modeller_name, pod_identifier)
+        return _LocalAuthorisation(
+            expected_pod_response,
+            _ProtocolDetails(
+                "bitfount.FederatedAveraging",
+                "bitfount.FederatedModelTraining",
+                aggregator="bitfount.SecureAggregator",
+                model="bitfount.PyTorchTabularClassifier",
+            ),
+        )
+
+    def test_verify_protocol_with_unmatching_protocol(
+        self, authoriser: _AuthorisationChecker
+    ) -> None:
+        """Test verify_protocol returns False when the protocol does not match."""
+        protocol = Mock()
+        protocol.name = "ResultsOnly"
+
+        assert not authoriser.verify_protocol(protocol)
+
+    def test_verify_protocol_with_unmatching_algorithm(
+        self, authoriser: _AuthorisationChecker
+    ) -> None:
+        """Test verify_protocol returns False when the algorithm does not match."""
+        protocol = Mock(algorithm=Mock())
+        protocol.name = "FederatedAveraging"
+        protocol.algorithm.name = "ModelTrainingAndEvaluation"
+
+        assert not authoriser.verify_protocol(protocol)
+
+    def test_verify_protocol_with_unmatching_aggregator(
+        self, authoriser: _AuthorisationChecker
+    ) -> None:
+        """Test verify_protocol returns False when the aggregator does not match."""
+        protocol = Mock(algorithm=Mock(), aggregator=Mock())
+        protocol.name = "FederatedAveraging"
+        protocol.algorithm.name = "FederatedModelTraining"
+        protocol.aggregator.name = "Aggregator"
+
+        assert not authoriser.verify_protocol(protocol)
+
+    def test_verify_protocol_with_unmatching_builtin_model(
+        self, authoriser: _AuthorisationChecker
+    ) -> None:
+        """Test verify_protocol returns False when the bultin model does not match."""
+        protocol = Mock(
+            algorithm=Mock(spec=_BaseModelAlgorithmFactory, model=Mock()),
+            aggregator=Mock(),
+        )
+        protocol.name = "FederatedAveraging"
+        protocol.algorithm.name = "FederatedModelTraining"
+        protocol.aggregator.name = "SecureAggregator"
+        protocol.algorithm.model.name = "LGBMRandomForestClassifier"
+
+        assert not authoriser.verify_protocol(protocol)
+
+    def test_verify_protocol_with_unmatching_custom_model(
+        self, authoriser: _AuthorisationChecker
+    ) -> None:
+        """Test verify_protocol returns False when the custom model does not match."""
+        protocol = Mock(
+            algorithm=Mock(
+                spec=_BaseModelAlgorithmFactory,
+                model=BitfountModelReference(
+                    model_ref="MyModel",
+                    username="username",
+                    datastructure=Mock(),
+                    schema=Mock(),
+                ),
+            ),
+            aggregator=Mock(),
+        )
+        protocol.name = "FederatedAveraging"
+        protocol.algorithm.name = "FederatedModelTraining"
+        protocol.aggregator.name = "SecureAggregator"
+
+        assert not authoriser.verify_protocol(protocol)
+
+    def test_verify_protocol_exception_returns_false(
+        self, authoriser: _AuthorisationChecker, mocker: MockerFixture
+    ) -> None:
+        """Test verify_protocol returns False when an exception is thrown."""
+        protocol = Mock(algorithm=Mock(), aggregator=Mock())
+        protocol.name = "FederatedAveraging"
+        protocol.algorithm.name = "FederatedModelTraining"
+        protocol.aggregator.name = "SecureAggregator"
+
+        # 'hasattr' is simply a builtin that is used in the `verify_protocol` method
+        mocker.patch(
+            "bitfount.federated.authorisation_checkers.hasattr",
+            side_effect=ValueError(),
+        )
+
+        assert not authoriser.verify_protocol(protocol)
+
+    def test_verify_protocol_returns_true_when_protocol_matches(
+        self, authoriser: _AuthorisationChecker
+    ) -> None:
+        """Test verify_protocol returns False when an exception is thrown."""
+        protocol = Mock(algorithm=Mock(spec=["model"]), aggregator=Mock())
+        protocol.name = "FederatedAveraging"
+        protocol.algorithm.name = "FederatedModelTraining"
+        protocol.aggregator.name = "SecureAggregator"
+        protocol.algorithm.model.name = "PyTorchTabularClassifier"
+
+        assert authoriser.verify_protocol(protocol)
+
+
+@unit_test
 class TestLocalAuthorisation:
     """Tests Local Authorisation."""
 
     async def test_local_authorisation_approves(
-        self,
-        modeller_name: str,
-        pod_identifier: str,
-        serialized_protocol: SerializedProtocol,
+        self, modeller_name: str, pod_identifier: str
     ) -> None:
         """Tests local authorisation.
 
         This authoriser 'approves' all requests for local use
         it simply returns the PodResponseMessage given to it.
         """
         expected_pod_response = _PodResponseMessage(modeller_name, pod_identifier)
-        authoriser = _LocalAuthorisation(expected_pod_response, serialized_protocol)
+        authoriser = _LocalAuthorisation(
+            expected_pod_response, _ProtocolDetails("protocol", "algorithm")
+        )
 
         result = await authoriser.check_authorisation()
 
         assert result == expected_pod_response
         assert result.messages == {}
 
     def test__generate_task_request_message(
@@ -360,49 +679,44 @@
         mock_encrypted_tr_import: Mock,
         mock_pod_public_key: NonCallableMock,
         mock_rsa_encryption: Mock,
         mock_task_request: NonCallableMock,
         mock_task_request_import: Mock,
         mock_task_request_message: NonCallableMock,
         mock_task_request_message_import: Mock,
-        opt_project_id: Optional[str],
         pod_identifiers: List[str],
-        serialized_protocol: SerializedProtocol,
+        protocol_details: _ProtocolDetails,
     ) -> None:
         """Tests the _generate_task_request_message method for Local Authorisation."""
         trm = _LocalAuthorisation._generate_task_request_message(
-            serialized_protocol=serialized_protocol,
+            protocol_details=protocol_details,
             pod_identifiers=pod_identifiers,
             aes_key=aes_key,
             pod_public_key=mock_pod_public_key,
-            project_id=opt_project_id,
         )
 
         # Check TaskRequest constructed as expected
         mock_task_request_import.assert_called_once_with(
-            serialized_protocol, pod_identifiers, aes_key
+            protocol_details, pod_identifiers, aes_key
         )
         # Check EncryptedTaskRequest constructed as expected (mocked RSA encryption
         # means serialize return value will be passed)
         mock_encrypted_tr_import.assert_called_once_with(
             mock_task_request.serialize.return_value
         )
         # Check encryption done
         mock_rsa_encryption.assert_called_once_with(
             mock_task_request.serialize.return_value,
             mock_pod_public_key,
         )
         # Check TaskRequestMessage constructed as expected
         mock_task_request_message_import.assert_called_once_with(
-            serialized_protocol,
+            protocol_details,
             IdentityVerificationMethod.LOCAL.value,
             mock_encrypted_task_request.serialize.return_value,
-            opt_project_id,
-            False,
-            False,
         )
         # Check that output value is serialized TaskRequestMessage
         assert trm == mock_task_request_message.serialize.return_value
 
     def test_extract_from_task_request_message(
         self,
         encrypted_task_request: _EncryptedTaskRequest,
@@ -474,58 +788,48 @@
         return hub
 
     @pytest.mark.parametrize(
         "response_type,expected_messages",
         [
             (_PodResponseType.ACCEPT, {}),
             (
-                _PodResponseType.UNAUTHORISED,
-                {_PodResponseType.UNAUTHORISED.name: []},
+                _PodResponseType.ACCESS_MANAGER_NOT_AUTHORISED,
+                {_PodResponseType.ACCESS_MANAGER_NOT_AUTHORISED.name: []},
             ),
         ],
     )
     async def test_signature_authorisation_returns_responses(
         self,
         expected_messages: Dict[str, str],
-        mock_access_manager: Mock,
-        mock_worker_mailbox: Mock,
+        mock_hub: Mock,
         modeller_name: str,
         pod_identifier: str,
+        protocol_details: _ProtocolDetails,
         response_type: _PodResponseType,
-        serialized_protocol: SerializedProtocol,
     ) -> None:
         """Test Signature Authorisation returns expected PodResponseMessages."""
+        check_access_requests = create_autospec(_CheckAccessRequests, instance=True)
+        check_access_requests.check.return_value = response_type
         encrypted_task = b"some task"
+        task_request = _ProtocolDetails("some protocol", "some algorithm")
         signature = b"some signature"
 
-        # Mock out API _check_access call
-        mock_access_manager.check_signature_based_access_request.return_value = (
-            response_type
-        )
-
         authoriser = _SignatureBasedAuthorisation(
             pod_response_message=_PodResponseMessage(modeller_name, pod_identifier),
-            access_manager=mock_access_manager,
-            modeller_name=mock_worker_mailbox.modeller_name,
+            access_request_checker=check_access_requests,
             encrypted_task_request=encrypted_task,
             signature=signature,
-            serialized_protocol=serialized_protocol,
+            task_protocol_details=task_request,
         )
 
         result = await authoriser.check_authorisation()
 
-        # Check API call made correctly
-        mock_access_manager.check_signature_based_access_request.assert_called_once_with(
-            unsigned_task=encrypted_task,
-            task_signature=signature,
-            pod_identifier=pod_identifier,
-            serialized_protocol=serialized_protocol,
-            modeller_name=modeller_name,
+        check_access_requests.check.assert_called_once_with(
+            encrypted_task, task_request, signature, modeller_name, pod_identifier
         )
-
         assert result.messages == expected_messages
 
     def test__generate_task_request_message(
         self,
         aes_key: bytes,
         mock_pod_private_key: NonCallableMock,
         mock_pod_public_key: NonCallableMock,
@@ -533,36 +837,34 @@
         mock_rsa_sign_message: Mock,
         mock_signed_encrypted_task_request: NonCallableMock,
         mock_signed_encrypted_tr_import: Mock,
         mock_task_request: NonCallableMock,
         mock_task_request_import: Mock,
         mock_task_request_message: NonCallableMock,
         mock_task_request_message_import: Mock,
-        opt_project_id: Optional[str],
         pod_identifiers: List[str],
-        serialized_protocol: SerializedProtocol,
+        protocol_details: _ProtocolDetails,
     ) -> None:
         """Test the generate_task_request_message method in sig-based authorisation."""
         # Create inner message maker class
         message_maker = _SignatureBasedAuthorisation._SignedMessageMaker(
             mock_pod_private_key
         )
 
         # Call
         trm = message_maker.generate_task_request_message(
-            serialized_protocol=serialized_protocol,
+            protocol_details=protocol_details,
             pod_identifiers=pod_identifiers,
             aes_key=aes_key,
             pod_public_key=mock_pod_public_key,
-            project_id=opt_project_id,
         )
 
         # Check TaskRequest constructed as expected
         mock_task_request_import.assert_called_once_with(
-            serialized_protocol, pod_identifiers, aes_key
+            protocol_details, pod_identifiers, aes_key
         )
         # Check encryption done
         mock_rsa_encryption.assert_called_once_with(
             mock_task_request.serialize.return_value,
             mock_pod_public_key,
         )
         # Check signing done (mocked RSA encryption means serialize() return value
@@ -575,20 +877,17 @@
         # means static value passed)
         mock_signed_encrypted_tr_import.assert_called_once_with(
             mock_task_request.serialize.return_value,
             mock_rsa_sign_message(None, None),  # extract signature value
         )
         # Check TaskRequestMessage constructed as expected
         mock_task_request_message_import.assert_called_once_with(
-            serialized_protocol,
+            protocol_details,
             IdentityVerificationMethod.KEYS.value,
             mock_signed_encrypted_task_request.serialize.return_value,
-            opt_project_id,
-            False,
-            False,
         )
         # Check that output value is serialized TaskRequestMessage
         assert trm == mock_task_request_message.serialize.return_value
 
     def test_create_task_request_message_generator(
         self,
         mock_pod_private_key: NonCallableMock,
@@ -705,48 +1004,48 @@
         return mailbox
 
     @pytest.mark.parametrize(
         "response_type,expected_messages",
         [
             (_PodResponseType.ACCEPT, {}),
             (
-                _PodResponseType.UNAUTHORISED,
-                {_PodResponseType.UNAUTHORISED.name: []},
+                _PodResponseType.ACCESS_MANAGER_NOT_AUTHORISED,
+                {_PodResponseType.ACCESS_MANAGER_NOT_AUTHORISED.name: []},
             ),
         ],
     )
     async def test_saml_authorisation_returns_responses(
         self,
         access_manager: Mock,
         expected_messages: Dict[str, List[str]],
         modeller_name: str,
         pod_identifier: str,
         pod_name: str,
+        protocol_details: _ProtocolDetails,
         response_type: _PodResponseType,
         saml_id: str,
         saml_request: str,
         saml_response: str,
-        serialized_protocol: SerializedProtocol,
         worker_mailbox: Mock,
     ) -> None:
         """Test SAML Authorisation returns expected PodResponseMessages."""
         access_manager.validate_saml_response.return_value = response_type
         response_message = _PodResponseMessage(modeller_name, pod_identifier)
         authoriser = _SAMLAuthorisation(
             pod_response_message=response_message,
             access_manager=access_manager,
             mailbox=worker_mailbox,
-            serialized_protocol=serialized_protocol,
+            task_protocol_details=protocol_details,
         )
 
         result = await authoriser.check_authorisation()
 
         worker_mailbox.issue_saml_challenge.assert_called_once_with(saml_request)
         access_manager.validate_saml_response.assert_called_once_with(
-            saml_response, saml_id, pod_identifier, modeller_name, serialized_protocol
+            saml_response, saml_id, pod_identifier, modeller_name, protocol_details
         )
 
         assert result.messages == expected_messages
 
     def test_extract_from_task_request_message(
         self,
         encrypted_task_request: _EncryptedTaskRequest,
@@ -769,24 +1068,14 @@
             extracted = _SAMLAuthorisation.extract_from_task_request_message(
                 task_request_message
             )
 
         assert extracted == encrypted_task_request
 
 
-@fixture(params=(True, False), ids=("project_id_incl", "no_project_id"))
-def opt_project_id(request: PytestRequest) -> Optional[str]:
-    """Project ID."""
-    incl_project_id: bool = request.param
-    if incl_project_id:
-        return "this-is-a-project-id"
-    else:
-        return None
-
-
 @fixture
 def mock_access_manager() -> Mock:
     """Mock access manager."""
     access_manager: Mock = create_autospec(BitfountAM, instance=True)
     return access_manager
 
 
@@ -847,23 +1136,23 @@
         self,
         fake_auth_domain: str,
         fake_client_id: str,
         mock_access_manager: Mock,
         mock_worker_mailbox: Mock,
         modeller_name: str,
         pod_identifier: str,
-        serialized_protocol: SerializedProtocol,
+        protocol_details: _ProtocolDetails,
     ) -> _OIDCAuthorisationCode:
         """Constructed OIDC authoriser for tests."""
         pod_response_message = _PodResponseMessage(modeller_name, pod_identifier)
         return _OIDCAuthorisationCode(
             pod_response_message=pod_response_message,
             access_manager=mock_access_manager,
             mailbox=mock_worker_mailbox,
-            serialized_protocol=serialized_protocol,
+            task_protocol_details=protocol_details,
             _auth_domain=fake_auth_domain,
             _client_id=fake_client_id,
         )
 
     def test__generate_task_request_message(
         self,
         aes_key: bytes,
@@ -871,49 +1160,44 @@
         mock_encrypted_tr_import: Mock,
         mock_pod_public_key: NonCallableMock,
         mock_rsa_encryption: Mock,
         mock_task_request: NonCallableMock,
         mock_task_request_import: Mock,
         mock_task_request_message: NonCallableMock,
         mock_task_request_message_import: Mock,
-        opt_project_id: Optional[str],
         pod_identifiers: List[str],
-        serialized_protocol: SerializedProtocol,
+        protocol_details: _ProtocolDetails,
     ) -> None:
         """Test _generate_task_request_message for OIDC Authorisation."""
         trm = _OIDCAuthorisationCode._generate_task_request_message(
-            serialized_protocol=serialized_protocol,
+            protocol_details=protocol_details,
             pod_identifiers=pod_identifiers,
             aes_key=aes_key,
             pod_public_key=mock_pod_public_key,
-            project_id=opt_project_id,
         )
 
         # Check TaskRequest constructed as expected
         mock_task_request_import.assert_called_once_with(
-            serialized_protocol, pod_identifiers, aes_key
+            protocol_details, pod_identifiers, aes_key
         )
         # Check EncryptedTaskRequest constructed as expected (mocked RSA encryption
         # means serialize return value will be passed)
         mock_encrypted_tr_import.assert_called_once_with(
             mock_task_request.serialize.return_value
         )
         # Check encryption done
         mock_rsa_encryption.assert_called_once_with(
             mock_task_request.serialize.return_value,
             mock_pod_public_key,
         )
         # Check TaskRequestMessage constructed as expected
         mock_task_request_message_import.assert_called_once_with(
-            serialized_protocol,
+            protocol_details,
             IdentityVerificationMethod.OIDC_ACF_PKCE.value,
             mock_encrypted_task_request.serialize.return_value,
-            opt_project_id,
-            False,
-            False,
         )
         # Check that output value is serialized TaskRequestMessage
         assert trm == mock_task_request_message.serialize.return_value
 
     def test_create_task_request_message_generator(self) -> None:
         """Tests return of OIDC create_task_request_message_generator()."""
         assert isinstance(
@@ -1006,16 +1290,16 @@
         mock_access_manager: Mock,
         mock_worker_mailbox: Mock,
         mocker: MockerFixture,
         modeller_name: str,
         oidc_auth_code_authoriser: _OIDCAuthorisationCode,
         pkce_access_token_response_json: _PKCEAccessTokenResponseJSON,
         pod_identifier: str,
+        protocol_details: _ProtocolDetails,
         redirect_uri: str,
-        serialized_protocol: SerializedProtocol,
     ) -> None:
         """Test authorisation checking for OIDC flow."""
         # Mock out received values from modeller
         mock_worker_mailbox.get_oidc_auth_flow_response.return_value = (
             _OIDCAuthFlowResponse(
                 auth_code,
                 code_verifier,
@@ -1044,15 +1328,15 @@
         # Check _get_access_token called as expected
         mock__get_access_token.assert_called_once_with(
             auth_code, code_verifier, redirect_uri
         )
         # Check API call made correctly
         mock_access_manager.check_oidc_access_request.assert_called_once_with(
             pod_identifier=pod_identifier,
-            serialized_protocol=serialized_protocol,
+            task_protocol_details=protocol_details,
             modeller_name=modeller_name,
             modeller_access_token=pkce_access_token_response_json["access_token"],
         )
         # Check logs
         info_logs = get_info_logs(caplog)
         assert (
             f"OIDC authorisation check complete. "
@@ -1074,37 +1358,37 @@
         self,
         fake_auth_domain: str,
         fake_client_id: str,
         mock_access_manager: Mock,
         mock_worker_mailbox: Mock,
         modeller_name: str,
         pod_identifier: str,
-        serialized_protocol: SerializedProtocol,
+        protocol_details: _ProtocolDetails,
     ) -> _OIDCDeviceCode:
         """An _OIDCDeviceCode authoriser instance."""
         pod_response_message = _PodResponseMessage(modeller_name, pod_identifier)
         return _OIDCDeviceCode(
             pod_response_message=pod_response_message,
             access_manager=mock_access_manager,
             mailbox=mock_worker_mailbox,
-            serialized_protocol=serialized_protocol,
+            task_protocol_details=protocol_details,
             _auth_domain=fake_auth_domain,
             _client_id=fake_client_id,
         )
 
     def test_auth_domain_and_client_id_always_set(
         self,
         fake_auth_domain: str,
         fake_client_id: str,
         mock_access_manager: Mock,
         mock_worker_mailbox: Mock,
         mocker: MockerFixture,
         modeller_name: str,
         pod_identifier: str,
-        serialized_protocol: SerializedProtocol,
+        protocol_details: _ProtocolDetails,
     ) -> None:
         """Tests default values used if auth_domain and client_id are not set."""
         # Mock _get_auth_environment() function
         mocker.patch(
             "bitfount.federated.authorisation_checkers._get_auth_environment",
             autospec=True,
             return_value=_AuthEnv(
@@ -1116,15 +1400,15 @@
 
         # Create _OIDCDeviceCode without providing auth_domain or client_id
         pod_response_message = _PodResponseMessage(modeller_name, pod_identifier)
         oidc_device_code_authoriser = _OIDCDeviceCode(
             pod_response_message=pod_response_message,
             access_manager=mock_access_manager,
             mailbox=mock_worker_mailbox,
-            serialized_protocol=serialized_protocol,
+            task_protocol_details=protocol_details,
         )
 
         # Check defaults set
         assert oidc_device_code_authoriser._auth_domain == fake_auth_domain
         assert oidc_device_code_authoriser._client_id == fake_client_id
 
     def test__generate_task_request_message(
@@ -1134,49 +1418,44 @@
         mock_encrypted_tr_import: Mock,
         mock_pod_public_key: NonCallableMock,
         mock_rsa_encryption: Mock,
         mock_task_request: NonCallableMock,
         mock_task_request_import: Mock,
         mock_task_request_message: NonCallableMock,
         mock_task_request_message_import: Mock,
-        opt_project_id: Optional[str],
         pod_identifiers: List[str],
-        serialized_protocol: SerializedProtocol,
+        protocol_details: _ProtocolDetails,
     ) -> None:
         """Test _generate_task_request_message for OIDC Authorisation."""
         trm = _OIDCDeviceCode._generate_task_request_message(
-            serialized_protocol=serialized_protocol,
+            protocol_details=protocol_details,
             pod_identifiers=pod_identifiers,
             aes_key=aes_key,
             pod_public_key=mock_pod_public_key,
-            project_id=opt_project_id,
         )
 
         # Check TaskRequest constructed as expected
         mock_task_request_import.assert_called_once_with(
-            serialized_protocol, pod_identifiers, aes_key
+            protocol_details, pod_identifiers, aes_key
         )
         # Check EncryptedTaskRequest constructed as expected (mocked RSA encryption
         # means serialize return value will be passed)
         mock_encrypted_tr_import.assert_called_once_with(
             mock_task_request.serialize.return_value
         )
         # Check encryption done
         mock_rsa_encryption.assert_called_once_with(
             mock_task_request.serialize.return_value,
             mock_pod_public_key,
         )
         # Check TaskRequestMessage constructed as expected
         mock_task_request_message_import.assert_called_once_with(
-            serialized_protocol,
+            protocol_details,
             IdentityVerificationMethod.OIDC_DEVICE_CODE.value,
             mock_encrypted_task_request.serialize.return_value,
-            opt_project_id,
-            False,
-            False,
         )
         # Check that output value is serialized TaskRequestMessage
         assert trm == mock_task_request_message.serialize.return_value
 
     def test_create_task_request_message_generator(self) -> None:
         """Tests return of OIDC create_task_request_message_generator()."""
         assert isinstance(
@@ -1546,23 +1825,19 @@
         content_value: Union[dict, str],
         device_code: str,
         expected_error_msg: str,
         expected_error_type: Type[Exception],
         expires_at: datetime,
         interval: int,
         oidc_device_code_authoriser: _OIDCDeviceCode,
-        remove_web_retry_backoff_sleep: Optional[Mock],
         status_code: int,
         token_endpoint: str,
         token_request: _DeviceAccessTokenRequestDict,
     ) -> None:
         """Tests _poll_for_access_token() fails if non-200/400 response."""
-        # Check retry backoff is patched
-        assert remove_web_retry_backoff_sleep is not None
-
         # We have to use the context manager as decorator doesn't work on async.
         # Can change when: https://github.com/getsentry/responses/pull/478 is released.
         with responses.RequestsMock() as rsps:
             partial_response_add = functools.partial(
                 rsps.add,
                 responses.POST,
                 token_endpoint,
@@ -1582,23 +1857,14 @@
                 expected_error_type,
                 match=expected_error_msg,
             ):
                 await oidc_device_code_authoriser._poll_for_access_token(
                     device_code, expires_at, interval
                 )
 
-        # Check retries occurred if expected
-        if status_code in web_utils._RETRY_STATUS_CODES:
-            assert (
-                remove_web_retry_backoff_sleep.call_count
-                == web_utils._DEFAULT_MAX_RETRIES
-            )
-        else:
-            remove_web_retry_backoff_sleep.assert_not_called()
-
     async def test__poll_for_access_token_fails_when_expired(
         self,
         device_code: str,
         expires_at: datetime,
         interval: int,
         mocker: MockerFixture,
         oidc_device_code_authoriser: _OIDCDeviceCode,
@@ -1645,15 +1911,15 @@
         interval: int,
         mock_access_manager: Mock,
         mock_worker_mailbox: Mock,
         mocker: MockerFixture,
         modeller_name: str,
         oidc_device_code_authoriser: _OIDCDeviceCode,
         pod_identifier: str,
-        serialized_protocol: SerializedProtocol,
+        protocol_details: _ProtocolDetails,
     ) -> None:
         """Tests check_authorisation() works as expected."""
         # Patch out receiving device code details
         mock_worker_mailbox.get_oidc_device_code_response.return_value = (
             _PodDeviceCodeDetails(
                 device_code=device_code,
                 expires_at=expires_at,
@@ -1682,15 +1948,15 @@
         mock_poll_for_access_token.assert_awaited_once_with(
             device_code, expires_at, interval
         )
 
         # Check API call made correctly
         mock_access_manager.check_oidc_access_request.assert_called_once_with(
             pod_identifier=pod_identifier,
-            serialized_protocol=serialized_protocol,
+            task_protocol_details=protocol_details,
             modeller_name=modeller_name,
             modeller_access_token=access_token,
         )
 
         # Check logs
         info_logs = get_info_logs(caplog)
         assert (
```

### Comparing `bitfount-0.5.86/tests/bitfount/federated/test_early_stopping.py` & `bitfount-0.5.9/tests/bitfount/federated/test_early_stopping.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,12 +1,13 @@
 """Tests early_stopping.py module."""
 from typing import Callable, Dict, Generator, List
 
+from _pytest.logging import LogCaptureFixture
 import pytest
-from pytest import LogCaptureFixture, fixture
+from pytest import fixture
 
 from bitfount.federated.early_stopping import FederatedEarlyStopping
 from tests.utils.helper import unit_test
 
 ValidationMetrics = List[Dict[str, float]]
 ValidationMetricsGenerator = Generator[ValidationMetrics, None, None]
 ValidationMetricsCreator = Callable[..., Generator[List[Dict[str, float]], None, None]]
```

### Comparing `bitfount-0.5.86/tests/bitfount/federated/test_logging.py` & `bitfount-0.5.9/tests/bitfount/federated/test_logging.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,24 +1,23 @@
 """Tests logging.py."""
+
 import logging
 from typing import Dict
-from unittest.mock import AsyncMock, Mock
+from unittest.mock import Mock
 
+from _pytest.logging import LogCaptureFixture
 import pytest
-from pytest import LogCaptureFixture
-from pytest_mock import MockerFixture
 
 from bitfount.federated.logging import (
     _federate_logger,
     _FederatedLogFilter,
     _FederatedLogger,
     _get_federated_logger,
     _MailboxHandler,
 )
-from bitfount.federated.transport.base_transport import _BaseMailbox
 from tests.utils.helper import unit_test
 
 LOGGING_METHODS = ["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"]
 
 
 @pytest.fixture
 def log_message() -> str:
@@ -40,17 +39,17 @@
         method_to_call = getattr(logger, f"federated_{level.lower()}")
 
         # Confirming that the method can be called - an error will be raised if it can't
         method_to_call(msg)
     else:
         assert not hasattr(logger, f"federated_{level.lower()}")
 
-    if caplog.records:
-        assert level in [record.levelname for record in caplog.records]
-        assert msg in [record.msg for record in caplog.records]
+    for record in caplog.records:
+        assert record.levelname == level
+        assert record.msg == msg
 
 
 @unit_test
 def test_federate_logger() -> None:
     """Tests that `federate_logger` has the right handlers and filters."""
     mock_mailbox = Mock()
     _federate_logger(mock_mailbox)
@@ -160,42 +159,7 @@
     assert record.levelname == "WARNING"
     assert record.msg == log_message
 
     if federated:
         mock_mailbox.log.assert_called_once()
     else:
         mock_mailbox.log.assert_not_called()
-
-
-@pytest.mark.parametrize("levelno", [10, 20, 30, 40, 50])
-@unit_test
-def test_emit_method_handles_level_appropriately(
-    levelno: int, mocker: MockerFixture
-) -> None:
-    """Tests that the emit method handles the level appropriately.
-
-    DEBUG, INFO, and WARNING should create an asyncio Task.
-    ERROR and CRITICAL should run the message sending in a separate thread.
-
-    It also tests that messages with a levelno greater than or equal to `ERROR` are sent
-    with priority.
-    """
-    if levelno < logging.ERROR:
-        # Task-based approach
-        mock_asyncio = mocker.patch("bitfount.federated.logging.asyncio")
-
-    mock_mailbox = Mock(spec=_BaseMailbox)
-    mock_mailbox.log = AsyncMock(spec=_BaseMailbox.log)
-    handler = _MailboxHandler(mock_mailbox)
-
-    record = Mock(spec=logging.LogRecord)
-    record.levelno = levelno
-
-    handler.emit(record)
-
-    if levelno >= logging.ERROR:
-        # Assert that messages of type ERROR or higher are sent with priority
-        # (i.e. before the end of handler.emit())
-        mock_mailbox.log.assert_awaited_once()
-    else:
-        # Assert that asyncio task is created
-        mock_asyncio.create_task.assert_called_once()
```

### Comparing `bitfount-0.5.86/tests/bitfount/federated/test_roles.py` & `bitfount-0.5.9/tests/bitfount/federated/test_roles.py`

 * *Files identical despite different names*

### Comparing `bitfount-0.5.86/tests/bitfount/federated/transport/identity_verification/test_oidc.py` & `bitfount-0.5.9/tests/bitfount/federated/transport/identity_verification/test_oidc.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,28 +1,29 @@
 """Tests for modeller-side OIDC handling."""
 import asyncio
-from asyncio import Task
 import base64
 from datetime import datetime, timedelta, timezone
 import functools
 import hashlib
 import logging
 import re
 import secrets
-from typing import Dict, List, Literal, Optional, Type, cast
+from typing import Dict, List, Literal, Type, cast
 from unittest.mock import Mock, call, create_autospec
 from urllib.parse import urlencode
 
+from _pytest.capture import CaptureFixture
+from _pytest.logging import LogCaptureFixture
 from aiohttp import web
 from aiohttp.pytest_plugin import AiohttpClient
 from aiohttp.test_utils import TestClient
 from aiohttp.web import Application
 from aiohttp.web_runner import TCPSite
 import pytest
-from pytest import CaptureFixture, LogCaptureFixture, fixture
+from pytest import fixture
 from pytest_mock import MockerFixture
 from requests import HTTPError
 from requests.exceptions import InvalidJSONError
 import responses
 from responses import matchers
 
 from bitfount.federated.transport.identity_verification import _BITFOUNT_MODELLER_PORT
@@ -40,17 +41,15 @@
     _OIDCAuthEndpointResponse,
     _OIDCAuthFlowResponse,
     _OIDCClientID,
     _PodDeviceCodeDetails,
 )
 from bitfount.hub.authentication_flow import AuthEnvironmentError, _AuthEnv
 from bitfount.hub.types import _DeviceCodeRequestDict, _DeviceCodeResponseJSON
-from bitfount.utils import web_utils
 from tests.utils.helper import get_critical_logs, get_info_logs, unit_test
-from tests.utils.mocks import AwaitableMock
 
 
 @unit_test
 class TestOIDCWebEndpoint:
     """Tests for the OIDC web endpoint."""
 
     @fixture
@@ -99,15 +98,15 @@
         self, aiohttp_client: AiohttpClient, oidc_web_endpoint: _OIDCWebEndpoint
     ) -> TestClient:
         """Test client for AioHTTP interactions."""
         app = Application()
         app.add_routes([web.get(_CALLBACK_ROUTE, oidc_web_endpoint.process_callback)])
         return await aiohttp_client(app)
 
-    async def test_start_processing(
+    def test_start_processing(
         self,
         authorize_urls: List[str],
         caplog: LogCaptureFixture,
         mock_webbrowser: Mock,
         oidc_web_endpoint: _OIDCWebEndpoint,
         states: Dict[str, str],
     ) -> None:
@@ -126,15 +125,15 @@
             f"Attempting to open browser. Running a headless client? "
             f"You'll need to open this link in a browser: {authorize_urls[0]}"
             in info_logs
         )
         # Check first URL dropped from iterator
         assert list(oidc_web_endpoint._urls_iter) == authorize_urls[1:]
 
-    async def test_start_processing_throws_error_if_not_initialized(
+    def test_start_processing_throws_error_if_not_initialized(
         self,
         caplog: LogCaptureFixture,
         mock_webbrowser: Mock,
         oidc_web_endpoint: _OIDCWebEndpoint,
     ) -> None:
         """Test start processing throws exception if not initialised."""
         expected_log_message = (
@@ -147,15 +146,15 @@
 
         # Check logged out error
         critical_logs = get_critical_logs(caplog)
         assert expected_log_message in critical_logs
         # Check webbrowser not opened
         mock_webbrowser.open.assert_not_called()
 
-    async def test_initialise_sets_initialised_variable(
+    def test_initialise_sets_initialised_variable(
         self,
         authorize_urls: List[str],
         oidc_web_endpoint: _OIDCWebEndpoint,
         states: Dict[str, str],
     ) -> None:
         """Tests that initialise() sets initialised variable."""
         assert oidc_web_endpoint._initialised is False
@@ -443,114 +442,41 @@
             pod_id: _OIDCAuthEndpointResponse(
                 auth_codes[i],
                 states[i],
             )
             for i, pod_id in enumerate(pod_identifiers)
         }
 
-    async def test_start_server(
+    async def test_start(
         self,
         mocker: MockerFixture,
         oidc_auth_flow_handler: _OIDCAuthFlowChallengeHandler,
     ) -> None:
         """Test start() correctly starts the server."""
         # Mock out site start-up method and init
         mock_tcp_site = create_autospec(TCPSite)
         mock_tcp_site_init = mocker.patch(
             "bitfount.federated.transport.identity_verification.oidc.TCPSite",
             autospec=True,
             return_value=mock_tcp_site,
         )
 
         # Check server not marked as started
-        with pytest.raises(AttributeError):
-            oidc_auth_flow_handler._server_start_task
+        assert not oidc_auth_flow_handler._server_started.is_set()
 
-        server_start_task = oidc_auth_flow_handler.start_server()
-        await server_start_task
+        await oidc_auth_flow_handler.start()
 
         # Check server now marked as started
-        assert oidc_auth_flow_handler._server_start_task.done()
+        assert oidc_auth_flow_handler._server_started.is_set()
         # Check TCP site created and started
         mock_tcp_site_init.assert_called_once_with(
             oidc_auth_flow_handler._runner, "localhost", _BITFOUNT_MODELLER_PORT
         )
         mock_tcp_site.start.assert_awaited_once()
 
-    async def test__start_times_out_after_time(
-        self,
-        caplog: LogCaptureFixture,
-        mocker: MockerFixture,
-        oidc_auth_flow_handler: _OIDCAuthFlowChallengeHandler,
-    ) -> None:
-        """Tests that _start() will timeout if server start hangs."""
-        # Mock out runner
-        mock_runner = mocker.patch.object(
-            oidc_auth_flow_handler, "_runner", autospec=True
-        )
-
-        # Mock out site start-up method and init
-        mock_tcp_site = create_autospec(TCPSite)
-        mock_tcp_site_init = mocker.patch(
-            "bitfount.federated.transport.identity_verification.oidc.TCPSite",
-            autospec=True,
-            return_value=mock_tcp_site,
-        )
-
-        # Make wait_for raise timeout exception
-        mocker.patch.object(
-            asyncio, "wait_for", side_effect=TimeoutError("custom timeout")
-        )
-
-        with pytest.raises(TimeoutError, match="custom timeout"):
-            await oidc_auth_flow_handler._start()
-
-        # Check setup called and site created
-        mock_runner.setup.assert_awaited_once()
-        mock_tcp_site_init.assert_called_once()
-
-        # Check logged exception
-        critical_logs = get_critical_logs(caplog)
-        assert (
-            "Timeout reached whilst trying to bind OIDC web endpoint" in critical_logs
-        )
-
-    async def test__start_handles_address_in_use_error(
-        self,
-        caplog: LogCaptureFixture,
-        mocker: MockerFixture,
-        oidc_auth_flow_handler: _OIDCAuthFlowChallengeHandler,
-    ) -> None:
-        """Tests that _start() handles address being unavailable."""
-        # Mock out runner
-        mock_runner = mocker.patch.object(
-            oidc_auth_flow_handler, "_runner", autospec=True
-        )
-
-        # Mock out site start-up method and init
-        mock_tcp_site = create_autospec(TCPSite)
-        mock_tcp_site_init = mocker.patch(
-            "bitfount.federated.transport.identity_verification.oidc.TCPSite",
-            autospec=True,
-            return_value=mock_tcp_site,
-        )
-        # Make site.start() raise error indicating address unavailable
-        mock_tcp_site.start.side_effect = OSError()
-
-        with pytest.raises(OSError):
-            await oidc_auth_flow_handler._start()
-
-        # Check setup called and site created
-        mock_runner.setup.assert_awaited_once()
-        mock_tcp_site_init.assert_called_once()
-
-        # Check logged exception
-        critical_logs = get_critical_logs(caplog)
-        assert "Unable to bind OIDC web endpoint" in critical_logs
-
     def test__verify_client_ids_successful(
         self,
         client_id: str,
         oidc_auth_flow_handler: _OIDCAuthFlowChallengeHandler,
         patch_get_auth_environment: Mock,
     ) -> None:
         """Test _verify_client_ids() passes correctly."""
@@ -646,18 +572,21 @@
             oidc_auth_flow_handler, "_runner", autospec=True
         )
         mock_endpoint = mocker.patch.object(
             oidc_auth_flow_handler, "_oidc_endpoint", autospec=True
         )
         mock_endpoint.get_responses.return_value = fake_endpoint_responses
 
-        # Mock out server_start_task
-        mock_server_start_task = AwaitableMock(spec=Task)
-        mock_server_start_task.done.return_value = False
-        oidc_auth_flow_handler._server_start_task = mock_server_start_task
+        # Mock out server_started event
+        mock_server_started = mocker.patch.object(
+            oidc_auth_flow_handler,
+            "_server_started",
+            autospec=True,
+        )
+        mock_server_started.is_set.return_value = False
 
         # Make call
         with caplog.at_level(logging.INFO):
             await oidc_auth_flow_handler.handle(mock_modeller_mailbox)
 
         # Check logs
         info_logs = get_info_logs(caplog)
@@ -666,27 +595,25 @@
         # Check endpoint interactions
         mock_endpoint.initialise.assert_called_once_with(
             expected_authorize_urls,
             states_dict,
         )
         mock_endpoint.start_processing.assert_called_once()
 
-        # Check server_start_task and runner interactions
-        mock_server_start_task.done.assert_called_once()
-        mock_server_start_task.assert_awaited_once()
-        mock_server_start_task.result.assert_called_once()
+        # Check server_started event and runner interactions
+        mock_server_started.is_set.assert_called_once()
+        mock_server_started.wait.assert_awaited_once()
+        mock_runner.cleanup.assert_called_once()
 
         # Check correct details sent onwards
+
         mock_modeller_mailbox.send_oidc_auth_flow_responses.assert_awaited_once_with(
             final_oidc_details
         )
 
-        # Check server is shutdown
-        mock_runner.cleanup.assert_awaited()
-
     async def test_handle_fails_different_states(
         self,
         caplog: LogCaptureFixture,
         code_verifiers: List[str],
         expected_authorize_urls: List[str],
         fake_endpoint_responses: Dict[str, _OIDCAuthEndpointResponse],
         mock_modeller_mailbox: Mock,
@@ -708,28 +635,28 @@
             autospec=True,
         )
         mock_secrets.token_urlsafe.side_effect = code_verifiers
 
         # Mock out state generation
         mock_secrets.token_hex.side_effect = states
 
-        # Mock out runner and web endpoint
-        mock_runner = mocker.patch.object(
-            oidc_auth_flow_handler, "_runner", autospec=True
-        )
+        # Mock out web endpoint
         mock_endpoint = mocker.patch.object(
             oidc_auth_flow_handler, "_oidc_endpoint", autospec=True
         )
         # Modify states to make not match on one
         fake_endpoint_responses[pod_identifiers[-1]].state = "incorrect_state"
         mock_endpoint.get_responses.return_value = fake_endpoint_responses
 
-        mock_server_start_task = AwaitableMock(spec=Task)
-        mock_server_start_task.done.return_value = True
-        oidc_auth_flow_handler._server_start_task = mock_server_start_task
+        # Mock out server_started event
+        mock_server_started = mocker.patch.object(
+            oidc_auth_flow_handler,
+            "_server_started",
+            autospec=True,
+        )
 
         # Make call
         with pytest.raises(
             ValueError,
             match=re.escape(
                 f"Unable to validate response intended for {pod_identifiers[-1]}"
             ),
@@ -743,96 +670,16 @@
         # Check endpoint interactions
         mock_endpoint.initialise.assert_called_once_with(
             expected_authorize_urls,
             states_dict,
         )
         mock_endpoint.start_processing.assert_called_once()
 
-        # Check server_start_task and runner interactions
-        mock_server_start_task.done.assert_called_once()
-        mock_server_start_task.result.assert_called_once()
-        mock_server_start_task.assert_not_awaited()
-        # Check server is still shutdown
-        mock_runner.cleanup.assert_awaited()
-
-    async def test_handle_fails_if_server_not_started(
-        self,
-        mock_modeller_mailbox: Mock,
-        mocker: MockerFixture,
-        oidc_auth_flow_handler: _OIDCAuthFlowChallengeHandler,
-    ) -> None:
-        """Tests handle() fails if server not started."""
-        # Mock out runner
-        mock_runner = mocker.patch.object(
-            oidc_auth_flow_handler, "_runner", autospec=True
-        )
-
-        with pytest.raises(
-            RuntimeError,
-            match=re.escape(
-                "OIDC server has not been started; "
-                "ensure start_server() has been called."
-            ),
-        ):
-            await oidc_auth_flow_handler.handle(mock_modeller_mailbox)
-
-        # Check server is still shutdown
-        mock_runner.cleanup.assert_awaited()
-
-    @pytest.mark.parametrize(
-        argnames="server_start_exception_cls",
-        argvalues=(TimeoutError, OSError, Exception),
-    )
-    async def test_handle_reraises_exception_from_start_task(
-        self,
-        code_verifiers: List[str],
-        mock_modeller_mailbox: Mock,
-        mocker: MockerFixture,
-        oidc_auth_flow_handler: _OIDCAuthFlowChallengeHandler,
-        patch_get_auth_environment: Mock,
-        received_oidc_details: Dict[str, _OIDCClientID],
-        server_start_exception_cls: Type[Exception],
-        states: List[str],
-    ) -> None:
-        """Tests that server start exceptions are reraised when awaited."""
-        # Mock server starting to make it throw an exception
-        mocker.patch.object(
-            oidc_auth_flow_handler,
-            "_start",
-            autospec=True,
-            side_effect=server_start_exception_cls("specific exception"),
-        )
-        oidc_auth_flow_handler.start_server()
-        assert oidc_auth_flow_handler._server_start_task
-
-        # Mock out receiving of OIDC details
-        mock_modeller_mailbox.get_oidc_client_ids.return_value = received_oidc_details
-
-        # Mock out code_verifier generation
-        mock_secrets = mocker.patch(
-            "bitfount.federated.transport.identity_verification.oidc.secrets",
-            autospec=True,
-        )
-        mock_secrets.token_urlsafe.side_effect = code_verifiers
-
-        # Mock out state generation
-        mock_secrets.token_hex.side_effect = states
-
-        # Mock out runner and web endpoint
-        mock_runner = mocker.patch.object(
-            oidc_auth_flow_handler, "_runner", autospec=True
-        )
-        mocker.patch.object(oidc_auth_flow_handler, "_oidc_endpoint", autospec=True)
-
-        # Check expected server exception is re-raised
-        with pytest.raises(server_start_exception_cls, match="specific exception"):
-            await oidc_auth_flow_handler.handle(mock_modeller_mailbox)
-
-        # Check server is still shutdown
-        mock_runner.cleanup.assert_awaited()
+        # Check server_started event and runner interactions
+        mock_server_started.is_set.assert_called_once()
 
 
 @unit_test
 class TestOIDCDeviceCodeHandler:
     """Tests for _OIDCDeviceCodeHandler."""
 
     @fixture
@@ -1076,21 +923,17 @@
     def test__get_device_code_fails_non_200_response(
         self,
         client_id: str,
         device_code_endpoint: str,
         device_code_request: _DeviceCodeRequestDict,
         expected_error_msg: str,
         oidc_device_code_handler: _OIDCDeviceCodeHandler,
-        remove_web_retry_backoff_sleep: Optional[Mock],
         status_code: int,
     ) -> None:
         """Test _get_device_code() fails on non-200 response."""
-        # Check retry backoff is patched
-        assert remove_web_retry_backoff_sleep is not None
-
         # Add a randomly filled or empty body to test different responses
         body = "not json error message" if status_code % 100 else ""
         responses.add(
             responses.POST,
             url=device_code_endpoint,
             body=body,
             status=status_code,
@@ -1108,23 +951,14 @@
 
         with pytest.raises(
             HTTPError,
             match=re.escape(expected_error_msg),
         ):
             oidc_device_code_handler._get_device_code(client_id)
 
-        # Check retries occurred if expected
-        if status_code in web_utils._RETRY_STATUS_CODES:
-            assert (
-                remove_web_retry_backoff_sleep.call_count
-                == web_utils._DEFAULT_MAX_RETRIES
-            )
-        else:
-            remove_web_retry_backoff_sleep.assert_not_called()
-
     @pytest.mark.parametrize(
         argnames=("content_type", "content_value", "expected_error"),
         argvalues=(
             pytest.param("json", "wrong json type", TypeError, id="wrong_json_type"),
             pytest.param("json", {}, KeyError, id="missing_json_key"),
             pytest.param(
                 "body", "not json error message", InvalidJSONError, id="not_json"
```

### Comparing `bitfount-0.5.86/tests/bitfount/federated/transport/identity_verification/test_saml.py` & `bitfount-0.5.9/tests/bitfount/federated/transport/identity_verification/test_saml.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,39 +1,34 @@
 """Test SAML handling."""
 import asyncio
-from asyncio import Task, create_task
-import re
-from typing import Dict, List, Type
-from unittest.mock import Mock, create_autospec
+from typing import Dict, List, cast
+from unittest.mock import AsyncMock, Mock, create_autospec
 
 from aiohttp import web
 from aiohttp.pytest_plugin import AiohttpClient
 from aiohttp.test_utils import TestClient
 from aiohttp.web_app import Application
-from aiohttp.web_runner import AppRunner, TCPSite
+from aiohttp.web_runner import AppRunner
 import pytest
-from pytest import LogCaptureFixture, fixture
+from pytest import fixture
 from pytest_mock import MockerFixture
 
+from bitfount.federated.transport.base_transport import _WorkerMailboxDetails
 from bitfount.federated.transport.identity_verification import _BITFOUNT_MODELLER_PORT
 from bitfount.federated.transport.identity_verification.saml import (
     _SAMLChallengeHandler,
     _SAMLWebEndpoint,
 )
 from bitfount.federated.transport.message_service import (
     _BitfountMessageType,
     _DecryptedBitfountMessage,
 )
-from bitfount.federated.transport.modeller_transport import (
-    _ModellerMailbox,
-    _WorkerMailboxDetails,
-)
+from bitfount.federated.transport.modeller_transport import _ModellerMailbox
 from bitfount.types import _SAMLResponse
-from tests.utils.helper import get_critical_logs, unit_test
-from tests.utils.mocks import AwaitableMock
+from tests.utils.helper import unit_test
 
 
 @fixture
 def saml_challenges_count() -> int:
     """Number of SAML challenges."""
     return 3
 
@@ -45,15 +40,14 @@
         _DecryptedBitfountMessage(
             message_type=_BitfountMessageType.SAML_REQUEST,
             body=f"saml-challenge-{i}",
             recipient="modeller",
             recipient_mailbox_id="modeller-mailbox-id",
             sender=f"sender-{i}",
             sender_mailbox_id=f"sender-mailbox-{i}",
-            task_id="task-id",
         )
         for i in range(saml_challenges_count)
     ]
 
 
 @fixture
 def saml_responses(saml_challenges_count: int) -> List[Dict[str, str]]:
@@ -206,292 +200,119 @@
             await endpoint.handle_saml_idp_response(Mock())
 
 
 @unit_test
 class TestSAMLChallengeHandler:
     """Test SAMLChallengeHandler."""
 
-    @fixture
-    def saml_challenge_handler(self, idp_url: str) -> _SAMLChallengeHandler:
-        """Handler to use in tests."""
-        return _SAMLChallengeHandler(idp_url)
-
-    @fixture
-    def mock_modeller_mailbox(
-        self, mailbox_details: Dict[str, _WorkerMailboxDetails]
-    ) -> Mock:
-        """Mock _ModellerMailbox instance."""
-        mock_mailbox: Mock = create_autospec(_ModellerMailbox, instance=True)
-        mock_mailbox.worker_mailboxes = mailbox_details
-        return mock_mailbox
-
-    async def test_start_server_sets_server_start_task(
-        self,
-        saml_challenge_handler: _SAMLChallengeHandler,
-    ) -> None:
-        """Tests that the server start task is set.
-
-        Nothing is mocked here, to ensure that the calls
-        to aiohttp actually work!
-        """
-        saml_challenge_handler.start_server()
-
-        assert saml_challenge_handler._server_start_task
-
-    async def test__start_aiohttp_calls_work(
+    async def test_saml_challenge_handler_sets_server_started_event(
         self,
-        saml_challenge_handler: _SAMLChallengeHandler,
+        idp_url: str,
     ) -> None:
-        """Tests that the server start calls work.
+        """Tests that the server started event is set.
 
         Nothing is mocked here, to ensure that the calls
         to aiohttp actually work!
         """
-        await saml_challenge_handler._start()
-
-    async def test__start_times_out_after_time(
-        self,
-        caplog: LogCaptureFixture,
-        mocker: MockerFixture,
-        saml_challenge_handler: _SAMLChallengeHandler,
-    ) -> None:
-        """Tests that _start() will timeout if server start hangs."""
-        # Mock out runner
-        mock_runner = mocker.patch.object(
-            saml_challenge_handler, "runner", autospec=True
-        )
-
-        # Mock out site start-up method and init
-        mock_tcp_site = create_autospec(TCPSite)
-        mock_tcp_site_init = mocker.patch(
-            "bitfount.federated.transport.identity_verification.saml.TCPSite",
-            autospec=True,
-            return_value=mock_tcp_site,
-        )
-
-        # Make wait_for raise timeout exception
-        mocker.patch.object(
-            asyncio, "wait_for", side_effect=TimeoutError("custom timeout")
-        )
-
-        with pytest.raises(TimeoutError, match="custom timeout"):
-            await saml_challenge_handler._start()
+        challenge_handler = _SAMLChallengeHandler(idp_url)
+        await challenge_handler.start()
 
-        # Check setup called and site created
-        mock_runner.setup.assert_awaited_once()
-        mock_tcp_site_init.assert_called_once()
+        assert challenge_handler.server_started.is_set()
 
-        # Check logged exception
-        critical_logs = get_critical_logs(caplog)
-        assert (
-            "Timeout reached whilst trying to bind SAML web endpoint" in critical_logs
-        )
-
-    async def test__start_handles_address_in_use_error(
-        self,
-        caplog: LogCaptureFixture,
-        mocker: MockerFixture,
-        saml_challenge_handler: _SAMLChallengeHandler,
+    async def test_saml_challenge_handler_sets_up_server(
+        self, idp_url: str, mocker: MockerFixture
     ) -> None:
-        """Tests that _start() handles address being unavailable."""
-        # Mock out runner
-        mock_runner = mocker.patch.object(
-            saml_challenge_handler, "runner", autospec=True
-        )
-
-        # Mock out site start-up method and init
-        mock_tcp_site = create_autospec(TCPSite)
-        mock_tcp_site_init = mocker.patch(
-            "bitfount.federated.transport.identity_verification.saml.TCPSite",
-            autospec=True,
-            return_value=mock_tcp_site,
-        )
-        # Make site.start() raise error indicating address unavailable
-        mock_tcp_site.start.side_effect = OSError()
-
-        with pytest.raises(OSError):
-            await saml_challenge_handler._start()
-
-        # Check setup called and site created
-        mock_runner.setup.assert_awaited_once()
-        mock_tcp_site_init.assert_called_once()
-
-        # Check logged exception
-        critical_logs = get_critical_logs(caplog)
-        assert "Unable to bind SAML web endpoint" in critical_logs
-
-    async def test_start_server_sets_up_server_in_background(
-        self,
-        mocker: MockerFixture,
-        saml_challenge_handler: _SAMLChallengeHandler,
-    ) -> None:
-        """Ensure server is started in background with expected settings."""
+        """Ensure server is started with expected settings."""
         mock_tcp_site = mocker.patch(
-            "bitfount.federated.transport.identity_verification.saml.TCPSite",
-            autospec=True,
-        )
-        mock_create_task = mocker.patch(
-            "bitfount.federated.transport.identity_verification.saml.create_task",
-            wraps=create_task,
+            "bitfount.federated.transport.identity_verification.saml.TCPSite"
         )
+        mock_tcp_site.return_value = AsyncMock()
 
-        saml_challenge_handler.start_server()
-        # Await start task to give time for background to run
-        await saml_challenge_handler._server_start_task
+        challenge_handler = _SAMLChallengeHandler(idp_url)
+        await challenge_handler.start()
 
-        mock_create_task.assert_called_once()
         mock_tcp_site.assert_called_once_with(
-            saml_challenge_handler.runner, "localhost", _BITFOUNT_MODELLER_PORT
+            challenge_handler.runner, "localhost", _BITFOUNT_MODELLER_PORT
         )
         mock_tcp_site.return_value.start.assert_awaited_once()
+        assert challenge_handler.server_started.is_set()
 
     async def test_saml_challenge_handler_opens_first_challenge(
         self,
         idp_url: str,
         mailbox_details: Dict[str, _WorkerMailboxDetails],
         mocker: MockerFixture,
-        saml_challenge_handler: _SAMLChallengeHandler,
         saml_challenges: List[_DecryptedBitfountMessage],
     ) -> None:
         """Tests that the SAML authentication flow is triggered.
 
         This ensures that we set up the SAMLWebEndpoint as expected,
         and that we open a browser to begin the SAML auth flow.
 
         It also checks that we shut down our web server.
         """
-        mock_endpoint = create_autospec(_SAMLWebEndpoint, instance=True)
-        saml_challenge_handler.saml_endpoint = mock_endpoint
-
-        challenges_handled = asyncio.Event()
-        saml_challenge_handler.all_saml_challenges_handled = challenges_handled
-
+        mock_endpoint = mocker.patch(
+            "bitfount.federated.transport.identity_verification.saml._SAMLWebEndpoint"
+        )
         mock_webbrowser = mocker.patch(
             "bitfount.federated.transport.identity_verification.saml.webbrowser"
         )
+        challenge_handler = _SAMLChallengeHandler(idp_url)
+        challenges_handled = asyncio.Event()
+        challenge_handler.all_saml_challenges_handled = challenges_handled
         # Set event; mimicking the side effect of the listener
         mock_webbrowser.open.side_effect = lambda _: challenges_handled.set()
 
-        # Mock the server start task
-        saml_challenge_handler._server_start_task = AwaitableMock(spec=Task)
-        saml_challenge_handler._server_start_task.done.return_value = False
-        saml_challenge_handler.runner = create_autospec(AppRunner, instance=True)
+        # Mock the server as having started
+        challenge_handler.server_started.set()
+        challenge_handler.runner = create_autospec(AppRunner, instance=True)
         mock_send = Mock()
 
-        await saml_challenge_handler._handle_saml_challenges(
+        await challenge_handler._handle_saml_challenges(
             mock_send, saml_challenges, mailbox_details
         )
 
         # Ensure browser is opened to the first SAML URL
         mock_webbrowser.open.assert_called_once_with(
             f"{idp_url}{saml_challenges[0].body}"
         )
+        # Ensure the server is shut down
+        cast(Mock, challenge_handler.runner.cleanup).assert_called_once()
         # Ensure the web endpoint is given the SAML challenges
-        mock_endpoint.set_saml_challenges.assert_called_once_with(
+        mock_endpoint.return_value.set_saml_challenges.assert_called_once_with(
             list(zip(saml_challenges, mailbox_details.values())), mock_send
         )
 
-        # Check that server task was awaited and interacted with
-        saml_challenge_handler._server_start_task.done.assert_called_once()
-        saml_challenge_handler._server_start_task.assert_awaited_once()
-        saml_challenge_handler._server_start_task.result.assert_called_once()
-
     async def test_handle_method(
         self,
-        mock_modeller_mailbox: Mock,
+        idp_url: str,
         mocker: MockerFixture,
-        saml_challenge_handler: _SAMLChallengeHandler,
         saml_challenges: List[_DecryptedBitfountMessage],
     ) -> None:
         """Tests that the handle() method performs expected operations.
 
         In particular that it awaits the SAML challenges being sent and then
         passes these off to be handled.
         """
-        # Mock out runner
-        mock_runner = mocker.patch.object(
-            saml_challenge_handler, "runner", autospec=True
-        )
+        # Create challenge handler
+        challenge_handler = _SAMLChallengeHandler(idp_url)
 
         # Create mock ModellerMailbox
+        mock_modeller_mailbox: Mock = create_autospec(_ModellerMailbox, instance=True)
         mock_modeller_mailbox.get_saml_challenges.return_value = saml_challenges
+        mock_modeller_mailbox.worker_mailboxes = Mock()
 
         # Mock out _handle_saml_challenges method
         mock__handle_saml_challenges = mocker.patch.object(
-            saml_challenge_handler, "_handle_saml_challenges", autospec=True
+            challenge_handler, "_handle_saml_challenges", autospec=True
         )
 
-        await saml_challenge_handler.handle(mock_modeller_mailbox)
+        await challenge_handler.handle(mock_modeller_mailbox)
 
         # Check SAML challenges awaited
         mock_modeller_mailbox.get_saml_challenges.assert_awaited_once()
         # Check passed on to actual handler
         mock__handle_saml_challenges.assert_awaited_once_with(
             mock_modeller_mailbox.send_saml_responses,
             saml_challenges,
             mock_modeller_mailbox.worker_mailboxes,
         )
-        # Check server shutdown at end
-        mock_runner.cleanup.assert_awaited()
-
-    async def test_handle_fails_if_server_not_started(
-        self,
-        mock_modeller_mailbox: Mock,
-        mocker: MockerFixture,
-        saml_challenge_handler: _SAMLChallengeHandler,
-        saml_challenges: List[_DecryptedBitfountMessage],
-    ) -> None:
-        """Tests handle() fails if server not started."""
-        # Mock out runner
-        mock_runner = mocker.patch.object(
-            saml_challenge_handler, "runner", autospec=True
-        )
-
-        with pytest.raises(
-            RuntimeError,
-            match=re.escape(
-                "SAML server has not been started; "
-                "ensure start_server() has been called."
-            ),
-        ):
-            await saml_challenge_handler.handle(mock_modeller_mailbox)
-
-        # Check server is still shutdown
-        mock_runner.cleanup.assert_awaited()
-
-    @pytest.mark.parametrize(
-        argnames="server_start_exception_cls",
-        argvalues=(TimeoutError, OSError, Exception),
-    )
-    async def test_handle_reraises_exception_from_start_task(
-        self,
-        mock_modeller_mailbox: Mock,
-        mocker: MockerFixture,
-        saml_challenge_handler: _SAMLChallengeHandler,
-        saml_challenges: List[_DecryptedBitfountMessage],
-        server_start_exception_cls: Type[Exception],
-    ) -> None:
-        """Tests that server start exceptions are reraised when awaited."""
-        # Mock server starting to make it throw an exception
-        mocker.patch.object(
-            saml_challenge_handler,
-            "_start",
-            autospec=True,
-            side_effect=server_start_exception_cls("specific exception"),
-        )
-        saml_challenge_handler.start_server()
-        assert saml_challenge_handler._server_start_task
-
-        # Mock out runner and web endpoint
-        mock_runner = mocker.patch.object(
-            saml_challenge_handler, "runner", autospec=True
-        )
-        mocker.patch.object(saml_challenge_handler, "saml_endpoint", autospec=True)
-
-        # Mock out SAML challenge reception
-        mock_modeller_mailbox.get_saml_challenges.return_value = saml_challenges
-
-        with pytest.raises(server_start_exception_cls):
-            await saml_challenge_handler.handle(mock_modeller_mailbox)
-
-        # Check server shutdown at end
-        mock_runner.cleanup.assert_awaited()
```

### Comparing `bitfount-0.5.86/tests/bitfount/federated/transport/test_config.py` & `bitfount-0.5.9/tests/bitfount/federated/transport/test_config.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,21 +1,16 @@
 """Tests config.py."""
 import inspect
 
+from _pytest.logging import LogCaptureFixture
+from _pytest.monkeypatch import MonkeyPatch
 import pytest
-from pytest import LogCaptureFixture, MonkeyPatch, fixture
+from pytest import fixture
 
-from bitfount.federated.transport.config import (
-    _DEV_MESSAGE_SERVICE_PORT,
-    _DEV_MESSAGE_SERVICE_TLS,
-    _DEV_MESSAGE_SERVICE_URL,
-    _STAGING_MESSAGE_SERVICE_URL,
-    PRODUCTION_MESSAGE_SERVICE_URL,
-    MessageServiceConfig,
-)
+from bitfount.federated.transport.config import MessageServiceConfig
 from bitfount.federated.transport.protos.messages_pb2_grpc import MessageServiceStub
 from tests.utils.helper import unit_test
 
 
 @unit_test
 class TestMessageServiceConfig:
     """Tests for MessageServiceConfig."""
@@ -37,50 +32,25 @@
         self, monkeypatch: MonkeyPatch
     ) -> None:
         """Ensure TLS can't be disabled with staging hub url."""
         monkeypatch.setenv("BITFOUNT_ENVIRONMENT", "staging")
         with pytest.raises(ValueError):
             MessageServiceConfig(tls=False)
 
-    def test_ms_config_url_set_on_staging(self, monkeypatch: MonkeyPatch) -> None:
-        """Test url correctly set on staging.."""
-        monkeypatch.setenv("BITFOUNT_ENVIRONMENT", "staging")
-        ms_config = MessageServiceConfig()
-        assert ms_config.url == _STAGING_MESSAGE_SERVICE_URL
-
-    def test_ms_config_url_set_on_production(self, monkeypatch: MonkeyPatch) -> None:
-        """Test url correctly set on production."""
-        monkeypatch.setenv("BITFOUNT_ENVIRONMENT", "production")
-        ms_config = MessageServiceConfig()
-        assert ms_config.url == PRODUCTION_MESSAGE_SERVICE_URL
-
-    def test_ms_config_url_set_on_dev(self, monkeypatch: MonkeyPatch) -> None:
-        """Test url correctly set on dev."""
-        monkeypatch.setenv("BITFOUNT_ENVIRONMENT", "dev")
-        ms_config = MessageServiceConfig()
-        assert ms_config.url == _DEV_MESSAGE_SERVICE_URL
-        assert ms_config.port == _DEV_MESSAGE_SERVICE_PORT
-        assert ms_config.tls == _DEV_MESSAGE_SERVICE_TLS
-
     def test_ms_config_issues_warning_if_tls_disabled_with_custom_url(
         self, caplog: LogCaptureFixture
     ) -> None:
-        """Ensure that warning is issued if tls is disabled with non-bitfount URL."""
-        MessageServiceConfig(url="custom.ms.url.com", tls=False)
+        """Ensure that warning is issued if tls is disabled with non-bitfount URL.
 
-        assert caplog.records[0].levelname == "WARNING"
-        assert (
-            caplog.records[0].getMessage()
-            == "Message service communication without TLS."
-        )
-        assert caplog.records[1].levelname == "DEBUG"
-        assert (
-            caplog.records[1].getMessage()
-            == "Message service configuration: {'url': 'custom.ms.url.com', 'port': 443, 'tls': False, 'use_local_storage': False}"  # noqa: B950
-        )
+        NB: There should only be one record in caplog.records.
+        """
+        MessageServiceConfig(url="custom.ms.url.com")
+        for record in caplog.records:
+            assert record.levelname == "WARNING"
+            assert record.getMessage() == "Message service communication without TLS."
 
     async def test_creates_message_service_stub_secure(self, test_url: str) -> None:
         """Tests create_message_service_stub with TLS."""
         ms_config = MessageServiceConfig(tls=False, url=test_url)
         stub = await ms_config.stub
         assert isinstance(stub, MessageServiceStub)
```

### Comparing `bitfount-0.5.86/tests/bitfount/federated/transport/test_message_service_usage.py` & `bitfount-0.5.9/tests/bitfount/federated/transport/test_message_service_usage.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,22 +1,25 @@
 """Tests for the message service implementations."""
-from collections.abc import Iterable
+import asyncio
+import logging
+import sys
 from typing import Callable
 from unittest.mock import Mock
 
-import pytest
 from pytest import fixture
 
-from tests.utils.concurrency_utils import run_local_modeller_and_workers
+from bitfount.federated.transport.base_transport import _run_func_and_listen_to_mailbox
 from tests.utils.helper import (
     AUC_THRESHOLD,
     create_local_modeller_and_workers,
     integration_test,
 )
 
+logging.basicConfig(level=logging.INFO, stream=sys.stdout)
+
 
 @integration_test
 class TestMessageServiceUsage:
     """Tests for Message Service federated transport layer for training jobs."""
 
     # TODO: [BIT-983] Add tests to check the following:
     #           - Other models
@@ -30,17 +33,14 @@
         self, apply_mock_get_pod_public_keys: Callable[[str], Mock]
     ) -> Mock:
         """Mocks out get_pod_public_keys function in modeller.py."""
         return apply_mock_get_pod_public_keys(
             "bitfount.federated.modeller._get_pod_public_keys"
         )
 
-    @pytest.mark.skip(
-        "[BIT-1914] LGBMRandomForestClassifier incompatible with ResultsOnly"
-    )
     async def test_classifier_runs(
         self,
         mock_get_pod_public_keys: Mock,
         mock_message_aes_decryption: Mock,
         mock_message_aes_encryption: Mock,
         mock_rsa_encryption: Mock,
         mock_rsa_sign_message: Mock,
@@ -48,15 +48,16 @@
         """Tests that a Logistic Regression classifier runs."""
         modeller, workers = create_local_modeller_and_workers(
             model_name="LogisticRegressionClassifier",
             protocol_name="ResultsOnly",
             algorithm_name="ModelTrainingAndEvaluation",
         )
 
-        modeller_results = await run_local_modeller_and_workers(modeller, workers)
-
-        assert isinstance(modeller_results, Iterable)
+        modeller_results, *_ = await asyncio.gather(
+            modeller.run_async([worker.mailbox.pod_identifier for worker in workers]),
+            *[_run_func_and_listen_to_mailbox(w.run(), w.mailbox) for w in workers]
+        )
         for result in modeller_results:
             assert result is not None
             assert isinstance(result, dict)
             auc = result["AUC"]
             assert auc > AUC_THRESHOLD
```

### Comparing `bitfount-0.5.86/tests/bitfount/federated/transport/test_modeller_transport.py` & `bitfount-0.5.9/tests/bitfount/federated/transport/test_modeller_transport.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,75 +1,61 @@
 """Tests that modeller can initiate training with Pods."""
 from __future__ import annotations
 
 from collections import namedtuple
 from datetime import datetime, timedelta, timezone
-from typing import Callable, Dict, Iterable, List, Mapping, Optional, Tuple
+from typing import Callable, Dict, Iterable, List, Mapping
 from unittest.mock import Mock, call, create_autospec
 
 from grpc import RpcError
 import msgpack
-import pytest
-from pytest import LogCaptureFixture, fixture, raises
+from pytest import fixture, raises
 from pytest_mock import MockerFixture
 
 from bitfount.federated.authorisation_checkers import IdentityVerificationMethod
 from bitfount.federated.encryption import _AESEncryption, _RSAEncryption
 from bitfount.federated.exceptions import BitfountTaskStartError
 from bitfount.federated.task_requests import (
     _EncryptedTaskRequest,
+    _ProtocolDetails,
     _SignedEncryptedTaskRequest,
     _TaskRequest,
     _TaskRequestMessage,
 )
-from bitfount.federated.transport.base_transport import _run_func_and_listen_to_mailbox
+from bitfount.federated.transport.base_transport import (
+    _run_func_and_listen_to_mailbox,
+    _WorkerMailboxDetails,
+)
 from bitfount.federated.transport.handlers import _AsyncMultipleResponsesHandler
 from bitfount.federated.transport.message_service import (
     _BitfountMessage,
     _BitfountMessageType,
     _DecryptedBitfountMessage,
     _MessageEncryption,
     _MessageService,
 )
 from bitfount.federated.transport.modeller_transport import (
     _get_parameter_updates_from_workers,
-    _get_psi_datasets_from_workers,
-    _get_public_key,
     _get_training_metrics_from_workers,
     _ModellerMailbox,
     _parameter_updates_handler,
-    _psi_dataset_handler,
-    _public_key_handler,
     _training_metrics_handler,
-    _WorkerMailboxDetails,
 )
-from bitfount.federated.transport.protos.messages_pb2 import TaskMetadata
 from bitfount.federated.transport.types import (
     _OIDCAuthFlowResponse,
     _OIDCClientID,
     _PodDeviceCodeDetails,
 )
-from bitfount.federated.types import (
-    SerializedAlgorithm,
-    SerializedProtocol,
-    _PodResponseType,
-    _TaskRequestMessageGenerator,
-)
+from bitfount.federated.types import _PodResponseType, _TaskRequestMessageGenerator
 from bitfount.types import _SerializedWeights
-from tests.utils import PytestRequest
-from tests.utils.fixtures import (
-    KeyBasedGenerator,
-    MessageOrException,
-    PollForMessagesPatcher,
-)
-from tests.utils.helper import get_info_logs, unit_test
+from tests.utils.fixtures import KeyBasedGenerator, MessageOrException
+from tests.utils.helper import unit_test
+from tests.utils.mocks import AsyncIteratorMock
 
-WorkerDetails = namedtuple(
-    "WorkerDetails", ["pod_identifier", "mailbox_id", "aes_encryption_key"]
-)
+WorkerDetails = namedtuple("WorkerDetails", ["pod_identifier", "mailbox_id"])
 
 
 @fixture
 def modeller_name() -> str:
     """Name of the modeller."""
     return "modeller"
 
@@ -96,15 +82,15 @@
 
 
 @fixture
 def worker_mailbox_details(worker_mailbox_ids: Dict[str, str]) -> List[WorkerDetails]:
     """Provides an indexable version of worker mailbox IDs."""
     # Need to be able to more easily index into the worker_mailbox_ids
     worker_details = [
-        WorkerDetails(pod_identifier, mailbox_id, b"aes_encryption_key")
+        WorkerDetails(pod_identifier, mailbox_id)
         for pod_identifier, mailbox_id in worker_mailbox_ids.items()
     ]
     return worker_details
 
 
 @fixture
 def pod_public_keys(pod_identifiers: List[str]) -> Dict[str, Mock]:
@@ -146,59 +132,25 @@
 @fixture
 def idp_url() -> str:
     """Identity Provider URL."""
     return "https://idp-url.unit-testing.bitfount.com"
 
 
 @fixture
-def task_id() -> str:
-    """Task ID."""
-    return "this-is-a-task-id"
-
-
-@fixture(params=(True, False), ids=("project_id_incl", "no_project_id"))
-def opt_project_id(request: PytestRequest) -> Optional[str]:
-    """Project ID."""
-    incl_project_id: bool = request.param
-    if incl_project_id:
-        return "this-is-a-project-id"
-    else:
-        return None
-
-
-@fixture(params=(True, False), ids=("task_id_incl", "no_task_id"))
-def opt_task_id(request: PytestRequest, task_id: str) -> Optional[str]:
-    """Returns a task ID or None, to cover both cases."""
-    incl_task_id: bool = request.param
-    if incl_task_id:
-        return task_id
-    else:
-        return None
-
-
-@fixture
-def online_check_uuid() -> str:
-    """UUID for online checking."""
-    return "auuidforonlinecheck"
-
-
-@fixture
 def modeller_mailbox(
     idp_url: str,
     mock_message_service: Mock,
     modeller_mailbox_id: str,
-    task_id: str,
     worker_mailboxes: Dict[str, _WorkerMailboxDetails],
 ) -> _ModellerMailbox:
     """The ModellerMailbox under test."""
     return _ModellerMailbox(
         mailbox_id="modeller_mailbox_id",
         worker_mailboxes=worker_mailboxes,
         message_service=mock_message_service,
-        task_id=task_id,
     )
 
 
 @fixture
 def signatures() -> List[bytes]:
     """List of fake signatures for message signing."""
     signatures = [b"signature1", b"signature2", b"signature3"]
@@ -237,142 +189,125 @@
     # and instead replace it with just returning message bodies.
     mock_decrypter = mocker.patch.object(_MessageEncryption, "decrypt_incoming_message")
     mock_decrypter.side_effect = lambda body, key: body
     return mock_decrypter
 
 
 @fixture
-def serialized_protocol() -> SerializedProtocol:
-    """Protocol details."""
-    return SerializedProtocol(
-        class_name="some protocol",
-        algorithm=SerializedAlgorithm(class_name="some algorithm"),
-    )
+def task_protocol_details() -> _ProtocolDetails:
+    """Mock task request body."""
+    return _ProtocolDetails("some protocol", "some algorithm")
 
 
 @unit_test
 class TestModellerMailbox:
     """Tests for ModellerMailbox class."""
 
     async def test_send_task_requests(
         self,
         default_task_request_msg_gen: _TaskRequestMessageGenerator,
         idp_url: str,
         mock_message_service: Mock,
         mocker: MockerFixture,
         modeller_mailbox_id: str,
-        task_id: str,
         worker_mailboxes: Dict[str, _WorkerMailboxDetails],
     ) -> None:
         """Test send_task_requests factory method.
 
         This is the class method and so should return us a created ModellerMailbox.
         """
         # Mock out underlying send method
         mocked_sender = mocker.patch.object(_ModellerMailbox, "_send_task_requests")
-        mocked_sender.return_value = (modeller_mailbox_id, worker_mailboxes, task_id)
+        mocked_sender.return_value = (modeller_mailbox_id, worker_mailboxes)
 
         modeller_mailbox = await _ModellerMailbox.send_task_requests(
-            serialized_protocol=SerializedProtocol(
-                class_name="some protocol",
-                algorithm=SerializedAlgorithm(class_name="some algorithm"),
-            ),
+            protocol_details=_ProtocolDetails("protocol", "algorithm"),
             pod_public_keys=Mock(),
             task_request_msg_gen=default_task_request_msg_gen,
             message_service=mock_message_service,
         )
 
         assert modeller_mailbox.mailbox_id == modeller_mailbox_id
         assert modeller_mailbox.worker_mailboxes == worker_mailboxes
         assert modeller_mailbox.message_service == mock_message_service
-        assert modeller_mailbox._task_id == task_id
 
     async def test__send_task_requests_is_successful(
         self,
         aes_keys: List[bytes],
         encrypted_task_requests: List[bytes],
         key_based_task_request_msg_gen: KeyBasedGenerator,
         mock_message_service: Mock,
         mock_rsa_encrypter: Mock,
         mock_signer: Mock,
         mocker: MockerFixture,
         modeller_mailbox_id: str,
-        opt_project_id: Optional[str],
         pod_identifiers: List[str],
         pod_public_keys: Dict[str, Mock],
-        serialized_protocol: SerializedProtocol,
         signatures: List[bytes],
-        task_id: str,
+        task_protocol_details: _ProtocolDetails,
         worker_mailbox_ids: Dict[str, str],
         worker_mailboxes: Dict[str, _WorkerMailboxDetails],
     ) -> None:
         """Happy path, job requests are all sent successfully.
 
         Key-based identity verification.
         """
         # Mock AES key generation
         mock_key_generator = mocker.patch.object(_AESEncryption, "generate_key")
         mock_key_generator.side_effect = aes_keys
 
         # Mock the message services setup method
-        mock_message_service.setup_task.return_value = (
+        mock_message_service.setup_communication_with_pods.return_value = (
             modeller_mailbox_id,
             worker_mailbox_ids,
-            task_id,
         )
 
         # Mock/create other args
         private_key = key_based_task_request_msg_gen.key
         task_request_msg_gen = key_based_task_request_msg_gen.gen
 
         # Call method
         (
             received_modeller_mailbox_id,
             received_worker_mailboxes,
-            received_task_id,
         ) = await _ModellerMailbox._send_task_requests(
-            serialized_protocol=serialized_protocol,
+            protocol_details=task_protocol_details,
             pod_public_keys=pod_public_keys,
             task_request_msg_gen=task_request_msg_gen,
             message_service=mock_message_service,
-            project_id=opt_project_id,
         )
 
         # Check return values
         assert received_modeller_mailbox_id == modeller_mailbox_id
         assert received_worker_mailboxes == worker_mailboxes
-        assert received_task_id == task_id
 
         # Check message service method call
-        mock_message_service.setup_task.assert_called_once_with(
+        mock_message_service.setup_communication_with_pods.assert_called_once_with(
             {
                 pod_identifier: _TaskRequestMessage(
-                    serialized_protocol=serialized_protocol,
+                    protocol_details=task_protocol_details,
                     auth_type=IdentityVerificationMethod.KEYS.value,
                     request=_SignedEncryptedTaskRequest(
                         encrypted_request=encrypted_task_requests[i],
                         signature=signatures[i],
                     ).serialize(),
-                    project_id=opt_project_id,
                 ).serialize()
                 for i, pod_identifier in enumerate(pod_identifiers)
             },
-            TaskMetadata(protocol=serialized_protocol["class_name"]),
-            opt_project_id,
         )
 
         # Ensure we encrypt and sign correctly
         mock_rsa_encrypter.assert_has_calls(
             [
                 # Each call to the encryption should be the (task request body,
                 # the list of all pods involved, generated AES key) and the public
                 # key to encrypt with.
                 call(
                     _TaskRequest(
-                        serialized_protocol=serialized_protocol,
+                        protocol_details=task_protocol_details,
                         pod_identifiers=pod_identifiers,
                         aes_key=aes_keys[i],
                     ).serialize(),
                     pod_public_key,
                 )
                 for i, pod_public_key in enumerate(pod_public_keys.values())
             ]
@@ -389,20 +324,18 @@
         encrypted_task_requests: List[bytes],
         key_based_task_request_msg_gen: KeyBasedGenerator,
         mock_message_service: Mock,
         mock_rsa_encrypter: Mock,
         mock_signer: Mock,
         mocker: MockerFixture,
         modeller_mailbox_id: str,
-        opt_project_id: Optional[str],
         pod_identifiers: List[str],
         pod_public_keys: Dict[str, Mock],
-        serialized_protocol: SerializedProtocol,
         signatures: List[bytes],
-        task_id: str,
+        task_protocol_details: _ProtocolDetails,
         worker_mailbox_ids: Dict[str, str],
         worker_mailboxes: Dict[str, _WorkerMailboxDetails],
     ) -> None:
         """Some pods are offline, so there aren't mailboxes returned for them.
 
         In the case where some pods are offline we continue the task,
         except we only receive mailboxes for some of the pods for the
@@ -416,76 +349,69 @@
 
         # For this test we ensure the message service returns fewer
         # pods than were requested.
         worker_mailboxes_from_message_service = dict(worker_mailbox_ids)
         worker_mailboxes_from_message_service.popitem()
 
         # Mock the message services setup method
-        mock_message_service.setup_task.return_value = (
+        mock_message_service.setup_communication_with_pods.return_value = (
             modeller_mailbox_id,
             worker_mailboxes_from_message_service,
-            task_id,
         )
 
         # Mock/create other args
         private_key = key_based_task_request_msg_gen.key
         task_request_msg_gen = key_based_task_request_msg_gen.gen
 
         # Call method
         (
             received_modeller_mailbox_id,
             received_worker_mailboxes,
-            received_task_id,
         ) = await _ModellerMailbox._send_task_requests(
-            serialized_protocol=serialized_protocol,
+            protocol_details=task_protocol_details,
             pod_public_keys=pod_public_keys,
             task_request_msg_gen=task_request_msg_gen,
             message_service=mock_message_service,
-            project_id=opt_project_id,
         )
 
         # Modeller mailbox ID matches the mocked message service response
         assert received_modeller_mailbox_id == modeller_mailbox_id
         # We only create WorkerMailboxDetails for the mailboxes
         # returned by the message service
         worker_mailboxes.popitem()
         assert received_worker_mailboxes == worker_mailboxes
-        assert received_task_id == task_id
 
         # Check message service method call included all pod identifiers
         # This includes the ones that aren't online, we don't know that one is
         # offline until the message service tells us!
-        mock_message_service.setup_task.assert_called_once_with(
+        mock_message_service.setup_communication_with_pods.assert_called_once_with(
             {
                 pod_identifier: _TaskRequestMessage(
-                    serialized_protocol=serialized_protocol,
+                    protocol_details=task_protocol_details,
                     auth_type=IdentityVerificationMethod.KEYS.value,
                     request=_SignedEncryptedTaskRequest(
                         encrypted_request=encrypted_task_requests[i],
                         signature=signatures[i],
                     ).serialize(),
-                    project_id=opt_project_id,
                 ).serialize()
                 for i, pod_identifier in enumerate(worker_mailbox_ids.keys())
             },
-            TaskMetadata(protocol=serialized_protocol["class_name"]),
-            opt_project_id,
         )
 
         # Ensure we encrypt and sign correctly the first task messages
         # This includes the ones that aren't online, we don't know that one is
         # offline until the message service tells us!
         mock_rsa_encrypter.assert_has_calls(
             [
                 # Each call to the encryption should be the (task request body,
                 # the list of all pods involved, generated AES key) and the public
                 # key to encrypt with.
                 call(
                     _TaskRequest(
-                        serialized_protocol=serialized_protocol,
+                        protocol_details=task_protocol_details,
                         pod_identifiers=pod_identifiers,
                         aes_key=aes_keys[i],
                     ).serialize(),
                     pod_public_key,
                 )
                 for i, pod_public_key in enumerate(pod_public_keys.values())
             ]
@@ -499,151 +425,137 @@
     async def test__send_task_requests_fails(
         self,
         encrypted_task_requests: List[bytes],
         key_based_task_request_msg_gen: KeyBasedGenerator,
         mock_message_service: Mock,
         mock_rsa_encrypter: Mock,
         mock_signer: Mock,
-        opt_project_id: Optional[str],
         pod_identifiers: List[str],
         pod_public_keys: Dict[str, Mock],
-        rpc_error: RpcError,
-        serialized_protocol: SerializedProtocol,
         signatures: List[bytes],
+        task_protocol_details: _ProtocolDetails,
     ) -> None:
         """RPC Error occurs for one attempt at starting training.
 
         Uses key-based identity verification method.
         """
         # Mock out error raising in message service.
-        mock_message_service.setup_task.side_effect = rpc_error
+        mock_message_service.setup_communication_with_pods.side_effect = RpcError()
 
         # Mock/create other args
         task_request_msg_gen = key_based_task_request_msg_gen.gen
 
         # Call method
         with raises(RuntimeError, match="Failed to start task with pods:"):
             await _ModellerMailbox._send_task_requests(
-                serialized_protocol=serialized_protocol,
+                protocol_details=task_protocol_details,
                 pod_public_keys=pod_public_keys,
                 task_request_msg_gen=task_request_msg_gen,
                 message_service=mock_message_service,
-                project_id=opt_project_id,
             )
 
         # Check that the encrypted and signed messages were passed to
-        # setup_task
-        mock_message_service.setup_task.assert_called_once_with(
+        # setup_communication_with_pods
+        mock_message_service.setup_communication_with_pods.assert_called_once_with(
             {
                 pod_identifier: _TaskRequestMessage(
-                    serialized_protocol=serialized_protocol,
+                    protocol_details=task_protocol_details,
                     auth_type=IdentityVerificationMethod.KEYS.value,
                     request=_SignedEncryptedTaskRequest(
                         encrypted_request=encrypted_task_requests[i],
                         signature=signatures[i],
                     ).serialize(),
-                    project_id=opt_project_id,
                 ).serialize()
                 for i, pod_identifier in enumerate(pod_identifiers)
             },
-            TaskMetadata(protocol=serialized_protocol["class_name"]),
-            opt_project_id,
         )
 
     async def test__send_task_requests_has_no_signatures_if_saml_based(
         self,
         aes_keys: List[bytes],
         encrypted_task_requests: List[bytes],
         mock_message_service: Mock,
         mock_rsa_encrypter: Mock,
         mocker: MockerFixture,
         modeller_mailbox_id: str,
-        opt_project_id: Optional[str],
         pod_identifiers: List[str],
         pod_public_keys: Dict[str, Mock],
         saml_task_request_msg_gen: _TaskRequestMessageGenerator,
-        serialized_protocol: SerializedProtocol,
-        task_id: str,
+        task_protocol_details: _ProtocolDetails,
         worker_mailbox_ids: Dict[str, str],
         worker_mailboxes: Dict[str, _WorkerMailboxDetails],
     ) -> None:
         """Tests _send_task_requests has no sigs if SAML-based verification."""
         # Mock AES key generation
         mock_key_generator = mocker.patch.object(_AESEncryption, "generate_key")
         mock_key_generator.side_effect = aes_keys
 
         # Mock the message services setup method
-        mock_message_service.setup_task.return_value = (
+        mock_message_service.setup_communication_with_pods.return_value = (
             modeller_mailbox_id,
             worker_mailbox_ids,
-            task_id,
         )
 
         # Call method
         (
             received_modeller_mailbox_id,
             received_worker_mailboxes,
-            received_task_id,
         ) = await _ModellerMailbox._send_task_requests(
-            serialized_protocol=serialized_protocol,
+            protocol_details=task_protocol_details,
             pod_public_keys=pod_public_keys,
             task_request_msg_gen=saml_task_request_msg_gen,
             message_service=mock_message_service,
-            project_id=opt_project_id,
         )
 
         # Check return values
         assert received_modeller_mailbox_id == modeller_mailbox_id
         assert received_worker_mailboxes == worker_mailboxes
-        assert received_task_id == task_id
 
         # Check message service method call
-        mock_message_service.setup_task.assert_called_once_with(
+        mock_message_service.setup_communication_with_pods.assert_called_once_with(
             {
                 pod_identifier: _TaskRequestMessage(
-                    serialized_protocol=serialized_protocol,
+                    protocol_details=task_protocol_details,
                     auth_type=IdentityVerificationMethod.SAML.value,
                     request=_EncryptedTaskRequest(
                         encrypted_request=encrypted_task_requests[i],
                     ).serialize(),
-                    project_id=opt_project_id,
                 ).serialize()
                 for i, pod_identifier in enumerate(pod_identifiers)
             },
-            TaskMetadata(protocol=serialized_protocol["class_name"]),
-            opt_project_id,
         )
 
         # Ensure we encrypt correctly
         mock_rsa_encrypter.assert_has_calls(
             [
                 # Each call to the encryption should be the (task request body,
                 # the list of all pods involved, generated AES key) and the public
                 # key to encrypt with.
                 call(
                     _TaskRequest(
-                        serialized_protocol=serialized_protocol,
+                        protocol_details=task_protocol_details,
                         pod_identifiers=pod_identifiers,
                         aes_key=aes_keys[i],
                     ).serialize(),
                     pod_public_key,
                 )
                 for i, pod_public_key in enumerate(pod_public_keys.values())
             ]
         )
 
     async def test_process_task_request_responses(
         self,
         mock_aes_decrypter: Mock,
         mock_message_service: Mock,
+        mock_poll_for_messages: Callable[
+            [Iterable[MessageOrException]], AsyncIteratorMock
+        ],
         modeller_mailbox: _ModellerMailbox,
         modeller_mailbox_id: str,
         modeller_name: str,
-        opt_task_id: Optional[str],
-        patch_poll_for_messages: PollForMessagesPatcher,
         worker_mailbox_details: List[WorkerDetails],
         worker_mailbox_ids: Dict[str, str],
         worker_mailboxes: Dict[str, _WorkerMailboxDetails],
     ) -> None:
         """Happy path, all pods send responses to training requests."""
         # Create fake task response messages
         task_response_messages = [
@@ -656,68 +568,66 @@
                         ].pod_identifier
                     }
                 ),
                 recipient=modeller_name,
                 recipient_mailbox_id=modeller_mailbox_id,
                 sender=worker_mailbox_details[0].pod_identifier,
                 sender_mailbox_id=worker_mailbox_details[0].mailbox_id,
-                task_id=opt_task_id,
             ),
             _BitfountMessage(
                 message_type=_BitfountMessageType.JOB_ACCEPT,
                 body=msgpack.dumps(
                     {
                         _PodResponseType.ACCEPT.name: worker_mailbox_details[
                             1
                         ].pod_identifier
                     }
                 ),
                 recipient=modeller_name,
                 recipient_mailbox_id=modeller_mailbox_id,
                 sender=worker_mailbox_details[1].pod_identifier,
                 sender_mailbox_id=worker_mailbox_details[1].mailbox_id,
-                task_id=opt_task_id,
             ),
             _BitfountMessage(
                 message_type=_BitfountMessageType.JOB_REJECT,
                 body=msgpack.dumps(
                     {
-                        _PodResponseType.NO_ACCESS.name: "some/pod",
-                        _PodResponseType.NO_ACCESS.name: "some/pod",
+                        _PodResponseType.SECURE_AGGREGATION_WORKERS_NOT_AUTHORISED.name: "some/pod",  # noqa: B950
+                        _PodResponseType.MODELLER_SIGNATURE_DOES_NOT_MATCH.name: "some/pod",  # noqa: B950
                     }
                 ),
                 recipient=modeller_name,
                 recipient_mailbox_id=modeller_mailbox_id,
                 sender=worker_mailbox_details[2].pod_identifier,
                 sender_mailbox_id=worker_mailbox_details[2].mailbox_id,
-                task_id=opt_task_id,
             ),
         ]
 
         # Assign these to poll_for_messages which is the underlying method we
         # expect to yield messages.
-        patch_poll_for_messages(task_response_messages)
+        mock_poll_for_messages(task_response_messages)
 
         # Start listening and processing messages
         accepted_pod_mailboxes = await _run_func_and_listen_to_mailbox(
             modeller_mailbox.process_task_request_responses(), modeller_mailbox
         )
 
         # Pod3 rejected the task, so we aren't expecting it!
         worker_mailboxes.pop(worker_mailbox_details[2].pod_identifier)
         assert accepted_pod_mailboxes == worker_mailboxes
 
     async def test_get_saml_challenges_retrieves_saml_responses(
         self,
         mock_aes_decrypter: Mock,
+        mock_poll_for_messages: Callable[
+            [Iterable[MessageOrException]], AsyncIteratorMock
+        ],
         modeller_mailbox: _ModellerMailbox,
         modeller_mailbox_id: str,
         modeller_name: str,
-        opt_task_id: Optional[str],
-        patch_poll_for_messages: PollForMessagesPatcher,
         worker_mailbox_details: List[WorkerDetails],
     ) -> None:
         """Happy path, all pods send responses to training requests."""
         saml_challenges = [
             msgpack.dumps(bytes(f"saml challenge {i}", "utf-8")) for i in range(3)
         ]
 
@@ -726,85 +636,78 @@
             _BitfountMessage(
                 message_type=_BitfountMessageType.SAML_REQUEST,
                 body=saml_challenges[0],
                 recipient=modeller_name,
                 recipient_mailbox_id=modeller_mailbox_id,
                 sender=worker_mailbox_details[0].pod_identifier,
                 sender_mailbox_id=worker_mailbox_details[0].mailbox_id,
-                task_id=opt_task_id,
             ),
             _BitfountMessage(
                 message_type=_BitfountMessageType.SAML_REQUEST,
                 body=saml_challenges[1],
                 recipient=modeller_name,
                 recipient_mailbox_id=modeller_mailbox_id,
                 sender=worker_mailbox_details[1].pod_identifier,
                 sender_mailbox_id=worker_mailbox_details[1].mailbox_id,
-                task_id=opt_task_id,
             ),
             _BitfountMessage(
                 message_type=_BitfountMessageType.SAML_REQUEST,
                 body=saml_challenges[2],
                 recipient=modeller_name,
                 recipient_mailbox_id=modeller_mailbox_id,
                 sender=worker_mailbox_details[2].pod_identifier,
                 sender_mailbox_id=worker_mailbox_details[2].mailbox_id,
-                task_id=opt_task_id,
             ),
         ]
 
         # Assign these to poll_for_messages which is the underlying method we
         # expect to yield messages.
-        patch_poll_for_messages(saml_requests)
+        mock_poll_for_messages(saml_requests)
 
         # Start listening and processing messages
         received_saml_challenges = await _run_func_and_listen_to_mailbox(
             modeller_mailbox.get_saml_challenges(), modeller_mailbox
         )
 
         # Ensure messages returned match those that we mocked
-        # Order is not guaranteed but _DecryptedBitfountMessage is not hashable
-        # or sortable so need to manually check equality.
-        expected_saml_challenges = [
+        assert received_saml_challenges == [
             challenge.decrypt(
                 modeller_mailbox.worker_mailboxes[challenge.sender].aes_encryption_key
             )
             for challenge in saml_requests
         ]
-        assert len(received_saml_challenges) == len(expected_saml_challenges)
-        assert all(
-            challenge in expected_saml_challenges
-            for challenge in received_saml_challenges
-        )
 
     async def test_get_saml_challenges_throws_error(
         self,
+        mock_poll_for_messages: Callable[
+            [Iterable[MessageOrException]], AsyncIteratorMock
+        ],
         modeller_mailbox: _ModellerMailbox,
-        patch_poll_for_messages: PollForMessagesPatcher,
         rpc_error: RpcError,
     ) -> None:
         """Test Start Task error thrown on error."""
-        patch_poll_for_messages([rpc_error])
+        mock_poll_for_messages([rpc_error])
         with raises(
             BitfountTaskStartError, match="Failed to start task with all pods."
         ):
             await _run_func_and_listen_to_mailbox(
                 modeller_mailbox.get_saml_challenges(),
                 modeller_mailbox,
             )
 
     async def test_handles_pod_never_responds(
         self,
         mock_aes_decrypter: Mock,
         mock_message_service: Mock,
+        mock_poll_for_messages: Callable[
+            [Iterable[MessageOrException]], AsyncIteratorMock
+        ],
         modeller_mailbox: _ModellerMailbox,
         modeller_mailbox_id: str,
         modeller_name: str,
-        opt_task_id: Optional[str],
-        patch_poll_for_messages: PollForMessagesPatcher,
         worker_mailbox_details: List[WorkerDetails],
         worker_mailboxes: Dict[str, _WorkerMailboxDetails],
     ) -> None:
         """Some pods never send a response to the training request."""
         # Create only a single response message
         task_response_messages = [
             _BitfountMessage(
@@ -816,20 +719,19 @@
                         ].pod_identifier
                     }
                 ),
                 recipient=modeller_name,
                 recipient_mailbox_id=modeller_mailbox_id,
                 sender=worker_mailbox_details[0].pod_identifier,
                 sender_mailbox_id=worker_mailbox_details[0].mailbox_id,
-                task_id=opt_task_id,
             ),
         ]
 
         # Assign this message to poll_for_messages yield
-        patch_poll_for_messages(task_response_messages)
+        mock_poll_for_messages(task_response_messages)
 
         # Start listening and processing messages. We timeout on the responses to
         # simulate non-responding pods. This may need to be increased if the test
         # is failing but should need to be no higher than 5 seconds.
         accepted_pod_mailboxes = await _run_func_and_listen_to_mailbox(
             modeller_mailbox.process_task_request_responses(timeout=1), modeller_mailbox
         )
@@ -840,19 +742,20 @@
 
         assert accepted_pod_mailboxes == worker_mailboxes
 
     async def test_handles_message_retrieval_exception(
         self,
         mock_aes_decrypter: Mock,
         mock_message_service: Mock,
+        mock_poll_for_messages: Callable[
+            [Iterable[MessageOrException]], AsyncIteratorMock
+        ],
         modeller_mailbox: _ModellerMailbox,
         modeller_mailbox_id: str,
         modeller_name: str,
-        opt_task_id: Optional[str],
-        patch_poll_for_messages: PollForMessagesPatcher,
         rpc_error: RpcError,
         worker_mailbox_details: List[WorkerDetails],
     ) -> None:
         """RPC Exception when trying to check for responses from pods."""
         task_response_messages: List[MessageOrException] = [
             _BitfountMessage(
                 message_type=_BitfountMessageType.JOB_ACCEPT,
@@ -863,20 +766,19 @@
                         ].pod_identifier
                     }
                 ),
                 recipient=modeller_name,
                 recipient_mailbox_id=modeller_mailbox_id,
                 sender=worker_mailbox_details[0].pod_identifier,
                 sender_mailbox_id=worker_mailbox_details[0].mailbox_id,
-                task_id=opt_task_id,
             ),
             rpc_error,
         ]
 
-        patch_poll_for_messages(task_response_messages)
+        mock_poll_for_messages(task_response_messages)
 
         with raises(
             BitfountTaskStartError, match="Failed to start task with all pods."
         ):
             await _run_func_and_listen_to_mailbox(
                 modeller_mailbox.process_task_request_responses(),
                 modeller_mailbox,
@@ -887,15 +789,14 @@
         mock_message_service: Mock,
         mock_message_timestamps: Callable[[Iterable[str]], Mock],
         mocker: MockerFixture,
         modeller_mailbox: _ModellerMailbox,
         modeller_mailbox_id: str,
         modeller_name: str,
         pod_identifiers: List[str],
-        task_id: str,
         worker_mailboxes: Dict[str, _WorkerMailboxDetails],
     ) -> None:
         """Message is sent AES encrypted using provided key."""
         fake_timestamps = ["Hello", "World", "!"]
         mock_message_timestamps(fake_timestamps)
 
         encrypt = mocker.patch.object(_MessageEncryption, "encrypt_outgoing_message")
@@ -942,45 +843,42 @@
                         recipient=pod_identifiers[0],
                         recipient_mailbox_id=worker_mailboxes[
                             pod_identifiers[0]
                         ].mailbox_id,
                         sender=modeller_name,
                         sender_mailbox_id=modeller_mailbox_id,
                         timestamp=fake_timestamps[0],
-                        task_id=task_id,
                     ),
                     already_packed=True,
                 ),
                 call(
                     _BitfountMessage(
                         message_type=_BitfountMessageType.TRAINING_COMPLETE,
                         body=encrypted_messages[1],
                         recipient=pod_identifiers[1],
                         recipient_mailbox_id=worker_mailboxes[
                             pod_identifiers[1]
                         ].mailbox_id,
                         sender=modeller_name,
                         sender_mailbox_id=modeller_mailbox_id,
                         timestamp=fake_timestamps[1],
-                        task_id=task_id,
                     ),
                     already_packed=True,
                 ),
                 call(
                     _BitfountMessage(
                         message_type=_BitfountMessageType.TRAINING_COMPLETE,
                         body=encrypted_messages[2],
                         recipient=pod_identifiers[2],
                         recipient_mailbox_id=worker_mailboxes[
                             pod_identifiers[2]
                         ].mailbox_id,
                         sender=modeller_name,
                         sender_mailbox_id=modeller_mailbox_id,
                         timestamp=fake_timestamps[2],
-                        task_id=task_id,
                     ),
                     already_packed=True,
                 ),
             ]
         )
         assert mock_message_service.send_message.call_count == 3
 
@@ -989,29 +887,27 @@
         mock_message_service: Mock,
         mock_message_timestamps: Callable[[Iterable[str]], Mock],
         mocker: MockerFixture,
         modeller_mailbox: _ModellerMailbox,
         modeller_mailbox_id: str,
         modeller_name: str,
         pod_identifiers: List[str],
-        rpc_error: RpcError,
-        task_id: str,
         worker_mailboxes: Dict[str, _WorkerMailboxDetails],
     ) -> None:
         """Sending encrypted message receives RpcError."""
         fake_timestamps = ["Hello"]
         mock_message_timestamps(fake_timestamps)
 
         expected_encrypted_message = b"encrypted_message"
         encrypt = mocker.patch.object(_MessageEncryption, "encrypt_outgoing_message")
         encrypt.return_value = expected_encrypted_message
         message = "some message"
         dumped_message = msgpack.dumps(message)
         mock_message_service.username = modeller_name
-        mock_message_service.send_message.side_effect = rpc_error
+        mock_message_service.send_message.side_effect = RpcError()
 
         modeller_mailbox.accepted_worker_mailboxes = worker_mailboxes
 
         with raises(RpcError):
             await modeller_mailbox._send_to_all_pods_aes_encrypt(
                 message, _BitfountMessageType.ALGORITHM_EXCHANGE
             )
@@ -1024,24 +920,22 @@
                 message_type=_BitfountMessageType.ALGORITHM_EXCHANGE,
                 body=expected_encrypted_message,
                 recipient=pod_identifiers[0],
                 recipient_mailbox_id=worker_mailboxes[pod_identifiers[0]].mailbox_id,
                 sender=modeller_name,
                 sender_mailbox_id=modeller_mailbox_id,
                 timestamp=fake_timestamps[0],
-                task_id=task_id,
             ),
             already_packed=True,
         )
 
     def test_receive_aes_decrypt_successful(
         self,
         modeller_mailbox: _ModellerMailbox,
         modeller_mailbox_id: str,
-        opt_task_id: Optional[str],
         pod_identifiers: List[str],
         worker_mailboxes: Dict[str, _WorkerMailboxDetails],
     ) -> None:
         """Receives and decrypts messages using provided AES key."""
         expected_decrypted_message = b"some message"
         original_message = _BitfountMessage(
             message_type=_BitfountMessageType.TRAINING_COMPLETE,
@@ -1049,58 +943,38 @@
                 msgpack.dumps(expected_decrypted_message),
                 worker_mailboxes[pod_identifiers[1]].aes_encryption_key,
             ),
             recipient=pod_identifiers[0],
             recipient_mailbox_id=worker_mailboxes[pod_identifiers[0]].mailbox_id,
             sender=pod_identifiers[1],
             sender_mailbox_id=modeller_mailbox_id,
-            task_id=opt_task_id,
         )
         modeller_mailbox.accepted_worker_mailboxes = worker_mailboxes
 
         message = modeller_mailbox._decrypt_message(original_message)
 
         assert message == _DecryptedBitfountMessage(
             message_type=_BitfountMessageType.TRAINING_COMPLETE,
             body=expected_decrypted_message,
             recipient=pod_identifiers[0],
             recipient_mailbox_id=worker_mailboxes[pod_identifiers[0]].mailbox_id,
             sender=pod_identifiers[1],
             sender_mailbox_id=modeller_mailbox_id,
             timestamp=original_message.timestamp,
-            task_id=original_message.task_id,
-        )
-
-    async def test_send_task_start_message(
-        self,
-        mocker: MockerFixture,
-        modeller_mailbox: _ModellerMailbox,
-    ) -> None:
-        """Tests modeller sends task start empty message."""
-        mock_message_decrypt = mocker.patch.object(
-            modeller_mailbox,
-            "_send_to_all_pods_aes_encrypt",
-            return_value=None,
-        )
-
-        await modeller_mailbox.send_task_start_message()
-        mock_message_decrypt.assert_awaited_once_with(
-            None, _BitfountMessageType.TASK_START
         )
 
     async def test_send_task_complete_message(
         self,
         mock_message_service: Mock,
         mock_message_timestamps: Callable[[Iterable[str]], Mock],
         mocker: MockerFixture,
         modeller_mailbox: _ModellerMailbox,
         modeller_mailbox_id: str,
         modeller_name: str,
         pod_identifiers: List[str],
-        task_id: str,
         worker_mailboxes: Dict[str, _WorkerMailboxDetails],
     ) -> None:
         """Task complete message is sent AES encrypted using provided key."""
         fake_timestamps = ["Hello", "World", "!"]
         mock_message_timestamps(fake_timestamps)
         encrypt = mocker.patch.object(_MessageEncryption, "encrypt_outgoing_message")
         encrypt.side_effect = [None, None, None]
@@ -1136,60 +1010,58 @@
                         recipient=pod_identifiers[0],
                         recipient_mailbox_id=worker_mailboxes[
                             pod_identifiers[0]
                         ].mailbox_id,
                         sender=modeller_name,
                         sender_mailbox_id=modeller_mailbox_id,
                         timestamp=fake_timestamps[0],
-                        task_id=task_id,
                     ),
                     already_packed=True,
                 ),
                 call(
                     _BitfountMessage(
                         message_type=_BitfountMessageType.TASK_COMPLETE,
                         body=None,  # type: ignore[arg-type] # reason: message body should be empty # noqa: B950
                         recipient=pod_identifiers[1],
                         recipient_mailbox_id=worker_mailboxes[
                             pod_identifiers[1]
                         ].mailbox_id,
                         sender=modeller_name,
                         sender_mailbox_id=modeller_mailbox_id,
                         timestamp=fake_timestamps[1],
-                        task_id=task_id,
                     ),
                     already_packed=True,
                 ),
                 call(
                     _BitfountMessage(
                         message_type=_BitfountMessageType.TASK_COMPLETE,
                         body=None,  # type: ignore[arg-type] # reason: message body should be empty # noqa: B950
                         recipient=pod_identifiers[2],
                         recipient_mailbox_id=worker_mailboxes[
                             pod_identifiers[2]
                         ].mailbox_id,
                         sender=modeller_name,
                         sender_mailbox_id=modeller_mailbox_id,
                         timestamp=fake_timestamps[2],
-                        task_id=task_id,
                     ),
                     already_packed=True,
                 ),
             ]
         )
         assert mock_message_service.send_message.call_count == 3
 
     async def test_get_oidc_client_ids(
         self,
         mock_aes_decrypter: Mock,
+        mock_poll_for_messages: Callable[
+            [Iterable[MessageOrException]], AsyncIteratorMock
+        ],
         modeller_mailbox: _ModellerMailbox,
         modeller_mailbox_id: str,
         modeller_name: str,
-        opt_task_id: Optional[str],
-        patch_poll_for_messages: PollForMessagesPatcher,
         worker_mailbox_details: List[WorkerDetails],
     ) -> None:
         """Test get_oidc_client_ids() extracts needed details."""
         # Create fake task response messages
         oidc_client_ids = [
             _OIDCClientID(f"client_id_{i}").serialize() for i in range(3)
         ]
@@ -1197,22 +1069,21 @@
             _BitfountMessage(
                 message_type=_BitfountMessageType.OIDC_CHALLENGE,
                 body=msgpack.dumps(oidc_client_ids[i]),
                 recipient=modeller_name,
                 recipient_mailbox_id=modeller_mailbox_id,
                 sender=worker_mailbox_details[i].pod_identifier,
                 sender_mailbox_id=worker_mailbox_details[i].mailbox_id,
-                task_id=opt_task_id,
             )
             for i in range(3)
         ]
 
         # Assign these to poll_for_messages which is the underlying method we
         # expect to yield messages.
-        patch_poll_for_messages(oidc_challenges)
+        mock_poll_for_messages(oidc_challenges)
 
         # Start listening and processing messages
         received_oidc_challenges = await _run_func_and_listen_to_mailbox(
             modeller_mailbox.get_oidc_client_ids(), modeller_mailbox
         )
 
         # Ensure extracted details match expected
@@ -1221,20 +1092,22 @@
                 oidc_client_ids[i]["client_id"]
             )
             for i in range(3)
         }
 
     async def test_get_oidc_client_ids_throws_error(
         self,
+        mock_poll_for_messages: Callable[
+            [Iterable[MessageOrException]], AsyncIteratorMock
+        ],
         modeller_mailbox: _ModellerMailbox,
-        patch_poll_for_messages: PollForMessagesPatcher,
         rpc_error: RpcError,
     ) -> None:
         """Test Start Task error thrown on error."""
-        patch_poll_for_messages([rpc_error])
+        mock_poll_for_messages([rpc_error])
         with raises(
             BitfountTaskStartError, match="Failed to start task with all pods."
         ):
             await _run_func_and_listen_to_mailbox(
                 modeller_mailbox.get_oidc_client_ids(),
                 modeller_mailbox,
             )
@@ -1243,15 +1116,14 @@
         self,
         mock_message_service: Mock,
         mocker: MockerFixture,
         modeller_mailbox: _ModellerMailbox,
         modeller_mailbox_id: str,
         modeller_name: str,
         pod_identifiers: List[str],
-        task_id: str,
         worker_mailboxes: Dict[str, _WorkerMailboxDetails],
     ) -> None:
         """Test send_oidc_auth_flow_responses() sends expected messages."""
         # Set modeller username
         mock_message_service.username = modeller_name
 
         # Mock out _send_aes_encrypted_message()
@@ -1283,29 +1155,27 @@
                     aes_encryption_key=worker_mailboxes[pod_id].aes_encryption_key,
                     message_service=modeller_mailbox.message_service,
                     message_type=_BitfountMessageType.OIDC_AFC_PKCE_RESPONSE,
                     recipient=worker_mailboxes[pod_id].pod_identifier,
                     recipient_mailbox_id=worker_mailboxes[pod_id].mailbox_id,
                     sender=modeller_name,
                     sender_mailbox_id=modeller_mailbox_id,
-                    task_id=task_id,
                 )
                 for pod_id in pod_identifiers
             ]
         )
 
     async def test_send_oidc_device_code_responses(
         self,
         mock_message_service: Mock,
         mocker: MockerFixture,
         modeller_mailbox: _ModellerMailbox,
         modeller_mailbox_id: str,
         modeller_name: str,
         pod_identifiers: List[str],
-        task_id: str,
         worker_mailboxes: Dict[str, _WorkerMailboxDetails],
     ) -> None:
         """Test send_oidc_auth_flow_responses() sends expected messages."""
         # Set modeller username
         mock_message_service.username = modeller_name
 
         # Mock out _send_aes_encrypted_message()
@@ -1342,303 +1212,120 @@
                     aes_encryption_key=worker_mailboxes[pod_id].aes_encryption_key,
                     message_service=modeller_mailbox.message_service,
                     message_type=_BitfountMessageType.OIDC_DEVICE_CODE_RESPONSE,
                     recipient=worker_mailboxes[pod_id].pod_identifier,
                     recipient_mailbox_id=worker_mailboxes[pod_id].mailbox_id,
                     sender=modeller_name,
                     sender_mailbox_id=modeller_mailbox_id,
-                    task_id=task_id,
                 )
                 for pod_id in pod_identifiers
             ]
         )
 
-    def test__setup_online_status_handler(
-        self, mocker: MockerFixture, modeller_mailbox: _ModellerMailbox
-    ) -> None:
-        """Test that modeller mailbox registers ONLINE_CHECK message handler."""
-        mock_register_handler = mocker.patch.object(
-            modeller_mailbox, "register_handler"
-        )
-        modeller_mailbox._setup_online_status_handler()
-        mock_register_handler.assert_called_once()
-        assert (
-            mock_register_handler.call_args[0][0] == _BitfountMessageType.ONLINE_CHECK
-        )
-
-    async def test_modeller_responds_to_online_status_request(
-        self,
-        caplog: LogCaptureFixture,
-        mock_aes_decrypter: Mock,
-        mock_message_service: Mock,
-        mocker: MockerFixture,
-        modeller_mailbox: _ModellerMailbox,
-        modeller_mailbox_id: str,
-        modeller_name: str,
-        online_check_uuid: str,
-        opt_task_id: Optional[str],
-        patch_poll_for_messages: PollForMessagesPatcher,
-        task_id: str,
-        worker_mailbox_details: List[WorkerDetails],
-    ) -> None:
-        """Some pods never send a response to the training request."""
-        caplog.set_level("INFO")
-        # Pod sends message to modeller checking if they are online
-        pod_messages = [
-            _BitfountMessage(
-                message_type=_BitfountMessageType.ONLINE_CHECK,
-                body=online_check_uuid,  # type: ignore[arg-type] # reason: should return unchanged # noqa: B950
-                recipient=modeller_name,
-                recipient_mailbox_id=modeller_mailbox_id,
-                sender=worker_mailbox_details[0].pod_identifier,
-                sender_mailbox_id=worker_mailbox_details[0].mailbox_id,
-                task_id=opt_task_id,
-            ),
-        ]
-
-        # For ease of testing, we pretend that the pod has already accepted the task
-        modeller_mailbox.accepted_worker_mailboxes = {
-            worker_mailbox_details[0].pod_identifier: worker_mailbox_details[0]  # type: ignore[dict-item] # reason: ease of testing # noqa: B950
-        }
-
-        mock_send_message = mocker.patch(
-            "bitfount.federated.transport.modeller_transport._send_aes_encrypted_message"  # noqa: B950
-        )
-
-        # Mock out _BitfountMessage constructor
-        mock_bitfount_message_cls = mocker.patch(
-            "bitfount.federated.transport.modeller_transport._BitfountMessage"
-        )
-
-        # Assign this message to poll_for_messages yield
-        patch_poll_for_messages(pod_messages)
-
-        # Start listening and processing messages. We timeout on the responses to
-        # simulate non-responding pods. This may need to be increased if the test
-        # is failing but should need to be no higher than 5 seconds.
-        await _run_func_and_listen_to_mailbox(
-            modeller_mailbox.process_task_request_responses(timeout=3), modeller_mailbox
-        )
-
-        # Assert that the response was sent to the correct pod and is of the right type
-        assert "Informing user1/pod_1 that we are still online." in get_info_logs(
-            caplog
-        )
-
-        # Check correct message sending was used
-        mock_send_message.assert_not_called()
-        mock_message_service.send_message.assert_called_once_with(
-            mock_bitfount_message_cls.return_value, already_packed=True
-        )
-        mock_bitfount_message_cls.assert_called_once_with(
-            body=online_check_uuid,
-            message_type=_BitfountMessageType.ONLINE_RESPONSE,
-            recipient=worker_mailbox_details[0].pod_identifier,
-            recipient_mailbox_id=worker_mailbox_details[0].mailbox_id,
-            sender=modeller_mailbox.message_service.username,
-            sender_mailbox_id=modeller_mailbox.mailbox_id,
-            task_id=task_id,
-        )
-
-    async def test_log(
-        self, mocker: MockerFixture, modeller_mailbox: _ModellerMailbox
-    ) -> None:
-        """Test that log message is sent appropriately."""
-        mock_log = mocker.patch.object(
-            modeller_mailbox, "_send_to_all_pods_aes_encrypt"
-        )
-        await modeller_mailbox.log({"msg": "message"})
-        mock_log.assert_awaited_once_with(
-            {"msg": "message"}, _BitfountMessageType.LOG_MESSAGE
-        )
-
-    @pytest.mark.parametrize(
-        "process_name",
-        ("aProcessName", None),
-        ids=lambda x: f"process_name_present={bool(x)}",
-    )
-    @pytest.mark.parametrize(
-        "thread_name",
-        ("aThreadName", None),
-        ids=lambda x: f"thread_name_present={bool(x)}",
-    )
-    def test_log_message_handler(
-        self,
-        mock_aes_decrypter: Mock,
-        mocker: MockerFixture,
-        modeller_mailbox: _ModellerMailbox,
-        modeller_mailbox_id: str,
-        modeller_name: str,
-        process_name: Optional[str],
-        task_id: str,
-        thread_name: Optional[str],
-        worker_mailbox_details: List[WorkerDetails],
-    ) -> None:
-        """Test LOG_MESSAGE handler modifies record as expected."""
-        # Mock out logging framework interaction
-        mock_logger = mocker.patch(
-            "bitfount.federated.transport.modeller_transport.logger", autospec=True
-        )
-        mock_logging = mocker.patch(
-            "bitfount.federated.transport.modeller_transport.logging", autospec=True
-        )
-
-        # Create mock log message
-        mock_log_message_contents = {
-            "msg": "An important log message",
-            "anotherField": "notToBeChanged",
-            "federated": True,
-        }
-        if process_name:
-            mock_log_message_contents["processName"] = process_name
-        if thread_name:
-            mock_log_message_contents["threadName"] = thread_name
-
-        mock_log_message = _BitfountMessage(
-            message_type=_BitfountMessageType.LOG_MESSAGE,
-            body=msgpack.dumps(mock_log_message_contents),
-            recipient=modeller_name,
-            recipient_mailbox_id=modeller_mailbox_id,
-            sender=worker_mailbox_details[0].pod_identifier,
-            sender_mailbox_id=worker_mailbox_details[0].mailbox_id,
-            task_id=task_id,
-        )
-
-        log_message_handler = modeller_mailbox._get_log_message_handler()
-        log_message_handler(mock_log_message)
-
-        # Message contents should have been manipulated and logged out
-        expected_log_message = {
-            "msg": (
-                f"<FROM POD {worker_mailbox_details[0].pod_identifier}>:"
-                f" An important log message"
-            ),
-            "anotherField": "notToBeChanged",
-        }
-        if process_name:
-            expected_log_message["processName"] = f"<{process_name}>"
-        if thread_name:
-            expected_log_message["threadName"] = f"<{thread_name}>"
-        mock_logging.makeLogRecord.assert_called_once_with(expected_log_message)
-        mock_logger.handle.assert_called_once_with(
-            mock_logging.makeLogRecord.return_value
-        )
-
 
 @unit_test
 class TestModellerTransportFunctions:
     """Tests for functions in modeller_transport module."""
 
     async def test__parameters_update_handler(
         self,
         mocker: MockerFixture,
         modeller_mailbox: _ModellerMailbox,
         modeller_mailbox_id: str,
-        opt_task_id: Optional[str],
         pod_identifiers: List[str],
         worker_mailboxes: Dict[str, _WorkerMailboxDetails],
     ) -> None:
         """Tests that weight updates are populated by handler."""
         # Decrypted & message
         expected_message = {"some parameter": ["some weight list"]}
         decrypt = mocker.patch.object(_BitfountMessage, "decrypt")
         decrypt.return_value = _DecryptedBitfountMessage(
             message_type=_BitfountMessageType.TRAINING_UPDATE,
             body=expected_message,
             recipient=pod_identifiers[0],
             recipient_mailbox_id=worker_mailboxes[pod_identifiers[0]].mailbox_id,
             sender=pod_identifiers[1],
             sender_mailbox_id=modeller_mailbox_id,
-            task_id=opt_task_id,
         )
 
         modeller_mailbox.accepted_worker_mailboxes = worker_mailboxes
 
         # Message from pod
         expected_encrypted_message = b"encrypted_message"
         message = _BitfountMessage(
             message_type=_BitfountMessageType.TRAINING_UPDATE,
             body=expected_encrypted_message,
             recipient=pod_identifiers[0],
             recipient_mailbox_id=worker_mailboxes[pod_identifiers[0]].mailbox_id,
             sender=pod_identifiers[1],
             sender_mailbox_id=modeller_mailbox_id,
-            task_id=opt_task_id,
         )
 
-        weight_updates: Dict[str, _SerializedWeights] = {}
+        weight_updates: List[_SerializedWeights] = []
         model_update_handler = _parameter_updates_handler(
             modeller_mailbox, weight_updates
         )
         with _AsyncMultipleResponsesHandler(
             handler=model_update_handler,
             message_types=_BitfountMessageType.TRAINING_UPDATE,
             mailbox=modeller_mailbox,
             responders=modeller_mailbox.accepted_worker_mailboxes.keys(),
         ) as response_handler:
             await response_handler.handler(message)
 
-        assert weight_updates == {pod_identifiers[1]: expected_message}
+        assert weight_updates == [expected_message]
 
     async def test__training_metrics_handler(
         self,
         mocker: MockerFixture,
         modeller_mailbox: _ModellerMailbox,
         modeller_mailbox_id: str,
-        opt_task_id: Optional[str],
         pod_identifiers: List[str],
         worker_mailboxes: Dict[str, _WorkerMailboxDetails],
     ) -> None:
         """Tests that validation metrics are populated by handler."""
         # Decrypted & message
-        expected_message = {"some parameter": "some metric"}
+        expected_message = [{"some parameter": "some metric"}]
         decrypt = mocker.patch.object(_BitfountMessage, "decrypt")
         decrypt.return_value = _DecryptedBitfountMessage(
             message_type=_BitfountMessageType.TRAINING_METRICS,
             body=expected_message,
             recipient=pod_identifiers[0],
             recipient_mailbox_id=worker_mailboxes[pod_identifiers[0]].mailbox_id,
             sender=pod_identifiers[1],
             sender_mailbox_id=modeller_mailbox_id,
-            task_id=opt_task_id,
         )
 
         modeller_mailbox.accepted_worker_mailboxes = worker_mailboxes
 
         # Message from pod
         expected_encrypted_message = b"encrypted_message"
         message = _BitfountMessage(
             message_type=_BitfountMessageType.TRAINING_METRICS,
             body=expected_encrypted_message,
             recipient=pod_identifiers[0],
             recipient_mailbox_id=worker_mailboxes[pod_identifiers[0]].mailbox_id,
             sender=pod_identifiers[1],
             sender_mailbox_id=modeller_mailbox_id,
-            task_id=opt_task_id,
         )
 
         validation_metrics: List[Mapping[str, str]] = []
+
         validation_metrics_handler = _training_metrics_handler(
             modeller_mailbox, validation_metrics
         )
 
         with _AsyncMultipleResponsesHandler(
             handler=validation_metrics_handler,
             message_types=_BitfountMessageType.TRAINING_UPDATE,
             mailbox=modeller_mailbox,
             responders=modeller_mailbox.accepted_worker_mailboxes.keys(),
         ) as response_handler:
             await response_handler.handler(message)
-
-        # Only single element here but order may be arbitrary in larger numbers
-        # As `Mapping`/`Dict` is unsortable and unhashable, have to manually
-        # do comparison.
-        expected_message_list = [expected_message]
-        assert len(validation_metrics) == len(expected_message_list)
-        assert all(metrics in expected_message_list for metrics in validation_metrics)
+        assert validation_metrics == [expected_message]
 
     async def test_get_metrics_updates_calls_handler(
         self, mocker: MockerFixture, modeller_mailbox: _ModellerMailbox
     ) -> None:
         """Tests that the appropriate handler is called."""
         mock_handler = Mock()
         mocker.patch(
@@ -1656,136 +1343,8 @@
         mock_handler = Mock()
         mocker.patch(
             "bitfount.federated.transport.modeller_transport._parameter_updates_handler",  # noqa: B950 ignore line too long
             mock_handler,
         )
 
         await _get_parameter_updates_from_workers(modeller_mailbox, timeout=1)
-        mock_handler.assert_called_once_with(modeller_mailbox, {})
-
-    async def test__public_key_handler(
-        self,
-        mocker: MockerFixture,
-        modeller_mailbox: _ModellerMailbox,
-        modeller_mailbox_id: str,
-        opt_task_id: Optional[str],
-        pod_identifiers: List[str],
-        worker_mailboxes: Dict[str, _WorkerMailboxDetails],
-    ) -> None:
-        """Tests that public key is given by handler."""
-        # Decrypted & message
-        expected_message = [{"some public_key"}]
-        decrypt = mocker.patch.object(_BitfountMessage, "decrypt")
-        decrypt.return_value = _DecryptedBitfountMessage(
-            message_type=_BitfountMessageType.KEY_EXCHANGE,
-            body=expected_message,
-            recipient=pod_identifiers[0],
-            recipient_mailbox_id=worker_mailboxes[pod_identifiers[0]].mailbox_id,
-            sender=pod_identifiers[1],
-            sender_mailbox_id=modeller_mailbox_id,
-            task_id=opt_task_id,
-        )
-
-        modeller_mailbox.accepted_worker_mailboxes = worker_mailboxes
-
-        # Message from pod
-        expected_encrypted_message = b"encrypted_message"
-        message = _BitfountMessage(
-            message_type=_BitfountMessageType.KEY_EXCHANGE,
-            body=expected_encrypted_message,
-            recipient=pod_identifiers[0],
-            recipient_mailbox_id=worker_mailboxes[pod_identifiers[0]].mailbox_id,
-            sender=pod_identifiers[1],
-            sender_mailbox_id=modeller_mailbox_id,
-            task_id=opt_task_id,
-        )
-
-        pub_keys: List[bytes] = []
-        pub_keys_handler = _public_key_handler(modeller_mailbox, pub_keys)
-
-        with _AsyncMultipleResponsesHandler(
-            handler=pub_keys_handler,
-            message_types=_BitfountMessageType.KEY_EXCHANGE,
-            mailbox=modeller_mailbox,
-            responders=modeller_mailbox.accepted_worker_mailboxes.keys(),
-        ) as response_handler:
-            await response_handler.handler(message)
-
-        # Only single element here but order may be arbitrary in larger numbers.
-        # Order is not guaranteed so need to sort.
-        assert sorted(pub_keys) == sorted([expected_message])
-
-    async def test__get_public_key_calls_handler(
-        self, mocker: MockerFixture, modeller_mailbox: _ModellerMailbox
-    ) -> None:
-        """Tests that the appropriate handler is called."""
-        mock_handler = Mock()
-        mocker.patch(
-            "bitfount.federated.transport.modeller_transport._public_key_handler",  # noqa: B950 ignore line too long
-            mock_handler,
-        )
-        await _get_public_key(modeller_mailbox, timeout=1)
-        mock_handler.assert_called_once_with(modeller_mailbox, [])
-
-    async def test__psi_dataset_handler(
-        self,
-        mocker: MockerFixture,
-        modeller_mailbox: _ModellerMailbox,
-        modeller_mailbox_id: str,
-        opt_task_id: Optional[str],
-        pod_identifiers: List[str],
-        worker_mailboxes: Dict[str, _WorkerMailboxDetails],
-    ) -> None:
-        """Tests that psi dataset is given by handler."""
-        # Decrypted & message
-        expected_message = [{"some data"}]
-        decrypt = mocker.patch.object(_BitfountMessage, "decrypt")
-        decrypt.return_value = _DecryptedBitfountMessage(
-            message_type=_BitfountMessageType.PSI_DATASET,
-            body=expected_message,
-            recipient=pod_identifiers[0],
-            recipient_mailbox_id=worker_mailboxes[pod_identifiers[0]].mailbox_id,
-            sender=pod_identifiers[1],
-            sender_mailbox_id=modeller_mailbox_id,
-            task_id=opt_task_id,
-        )
-
-        modeller_mailbox.accepted_worker_mailboxes = worker_mailboxes
-
-        # Message from pod
-        expected_encrypted_message = b"encrypted_message"
-        message = _BitfountMessage(
-            message_type=_BitfountMessageType.PSI_DATASET,
-            body=expected_encrypted_message,
-            recipient=pod_identifiers[0],
-            recipient_mailbox_id=worker_mailboxes[pod_identifiers[0]].mailbox_id,
-            sender=pod_identifiers[1],
-            sender_mailbox_id=modeller_mailbox_id,
-            task_id=opt_task_id,
-        )
-
-        psi_dataset: List[Tuple[List[str], List[str]]] = []
-        psi_dataset_handler = _psi_dataset_handler(modeller_mailbox, psi_dataset)
-
-        with _AsyncMultipleResponsesHandler(
-            handler=psi_dataset_handler,
-            message_types=_BitfountMessageType.PSI_DATASET,
-            mailbox=modeller_mailbox,
-            responders=modeller_mailbox.accepted_worker_mailboxes.keys(),
-        ) as response_handler:
-            await response_handler.handler(message)
-
-        # Only single element here but order may be arbitrary in larger numbers
-        # Need to sort as order is not guaranteed
-        assert sorted(psi_dataset) == sorted([expected_message])
-
-    async def test__get_psi_datasets_from_workers_calls_handler(
-        self, mocker: MockerFixture, modeller_mailbox: _ModellerMailbox
-    ) -> None:
-        """Tests that the appropriate handler is called."""
-        mock_handler = Mock()
-        mocker.patch(
-            "bitfount.federated.transport.modeller_transport._psi_dataset_handler",  # noqa: B950 ignore line too long
-            mock_handler,
-        )
-        await _get_psi_datasets_from_workers(modeller_mailbox, timeout=1)
         mock_handler.assert_called_once_with(modeller_mailbox, [])
```

### Comparing `bitfount-0.5.86/tests/bitfount/federated/transport/test_pod_transport.py` & `bitfount-0.5.9/tests/bitfount/federated/transport/test_pod_transport.py`

 * *Files 10% similar despite different names*

```diff
@@ -54,27 +54,23 @@
         return _PodMailbox(pod_name, pod_mailbox_id, mock_message_service)
 
     async def test_connect_pod_successful(
         self, mock_message_service: Mock, pod_mailbox: _PodMailbox, pod_name: str
     ) -> None:
         """connect_pod successful."""
         mock_message_service.connect_pod.return_value = SuccessResponse()
-        await pod_mailbox.connect_pod(pod_name, None, mock_message_service)
-        mock_message_service.connect_pod.assert_called_once_with(pod_name, None)
+        await pod_mailbox.connect_pod(pod_name, mock_message_service)
+        mock_message_service.connect_pod.assert_called_once_with(pod_name)
 
     async def test_connect_pod_unsuccessful(
-        self,
-        mock_message_service: Mock,
-        pod_mailbox: _PodMailbox,
-        pod_name: str,
-        rpc_error: RpcError,
+        self, mock_message_service: Mock, pod_mailbox: _PodMailbox, pod_name: str
     ) -> None:
         """connect_pod fails."""
-        mock_message_service.connect_pod.side_effect = rpc_error
+        mock_message_service.connect_pod.side_effect = RpcError()
 
         with raises(
             PodConnectFailedError,
             match=f"Failed to connect to messaging service as pod: {pod_name}",
         ):
-            await pod_mailbox.connect_pod(pod_name, None, mock_message_service)
+            await pod_mailbox.connect_pod(pod_name, mock_message_service)
 
-        mock_message_service.connect_pod.assert_called_once_with(pod_name, None)
+        mock_message_service.connect_pod.assert_called_once_with(pod_name)
```

### Comparing `bitfount-0.5.86/tests/bitfount/federated/transport/test_utils.py` & `bitfount-0.5.9/tests/bitfount/federated/transport/test_utils.py`

 * *Files identical despite different names*

### Comparing `bitfount-0.5.86/tests/bitfount/federated/transport/test_worker_transport.py` & `bitfount-0.5.9/tests/bitfount/federated/transport/test_message_service.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,1393 +1,1373 @@
-"""Test worker can communicate with modeller."""
-from __future__ import annotations
-
-import asyncio
-from collections import namedtuple
-from datetime import datetime, timezone
-import logging
+"""Tests GRPC Wrapper class."""
+from datetime import datetime
+from pathlib import Path
 import re
-from typing import Callable, Dict, Iterable, List, Optional
-from unittest.mock import ANY, Mock, NonCallableMagicMock, call, create_autospec
+import tempfile
+from unittest.mock import (
+    ANY,
+    AsyncMock,
+    Mock,
+    NonCallableMock,
+    PropertyMock,
+    call,
+    create_autospec,
+)
 
-from cryptography.hazmat.primitives.asymmetric.rsa import RSAPrivateKey, RSAPublicKey
-from grpc import RpcError
+from grpc import RpcError, StatusCode
 import msgpack
 import pytest
-from pytest import LogCaptureFixture, fixture, raises
+from pytest import fixture, raises
 from pytest_mock import MockerFixture
+from requests import HTTPError, RequestException
 
-from bitfount.federated.transport.base_transport import (
-    MessageRetrievalError,
-    _run_func_and_listen_to_mailbox,
-)
+from bitfount.federated.encryption import _AESEncryption
+from bitfount.federated.transport.config import MessageServiceConfig
 from bitfount.federated.transport.message_service import (
+    _MAX_STORAGE_SIZE_BYTES,
+    _MAX_STORAGE_SIZE_MEGABYTES,
+    _SMALL_MESSAGE_UPPER_LIMIT_SIZE_BYTES,
     _BitfountMessage,
     _BitfountMessageType,
+    _DecryptedBitfountMessage,
+    _LargeObjectRequestHandler,
     _MessageEncryption,
     _MessageService,
 )
-from bitfount.federated.transport.protos.messages_pb2 import SuccessResponse
-from bitfount.federated.transport.worker_transport import (
-    _DEFAULT_AUTHENTICATION_MODELLER_RESPONSE_TIMEOUT,
-    _SOFT_LIMIT_MESSAGE_TIMEOUT,
-    _get_psi_dataset,
-    _get_worker_secure_shares,
-    _InterPodWorkerMailbox,
-    _send_secure_shares_to_others,
-    _WorkerMailbox,
+from bitfount.federated.transport.protos.messages_pb2 import (
+    BitfountMessage as GrpcBitfountMessage,
 )
-from bitfount.federated.types import _PodResponseType
-from tests.utils import PytestRequest
-from tests.utils.fixtures import PollForMessagesPatcher
-from tests.utils.helper import (
-    get_debug_logs,
-    get_error_logs,
-    get_info_logs,
-    get_warning_logs,
-    unit_test,
+from bitfount.federated.transport.protos.messages_pb2 import (
+    BitfountTask,
+    BitfountTasks,
+    BlobStorageData,
+    CommunicationDetails,
+    PodData,
+    SuccessResponse,
 )
+from bitfount.federated.transport.protos.messages_pb2 import Acknowledgement
+from bitfount.hub.authentication_flow import BitfountSession
+from bitfount.types import _S3PresignedPOSTFields, _S3PresignedPOSTURL, _S3PresignedURL
+from bitfount.utils import _get_mb_from_bytes
+from tests.utils.helper import unit_test
+from tests.utils.mocks import create_dataclass_mock
 
-WorkerDetails = namedtuple("WorkerDetails", ["pod_identifier", "mailbox_id"])
+access_token = "someFakeAccessToken"
 
 
-@fixture
-def pod_identifier() -> str:
-    """A pod identifier."""
-    return "someUser/somePod"
-
-
-@fixture
-def worker_mailbox_id() -> str:
-    """A mailbox ID for the worker."""
-    return "thisWorkersMailboxID"
-
-
-@fixture
-def modeller_mailbox_id() -> str:
-    """The mailbox ID for the modeller."""
-    return "someModellerMailboxId"
-
-
-@fixture
-def modeller_name() -> str:
-    """The name of the modeller."""
-    return "someModeller"
-
-
-@fixture
-def aes_key() -> bytes:
-    """The AES key to use in message encryption between worker and modeller."""
-    return b"SecretAESKey"
-
-
-@fixture
-def mock_aes_decrypter(mocker: MockerFixture) -> Mock:
-    """Mock AES decryption for cases where message bodies aren't encrypted."""
-    # If the message bodies are not encrypted we need to mock out the decryption
-    # and instead replace it with just returning message bodies.
-    mock_decrypter = mocker.patch.object(_MessageEncryption, "decrypt_incoming_message")
-    mock_decrypter.side_effect = lambda body, key: body
-    return mock_decrypter
-
-
-@fixture
-def mock_message_service() -> Mock:
-    """Mock message service."""
-    mock_message_service: Mock = create_autospec(_MessageService, instance=True)
-    return mock_message_service
-
-
-@fixture
-def other_pod_details() -> List[WorkerDetails]:
-    """Details of the other pods involved in the task."""
-    return [
-        WorkerDetails("user/differentpod", "someMailboxID2"),
-        WorkerDetails("another/pod", "someMailboxID3"),
-    ]
-
-
-@fixture
-def pod_mailbox_ids(
-    other_pod_details: List[WorkerDetails], pod_identifier: str, worker_mailbox_id: str
-) -> Dict[str, str]:
-    """A mapping of pod identifier to mailbox ID for all task workers.
-
-    This includes the worker itself.
-    """
-    return {
-        pod_identifier: worker_mailbox_id,
-        other_pod_details[0].pod_identifier: other_pod_details[0].mailbox_id,
-        other_pod_details[1].pod_identifier: other_pod_details[1].mailbox_id,
-    }
-
-
-@fixture(params=(True, False), ids=("task_id_incl", "no_task_id"))
-def opt_task_id(request: PytestRequest) -> Optional[str]:
-    """Returns a task ID or None, to cover both cases."""
-    incl_task_id: bool = request.param
-    if incl_task_id:
-        return "this-is-a-task-id"
-    else:
-        return None
-
-
-@fixture
-def online_check_uuid() -> str:
-    """UUID for online checking."""
-    return "auuidforonlinecheck"
-
-
-@fixture
-def worker_mailbox(
-    aes_key: bytes,
-    mock_message_service: Mock,
-    modeller_mailbox_id: str,
-    modeller_name: str,
-    opt_task_id: Optional[str],
-    pod_identifier: str,
-    pod_mailbox_ids: Dict[str, str],
-) -> _WorkerMailbox:
-    """A WorkerMailbox instance with components mocked out."""
-    return _WorkerMailbox(
-        pod_identifier=pod_identifier,
-        modeller_mailbox_id=modeller_mailbox_id,
-        modeller_name=modeller_name,
-        aes_encryption_key=aes_key,
-        message_service=mock_message_service,
-        pod_mailbox_ids=pod_mailbox_ids,
-        task_id=opt_task_id,
-    )
-
-
-@fixture
-def mock_pod_public_keys(other_pod_details: List[WorkerDetails]) -> Dict[str, Mock]:
-    """Mock public keys for each "other" pod."""
-    other_pod_ids = [wd.pod_identifier for wd in other_pod_details]
-    return {
-        pod_id: create_autospec(RSAPublicKey, instance=True) for pod_id in other_pod_ids
-    }
-
-
-@fixture
-def mock_private_key() -> Mock:
-    """Mock private key for target pod."""
-    mock_private_key: Mock = create_autospec(RSAPrivateKey, instance=True)
-    return mock_private_key
-
-
-@fixture
-def interpod_worker_mailbox(
-    aes_key: bytes,
-    mock_message_service: Mock,
-    mock_pod_public_keys: Dict[str, Mock],
-    mock_private_key: Mock,
-    modeller_mailbox_id: str,
-    modeller_name: str,
-    opt_task_id: Optional[str],
-    other_pod_details: List[WorkerDetails],
-    pod_identifier: str,
-    pod_mailbox_ids: Dict[str, str],
-) -> _InterPodWorkerMailbox:
-    """An InterPodWorkerMailbox instance with components mocked out."""
-    return _InterPodWorkerMailbox(
-        pod_public_keys=mock_pod_public_keys,
-        private_key=mock_private_key,
-        pod_identifier=pod_identifier,
-        modeller_mailbox_id=modeller_mailbox_id,
-        modeller_name=modeller_name,
-        aes_encryption_key=aes_key,
-        message_service=mock_message_service,
-        pod_mailbox_ids=pod_mailbox_ids,
-        task_id=opt_task_id,
-    )
-
-
-@fixture
-def mock_message_type() -> Mock:
-    """A mock instance of the _BitfountMessageType enum."""
-    mock_message_type: Mock = create_autospec(_BitfountMessageType, instance=True)
-    return mock_message_type
-
-
-@fixture
-def dummy_message_awaitable() -> (
-    Callable[[Optional[_BitfountMessage], Optional[bool]], object]
-):
-    """A dummy awaitable for testing."""
+@unit_test
+class TestMessageService:
+    """Test MessageService (GRPC wrapper)."""
 
-    class MockMessageAwaitable:
-        """Dummy awaitable in place of _AsyncCallback.
+    @fixture
+    def username(self) -> str:
+        """Username."""
+        return "theAuthenticatedUser"
+
+    @fixture
+    def session(self, username: str) -> Mock:
+        """Mocked session."""
+        session: Mock = create_autospec(BitfountSession, instance=True)
+        session.access_token = access_token
+        session.username = username
+        return session
+
+    @fixture
+    def grpc_stub(self) -> Mock:
+        """Mocked GRPC Stub.
 
-        The `timeout` indicates whether the first time the awaitable is called,
-        it should return a message or raise an `asyncio.TimeoutError`.
+        Mocking this is a bit unpleasant due to the way it is constructed.
         """
+        mock_stub = AsyncMock(
+            spec_set=[
+                "PodConnect",
+                "SetupTaskMailboxes",
+                "SendBitfountMessage",
+                "GetBitfountMessage",
+                "GetLargeObjectStorage",
+                "AcknowledgeMessage",
+            ]
+        )
 
-        def __init__(
-            self,
-            message: Optional[_BitfountMessage] = None,
-            timeout: Optional[bool] = False,
-        ) -> None:
-            self.message = message
-            self.timeout = timeout
-            self.timeout_handler = Mock()
-
-        async def result(
-            self, timeout: Optional[float] = None
-        ) -> Optional[_BitfountMessage]:
-            """Returns the result of the awaitable.
-
-            In this dummy implementation, the result is just the original message. If a
-            timeout is specified, this will be recorded using `self.timeout_handler`.
-            """
-            if self.timeout:
-                self.timeout = None
-                raise asyncio.TimeoutError()
+        # Need to explicitly set AsyncMocks on all the methods due to how
+        # spec_set works
+        mock_stub.PodConnect = AsyncMock()
+        mock_stub.SetupTaskMailboxes = AsyncMock()
+        mock_stub.SendBitfountMessage = AsyncMock()
+        mock_stub.GetBitfountMessage = AsyncMock()
+        mock_stub.GetLargeObjectStorage = AsyncMock()
+        mock_stub.AcknowledgeMessage = AsyncMock()
+
+        return mock_stub
+
+    @fixture
+    def mock_config_stub_property(self, grpc_stub: Mock) -> PropertyMock:
+        """Property mock of the MessageServiceConfig.stub property."""
+        # Need to use small helper function to get the async behaviour.
+        async def _mock_stub_property() -> Mock:
+            return grpc_stub
+
+        return PropertyMock(side_effect=_mock_stub_property)
+
+    @fixture
+    def mock_ms_config(
+        self, mock_config_stub_property: PropertyMock
+    ) -> NonCallableMock:
+        """Returns mock MessageServiceConfig."""
+        mock_ms_config = create_dataclass_mock(MessageServiceConfig)
+
+        # PropertyMock must be set on type(x), not x.
+        type(mock_ms_config).stub = mock_config_stub_property
+
+        # Explicitly set use_local_storage to False
+        mock_ms_config.use_local_storage = False
+
+        return mock_ms_config
+
+    @fixture
+    def message_service(
+        self, mock_ms_config: NonCallableMock, session: Mock
+    ) -> _MessageService:
+        """The MessageService under test."""
+        return _MessageService(session, mock_ms_config)
+
+    @fixture
+    def mock_upload_to_s3(self, mocker: MockerFixture) -> Mock:
+        """Mock out upload_data_to_s3 function in message_service.py."""
+        mock_upload_to_s3 = mocker.patch(
+            "bitfount.federated.transport.message_service._upload_data_to_s3",
+            autospec=True,
+        )
+        return mock_upload_to_s3
 
-            self.timeout_handler(timeout)
-            return self.message
+    @fixture
+    def mock_download_from_s3(self, mocker: MockerFixture) -> Mock:
+        """Mock out download_data_from_s3 function in message_service.py."""
+        mock_download_from_s3 = mocker.patch(
+            "bitfount.federated.transport.message_service._download_data_from_s3",
+            autospec=True,
+        )
+        return mock_download_from_s3
 
-    return MockMessageAwaitable
+    def test_username_property_returns_username(
+        self, message_service: _MessageService, username: str
+    ) -> None:
+        """Property returns username from session."""
+        assert message_service.username == username
 
+    async def test_stub_property(
+        self,
+        grpc_stub: Mock,
+        message_service: _MessageService,
+        mock_config_stub_property: PropertyMock,
+    ) -> None:
+        """Tests that the stub property/generates the stub from config."""
+        # Check that the stub generated matches the output of the
+        # MessageServiceConfig stub.
+        stub = await message_service.stub
+        assert stub == grpc_stub
+
+        # Check value is "cached"
+        stub2 = await message_service.stub
+        assert stub is stub2
+        assert message_service._stub is not None
+
+        # Check config stub only called once
+        mock_config_stub_property.assert_called_once()
+
+    async def test_connect_successful(
+        self, grpc_stub: Mock, message_service: _MessageService, mocker: MockerFixture
+    ) -> None:
+        """Pod Connect successful."""
+        platform = mocker.patch("bitfount.federated.transport.message_service.platform")
+        psutil = mocker.patch("bitfount.federated.transport.message_service.psutil")
+        get_gpu_metadata = mocker.patch(
+            "bitfount.federated.transport.message_service.get_gpu_metadata"
+        )
+        platform.processor.return_value = "someProcessor"
+        platform.system.return_value = "someOS"
+        psutil.cpu_count.return_value = 50
+        get_gpu_metadata.return_value = "someGPUName", 33
+
+        memory_data = Mock()
+        memory_data.total = 346346436
+        psutil.virtual_memory.return_value = memory_data
+        grpc_stub.PodConnect.return_value = SuccessResponse()
+        pod_name = "somePodName"
+
+        mailbox_id = await message_service.connect_pod(pod_name)
+
+        assert mailbox_id == pod_name
+        grpc_stub.PodConnect.assert_called_once_with(
+            PodData(
+                podName=pod_name,
+                processor="someProcessor",
+                podOS="someOS",
+                cpuCount=50,
+                gpuCount=33,
+                gpuName="someGPUName",
+                totalMemoryBytes=346346436,
+            ),
+            metadata=[("token", access_token)],
+        )
 
-@unit_test
-class TestWorkerMailbox:
-    """Test WorkerMailbox class."""
+    async def test_connect_raises_error_on_failure(
+        self, grpc_stub: Mock, message_service: _MessageService
+    ) -> None:
+        """Pod Connect receives RpcError."""
+        grpc_stub.PodConnect.side_effect = RpcError("Pod connect failed")
+        pod_name = "somePodName"
 
-    async def test__send_aes_encrypted_message_successful(
-        self,
-        aes_key: bytes,
-        mock_message_service: Mock,
-        mock_message_timestamps: Callable[[Iterable[str]], Mock],
-        mocker: MockerFixture,
-        modeller_mailbox_id: str,
-        modeller_name: str,
-        opt_task_id: Optional[str],
-        pod_identifier: str,
-        worker_mailbox: _WorkerMailbox,
-        worker_mailbox_id: str,
-    ) -> None:
-        """Message is sent AES encrypted using provided key."""
-        fake_timestamps = ["Hello"]
-        mock_message_timestamps(fake_timestamps)
-
-        expected_encrypted_message: bytes = b"encrypted_message"
-        encrypt = mocker.patch.object(_MessageEncryption, "encrypt_outgoing_message")
-        encrypt.return_value = expected_encrypted_message
-
-        message = "some message"
-        dumped_message = msgpack.dumps(message)
+        with raises(RpcError):
+            await message_service.connect_pod(pod_name)
 
-        await worker_mailbox._send_aes_encrypted_message(
-            message, _BitfountMessageType.TRAINING_UPDATE
+        # Sanity check that PodConnect was called at all
+        grpc_stub.PodConnect.assert_called_once_with(
+            ANY, metadata=[("token", access_token)]
         )
 
-        encrypt.assert_called_once_with(dumped_message, aes_key)
-        mock_message_service.send_message.assert_called_once_with(
-            _BitfountMessage(
-                message_type=_BitfountMessageType.TRAINING_UPDATE,
-                body=expected_encrypted_message,
-                recipient=modeller_name,
-                recipient_mailbox_id=modeller_mailbox_id,
-                sender=pod_identifier,
-                sender_mailbox_id=worker_mailbox_id,
-                timestamp=fake_timestamps[0],
-                task_id=opt_task_id,
+    async def test_setup_communication_with_pods_successful(
+        self, grpc_stub: Mock, message_service: _MessageService
+    ) -> None:
+        """Start Training successful."""
+        expected_modeller_mailbox_id = "someMailboxId"
+        expected_worker_mailboxes = {
+            "somePodName": "somePodMailboxId",
+            "someOtherPodName": "someOtherMailboxId",
+        }
+        grpc_stub.SetupTaskMailboxes.return_value = CommunicationDetails(
+            mailboxId=expected_modeller_mailbox_id,
+            podMailboxIds=expected_worker_mailboxes,
+        )
+        pod_identifier = "podOwner/somePodName"
+        training_request = b"some training request"
+
+        (
+            modeller_mailbox_id,
+            worker_mailboxes,
+        ) = await message_service.setup_communication_with_pods(
+            {pod_identifier: training_request}
+        )
+
+        assert modeller_mailbox_id == expected_modeller_mailbox_id
+        assert worker_mailboxes == expected_worker_mailboxes
+        grpc_stub.SetupTaskMailboxes.assert_called_once_with(
+            BitfountTasks(
+                tasks=[
+                    BitfountTask(
+                        podIdentifier=pod_identifier, encryptedTask=training_request
+                    )
+                ]
             ),
-            already_packed=True,
+            metadata=[("token", access_token)],
         )
 
-    async def test__send_aes_encrypted_message_throws_error(
-        self,
-        aes_key: bytes,
-        mock_message_service: Mock,
-        mock_message_timestamps: Callable[[Iterable[str]], Mock],
-        mocker: MockerFixture,
-        modeller_mailbox_id: str,
-        modeller_name: str,
-        opt_task_id: Optional[str],
-        pod_identifier: str,
-        rpc_error: RpcError,
-        worker_mailbox: _WorkerMailbox,
-        worker_mailbox_id: str,
-    ) -> None:
-        """Sending encrypted message receives RpcError."""
-        fake_timestamps = ["Hello"]
-        mock_message_timestamps(fake_timestamps)
-
-        expected_encrypted_message: bytes = b"encrypted_message"
-        encrypt = mocker.patch.object(_MessageEncryption, "encrypt_outgoing_message")
-        encrypt.return_value = expected_encrypted_message
-
-        message = "some message"
-        dumped_message = msgpack.dumps(message)
-
-        mock_message_service.send_message.side_effect = rpc_error
+    async def test_setup_communication_with_pods_raises_error_on_failure(
+        self, grpc_stub: Mock, message_service: _MessageService
+    ) -> None:
+        """Start Training receives RpcError."""
+        grpc_stub.SetupTaskMailboxes.side_effect = RpcError("SetupTaskMailboxes failed")
+        pod_identifier = "podOwner/somePodName"
+        training_request = b"some training request"
 
         with raises(RpcError):
-            await worker_mailbox._send_aes_encrypted_message(
-                message, _BitfountMessageType.TRAINING_UPDATE
+            await message_service.setup_communication_with_pods(
+                {pod_identifier: training_request}
             )
 
-        encrypt.assert_called_once_with(dumped_message, aes_key)
-        mock_message_service.send_message.assert_called_once_with(
+        grpc_stub.SetupTaskMailboxes.assert_called_once_with(
+            BitfountTasks(
+                tasks=[
+                    BitfountTask(
+                        podIdentifier=pod_identifier, encryptedTask=training_request
+                    )
+                ]
+            ),
+            metadata=[("token", access_token)],
+        )
+
+    async def test_send_message_successful_already_packed(
+        self, grpc_stub: Mock, message_service: _MessageService
+    ) -> None:
+        """Test send packed body to Pod."""
+        grpc_stub.SendBitfountMessage.return_value = SuccessResponse()
+        packed_message = b"an already packed message"
+        pod_identifier = "podOwner/somePodName"
+        mailbox_id = "someMailboxId"
+        reply_to_mailbox_id = "replyMailboxId"
+        sender = "someSender"
+        expected_timestamp = datetime.now().isoformat()
+
+        response = await message_service.send_message(
             _BitfountMessage(
                 message_type=_BitfountMessageType.TRAINING_UPDATE,
-                body=expected_encrypted_message,
-                recipient=modeller_name,
-                recipient_mailbox_id=modeller_mailbox_id,
-                sender=pod_identifier,
-                sender_mailbox_id=worker_mailbox_id,
-                timestamp=fake_timestamps[0],
-                task_id=opt_task_id,
+                body=packed_message,
+                recipient=pod_identifier,
+                recipient_mailbox_id=mailbox_id,
+                sender=sender,
+                sender_mailbox_id=reply_to_mailbox_id,
+                timestamp=expected_timestamp,
             ),
             already_packed=True,
         )
 
-    async def test__get_message_returns_message_without_breaching_timeout(
-        self,
-        caplog: LogCaptureFixture,
-        modeller_mailbox_id: str,
-        modeller_name: str,
-        opt_task_id: Optional[str],
-        patch_poll_for_messages: PollForMessagesPatcher,
-        pod_identifier: str,
-        worker_mailbox: _WorkerMailbox,
-        worker_mailbox_id: str,
-    ) -> None:
-        """Message is returned as expected without breaching timeout."""
-        # Set up the mocked messages to be received
-        patch_poll_for_messages(
-            [
+        assert response == SuccessResponse()
+        grpc_stub.SendBitfountMessage.assert_called_once_with(
+            GrpcBitfountMessage(
+                messageType=_BitfountMessageType.TRAINING_UPDATE.value,
+                body=packed_message,
+                recipient=pod_identifier,
+                recipientMailboxId=mailbox_id,
+                sender=sender,
+                senderMailboxId=reply_to_mailbox_id,
+                timestamp=expected_timestamp,
+            ),
+            metadata=[("token", access_token)],
+        )
+
+    async def test_send_message_error_is_raised(
+        self, grpc_stub: Mock, message_service: _MessageService
+    ) -> None:
+        """Test send to pod throws RpcError."""
+        grpc_stub.SendBitfountMessage.side_effect = RpcError()
+        packed_message = b"an already packed message"
+        pod_identifier = "podOwner/somePodName"
+        mailbox_id = "someMailboxId"
+        reply_to_mailbox_id = "replyMailboxId"
+        sender = "someModeller"
+        expected_timestamp = datetime.now().isoformat()
+
+        with raises(RpcError):
+            await message_service.send_message(
                 _BitfountMessage(
                     message_type=_BitfountMessageType.TRAINING_UPDATE,
-                    body=b"123",
-                    recipient=modeller_name,
-                    recipient_mailbox_id=modeller_mailbox_id,
-                    sender=pod_identifier,
-                    sender_mailbox_id=worker_mailbox_id,
-                    task_id=opt_task_id,
-                )
-            ]
-        )
+                    body=packed_message,
+                    recipient=pod_identifier,
+                    recipient_mailbox_id=mailbox_id,
+                    sender=sender,
+                    sender_mailbox_id=reply_to_mailbox_id,
+                    timestamp=expected_timestamp,
+                ),
+                already_packed=True,
+            )
 
-        # Start listening and processing messages
-        mock_soft_limit_timeout = 30
-        message = await _run_func_and_listen_to_mailbox(
-            worker_mailbox._get_message(
-                _BitfountMessageType.TRAINING_UPDATE, timeout=mock_soft_limit_timeout
+        grpc_stub.SendBitfountMessage.assert_called_once_with(
+            GrpcBitfountMessage(
+                messageType=_BitfountMessageType.TRAINING_UPDATE.value,
+                body=packed_message,
+                sender=sender,
+                senderMailboxId=reply_to_mailbox_id,
+                recipient=pod_identifier,
+                recipientMailboxId=mailbox_id,
+                timestamp=expected_timestamp,
             ),
-            worker_mailbox,
+            metadata=[("token", access_token)],
         )
-        assert message.body == b"123"
 
-        # Check no logs due to timeout not being reached
-        logs = get_info_logs(caplog, and_higher=True)
-        assert "Checking if Modeller is still online..." not in logs
+    async def test_send_message_successful_not_packed(
+        self, grpc_stub: Mock, message_service: _MessageService
+    ) -> None:
+        """Test send plain object to Modeller."""
+        grpc_stub.SendBitfountMessage.return_value = SuccessResponse()
+        unpacked_message = b"a message to be packed"
+        expected_packed_message = msgpack.dumps(unpacked_message)
+        modeller_name = "someModeller"
+        pod_identifier = "podOwner/somePodName"
+        mailbox_id = "someMailboxId"
+        reply_to_mailbox_id = "replyMailboxId"
+        expected_timestamp = datetime.now().isoformat()
 
-    async def test__get_message_breaches_soft_limit_timeout(
-        self,
-        caplog: LogCaptureFixture,
-        dummy_message_awaitable: Callable[
-            [Optional[_BitfountMessage], Optional[bool]], object
-        ],
-        mocker: MockerFixture,
-        modeller_mailbox_id: str,
-        modeller_name: str,
-        online_check_uuid: str,
-        opt_task_id: Optional[str],
-        pod_identifier: str,
-        worker_mailbox: _WorkerMailbox,
-        worker_mailbox_id: str,
-    ) -> None:
-        """Message is returned as expected after breaching soft timeout."""
-        caplog.set_level("INFO")
-        original_message_awaitable = dummy_message_awaitable(
+        response = await message_service.send_message(
             _BitfountMessage(
                 message_type=_BitfountMessageType.TRAINING_UPDATE,
-                body=b"123",
+                body=unpacked_message,
                 recipient=modeller_name,
-                recipient_mailbox_id=modeller_mailbox_id,
+                recipient_mailbox_id=mailbox_id,
                 sender=pod_identifier,
-                sender_mailbox_id=worker_mailbox_id,
-                task_id=opt_task_id,
+                sender_mailbox_id=reply_to_mailbox_id,
+                timestamp=expected_timestamp,
             ),
-            True,
-        )
-
-        # Patch out the waiting for response
-        mocker.patch.object(
-            worker_mailbox._online_response_handler,
-            "wait_for_response",
-            autospec=True,
-            return_value=None,
+            already_packed=False,
         )
 
-        # Mock the bitfount messages retrieved from the modeller
-        mocker.patch(
-            "bitfount.federated.transport.worker_transport._get_message_awaitable",
-            return_value=original_message_awaitable,
+        assert response == SuccessResponse()
+        grpc_stub.SendBitfountMessage.assert_called_once_with(
+            GrpcBitfountMessage(
+                messageType=_BitfountMessageType.TRAINING_UPDATE.value,
+                body=expected_packed_message,
+                sender=pod_identifier,
+                senderMailboxId=reply_to_mailbox_id,
+                recipient=modeller_name,
+                recipientMailboxId=mailbox_id,
+                timestamp=expected_timestamp,
+            ),
+            metadata=[("token", access_token)],
         )
 
-        # Mock checking the modeller is online
-        mock_check_modeller_online = mocker.patch.object(
-            worker_mailbox, "check_modeller_online"
+    async def test_get_message_successful(
+        self, grpc_stub: Mock, message_service: _MessageService, mocker: MockerFixture
+    ) -> None:
+        """Test get message returns message."""
+        expected_message_body = b"some message"
+        expected_sent_by = "some modeller"
+        expected_mailbox_id = "some mailbox"
+        expected_reply_to_mailbox = "some reply to mailbox"
+        expected_receipt_handle = "valid receipt handle"
+        expected_pod_mailbox_ids = {"somePod": "someId"}
+        expected_recipient = "someRecipient"
+        expected_timestamp = datetime.now().isoformat()
+
+        mock_sleep = mocker.patch("asyncio.sleep")
+
+        grpc_stub.GetBitfountMessage.return_value = GrpcBitfountMessage(
+            messageType=_BitfountMessageType.MODEL_PARAMETERS.value,
+            body=expected_message_body,
+            sender=expected_sent_by,
+            senderMailboxId=expected_reply_to_mailbox,
+            receiptHandle=expected_receipt_handle,
+            recipient=expected_recipient,
+            recipientMailboxId=expected_mailbox_id,
+            podMailboxIds=expected_pod_mailbox_ids,
+            timestamp=expected_timestamp,
+        )
+        grpc_stub.AcknowledgeMessage.return_value = SuccessResponse()
+
+        message = await message_service._get_message(
+            expected_mailbox_id, max_attempts=10, wait_between_errors=9001
+        )
+
+        # Retrieved message is as expected
+        assert message == _BitfountMessage(
+            message_type=_BitfountMessageType.MODEL_PARAMETERS,
+            body=expected_message_body,
+            recipient=expected_recipient,
+            recipient_mailbox_id=expected_mailbox_id,
+            sender=expected_sent_by,
+            sender_mailbox_id=expected_reply_to_mailbox,
+            pod_mailbox_ids=expected_pod_mailbox_ids,
+            timestamp=expected_timestamp,
+            receipt_handle=expected_receipt_handle,
+        )
+
+        # Message was requested correctly
+        grpc_stub.GetBitfountMessage.assert_called_once_with(
+            CommunicationDetails(mailboxId=expected_mailbox_id),
+            metadata=[("token", access_token)],
+        )
+
+        # Message was acknowledged correctly
+        grpc_stub.AcknowledgeMessage.assert_called_once_with(
+            Acknowledgement(
+                mailboxId=expected_mailbox_id,
+                receiptHandle=expected_receipt_handle,
+                deleteMailbox=False,
+            ),
+            metadata=[("token", access_token)],
         )
 
-        # Mock hard limit timeout
-        mocker.patch(
-            "bitfount.federated.transport.worker_transport._HARD_LIMIT_MESSAGE_TIMEOUT",
-            5,
-        )
+        # We didn't sleep unnecessarily
+        mock_sleep.assert_not_awaited()
 
-        # Start listening and processing messages
-        mock_soft_limit_timeout = 3
-        message = await worker_mailbox._get_message(
-            _BitfountMessageType.TRAINING_UPDATE, timeout=mock_soft_limit_timeout
+    async def test_get_message_successful_and_is_unpacked(
+        self, grpc_stub: Mock, message_service: _MessageService, mocker: MockerFixture
+    ) -> None:
+        """Test get message returns and unpacks message."""
+        packed_message_body = msgpack.dumps(b"some message")
+        expected_sent_by = "some modeller"
+        expected_mailbox_id = "some mailbox"
+        expected_reply_to_mailbox = "some reply to mailbox"
+        expected_receipt_handle = "valid receipt handle"
+        expected_recipient = "someRecipient"
+        expected_timestamp = datetime.now().isoformat()
+
+        mock_sleep = mocker.patch("asyncio.sleep")
+
+        grpc_stub.GetBitfountMessage.return_value = GrpcBitfountMessage(
+            messageType=_BitfountMessageType.MODEL_PARAMETERS.value,
+            body=packed_message_body,
+            sender=expected_sent_by,
+            senderMailboxId=expected_reply_to_mailbox,
+            receiptHandle=expected_receipt_handle,
+            recipient=expected_recipient,
+            recipientMailboxId=expected_mailbox_id,
+            podMailboxIds={},
+            timestamp=expected_timestamp,
+        )
+        grpc_stub.AcknowledgeMessage.return_value = SuccessResponse()
+
+        message = await message_service._get_message(
+            expected_mailbox_id, max_attempts=10, wait_between_errors=9001
+        )
+
+        # Retrieved message is as expected
+        assert message == _BitfountMessage(
+            message_type=_BitfountMessageType.MODEL_PARAMETERS,
+            body=packed_message_body,
+            recipient=expected_recipient,
+            recipient_mailbox_id=expected_mailbox_id,
+            sender=expected_sent_by,
+            sender_mailbox_id=expected_reply_to_mailbox,
+            timestamp=expected_timestamp,
+            receipt_handle=expected_receipt_handle,
+            pod_mailbox_ids={},
+        )
+
+        # Message was requested correctly
+        grpc_stub.GetBitfountMessage.assert_called_once_with(
+            CommunicationDetails(mailboxId=expected_mailbox_id),
+            metadata=[("token", access_token)],
+        )
+
+        # Message was acknowledged correctly
+        grpc_stub.AcknowledgeMessage.assert_called_once_with(
+            Acknowledgement(
+                mailboxId=expected_mailbox_id,
+                receiptHandle=expected_receipt_handle,
+                deleteMailbox=False,
+            ),
+            metadata=[("token", access_token)],
         )
 
-        assert message.body == b"123"
+        # We didn't sleep unnecessarily
+        mock_sleep.assert_not_awaited()
 
-        # Check logs
-        info_logs = get_info_logs(caplog)
-        assert "Checking if Modeller is still online..." in info_logs
-        assert "Modeller is online, responded with expected message." in info_logs
+    async def test_get_message_successful_mailbox_delete_flag_set_on_task_complete(
+        self, grpc_stub: Mock, message_service: _MessageService, mocker: MockerFixture
+    ) -> None:
+        """Test get message and delete mailbox.
 
-        mock_check_modeller_online.assert_called_once()
+        When a pod receives a TASK_COMPLETE message we acknowledge it,
+        informing the message service that the task queue can now
+        be deleted.
+        """
+        packed_message_body = msgpack.dumps(b"some message")
+        expected_sent_by = "some modeller"
+        expected_mailbox_id = "some mailbox"
+        expected_reply_to_mailbox = "some reply to mailbox"
+        expected_receipt_handle = "valid receipt handle"
+        expected_recipient = "someRecipient"
+        expected_timestamp = datetime.now().isoformat()
+
+        mock_sleep = mocker.patch("asyncio.sleep")
+
+        grpc_stub.GetBitfountMessage.return_value = GrpcBitfountMessage(
+            messageType=_BitfountMessageType.TASK_COMPLETE.value,
+            body=packed_message_body,
+            sender=expected_sent_by,
+            senderMailboxId=expected_reply_to_mailbox,
+            receiptHandle=expected_receipt_handle,
+            recipient=expected_recipient,
+            recipientMailboxId=expected_mailbox_id,
+            podMailboxIds={},
+            timestamp=expected_timestamp,
+        )
+        grpc_stub.AcknowledgeMessage.return_value = SuccessResponse()
+
+        message = await message_service._get_message(
+            expected_mailbox_id, max_attempts=10, wait_between_errors=9001
+        )
+
+        # Retrieved message is as expected
+        assert message == _BitfountMessage(
+            message_type=_BitfountMessageType.TASK_COMPLETE,
+            body=packed_message_body,
+            recipient=expected_recipient,
+            recipient_mailbox_id=expected_mailbox_id,
+            sender=expected_sent_by,
+            sender_mailbox_id=expected_reply_to_mailbox,
+            timestamp=expected_timestamp,
+            receipt_handle=expected_receipt_handle,
+            pod_mailbox_ids={},
+        )
+
+        # Message was requested correctly
+        grpc_stub.GetBitfountMessage.assert_called_once_with(
+            CommunicationDetails(mailboxId=expected_mailbox_id),
+            metadata=[("token", access_token)],
+        )
+
+        # Message was acknowledged correctly
+        grpc_stub.AcknowledgeMessage.assert_called_once_with(
+            Acknowledgement(
+                mailboxId=expected_mailbox_id,
+                receiptHandle=expected_receipt_handle,
+                deleteMailbox=True,
+            ),
+            metadata=[("token", access_token)],
+        )
 
-        # Should be one call to timeout_handler; the first call to result() raises
-        # a TimeoutError, the second call passes the timeout through to
-        # timeout_handler. This second call is made whilst we are waiting for the
-        # online response check event to be set and so has no timeout.
-        # mypy reason: Mypy can't detect the actual type of this mocked awaitable.
-        original_message_awaitable.timeout_handler.assert_called_once()  # type: ignore[attr-defined] # Reason: see above # noqa: B950
+        # We didn't sleep unnecessarily
+        mock_sleep.assert_not_awaited()
 
-    async def test__get_message_breaches_hard_limit_timeout(
+    async def test_get_message_downloads_from_large_object_storage(
         self,
-        caplog: LogCaptureFixture,
-        dummy_message_awaitable: Callable[
-            [Optional[_BitfountMessage], Optional[bool]], object
-        ],
+        grpc_stub: Mock,
+        message_service: _MessageService,
+        mock_download_from_s3: Mock,
         mocker: MockerFixture,
-        worker_mailbox: _WorkerMailbox,
+        s3_download_url: _S3PresignedURL,
     ) -> None:
-        """Test worker gives up on waiting for message after breaching hard limit.
+        """Test get message downloads from blob storage."""
+        mock_async_sleep = mocker.patch("asyncio.sleep")
 
-        This should raise an `asyncio.TimeoutError` exception.
-        """
-        caplog.set_level("INFO")
-
-        # No messages retrieved from the Modeller, just `TimeOutError`s
-        original_message_awaitable = dummy_message_awaitable(None, True)
+        # Construct GrpcBitfountMessage
+        expected_message_body = s3_download_url
+        packed_message_body = msgpack.dumps(expected_message_body)
+        expected_sent_by = "some_modeller"
+        expected_mailbox_id = "some_mailbox"
+        expected_reply_to_mailbox = "some reply to mailbox"
+        expected_receipt_handle = "valid receipt handle"
+        expected_recipient = "some_recipient"
+        expected_timestamp = datetime.now().isoformat()
 
-        # Mock the bitfount messages retrieved from the modeller
-        mocker.patch(
-            "bitfount.federated.transport.worker_transport._get_message_awaitable",
-            return_value=original_message_awaitable,
+        grpc_stub.GetBitfountMessage.return_value = GrpcBitfountMessage(
+            messageType=_BitfountMessageType.TRAINING_UPDATE.value,
+            body=packed_message_body,
+            sender=expected_sent_by,
+            senderMailboxId=expected_reply_to_mailbox,
+            recipient=expected_recipient,
+            recipientMailboxId=expected_mailbox_id,
+            receiptHandle=expected_receipt_handle,
+            timestamp=expected_timestamp,
         )
 
-        # Mock checking the modeller is online
-        mock_check_modeller_online = mocker.patch.object(
-            worker_mailbox, "check_modeller_online"
-        )
+        # Mock out message acknowledgement
+        grpc_stub.AcknowledgeMessage.return_value = SuccessResponse()
 
-        # Mock sending of task abort message
-        mock_send_task_abort_message = mocker.patch.object(
-            worker_mailbox, "send_task_abort_message"
-        )
+        # Mock out downloading from S3
+        # Messages are inherently packed when stored in S3, but we're mocking out
+        # the return here so can avoid that.
+        message_body_in_blob_storage = b"here is the actual message"
+        mock_download_from_s3.return_value = message_body_in_blob_storage
 
-        # Mock hard limit timeout
-        mocker.patch(
-            "bitfount.federated.transport.worker_transport._HARD_LIMIT_MESSAGE_TIMEOUT",
-            5,
-        )
-        mock_wait_for = mocker.patch.object(
-            asyncio, "wait_for", autospec=True, side_effect=asyncio.TimeoutError()
+        # Retrieve message
+        message = await message_service._get_message(
+            expected_mailbox_id,
+            max_attempts=10,
+            wait_between_errors=9001,
         )
 
-        # Start listening and processing messages
-        mock_soft_limit_timeout = 3
-        with pytest.raises(asyncio.TimeoutError):
-            await worker_mailbox._get_message(
-                _BitfountMessageType.TRAINING_UPDATE, timeout=mock_soft_limit_timeout
+        # Retrieved message is as expected
+        assert (
+            _BitfountMessage(
+                message_type=_BitfountMessageType.TRAINING_UPDATE,
+                body=message_body_in_blob_storage,
+                recipient=expected_recipient,
+                recipient_mailbox_id=expected_mailbox_id,
+                sender=expected_sent_by,
+                sender_mailbox_id=expected_reply_to_mailbox,
+                timestamp=expected_timestamp,
+                receipt_handle=expected_receipt_handle,
+                pod_mailbox_ids={},
             )
-
-        # Check logs
-        assert "Checking if Modeller is still online..." in get_info_logs(caplog)
-        assert "Modeller is offline. Aborting task." in get_warning_logs(caplog)
-
-        mock_check_modeller_online.assert_called_once()
-        mock_wait_for.assert_awaited_once_with(ANY, timeout=5)
-        mock_send_task_abort_message.assert_called_once()
-
-    async def test__get_message_and_decrypt_successful(
-        self,
-        aes_key: bytes,
-        mocker: MockerFixture,
-        modeller_mailbox_id: str,
-        modeller_name: str,
-        opt_task_id: Optional[str],
-        patch_poll_for_messages: PollForMessagesPatcher,
-        pod_identifier: str,
-        worker_mailbox: _WorkerMailbox,
-        worker_mailbox_id: str,
-    ) -> None:
-        """Receives and decrypts messages using provided AES key."""
-        expected_message = "some message"
-        dumped_message: bytes = msgpack.dumps(expected_message)
-        # We will be manually mocking out the decryption so this message can just
-        # be arbitrary.
-        mock_encrypted_message: bytes = b"encrypted_message"
-
-        # Mock out decryption
-        decrypt = mocker.patch.object(_MessageEncryption, "decrypt_incoming_message")
-        decrypt.return_value = dumped_message
-
-        # Set up the mocked messages to be received
-        patch_poll_for_messages(
-            [
-                _BitfountMessage(
-                    message_type=_BitfountMessageType.TRAINING_UPDATE,
-                    body=mock_encrypted_message,
-                    recipient=modeller_name,
-                    recipient_mailbox_id=modeller_mailbox_id,
-                    sender=pod_identifier,
-                    sender_mailbox_id=worker_mailbox_id,
-                    task_id=opt_task_id,
-                )
-            ]
+            == message
         )
 
-        # Start listening and processing messages
-        message = await _run_func_and_listen_to_mailbox(
-            worker_mailbox._get_message_and_decrypt(
-                _BitfountMessageType.TRAINING_UPDATE
-            ),
-            worker_mailbox,
+        # Message was requested correctly
+        grpc_stub.GetBitfountMessage.assert_called_once_with(
+            CommunicationDetails(mailboxId=expected_mailbox_id),
+            metadata=[("token", access_token)],
         )
 
-        assert message == expected_message
-        decrypt.assert_called_once_with(mock_encrypted_message, aes_key)
-
-    async def test__get_message_and_decrypt_error(
-        self,
-        patch_poll_for_messages: PollForMessagesPatcher,
-        rpc_error: RpcError,
-        worker_mailbox: _WorkerMailbox,
-    ) -> None:
-        """Receiving message throws RpcError."""
-        patch_poll_for_messages([rpc_error])
-
-        with raises(
-            MessageRetrievalError,
-            match="An error occurred when trying to communicate with the "
-            "messaging service",
-        ):
-            # Start listening and processing messages
-            await _run_func_and_listen_to_mailbox(
-                worker_mailbox._get_message_and_decrypt(
-                    _BitfountMessageType.TRAINING_UPDATE  # this is arbitrary
-                ),
-                worker_mailbox,
-            )
-
-    async def test_accept_job_successful(
-        self,
-        mocker: MockerFixture,
-        worker_mailbox: _WorkerMailbox,
-    ) -> None:
-        """Training job acceptance sent."""
-        mock_message_send = mocker.patch.object(
-            worker_mailbox,
-            "_send_aes_encrypted_message",
-            return_value=SuccessResponse(),
+        # Message was acknowledged correctly
+        grpc_stub.AcknowledgeMessage.assert_called_once_with(
+            Acknowledgement(
+                mailboxId=expected_mailbox_id,
+                receiptHandle=expected_receipt_handle,
+                deleteMailbox=False,
+            ),
+            metadata=[("token", access_token)],
         )
 
-        await worker_mailbox.accept_task()
+        # Download was called correctly
+        mock_download_from_s3.assert_called_once_with(s3_download_url)
 
-        mock_message_send.assert_called_once_with(
-            {_PodResponseType.ACCEPT.name: worker_mailbox.pod_identifier},
-            _BitfountMessageType.JOB_ACCEPT,
-        )
+        # Check we didn't sleep unnecessarily
+        mock_async_sleep.assert_not_awaited()
 
-    async def test_accept_job_unsuccessful(
-        self,
-        mocker: MockerFixture,
-        rpc_error: RpcError,
-        worker_mailbox: _WorkerMailbox,
+    async def test_get_message_none_found(
+        self, grpc_stub: Mock, message_service: _MessageService, mocker: MockerFixture
     ) -> None:
-        """Sending training job acceptance fails."""
-        mock_message_send = mocker.patch.object(
-            worker_mailbox,
-            "_send_aes_encrypted_message",
-            side_effect=rpc_error,
-        )
+        """Test get message when there are none."""
+        expected_mailbox_id = "some mailbox"
 
-        with pytest.raises(RpcError):
-            await worker_mailbox.accept_task()
+        mock_sleep = mocker.patch("asyncio.sleep")
 
-        mock_message_send.assert_called_once_with(
-            {_PodResponseType.ACCEPT.name: worker_mailbox.pod_identifier},
-            _BitfountMessageType.JOB_ACCEPT,
-        )
+        error = RpcError()
+        error.code = lambda: StatusCode.NOT_FOUND  # type: ignore[assignment] # Reason: only way to construct RpcError in python code # noqa: B950
 
-    async def test_reject_job_successful(
-        self,
-        mocker: MockerFixture,
-        worker_mailbox: _WorkerMailbox,
-    ) -> None:
-        """Training job rejection sent."""
-        expected_error_messages = {"error": "messages"}
-        mock_message_send = mocker.patch.object(
-            worker_mailbox,
-            "_send_aes_encrypted_message",
-            return_value=SuccessResponse(),
-        )
+        grpc_stub.GetBitfountMessage.side_effect = error
 
-        await worker_mailbox.reject_task(
-            expected_error_messages,
+        message = await message_service._get_message(
+            expected_mailbox_id, max_attempts=10, wait_between_errors=9001
         )
 
-        mock_message_send.assert_called_once_with(
-            expected_error_messages,
-            _BitfountMessageType.JOB_REJECT,
-        )
+        # Retrieved message is as expected
+        assert message is None
 
-    async def test_reject_job_unsuccessful(
-        self,
-        mocker: MockerFixture,
-        rpc_error: RpcError,
-        worker_mailbox: _WorkerMailbox,
-    ) -> None:
-        """Sending training job rejection fails."""
-        expected_error_messages = {"error": "messages"}
-        mock_message_send = mocker.patch.object(
-            worker_mailbox,
-            "_send_aes_encrypted_message",
-            side_effect=rpc_error,
+        # Message was requested correctly
+        grpc_stub.GetBitfountMessage.assert_called_once_with(
+            CommunicationDetails(mailboxId=expected_mailbox_id),
+            metadata=[("token", access_token)],
         )
 
-        with pytest.raises(RpcError):
-            await worker_mailbox.reject_task(
-                expected_error_messages,
-            )
+        # Nothing to acknowledge
+        grpc_stub.AcknowledgeMessage.assert_not_called()
 
-        mock_message_send.assert_called_once_with(
-            expected_error_messages,
-            _BitfountMessageType.JOB_REJECT,
-        )
+        # We didn't sleep unnecessarily
+        mock_sleep.assert_not_awaited()
 
-    async def test_issue_saml_challenge_successful(
-        self,
-        mocker: MockerFixture,
-        worker_mailbox: _WorkerMailbox,
+    async def test_get_message_throws_error_when_never_successful(
+        self, grpc_stub: Mock, message_service: _MessageService, mocker: MockerFixture
     ) -> None:
-        """Training SAML challenge sent."""
-        expected_saml_request = "some saml request"
-        mock_message_send = mocker.patch.object(
-            worker_mailbox,
-            "_send_aes_encrypted_message",
-            return_value=SuccessResponse(),
-        )
+        """Test get message eventually throws error if errors occur."""
+        expected_mailbox_id = "some mailbox"
+        expected_attempts = 10
+        expected_wait_interval = 9001
 
-        await worker_mailbox.issue_saml_challenge(
-            expected_saml_request,
-        )
+        mock_sleep = mocker.patch("asyncio.sleep")
 
-        mock_message_send.assert_called_once_with(
-            expected_saml_request,
-            _BitfountMessageType.SAML_REQUEST,
-        )
+        error = RpcError()
+        error.code = lambda: StatusCode.INTERNAL  # type: ignore[assignment] # Reason: only way to construct RpcError in python code # noqa: B950
 
-    async def test_issue_saml_challenge_unsuccessful(
-        self,
-        mocker: MockerFixture,
-        rpc_error: RpcError,
-        worker_mailbox: _WorkerMailbox,
-    ) -> None:
-        """Sending SAML challenge fails."""
-        expected_saml_request = "some saml request"
-        mock_message_send = mocker.patch.object(
-            worker_mailbox,
-            "_send_aes_encrypted_message",
-            side_effect=rpc_error,
-        )
+        grpc_stub.GetBitfountMessage.side_effect = error
 
-        with pytest.raises(RpcError):
-            await worker_mailbox.issue_saml_challenge(
-                expected_saml_request,
+        with raises(RpcError):
+            await message_service._get_message(
+                expected_mailbox_id,
+                max_attempts=expected_attempts,
+                wait_between_errors=expected_wait_interval,
             )
 
-        mock_message_send.assert_called_once_with(
-            expected_saml_request,
-            _BitfountMessageType.SAML_REQUEST,
-        )
+        expected_get_call = call(
+            CommunicationDetails(mailboxId=expected_mailbox_id),
+            metadata=[("token", access_token)],
+        )
+        # Message was requested correctly up to max attempts
+        grpc_stub.GetBitfountMessage.assert_has_calls(
+            [expected_get_call] * expected_attempts
+        )
+        assert grpc_stub.GetBitfountMessage.call_count == expected_attempts
+
+        # Nothing to acknowledge
+        grpc_stub.AcknowledgeMessage.assert_not_called()
+
+        # It waited before trying again each time (but not after the final failure!)
+        mock_sleep.assert_has_awaits(
+            [call(expected_wait_interval)] * (expected_attempts - 1)
+        )
+
+    async def test_get_message_throws_error_but_is_eventually_successful(
+        self, grpc_stub: Mock, message_service: _MessageService, mocker: MockerFixture
+    ) -> None:
+        """Test get message doesn't throw error, retries until threshold is reached."""
+        packed_message_body = msgpack.dumps(b"some message")
+        expected_sent_by = "some modeller"
+        expected_mailbox_id = "some mailbox"
+        expected_reply_to_mailbox = "some reply to mailbox"
+        expected_receipt_handle = "valid receipt handle"
+        expected_recipient = "someRecipient"
+        expected_timestamp = datetime.now().isoformat()
+        expected_wait_interval = 3001
+
+        mock_sleep = mocker.patch("asyncio.sleep")
+
+        error = RpcError()
+        error.code = lambda: StatusCode.INTERNAL  # type: ignore[assignment] # Reason: only way to construct RpcError in python code # noqa: B950
+
+        grpc_stub.GetBitfountMessage.side_effect = [
+            error,
+            GrpcBitfountMessage(
+                messageType=_BitfountMessageType.MODEL_PARAMETERS.value,
+                body=packed_message_body,
+                sender=expected_sent_by,
+                senderMailboxId=expected_reply_to_mailbox,
+                receiptHandle=expected_receipt_handle,
+                recipient=expected_recipient,
+                recipientMailboxId=expected_mailbox_id,
+                podMailboxIds={},
+                timestamp=expected_timestamp,
+            ),
+        ]
+        grpc_stub.AcknowledgeMessage.return_value = SuccessResponse()
 
-    async def test_get_saml_response(
-        self,
-        mocker: MockerFixture,
-        worker_mailbox: _WorkerMailbox,
-    ) -> None:
-        """SAML challenge response retrieved."""
-        expected_saml_response = "some saml response"
-        mock_message_decrypt = mocker.patch.object(
-            worker_mailbox,
-            "_get_message_and_decrypt",
-            return_value=expected_saml_response,
+        message = await message_service._get_message(
+            expected_mailbox_id,
+            max_attempts=10,
+            wait_between_errors=expected_wait_interval,
+        )
+
+        # Retrieved message is as expected
+        assert message == _BitfountMessage(
+            message_type=_BitfountMessageType.MODEL_PARAMETERS,
+            body=packed_message_body,
+            recipient=expected_recipient,
+            recipient_mailbox_id=expected_mailbox_id,
+            sender=expected_sent_by,
+            sender_mailbox_id=expected_reply_to_mailbox,
+            pod_mailbox_ids={},
+            timestamp=expected_timestamp,
+            receipt_handle=expected_receipt_handle,
+        )
+
+        expected_get_message_call = call(
+            CommunicationDetails(mailboxId=expected_mailbox_id),
+            metadata=[("token", access_token)],
+        )
+        # Message was requested correctly
+        grpc_stub.GetBitfountMessage.assert_has_calls([expected_get_message_call] * 2)
+        assert grpc_stub.GetBitfountMessage.call_count == 2
+
+        # Message was acknowledged correctly
+        grpc_stub.AcknowledgeMessage.assert_called_once_with(
+            Acknowledgement(
+                mailboxId=expected_mailbox_id,
+                receiptHandle=expected_receipt_handle,
+                deleteMailbox=False,
+            ),
+            metadata=[("token", access_token)],
         )
 
-        response = await worker_mailbox.get_saml_response()
-
-        assert response == expected_saml_response
-        mock_message_decrypt.assert_awaited_once_with(
-            _BitfountMessageType.SAML_RESPONSE,
-            _DEFAULT_AUTHENTICATION_MODELLER_RESPONSE_TIMEOUT,
-        )
+        # We didn't sleep unnecessarily
+        mock_sleep.assert_awaited_once_with(expected_wait_interval)
 
-    async def test_get_task_start_update(
-        self,
-        mocker: MockerFixture,
-        worker_mailbox: _WorkerMailbox,
+    async def test_get_message_is_called_again_if_acknowledgement_fails(
+        self, grpc_stub: Mock, message_service: _MessageService, mocker: MockerFixture
     ) -> None:
-        """Tests worker gets task start empty message."""
-        mock_message_decrypt = mocker.patch.object(
-            worker_mailbox,
-            "_get_message_and_decrypt",
-            return_value=None,
-        )
+        """Tests that the message is re-fetched if the acknowledgement fails.
 
-        await worker_mailbox.get_task_start_update()
-        mock_message_decrypt.assert_awaited_once_with(
-            _BitfountMessageType.TASK_START, timeout=_SOFT_LIMIT_MESSAGE_TIMEOUT
-        )
+        If acknowledgement fails then it may be due to an expired receipt handle,
+        to avoid this we make sure that the message is retrieved again before retrying,
+        as this will fetch a new receipt handle
+        """
+        packed_message_body = msgpack.dumps(b"some message")
+        expected_sent_by = "some modeller"
+        expected_mailbox_id = "some mailbox"
+        expected_reply_to_mailbox = "some reply to mailbox"
+        expected_receipt_handle = "invalid receipt handle"
+        expected_second_receipt_handle = "valid receipt handle"
+        expected_recipient = "someRecipient"
+        expected_timestamp = datetime.now().isoformat()
+        expected_wait_interval = 3001
+
+        mock_sleep = mocker.patch("asyncio.sleep")
+
+        error = RpcError()
+        error.code = lambda: StatusCode.INTERNAL  # type: ignore[assignment] # Reason: only way to construct RpcError in python code # noqa: B950
+
+        grpc_stub.GetBitfountMessage.side_effect = [
+            GrpcBitfountMessage(
+                messageType=_BitfountMessageType.MODEL_PARAMETERS.value,
+                body=packed_message_body,
+                sender=expected_sent_by,
+                senderMailboxId=expected_reply_to_mailbox,
+                receiptHandle=expected_receipt_handle,
+                recipient=expected_recipient,
+                recipientMailboxId=expected_mailbox_id,
+                podMailboxIds={},
+                timestamp=expected_timestamp,
+            ),
+            GrpcBitfountMessage(
+                messageType=_BitfountMessageType.MODEL_PARAMETERS.value,
+                body=packed_message_body,
+                sender=expected_sent_by,
+                senderMailboxId=expected_reply_to_mailbox,
+                receiptHandle=expected_second_receipt_handle,
+                recipient=expected_recipient,
+                recipientMailboxId=expected_mailbox_id,
+                podMailboxIds={},
+                timestamp=expected_timestamp,
+            ),
+        ]
 
-    async def test_get_task_complete_update(
-        self,
-        mocker: MockerFixture,
-        worker_mailbox: _WorkerMailbox,
-    ) -> None:
-        """Tests worker gets task complete empty message."""
-        mock_message_decrypt = mocker.patch.object(
-            worker_mailbox,
-            "_get_message_and_decrypt",
-            return_value=None,
-        )
+        grpc_stub.AcknowledgeMessage.side_effect = [error, SuccessResponse()]
 
-        await worker_mailbox.get_task_complete_update()
-        mock_message_decrypt.assert_awaited_once_with(
-            _BitfountMessageType.TASK_COMPLETE, timeout=_SOFT_LIMIT_MESSAGE_TIMEOUT
-        )
+        message = await message_service._get_message(
+            expected_mailbox_id,
+            max_attempts=10,
+            wait_between_errors=expected_wait_interval,
+        )
+
+        # Retrieved message is as expected
+        assert message == _BitfountMessage(
+            message_type=_BitfountMessageType.MODEL_PARAMETERS,
+            body=packed_message_body,
+            recipient=expected_recipient,
+            recipient_mailbox_id=expected_mailbox_id,
+            sender=expected_sent_by,
+            sender_mailbox_id=expected_reply_to_mailbox,
+            pod_mailbox_ids={},
+            timestamp=expected_timestamp,
+            receipt_handle=expected_second_receipt_handle,
+        )
+
+        expected_get_message_call = call(
+            CommunicationDetails(mailboxId=expected_mailbox_id),
+            metadata=[("token", access_token)],
+        )
+        # Message was requested correctly
+        grpc_stub.GetBitfountMessage.assert_has_calls([expected_get_message_call] * 2)
+        assert grpc_stub.GetBitfountMessage.call_count == 2
 
-    async def test_get_training_complete_update(
-        self,
-        mocker: MockerFixture,
-        worker_mailbox: _WorkerMailbox,
-    ) -> None:
-        """Tests worker gets training complete empty message."""
-        expected_message = True
-        mock_message_decrypt = mocker.patch.object(
-            worker_mailbox,
-            "_get_message_and_decrypt",
-            return_value=expected_message,
+        # Message was acknowledged correctly
+        grpc_stub.AcknowledgeMessage.assert_has_calls(
+            [
+                call(
+                    Acknowledgement(
+                        mailboxId=expected_mailbox_id,
+                        receiptHandle=expected_receipt_handle,
+                        deleteMailbox=False,
+                    ),
+                    metadata=[("token", access_token)],
+                ),
+                call(
+                    Acknowledgement(
+                        mailboxId=expected_mailbox_id,
+                        receiptHandle=expected_second_receipt_handle,
+                        deleteMailbox=False,
+                    ),
+                    metadata=[("token", access_token)],
+                ),
+            ]
         )
+        assert grpc_stub.AcknowledgeMessage.call_count == 2
 
-        response = await worker_mailbox.get_training_iteration_complete_update()
-        assert response == expected_message
-        mock_message_decrypt.assert_awaited_once_with(
-            _BitfountMessageType.TRAINING_COMPLETE, timeout=_SOFT_LIMIT_MESSAGE_TIMEOUT
-        )
+        # Sleep was called between errors
+        mock_sleep.assert_awaited_once_with(expected_wait_interval)
 
-    async def test_send_oidc_client_id(
-        self,
-        mocker: MockerFixture,
-        worker_mailbox: _WorkerMailbox,
+    async def test_poll_for_message_eventually_retrieves_message(
+        self, message_service: _MessageService, mocker: MockerFixture
     ) -> None:
-        """Test send_oidc_client_id() works correctly."""
-        # Patch out _WorkerMailbox._send_aes_encrypted_message()
-        mock_message_send = mocker.patch.object(
-            worker_mailbox,
-            "_send_aes_encrypted_message",
-            return_value=SuccessResponse(),
+        """Test poll for message receives a message after a few attempts."""
+        expected_message = _BitfountMessage(
+            message_type=_BitfountMessageType.UNDEFINED,
+            body=b"hello world",
+            recipient="to_you",
+            recipient_mailbox_id="your_mailbox_id",
+            sender="from_me",
+            sender_mailbox_id="my_mailbox_id",
+        )
+        expected_mailbox = "someMailboxID"
+        mocked__get_message = mocker.patch.object(
+            message_service,
+            "_get_message",
+            AsyncMock(
+                side_effect=[
+                    None,
+                    None,
+                    expected_message,
+                ]
+            ),
         )
 
-        client_id = "client_id_value"
-
-        await worker_mailbox.send_oidc_client_id(client_id)
+        # There is no anext() built-in so have to do it this way
+        message = await message_service.poll_for_messages(expected_mailbox).__anext__()
 
-        # Check called correctly
-        mock_message_send.assert_called_once_with(
-            {"client_id": client_id},
-            _BitfountMessageType.OIDC_CHALLENGE,
-        )
+        assert message == expected_message
+        mocked__get_message.assert_has_awaits([call(expected_mailbox)] * 3)
+        assert mocked__get_message.await_count == 3
 
-    async def test_get_oidc_auth_flow_response(
-        self,
-        mocker: MockerFixture,
-        worker_mailbox: _WorkerMailbox,
+    async def test_poll_for_message_throws_error(
+        self, message_service: _MessageService, mocker: MockerFixture
     ) -> None:
-        """Tests get_oidc_auth_flow_response() correctly extracts details."""
-        expected_message = {
-            "auth_code": "auth_code_value",
-            "code_verifier": "code_verifier_value",
-            "redirect_uri": "redirect_uri_value",
-        }
-
-        # Patch out _WorkerMailbox._get_message_and_decrypt()
-        mock_message_decrypt = mocker.patch.object(
-            worker_mailbox,
-            "_get_message_and_decrypt",
-            return_value=expected_message,
+        """Test poll for message throws RpcError."""
+        expected_mailbox = "someMailboxID"
+        mocked__get_message = mocker.patch.object(
+            message_service, "_get_message", AsyncMock(side_effect=[None, RpcError()])
         )
 
-        response = await worker_mailbox.get_oidc_auth_flow_response()
+        with raises(RpcError):
+            # There is no anext() built-in so have to do it this way
+            await message_service.poll_for_messages(expected_mailbox).__anext__()
 
-        # Check calls/returns
-        mock_message_decrypt.assert_awaited_once_with(
-            _BitfountMessageType.OIDC_AFC_PKCE_RESPONSE, ANY
-        )
-        assert response.auth_code == expected_message["auth_code"]
-        assert response.code_verifier == expected_message["code_verifier"]
-        assert response.redirect_uri == expected_message["redirect_uri"]
+        mocked__get_message.assert_has_awaits([call(expected_mailbox)] * 2)
+        assert mocked__get_message.await_count == 2
 
-    async def test_get_oidc_auth_flow_response_fails_wrong_type(
+    async def test_maybe_upload_to_large_object_storage_small_message(
         self,
-        mocker: MockerFixture,
-        worker_mailbox: _WorkerMailbox,
-    ) -> None:
-        """Tests get_oidc_auth_flow_response() fails with wrong type."""
-        expected_message = "not a dict"
-
-        # Patch out _WorkerMailbox._get_message_and_decrypt()
-        mock_message_decrypt = mocker.patch.object(
-            worker_mailbox,
-            "_get_message_and_decrypt",
-            return_value=expected_message,
-        )
+        grpc_stub: Mock,
+        message_service: _MessageService,
+        mock_upload_to_s3: Mock,
+        s3_download_url: _S3PresignedURL,
+        s3_upload_fields: _S3PresignedPOSTFields,
+        s3_upload_url: _S3PresignedPOSTURL,
+    ) -> None:
+        """Test small messages are sent to message service."""
+        # Set output as expected fields
+        grpc_stub.GetLargeObjectStorage.return_value = BlobStorageData(
+            uploadUrl=s3_upload_url,
+            downloadUrl=s3_download_url,
+            uploadFields=s3_upload_fields,
+        )
+
+        # Construct message
+        sender = "someSender"
+        sender_mailbox_id = "someSenderMailboxId"
+        expected_body = msgpack.dumps({"some": "body"})
+        bitfount_message = _BitfountMessage(
+            body=expected_body,  #
+            message_type=_BitfountMessageType.UNDEFINED,
+            recipient="someRecipient",
+            recipient_mailbox_id="someMailboxId",
+            sender=sender,
+            sender_mailbox_id=sender_mailbox_id,
+        )
+
+        body = await message_service._maybe_upload_to_large_object_storage(
+            bitfount_message
+        )
+
+        # Check the upload function wasn't called
+        mock_upload_to_s3.assert_not_called()
+        # Check that message body is unchanged
+        assert body == expected_body
+
+    async def test_maybe_upload_to_large_object_storage_large_message(
+        self,
+        grpc_stub: Mock,
+        message_service: _MessageService,
+        mock_upload_to_s3: Mock,
+        s3_download_url: _S3PresignedURL,
+        s3_upload_fields: _S3PresignedPOSTFields,
+        s3_upload_url: _S3PresignedPOSTURL,
+    ) -> None:
+        """Test large object gets uploaded."""
+        # Set output as expected fields
+        grpc_stub.GetLargeObjectStorage.return_value = BlobStorageData(
+            uploadUrl=s3_upload_url,
+            downloadUrl=s3_download_url,
+            uploadFields=s3_upload_fields,
+        )
+
+        # Construct message
+        sender = "someSender/somePod"
+        sender_mailbox_id = "someSenderMailboxId"
+        expected_body = b"a" * (_SMALL_MESSAGE_UPPER_LIMIT_SIZE_BYTES + 1)
+        bitfount_message = _BitfountMessage(
+            body=expected_body,
+            message_type=_BitfountMessageType.UNDEFINED,
+            recipient="someRecipient",
+            recipient_mailbox_id="someMailboxId",
+            sender=sender,
+            sender_mailbox_id=sender_mailbox_id,
+        )
+
+        body = await message_service._maybe_upload_to_large_object_storage(
+            bitfount_message
+        )
+
+        # Assert upload code is called
+        mock_upload_to_s3.assert_called_once_with(
+            upload_url=s3_upload_url,
+            presigned_fields=s3_upload_fields,
+            data=expected_body,
+        )
+        # Assert download URL is returned (packed) to user
+        assert body == msgpack.dumps(s3_download_url)
+
+    async def test_maybe_upload_to_large_object_storage_fails_too_large_message(
+        self, message_service: _MessageService, mocker: MockerFixture
+    ) -> None:
+        """Tests that exception raised if message body too big for storage."""
+        # Patch out schema size calculation so we can set it
+        too_large_size = _MAX_STORAGE_SIZE_BYTES + 1
+        mock_data_sizer = mocker.patch(
+            "bitfount.federated.transport.message_service._get_packed_data_object_size"
+        )
+        mock_data_sizer.return_value = too_large_size
+
+        # Create mock message, mock length is long enough to consider uploading
+        mock_message = create_dataclass_mock(_BitfountMessage)
+        mock_message.body.__len__ = lambda _: too_large_size
 
         with pytest.raises(
-            TypeError,
+            ValueError,
             match=re.escape(
-                f"Unable to access OIDC response contents; "
-                f"expected dict, got {type(expected_message)}"
+                f"Message body is too large to upload: "
+                f"expected max {_MAX_STORAGE_SIZE_MEGABYTES} megabytes, "
+                f"got {_get_mb_from_bytes(too_large_size).fractional} megabytes."
             ),
         ):
-            await worker_mailbox.get_oidc_auth_flow_response()
-
-        # Check calls/returns
-        mock_message_decrypt.assert_awaited_once_with(
-            _BitfountMessageType.OIDC_AFC_PKCE_RESPONSE, ANY
-        )
+            await message_service._maybe_upload_to_large_object_storage(mock_message)
 
-    @pytest.mark.parametrize(
-        "key_to_drop", ("auth_code", "code_verifier", "redirect_uri")
-    )
-    async def test_get_oidc_auth_flow_response_fails_missing_key(
+    async def test_maybe_upload_to_large_object_storage_bad_upload(
         self,
-        key_to_drop: str,
-        mocker: MockerFixture,
-        worker_mailbox: _WorkerMailbox,
-    ) -> None:
-        """Tests get_oidc_auth_flow_response() fails with missing key."""
-        expected_message = {
-            "auth_code": "auth_code_value",
-            "code_verifier": "code_verifier_value",
-            "redirect_uri": "redirect_uri_value",
-        }
-        expected_message.pop(key_to_drop)
-
-        # Patch out _WorkerMailbox._get_message_and_decrypt()
-        mock_message_decrypt = mocker.patch.object(
-            worker_mailbox,
-            "_get_message_and_decrypt",
-            return_value=expected_message,
+        grpc_stub: Mock,
+        message_service: _MessageService,
+        mock_upload_to_s3: Mock,
+        s3_download_url: _S3PresignedURL,
+        s3_upload_fields: _S3PresignedPOSTFields,
+        s3_upload_url: _S3PresignedPOSTURL,
+    ) -> None:
+        """Test failed upload throws exception."""
+        # Set output as expected fields
+        grpc_stub.GetLargeObjectStorage.return_value = BlobStorageData(
+            uploadUrl=s3_upload_url,
+            downloadUrl=s3_download_url,
+            uploadFields=s3_upload_fields,
+        )
+
+        # Construct message
+        sender = "someSender"
+        sender_mailbox_id = "someSenderMailboxId"
+        expected_body = b"a" * (_SMALL_MESSAGE_UPPER_LIMIT_SIZE_BYTES + 1)
+        bitfount_message = _BitfountMessage(
+            body=expected_body,
+            message_type=_BitfountMessageType.UNDEFINED,
+            recipient="someRecipient",
+            recipient_mailbox_id="someMailboxId",
+            sender=sender,
+            sender_mailbox_id=sender_mailbox_id,
         )
 
+        # Set upload mock to throw exception
+        mock_upload_to_s3.side_effect = RequestException("TEST ERROR")
+
         with pytest.raises(
-            KeyError,
+            RequestException,
             match=re.escape(
-                f"Expected auth_code, code_verifier, and redirect_uri to be in "
-                f"OIDC response; got {expected_message.keys()}"
+                "Failed to upload message to large message storage. Cause: TEST ERROR."
             ),
         ):
-            await worker_mailbox.get_oidc_auth_flow_response()
-
-        # Check calls/returns
-        mock_message_decrypt.assert_awaited_once_with(
-            _BitfountMessageType.OIDC_AFC_PKCE_RESPONSE, ANY
-        )
-
-    async def test_get_oidc_device_code_response(
-        self,
-        mocker: MockerFixture,
-        worker_mailbox: _WorkerMailbox,
-    ) -> None:
-        """Tests get_oidc_device_code_response() correctly extracts details."""
-        now = datetime.now(timezone.utc)
-        expected_message = {
-            "device_code": "someDeviceCode",
-            "expires_at": now.isoformat(),
-            "interval": 5,
-        }
-
-        # Patch out _WorkerMailbox._get_message_and_decrypt()
-        mock_message_decrypt = mocker.patch.object(
-            worker_mailbox,
-            "_get_message_and_decrypt",
-            return_value=expected_message,
-        )
-
-        response = await worker_mailbox.get_oidc_device_code_response()
+            await message_service._maybe_upload_to_large_object_storage(
+                bitfount_message
+            )
 
-        # Check calls/returns
-        mock_message_decrypt.assert_awaited_once_with(
-            _BitfountMessageType.OIDC_DEVICE_CODE_RESPONSE, ANY
-        )
-        assert response.device_code == expected_message["device_code"]
-        assert response.expires_at == now
-        assert response.interval == 5
+        # Assert upload function called correctly even if errored
+        mock_upload_to_s3.assert_called_once_with(
+            upload_url=s3_upload_url,
+            presigned_fields=s3_upload_fields,
+            data=expected_body,
+        )
+
+    async def test_maybe_upload_to_large_object_storage_rpc_error(
+        self, grpc_stub: Mock, message_service: _MessageService
+    ) -> None:
+        """Test failed upload throws exception."""
+        # Make GRPC error out
+        grpc_stub.GetLargeObjectStorage.side_effect = RpcError(
+            "Failed to create storage"
+        )
+
+        # Construct message that should be uploaded
+        sender = "someSender"
+        sender_mailbox_id = "someSenderMailboxId"
+        expected_body = b"a" * (_SMALL_MESSAGE_UPPER_LIMIT_SIZE_BYTES + 1)
+        bitfount_message = _BitfountMessage(
+            body=expected_body,
+            message_type=_BitfountMessageType.UNDEFINED,
+            recipient="someRecipient",
+            recipient_mailbox_id="someMailboxId",
+            sender=sender,
+            sender_mailbox_id=sender_mailbox_id,
+        )
+
+        with pytest.raises(RpcError, match="Failed to create storage"):
+            await message_service._maybe_upload_to_large_object_storage(
+                bitfount_message
+            )
 
-    async def test_get_oidc_device_code_response_fails_wrong_type(
+    def test_upload_large_object_bad_response(
         self,
-        mocker: MockerFixture,
-        worker_mailbox: _WorkerMailbox,
+        mock_upload_to_s3: Mock,
+        s3_upload_fields: _S3PresignedPOSTFields,
+        s3_upload_url: _S3PresignedPOSTURL,
     ) -> None:
-        """Tests get_oidc_device_code_response() fails with wrong type."""
-        expected_message = "not a dict"
+        """Uploading fails due to non 200 response code."""
+        # Set upload mock to throw error (non-200 response code)
+        mock_upload_to_s3.side_effect = HTTPError("NON-200/201 TEST")
 
-        # Patch out _WorkerMailbox._get_message_and_decrypt()
-        mock_message_decrypt = mocker.patch.object(
-            worker_mailbox,
-            "_get_message_and_decrypt",
-            return_value=expected_message,
-        )
+        object_to_upload = b"here is my byte string"
 
         with pytest.raises(
-            TypeError,
+            RequestException,
             match=re.escape(
-                f"Unable to access OIDC response contents; "
-                f"expected dict, got {type(expected_message)}"
+                "Failed to upload message to large message storage. "
+                "Cause: NON-200/201 TEST."
             ),
         ):
-            await worker_mailbox.get_oidc_device_code_response()
+            _LargeObjectRequestHandler.upload_large_object(
+                s3_upload_url, s3_upload_fields, object_to_upload
+            )
 
-        # Check calls/returns
-        mock_message_decrypt.assert_awaited_once_with(
-            _BitfountMessageType.OIDC_DEVICE_CODE_RESPONSE, ANY
+        # Assert upload function called correctly even if errored
+        mock_upload_to_s3.assert_called_once_with(
+            upload_url=s3_upload_url,
+            presigned_fields=s3_upload_fields,
+            data=object_to_upload,
         )
 
-    @pytest.mark.parametrize(
-        "key_to_drop",
-        ("device_code", "expires_at", "interval"),
-    )
-    async def test_get_oidc_device_code_response_fails_missing_key(
-        self,
-        key_to_drop: str,
-        mocker: MockerFixture,
-        worker_mailbox: _WorkerMailbox,
-    ) -> None:
-        """Tests get_oidc_device_code_response() fails with missing key."""
-        now = datetime.now(timezone.utc)
-        expected_message = {
-            "device_code": "someDeviceCode",
-            "expires_at": now.isoformat(),
-            "interval": 5,
-        }
-        expected_message.pop(key_to_drop)
-
-        # Patch out _WorkerMailbox._get_message_and_decrypt()
-        mock_message_decrypt = mocker.patch.object(
-            worker_mailbox,
-            "_get_message_and_decrypt",
-            return_value=expected_message,
-        )
+    def test_download_large_object_bad_response(
+        self, mock_download_from_s3: Mock, s3_download_url: _S3PresignedURL
+    ) -> None:
+        """Downloading fails due to non 200 response code."""
+        # Set mock to return HTTP Error (non-200 response code)
+        mock_download_from_s3.side_effect = HTTPError("NON-200 TEST ERROR")
 
         with pytest.raises(
-            KeyError,
+            RequestException,
             match=re.escape(
-                f"Expected device_code, expires_at, and interval to be in "
-                f"OIDC response; got {expected_message.keys()}"
+                "Failed to retrieve message from large message storage. "
+                "Cause: NON-200 TEST ERROR."
             ),
         ):
-            await worker_mailbox.get_oidc_device_code_response()
+            _LargeObjectRequestHandler.get_large_object_from_url(s3_download_url)
 
-        # Check calls/returns
-        mock_message_decrypt.assert_awaited_once_with(
-            _BitfountMessageType.OIDC_DEVICE_CODE_RESPONSE, ANY
-        )
+        # Assert upload function called correctly even if errored
+        mock_download_from_s3.assert_called_once_with(download_url=s3_download_url)
 
-    async def test__get_psi_dataset(
-        self,
-        mocker: MockerFixture,
-        worker_mailbox: _WorkerMailbox,
+    def test_save_object_to_local_storage(
+        self, message_service: _MessageService
     ) -> None:
-        """Test _get_psi_dataset method."""
-        # Patch out _WorkerMailbox._get_message_and_decrypt()
-        expected_message = ["some message"]
-        mock_message_decrypt = mocker.patch.object(
-            worker_mailbox,
-            "_get_message_and_decrypt",
-            return_value=expected_message,
-        )
-        await _get_psi_dataset(worker_mailbox)
-        # Check calls/returns
-        mock_message_decrypt.assert_awaited_once_with(
-            _BitfountMessageType.PSI_DATASET, timeout=_SOFT_LIMIT_MESSAGE_TIMEOUT
-        )
-
-    async def test_log(
-        self, mocker: MockerFixture, worker_mailbox: _WorkerMailbox
-    ) -> None:
-        """Test that log message is sent appropriately."""
-        mock_log = mocker.patch.object(worker_mailbox, "_send_aes_encrypted_message")
-        await worker_mailbox.log({"msg": "message"})
-        mock_log.assert_awaited_once_with(
-            {"msg": "message"}, _BitfountMessageType.LOG_MESSAGE
-        )
-
-    @pytest.mark.parametrize(
-        "process_name",
-        ("aProcessName", None),
-        ids=lambda x: f"process_name_present={bool(x)}",
-    )
-    @pytest.mark.parametrize(
-        "thread_name",
-        ("aThreadName", None),
-        ids=lambda x: f"thread_name_present={bool(x)}",
-    )
-    def test_log_message_handler(
-        self,
-        mock_aes_decrypter: Mock,
-        mocker: MockerFixture,
-        modeller_mailbox_id: str,
-        modeller_name: str,
-        opt_task_id: Optional[str],
-        pod_identifier: str,
-        process_name: Optional[str],
-        thread_name: Optional[str],
-        worker_mailbox: _WorkerMailbox,
-        worker_mailbox_id: str,
-    ) -> None:
-        """Test LOG_MESSAGE handler modifies record as expected."""
-        # Mock out logging framework interaction
-        mock_logger = mocker.patch(
-            "bitfount.federated.transport.worker_transport.logger", autospec=True
-        )
-        mock_logging = mocker.patch(
-            "bitfount.federated.transport.worker_transport.logging", autospec=True
-        )
-
-        # Create mock log message
-        mock_log_message_contents = {
-            "msg": "An important log message",
-            "anotherField": "notToBeChanged",
-            "federated": True,
-        }
-        if process_name:
-            mock_log_message_contents["processName"] = process_name
-        if thread_name:
-            mock_log_message_contents["threadName"] = thread_name
-
-        mock_log_message = _BitfountMessage(
-            message_type=_BitfountMessageType.LOG_MESSAGE,
-            body=msgpack.dumps(mock_log_message_contents),
+        """Tests saving object to local storage."""
+        sender = "someSender"
+        sender_mailbox_id = "someSenderMailboxId"
+        expected_body = b"a" * (_SMALL_MESSAGE_UPPER_LIMIT_SIZE_BYTES + 1)
+
+        bitfount_message = _BitfountMessage(
+            body=expected_body,
+            message_type=_BitfountMessageType.UNDEFINED,
+            recipient="someRecipient",
+            recipient_mailbox_id="someMailboxId",
+            sender=sender,
+            sender_mailbox_id=sender_mailbox_id,
+        )
+
+        filename = message_service._save_object_to_local_storage(bitfount_message)
+        assert type(filename) is bytes
+        filename_str: str = msgpack.loads(filename)
+        assert Path(filename_str).exists()
+        with open(filename_str, "rb") as f:
+            assert f.read() == expected_body
+
+    def test_read_local_bitfount_message_from_rpc(self) -> None:
+        """Test reading local object from storage."""
+        sender = "someSender"
+        mailbox_id = "someSenderMailboxId"
+        _, local_tempfile = tempfile.mkstemp()
+        packed_message = msgpack.dumps(local_tempfile)
+        expected_message_body = msgpack.dumps(["some message"])
+        with open(local_tempfile, "wb") as f:
+            f.write(expected_message_body)
+        pod_identifier = "podOwner/somePodName"
+        reply_to_mailbox_id = "replyMailboxId"
+        expected_timestamp = datetime.now().isoformat()
+
+        grpc_message = GrpcBitfountMessage(
+            messageType=_BitfountMessageType.TRAINING_UPDATE.value,
+            body=packed_message,
             recipient=pod_identifier,
-            recipient_mailbox_id=worker_mailbox_id,
-            sender=modeller_name,
-            sender_mailbox_id=modeller_mailbox_id,
-            task_id=opt_task_id,
-        )
-
-        log_message_handler = worker_mailbox._get_log_message_handler()
-        log_message_handler(mock_log_message)
-
-        # Message contents should have been manipulated and logged out
-        expected_log_message = {
-            "msg": "<FROM MODELLER>: An important log message",
-            "anotherField": "notToBeChanged",
-        }
-        if process_name:
-            expected_log_message["processName"] = f"<{process_name}>"
-        if thread_name:
-            expected_log_message["threadName"] = f"<{thread_name}>"
-        mock_logging.makeLogRecord.assert_called_once_with(expected_log_message)
-        mock_logger.handle.assert_called_once_with(
-            mock_logging.makeLogRecord.return_value
-        )
+            recipientMailboxId=mailbox_id,
+            sender=sender,
+            senderMailboxId=reply_to_mailbox_id,
+            timestamp=expected_timestamp,
+        )
+        bitfount_message = _BitfountMessage.from_rpc(grpc_message)
+        assert type(bitfount_message) == _BitfountMessage
+        assert bitfount_message.body == expected_message_body
 
 
 @unit_test
-class TestInterPodWorkerMailbox:
-    """Tests for _InterPodWorkerMailbox."""
+class TestMessageEncryption:
+    """Tests AES message encryption."""
 
-    def test_init_fails_if_missing_keys(
-        self,
-        aes_key: bytes,
-        mock_message_service: Mock,
-        mock_pod_public_keys: Dict[str, Mock],
-        mock_private_key: Mock,
-        modeller_mailbox_id: str,
-        modeller_name: str,
-        other_pod_details: List[WorkerDetails],
-        pod_identifier: str,
-        pod_mailbox_ids: Dict[str, str],
-    ) -> None:
-        """Tests that init raises exception if missing public keys for some pods."""
-        # Remove public key from dictionary
-        missing_key_pod_id = other_pod_details[-1].pod_identifier
-        del mock_pod_public_keys[missing_key_pod_id]
+    @fixture
+    def original_message(self) -> bytes:
+        """Message to encrypt."""
+        return b"this is my original message"
+
+    def test_encryption_is_polymorphic(self, original_message: bytes) -> None:
+        """Testing that we can decrypt an encrypted message."""
+        encryption_key = _AESEncryption.generate_key()
 
-        with pytest.raises(
-            ValueError,
-            match=re.escape(
-                f"We are missing public keys for the following pods: "
-                f"{missing_key_pod_id}. "
-                f"Unable to continue inter-pod communication."
-            ),
-        ):
-            _InterPodWorkerMailbox(
-                pod_public_keys=mock_pod_public_keys,
-                private_key=mock_private_key,
-                pod_identifier=pod_identifier,
-                modeller_mailbox_id=modeller_mailbox_id,
-                modeller_name=modeller_name,
-                aes_encryption_key=aes_key,
-                message_service=mock_message_service,
-                pod_mailbox_ids=pod_mailbox_ids,
+        assert (
+            _MessageEncryption.decrypt_incoming_message(
+                _MessageEncryption.encrypt_outgoing_message(
+                    original_message, encryption_key
+                ),
+                encryption_key,
             )
-
-    async def test__send_pod_to_pod_message(
-        self,
-        interpod_worker_mailbox: _InterPodWorkerMailbox,
-        mock_message_service: Mock,
-        mock_message_timestamps: Callable[[Iterable[str]], Mock],
-        mock_message_type: Mock,
-        mock_pod_public_keys: Dict[str, Mock],
-        mock_rsa_encryption: Mock,
-        opt_task_id: Optional[str],
-        other_pod_details: List[WorkerDetails],
-        pod_identifier: str,
-        worker_mailbox_id: str,
-    ) -> None:
-        """Test _send_pod_to_pod_message method works."""
-        fake_timestamps = ["Hello"]
-        mock_message_timestamps(fake_timestamps)
-
-        msg_body = {"a": "message", "to": "send"}
-        recipient_details = other_pod_details[0]
-        recipient_key = mock_pod_public_keys[recipient_details.pod_identifier]
-
-        await interpod_worker_mailbox._send_pod_to_pod_message(
-            recipient=recipient_details.pod_identifier,
-            recipient_mailbox_id=recipient_details.mailbox_id,
-            object_to_send=msg_body,
-            message_type=mock_message_type,
-        )
-
-        # Check encryption called correctly
-        mock_rsa_encryption.assert_called_once_with(
-            msgpack.dumps(msg_body), recipient_key
-        )
-        # Check expected message sent
-        expected_message = _BitfountMessage(
-            message_type=mock_message_type,
-            body=msgpack.dumps(msg_body),  # mock encryption just returns input
-            recipient=recipient_details.pod_identifier,
-            recipient_mailbox_id=recipient_details.mailbox_id,
-            sender=pod_identifier,
-            sender_mailbox_id=worker_mailbox_id,
-            timestamp=fake_timestamps[0],
-            task_id=opt_task_id,
-        )
-        mock_message_service.send_message.assert_awaited_once_with(
-            expected_message, already_packed=True
+            == original_message
         )
 
-    async def test__send_pod_to_pod_message_logs_error_if_no_key(
-        self,
-        caplog: LogCaptureFixture,
-        interpod_worker_mailbox: _InterPodWorkerMailbox,
-        mock_message_service: Mock,
-        mock_message_type: Mock,
-    ) -> None:
-        """Test _send_pod_to_pod_message logs error if no public key for recipient."""
-        await interpod_worker_mailbox._send_pod_to_pod_message(
-            recipient="not_a_real_recipient/pod",
-            recipient_mailbox_id="not_a_real_mailbox_id",
-            object_to_send=Mock(),
-            message_type=mock_message_type,
-        )
+    def test_encrypted_message_is_different(self, original_message: bytes) -> None:
+        """Sanity checking we haven't just returned the message."""
+        encryption_key = _AESEncryption.generate_key()
 
-        # Check error logged
-        error_logs = get_error_logs(caplog)
-        assert (
-            "Unable to find public key for pod not_a_real_recipient/pod. "
-            "Unable to send pod-to-pod message." in error_logs
+        encrypted_message = _MessageEncryption.encrypt_outgoing_message(
+            original_message, encryption_key
         )
-        # Check didn't try to send
-        mock_message_service.send_message.assert_not_called()
-
-    def test__pod_to_pod_message_handler(
-        self,
-        interpod_worker_mailbox: _InterPodWorkerMailbox,
-        mock_private_key: Mock,
-    ) -> None:
-        """Test _pod_to_pod_message_handler returns decrypted RSA message."""
-        mock_message = create_autospec(_BitfountMessage, instance=True)
-
-        decrypted = interpod_worker_mailbox._pod_to_pod_message_handler(mock_message)
 
-        # Check message decrypted correctly
-        mock_message.decrypt_rsa.assert_called_once_with(mock_private_key)
-        assert decrypted == mock_message.decrypt_rsa.return_value
+        assert encrypted_message != original_message
 
-    async def test_check_modeller_online(
-        self,
-        mocker: MockerFixture,
-        online_check_uuid: str,
-        worker_mailbox: _WorkerMailbox,
-    ) -> None:
-        """Tests worker `check_modeller_online` method."""
-        mock_message_send = mocker.patch.object(
-            worker_mailbox,
-            "_send_aes_encrypted_message",
-            return_value=None,
+    def test_provided_encryption_key_is_used(self, original_message: bytes) -> None:
+        """Sanity checking the key actually serves a purpose."""
+        encrypted_message = _MessageEncryption.encrypt_outgoing_message(
+            original_message, _AESEncryption.generate_key()
         )
+        wrong_encryption_key = _AESEncryption.generate_key()
 
-        await worker_mailbox.check_modeller_online(online_check_uuid)
-        mock_message_send.assert_awaited_once_with(
-            online_check_uuid,
-            _BitfountMessageType.ONLINE_CHECK,
+        decrypted_message = _MessageEncryption.decrypt_incoming_message(
+            encrypted_message, wrong_encryption_key
         )
 
-    async def test_send_task_abort_message(
-        self,
-        mocker: MockerFixture,
-        worker_mailbox: _WorkerMailbox,
-    ) -> None:
-        """Tests worker `send_task_abort_message` method."""
-        mock_message_send = mocker.patch.object(
-            worker_mailbox,
-            "_send_aes_encrypted_message",
-            return_value=None,
-        )
-
-        await worker_mailbox.send_task_abort_message()
-        mock_message_send.assert_awaited_once_with(
-            None, _BitfountMessageType.TASK_ABORT
-        )
+        assert decrypted_message != original_message
 
 
 @unit_test
-class TestWorkerTransportFunctions:
-    """Test the other functions not contained within WorkerMailbox."""
+class TestBitfountMessage:
+    """Tests for BitfountMessage."""
 
-    async def test_send_to_pods_sends_message(
-        self,
-        interpod_worker_mailbox: _InterPodWorkerMailbox,
-        mock_message_service: Mock,
-        mock_message_timestamps: Callable[[Iterable[str]], Mock],
-        mock_rsa_encryption: Mock,
-        opt_task_id: Optional[str],
-        other_pod_details: List[WorkerDetails],
-        pod_identifier: str,
-        worker_mailbox_id: str,
-    ) -> None:
-        """Tests send_secure_shares_to_others method works correctly.
+    def test_timestamps_unique(self) -> None:
+        """Tests the timestamps for two BitfountMessages are different.
 
-        Asserts each pod receives their own message generated by secure_share_generator.
+        This will help avoid a regression bug where we had the same default
+        timestamp for every instance of the class due the nature of default arg
+        values in dataclasses.
         """
-        fake_timestamps = ["Hello", "World"]
-        mock_message_timestamps(fake_timestamps)
-
-        mock_message_service.send_message.return_value = SuccessResponse()
-
-        class DummySecureShare:
-            def __init__(self) -> None:
-                self.counter = 0
-
-            def message_body_generator(self) -> int:
-                self.counter += 1
-                return self.counter
-
-        sec = DummySecureShare()
-
-        await _send_secure_shares_to_others(
-            sec.message_body_generator,
-            interpod_worker_mailbox,
+        bm1 = _BitfountMessage(
+            message_type=Mock(),
+            body=b"",
+            recipient="recipient",
+            recipient_mailbox_id="recipient_mailbox_id",
+            sender="sender",
+            sender_mailbox_id="sender_mailbox_id",
+        )
+        bm2 = _BitfountMessage(
+            message_type=Mock(),
+            body=b"",
+            recipient="recipient",
+            recipient_mailbox_id="recipient_mailbox_id",
+            sender="sender",
+            sender_mailbox_id="sender_mailbox_id",
         )
+        assert bm1.timestamp != bm2.timestamp
 
-        # Call count is pods in `pod_mailbox_ids`
-        # excluding this pod as it doesnt send to itself
-        assert mock_message_service.send_message.call_count == 2
-        mock_message_service.send_message.assert_has_calls(
-            [
-                call(
-                    _BitfountMessage(
-                        message_type=_BitfountMessageType.SECURE_SHARE,
-                        body=msgpack.dumps(1),
-                        recipient=other_pod_details[0].pod_identifier,
-                        recipient_mailbox_id=other_pod_details[0].mailbox_id,
-                        sender=pod_identifier,
-                        sender_mailbox_id=worker_mailbox_id,
-                        timestamp=fake_timestamps[0],
-                        task_id=opt_task_id,
-                    ),
-                    already_packed=True,
-                ),
-                call(
-                    _BitfountMessage(
-                        message_type=_BitfountMessageType.SECURE_SHARE,
-                        body=msgpack.dumps(2),
-                        recipient=other_pod_details[1].pod_identifier,
-                        recipient_mailbox_id=other_pod_details[1].mailbox_id,
-                        sender=pod_identifier,
-                        sender_mailbox_id=worker_mailbox_id,
-                        timestamp=fake_timestamps[1],
-                        task_id=opt_task_id,
-                    ),
-                    already_packed=True,
-                ),
-            ]
-        )
 
-    async def test__get_worker_secure_shares(
-        self,
-        caplog: LogCaptureFixture,
-        interpod_worker_mailbox: _InterPodWorkerMailbox,
-        mock_private_key: Mock,
-        other_pod_details: List[WorkerDetails],
-        patch_poll_for_messages: PollForMessagesPatcher,
-    ) -> None:
-        """Tests the _get_worker_secure_shares function works."""
-        # Create mock messages to process
-        mock_messages = [
-            NonCallableMagicMock(
-                spec=_BitfountMessage,
-                **{
-                    "sender": details.pod_identifier,
-                    "message_type": _BitfountMessageType.SECURE_SHARE,
-                    "decrypt_rsa.return_value.body": i,
-                },
-            )
-            for i, details in enumerate(other_pod_details)
-        ]
-        patch_poll_for_messages(mock_messages)
+@unit_test
+class TestDecryptedBitfountMessage:
+    """Tests for BitfountMessage."""
 
-        with caplog.at_level(logging.DEBUG):
-            shares = await _run_func_and_listen_to_mailbox(
-                _get_worker_secure_shares(interpod_worker_mailbox),
-                interpod_worker_mailbox,
-            )
+    def test_timestamps_unique(self) -> None:
+        """Tests the timestamps for two DecryptedBitfountMessages are different.
 
-        # Check decoded shares are what we expect and that each message was decoded
-        # Need to sort for comparison as order is not guaranteed!
-        assert sorted(shares) == [i for i in range(len(other_pod_details))]
-        for mock_message in mock_messages:
-            mock_message.decrypt_rsa.assert_called_once_with(mock_private_key)
-
-        # Check that we logged out the messages received
-        debug_logs = get_debug_logs(caplog)
-        for other_pod in other_pod_details:
-            assert (
-                f"Receiving secure share from worker {other_pod.pod_identifier}"
-                in debug_logs
-            )
+        This will help avoid a regression bug where we had the same default
+        timestamp for every instance of the class due the nature of default arg
+        values in dataclasses.
+        """
+        dbm1 = _DecryptedBitfountMessage(
+            message_type=Mock(),
+            body=Mock(),
+            recipient="recipient",
+            recipient_mailbox_id="recipient_mailbox_id",
+            sender="sender",
+            sender_mailbox_id="sender_mailbox_id",
+        )
+        dbm2 = _DecryptedBitfountMessage(
+            message_type=Mock(),
+            body=Mock(),
+            recipient="recipient",
+            recipient_mailbox_id="recipient_mailbox_id",
+            sender="sender",
+            sender_mailbox_id="sender_mailbox_id",
+        )
+        assert dbm1.timestamp != dbm2.timestamp
```

### Comparing `bitfount-0.5.86/tests/bitfount/hub/test_api.py` & `bitfount-0.5.9/tests/bitfount/hub/test_api.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,97 +1,77 @@
 """Tests for the BitfountAM and BitfountHub API calls."""
-import base64
-from datetime import datetime, timezone
 import inspect
 import json
 import logging
 from pathlib import Path
 import re
-from typing import List, Optional, Type, Union, cast
-from unittest.mock import Mock, PropertyMock
+from typing import List, Optional, Tuple, cast
+from unittest.mock import Mock
 
+from _pytest.logging import LogCaptureFixture
 from cryptography.hazmat.primitives import hashes
 from cryptography.hazmat.primitives.asymmetric import padding
 from cryptography.hazmat.primitives.asymmetric.rsa import RSAPrivateKey, RSAPublicKey
+from cryptography.hazmat.primitives.serialization import Encoding, PublicFormat
 import pytest
-from pytest import LogCaptureFixture, MonkeyPatch, fixture
+from pytest import fixture
 from pytest_lazyfixture import lazy_fixture
 from pytest_mock import MockerFixture
 import requests
-from requests import HTTPError, RequestException
+from requests import HTTPError, RequestException, Session
 from requests.exceptions import InvalidJSONError
 import responses
-from responses.matchers import json_params_matcher, query_param_matcher
+from responses.matchers import json_params_matcher
 
 from bitfount.federated.encryption import _RSAEncryption
-from bitfount.federated.types import SerializedProtocol, _PodResponseType
+from bitfount.federated.task_requests import _ProtocolDetails
+from bitfount.federated.types import _PodResponseType
 from bitfount.hub.api import (
-    _DEV_AM_URL,
     _MAX_CUSTOM_MODEL_SIZE_BYTES,
     _MAX_CUSTOM_MODEL_SIZE_MEGABYTES,
     _MAX_SCHEMA_SIZE_BYTES,
     _MAX_SCHEMA_SIZE_MEGABYTES,
-    _MAX_WEIGHTS_SIZE_BYTES,
-    _MAX_WEIGHTS_SIZE_MEGABYTES,
-    _STAGING_AM_URL,
-    PRODUCTION_AM_URL,
+    AccessRequest,
     BitfountAM,
     BitfountHub,
     PodPublicMetadata,
     _check_pod_id_details,
-    hash_file_contents,
 )
 from bitfount.hub.authentication_flow import BitfountSession
-from bitfount.hub.exceptions import ModelUploadError, SchemaUploadError
 from bitfount.hub.types import (
     _AccessManagerKeyResponseJSON,
+    _AccessRequestResponseJSON,
     _AMAccessCheckResponseJSON,
     _HubFailureResponseJSON,
     _HubSuccessResponseJSON,
     _ModelDetailsResponseJSON,
     _ModelUploadResponseJSON,
-    _MonitorPostJSON,
     _MultiModelDetailsResponseJSON,
     _MultiPodDetailsResponseJSON,
     _OIDCAccessCheckPostJSON,
     _PodDetailsResponseJSON,
-    _PublicKeyJSON,
     _SAMLAdditionalInfoPOSTJSON,
     _SAMLChallengeResponseJSON,
-    _SignatureBasedAccessCheckPostJSON,
-    _UserRSAPublicKeysResponseJSON,
 )
 from bitfount.models.bitfount_model import BitfountModel
 from bitfount.storage import _get_packed_data_object_size
 from bitfount.types import (
-    BaseDistributedModelProtocol,
     _JSONDict,
     _S3PresignedPOSTFields,
     _S3PresignedPOSTURL,
     _S3PresignedURL,
-    _SAMLResponse,
 )
-from bitfount.utils import (
-    _get_mb_from_bytes,
-    _get_non_abstract_classes_from_module,
-    web_utils,
-)
-from tests.utils import PytestRequest
+from bitfount.utils import _get_mb_from_bytes
 from tests.utils.helper import (
     get_debug_logs,
     get_error_logs,
     get_warning_logs,
-    integration_test,
     unit_test,
 )
 
-# This forces `requests` to make IPv4 connections
-# TODO: [BIT-1443] Remove this once Hub/AM support IPv6
-requests.packages.urllib3.util.connection.HAS_IPV6 = False  # type: ignore[attr-defined] # Reason: see above # noqa: B950
-
 
 def sign_message(message: bytes, private_key: RSAPrivateKey) -> bytes:
     """Signs provided `message` with provided `private_key` and returns signature."""
     signature = private_key.sign(
         message,
         padding.PSS(
             mgf=padding.MGF1(hashes.SHA256()), salt_length=20  # padding.PSS.MAX_LENGTH
@@ -99,14 +79,27 @@
         hashes.SHA256(),
     )
 
     return signature
 
 
 @fixture
+def good_access_request(
+    access_json: str, signed_permission: str
+) -> _AccessRequestResponseJSON:
+    """Format of a good response for an access request."""
+    return {
+        "accessJSON": access_json,
+        "modellerPublicKeyPem": "not a real public key",
+        "status": "REJECTED",  # Note that this is not used
+        "signedPermission": signed_permission,
+    }
+
+
+@fixture
 def pod_namespace() -> str:
     """Pod namespace information."""
     return "pod_namespace"
 
 
 @fixture
 def pod_name() -> str:
@@ -117,14 +110,64 @@
 @fixture
 def pod_identifier(pod_name: str, pod_namespace: str) -> str:
     """Pod identifier information."""
     return f"{pod_namespace}/{pod_name}"
 
 
 @unit_test
+class TestAccessRequest:
+    """Tests the AccessRequest class."""
+
+    @fixture
+    def modeller_signature(
+        self, modeller_message: str, modeller_private_key: RSAPrivateKey
+    ) -> bytes:
+        """Message signed by modeller as fixture."""
+        return sign_message(modeller_message.encode("ascii"), modeller_private_key)
+
+    def test_access_request_signatures_and_status(
+        self,
+        access_manager_public_key: RSAPublicKey,
+        good_access_request: _AccessRequestResponseJSON,
+        modeller_public_key: RSAPublicKey,
+    ) -> None:
+        """Tests request signatures works."""
+        access_request = AccessRequest(good_access_request)
+
+        # Access request should be signed by the Access Manager and NOT the modeller.
+        assert access_request.authoriser_signed(access_manager_public_key)
+        assert not access_request.authoriser_signed(modeller_public_key)
+        assert access_request.status == "APPROVED"  # Should use what's in accessJSON
+
+    def test_access_request_no_signature(
+        self, access_json: str, authoriser_public_key: RSAPublicKey
+    ) -> None:
+        """Tests request signatures works."""
+        access_request = AccessRequest(
+            {"accessJSON": access_json, "modellerPublicKeyPem": "", "status": ""}
+        )
+
+        # Is NOT signed by the authoriser
+        assert not access_request.authoriser_signed(authoriser_public_key)
+        assert access_request.status == "APPROVED"
+
+    def test_access_request_is_correct_modeller(
+        self, access_json: str, modeller_message: str, modeller_signature: bytes
+    ) -> None:
+        """Tests request modeller checking works."""
+        access_request = AccessRequest(
+            {"accessJSON": access_json, "modellerPublicKeyPem": "", "status": ""}
+        )
+
+        assert access_request.is_correct_modeller(
+            modeller_message.encode(), modeller_signature
+        )
+
+
+@unit_test
 class TestBitfountHub:
     """Tests for the BitfountHub API."""
 
     # START OF FIXTURES #
     @fixture
     def good_pod(self, s3_download_url: _S3PresignedURL) -> _PodDetailsResponseJSON:
         """JSON response for a good get pod."""
@@ -196,15 +239,16 @@
 
                 def evaluate(self, test_dl=None):
                     pass
 
                 def fit(self, *args, **kwargs):
                     pass
 
-                def predict(self, *args, **kwargs):
+                @property
+                def training_needed(self):
                     pass
 
                 def apply_weight_updates():
                     ...
 
                 def backend_tensor_shim():
                     ...
@@ -220,37 +264,32 @@
 
                 def tensor_precision():
                     ...
 
                 def update_params():
                     ...
 
-                def _get_import_statements():
-                    ...
-
-                def _get_model():
-                    ...
-
             class MyOtherModel(BitfountModel):
                 def _set_metrics(self, metrics=None):
                     pass
 
                 def serialize(self, filename):
                     pass
 
                 def deserialize(self, filename):
                     pass
 
                 def evaluate(self, test_dl=None):
                     pass
 
-                def predict(self, *args, **kwargs):
+                def fit(self, *args, **kwargs):
                     pass
 
-                def fit(self, *args, **kwargs):
+                @property
+                def training_needed(self):
                     pass
 
                 def apply_weight_updates():
                     ...
 
                 def backend_tensor_shim():
                     ...
@@ -265,68 +304,36 @@
                     ...
 
                 def tensor_precision():
                     ...
 
                 def update_params():
                     ...
-
-                def _get_import_statements():
-                    ...
-
-                def _get_model():
-                    ...
             """
         )
 
     @fixture
     def good_models_response(self) -> List[_MultiModelDetailsResponseJSON]:
         """Good model details in format for multiple models as fixture."""
         return [
             {
                 "modellerName": "username",
                 "modelName": "blah",
-                "modelStorageKey": "nice_hash",
-                "modelVersion": 1,
+                "modelStorageKey": "model_storage_key",
             },
         ]
 
     @fixture
     def good_model_response(
-        self,
-        bitfount_model_correct_structure: str,
-        s3_download_url: _S3PresignedURL,
-        tmp_path: Path,
+        self, s3_download_url: _S3PresignedURL
     ) -> _ModelDetailsResponseJSON:
         """Good model code and correct hash as fixture."""
-        model_file = tmp_path / "model.py"
-        model_file.touch()
-        model_file.write_text(bitfount_model_correct_structure)
         return {
+            "modelStorageKey": "model_storage_key",
             "modelDownloadUrl": s3_download_url,
-            "modelHash": hash_file_contents(model_file),
-            "modelVersion": 1,
-        }
-
-    @fixture
-    def good_model_response_w_weights(
-        self,
-        bitfount_model_correct_structure: str,
-        s3_download_url: _S3PresignedURL,
-        tmp_path: Path,
-    ) -> _ModelDetailsResponseJSON:
-        """Good model code and correct hash as fixture."""
-        model_file = tmp_path / "model.py"
-        model_file.touch()
-        model_file.write_text(bitfount_model_correct_structure)
-        return {
-            "modelDownloadUrl": s3_download_url,
-            "modelHash": hash_file_contents(model_file),
-            "weightsDownloadUrl": s3_download_url,
-            "modelVersion": 1,
         }
 
     @fixture
     def json_success(self) -> _HubSuccessResponseJSON:
         """Hub success message in JSON format as fixture."""
         return {
             "success": True,
@@ -338,39 +345,29 @@
         """Hub failure message in JSON format as fixture."""
         return {
             "success": False,
             "errorMessage": "Error",
         }
 
     @fixture
-    def username(self) -> str:
-        """Fake username for auth session."""
-        return "session_username"
-
-    @fixture
     def hub_url(self) -> str:
         """Test hub URL as fixture."""
         return "http://test.bitfount.com"
 
     @fixture
-    def hub(
-        self, hub_url: str, mocker: MockerFixture, tmp_path: Path, username: str
-    ) -> BitfountHub:
+    def hub(self, hub_url: str, tmp_path: Path) -> BitfountHub:
         """A BitfountHub instance as fixture."""
-        session = BitfountSession()
+        session = requests.Session()
         # Patch authenticated, username and user_storage_path onto session
-        mocker.patch.object(
-            type(session), "authenticated", PropertyMock(return_value=True)
-        )
-        mocker.patch.object(
-            type(session), "username", PropertyMock(return_value=username)
-        )
+        session.authenticated = True  # type: ignore[attr-defined] # Reason: see comment # noqa: B950
+        session.username = "test"  # type: ignore[attr-defined] # Reason: see comment # noqa: B950
+        session.user_storage_path = tmp_path  # type: ignore[attr-defined] # Reason: see comment # noqa: B950
 
         # Treat this as a BitfountSession for these tests
-        return BitfountHub(session, hub_url)
+        return BitfountHub(cast(BitfountSession, session), hub_url)
 
     @fixture
     def schema_json(self) -> _JSONDict:
         """Dictionary view of a false schema."""
         return {"I": "am", "a": "pod schema"}
 
     @fixture
@@ -384,69 +381,41 @@
     ) -> PodPublicMetadata:
         """Pod public metadata as fixture."""
         return PodPublicMetadata(
             pod_name,
             "pod_display_name",
             "pod_description",
             schema_json,
+            True,
         )
 
     # END OF FIXTURES #
 
-    def test_issubclass_check_for_correctly_structured_bitfount_model(
-        self, bitfount_model_correct_structure: str, tmp_path: Path
-    ) -> None:
-        """Tests that the bitfount model is correctly structured.
-
-        And that the issubclass protocol check picks this up correctly.
-        """
-        model_name = "MyModel"
-        model_file = tmp_path / f"{model_name}.py"
-        model_file.touch()
-        model_file.write_text(bitfount_model_correct_structure)
-        model_cls = _get_non_abstract_classes_from_module(model_file)[model_name]
-        assert issubclass(model_cls, BaseDistributedModelProtocol)
-
-    def test_issubclass_check_for_incorrectly_structured_bitfount_model(
-        self, bitfount_model_incorrect_structure: str, tmp_path: Path
-    ) -> None:
-        """Tests that the bitfount model is incorrectly structured.
-
-        And that the issubclass protocol check picks this up correctly.
-        """
-        model_name = "MyModel"
-        model_file = tmp_path / f"{model_name}.py"
-        model_file.touch()
-        model_file.write_text(bitfount_model_incorrect_structure)
-        model_cls = _get_non_abstract_classes_from_module(model_file)[model_name]
-        assert not issubclass(model_cls, BaseDistributedModelProtocol)
-
     def test_create_hub_without_bitfount_session(self, mocker: MockerFixture) -> None:
         """Tests that BitfountSession is created under the hood if not provided."""
         mock_session = Mock(
             spec=BitfountSession,
             authenticated=False,
             user_storage_path="",
             username="",
-            authentication_handler=Mock(),
         )
         mocker.patch("bitfount.hub.api.BitfountSession", return_value=mock_session)
         hub = BitfountHub()
         mock_session.authenticate.assert_called_once()
         assert isinstance(hub.session, BitfountSession)
 
     @pytest.mark.parametrize(
         argnames=["pod_identifier_", "pod_name_", "pod_namespace_"],
         argvalues=[
             [lazy_fixture("pod_identifier"), None, None],
             [None, lazy_fixture("pod_name"), lazy_fixture("pod_namespace")],
         ],
     )
     @responses.activate
-    def test_get_pod(
+    def test_bitfounthub_get_pod(
         self,
         good_pod: _PodDetailsResponseJSON,
         hub: BitfountHub,
         hub_url: str,
         pod_identifier: str,
         # We use these names to avoid recursive parameterisation issues due to the
         # parametrize() call above.
@@ -469,15 +438,15 @@
         argnames=["pod_identifier_", "pod_name_", "pod_namespace_"],
         argvalues=[
             [lazy_fixture("pod_identifier"), None, None],
             [None, lazy_fixture("pod_name"), lazy_fixture("pod_namespace")],
         ],
     )
     @responses.activate
-    def test_get_pod_request_exception(
+    def test_bitfounthub_get_pod_request_exception(
         self,
         caplog: LogCaptureFixture,
         hub: BitfountHub,
         hub_url: str,
         pod_identifier: str,
         # We use these names to avoid recursive parameterisation issues due to the
         # parametrize() call above.
@@ -498,80 +467,60 @@
         assert pod_details is None
         # Check call was made
         assert responses.assert_call_count(f"{hub_url}/api/pods/{pod_identifier}", 1)
         # Assert error message
         error_logs = get_error_logs(caplog)
         assert "Bitfount Hub connection failed with: TEST" in error_logs
 
-    @pytest.mark.parametrize("status_code", (400, 404, 500))
     @pytest.mark.parametrize(
         argnames=["pod_identifier_", "pod_name_", "pod_namespace_"],
         argvalues=[
             [lazy_fixture("pod_identifier"), None, None],
             [None, lazy_fixture("pod_name"), lazy_fixture("pod_namespace")],
         ],
     )
     @responses.activate
-    def test_get_pod_bad_response(
+    def test_bitfounthub_get_pod_bad_response(
         self,
         caplog: LogCaptureFixture,
         hub: BitfountHub,
         hub_url: str,
         pod_identifier: str,
-        remove_web_retry_backoff_sleep: Optional[Mock],
-        status_code: int,
         # We use these names to avoid recursive parameterisation issues due to the
         # parametrize() call above.
         pod_identifier_: Optional[str],
         pod_name_: Optional[str],
         pod_namespace_: Optional[str],
     ) -> None:
         """Test Hub.get_pod() returns None if a non-OK response is returned."""
-        # Check retry backoff is patched
-        assert remove_web_retry_backoff_sleep is not None
-
         responses.add(
             responses.GET,
             f"{hub_url}/api/pods/{pod_identifier}",
-            status=status_code,
+            status=400,
         )
 
         pod_details = hub.get_pod(pod_identifier_, pod_namespace_, pod_name_)
 
         # Pod details should be None due to exception
         assert pod_details is None
-        # Check call/retries were made
-        expected_url_calls = 1
-        if status_code in web_utils._RETRY_STATUS_CODES:
-            expected_url_calls += web_utils._DEFAULT_MAX_RETRIES
-        assert responses.assert_call_count(
-            f"{hub_url}/api/pods/{pod_identifier}", expected_url_calls
-        )
+        # Check call was made
+        assert responses.assert_call_count(f"{hub_url}/api/pods/{pod_identifier}", 1)
         # Assert error message
         error_logs = get_error_logs(caplog)
-        assert f"Bitfount Hub connection failed with: {status_code}" in error_logs
-
-        # Check retries occurred if expected
-        if status_code in web_utils._RETRY_STATUS_CODES:
-            assert (
-                remove_web_retry_backoff_sleep.call_count
-                == web_utils._DEFAULT_MAX_RETRIES
-            )
-        else:
-            remove_web_retry_backoff_sleep.assert_not_called()
+        assert "Bitfount Hub connection failed with: 400" in error_logs
 
     @pytest.mark.parametrize(
         argnames=["pod_identifier_", "pod_name_", "pod_namespace_"],
         argvalues=[
             [lazy_fixture("pod_identifier"), None, None],
             [None, lazy_fixture("pod_name"), lazy_fixture("pod_namespace")],
         ],
     )
     @responses.activate
-    def test_get_pod_bad_json(
+    def test_bitfounthub_get_pod_bad_json(
         self,
         hub: BitfountHub,
         hub_url: str,
         pod_identifier: str,
         # We use these names to avoid recursive parameterisation issues due to the
         # parametrize() call above.
         pod_identifier_: Optional[str],
@@ -587,23 +536,83 @@
 
         with pytest.raises(InvalidJSONError, match="Invalid JSON response "):
             hub.get_pod(pod_identifier_, pod_namespace_, pod_name_)
 
         # Check call was made
         assert responses.assert_call_count(f"{hub_url}/api/pods/{pod_identifier}", 1)
 
+    @responses.activate
+    def test_bitfounthub_get_access_requests_good(
+        self,
+        access_json: str,
+        good_access_request: _AccessRequestResponseJSON,
+        hub: BitfountHub,
+        hub_url: str,
+        signed_permission: str,
+    ) -> None:
+        """Checks get_access_requests works correctly."""
+        modeller_name = "someModeller"
+        pod_namespace = "somePodOwner"
+        pod_name = "somePodName"
+        pod_identifier = f"{pod_namespace}/{pod_name}"
+
+        responses.add(
+            responses.GET,
+            f"{hub_url}/api/access/access-request"
+            f"?modellerName={modeller_name}&podName={pod_name}",
+            json=[good_access_request],
+        )
+
+        results = hub.get_access_requests(modeller_name, pod_identifier)
+
+        # Results should be the single access request
+        assert len(results) == 1
+        access_request = results[0]
+        assert access_request.access_json_str == access_json
+        assert access_request.signed_permission == signed_permission
+        # Check that the correct request was made
+        assert responses.assert_call_count(
+            f"{hub_url}/api/access/access-request"
+            f"?modellerName={modeller_name}&podName={pod_name}",
+            1,
+        )
+
+    @responses.activate
+    def test_bitfounthub_get_access_requests_exception(
+        self, caplog: LogCaptureFixture, hub: BitfountHub, hub_url: str
+    ) -> None:
+        """Checks method handles RequestException during hub request."""
+        modeller_name = "modeller"
+        pod_name = "pod"
+        pod_identifier = f"username/{pod_name}"
+
+        responses.add(
+            responses.GET,
+            f"{hub_url}/api/access/access-request"
+            f"?modellerName={modeller_name}&podName={pod_name}",
+            body=RequestException("TEST"),
+        )
+
+        results = hub.get_access_requests(modeller_name, pod_identifier)
+
+        # Returned value is empty if exception occurred
+        assert len(results) == 0
+        # Assert error message
+        error_logs = get_error_logs(caplog)
+        assert "Bitfount Hub connection failed with: TEST" in error_logs
+
     @pytest.mark.parametrize(
         argnames=["pod_identifier_", "pod_name_", "pod_namespace_"],
         argvalues=[
             [lazy_fixture("pod_identifier"), None, None],
             [None, lazy_fixture("pod_name"), lazy_fixture("pod_namespace")],
         ],
     )
     @responses.activate
-    def test_get_pod_key(
+    def test_bitfounthub_get_pod_key(
         self,
         good_pod: _PodDetailsResponseJSON,
         hub: BitfountHub,
         hub_url: str,
         pod_identifier: str,
         # We use these names to avoid recursive parameterisation issues due to the
         # parametrize() call above.
@@ -629,15 +638,15 @@
         argnames=["pod_identifier_", "pod_name_", "pod_namespace_"],
         argvalues=[
             [lazy_fixture("pod_identifier"), None, None],
             [None, lazy_fixture("pod_name"), lazy_fixture("pod_namespace")],
         ],
     )
     @responses.activate
-    def test_get_pod_key_request_exception(
+    def test_bitfounthub_get_pod_key_request_exception(
         self,
         caplog: LogCaptureFixture,
         hub: BitfountHub,
         hub_url: str,
         pod_identifier: str,
         # We use these names to avoid recursive parameterisation issues due to the
         # parametrize() call above.
@@ -651,28 +660,28 @@
             f"{hub_url}/api/pods/{pod_identifier}",
             body=RequestException("TEST"),
         )
 
         key = hub.get_pod_key(pod_identifier_, pod_namespace_, pod_name_)
 
         # Returned value is empty if exception occurred
-        assert key is None
+        assert key == ""
         # Assert error message
         error_logs = get_error_logs(caplog)
         assert "Bitfount Hub connection failed with: TEST" in error_logs
 
     @pytest.mark.parametrize(
         argnames=["pod_identifier_", "pod_name_", "pod_namespace_"],
         argvalues=[
             [lazy_fixture("pod_identifier"), None, None],
             [None, lazy_fixture("pod_name"), lazy_fixture("pod_namespace")],
         ],
     )
     @responses.activate
-    def test_get_pod_key_bad_json(
+    def test_bitfounthub_get_pod_key_bad_json(
         self,
         caplog: LogCaptureFixture,
         hub: BitfountHub,
         hub_url: str,
         pod_identifier: str,
         # We use these names to avoid recursive parameterisation issues due to the
         # parametrize() call above.
@@ -697,15 +706,15 @@
         argnames=["pod_identifier_", "pod_name_", "pod_namespace_"],
         argvalues=[
             [lazy_fixture("pod_identifier"), None, None],
             [None, lazy_fixture("pod_name"), lazy_fixture("pod_namespace")],
         ],
     )
     @responses.activate
-    def test_get_pod_key_no_key_in_json(
+    def test_bitfounthub_get_pod_key_no_key_in_json(
         self,
         caplog: LogCaptureFixture,
         good_pod: _PodDetailsResponseJSON,
         hub: BitfountHub,
         hub_url: str,
         pod_identifier: str,
         # We use these names to avoid recursive parameterisation issues due to the
@@ -722,16 +731,16 @@
             responses.GET,
             f"{hub_url}/api/pods/{pod_identifier}",
             json=good_pod,
         )
 
         key = hub.get_pod_key(pod_identifier_, pod_namespace_, pod_name_)
 
-        # Key should be None
-        assert key is None
+        # Key should be empty string
+        assert key == ""
         # Check call was made
         assert responses.assert_call_count(f"{hub_url}/api/pods/{pod_identifier}", 1)
         # Assert error message
         error_logs = get_error_logs(caplog)
         assert "Response JSON contained no public key: " in error_logs
 
     @responses.activate
@@ -822,56 +831,50 @@
     def test_send_model_success(
         self,
         bitfount_model_correct_structure: str,
         bitfount_model_correct_structure_size: int,
         hub: BitfountHub,
         hub_url: str,
         mock_s3_file_upload_in_api_module: Mock,
-        mocker: MockerFixture,
         s3_upload_fields: _S3PresignedPOSTFields,
         s3_upload_url: _S3PresignedPOSTURL,
         tmp_path: Path,
     ) -> None:
         """Checks send model works correctly."""
         model_name = "MyModel"
         model_file = tmp_path / f"{model_name}.py"
         model_file.touch()
         model_file.write_text(bitfount_model_correct_structure)
-        mock_hash = mocker.patch("bitfount.hub.api.hash_file_contents")
-        file_hash = "file_hash"
-        mock_hash.return_value = file_hash
 
         # Create fake hub response
         model_upload_response: _ModelUploadResponseJSON = {
             "uploadUrl": s3_upload_url,
             "uploadFields": s3_upload_fields,
             "success": True,
             "errorMessage": "None",
-            "alreadyExisted": False,
-            "version": 1,
         }
 
         responses.add(
             responses.POST,
             f"{hub_url}/api/models",
             json=model_upload_response,
             match=[
                 json_params_matcher(
                     {
                         "modelName": model_name,
                         "modelSize": bitfount_model_correct_structure_size,
-                        "modelHash": file_hash,
-                        "privateModel": False,
                     }
                 )
             ],
         )
 
-        hub.send_model(model_file)
+        response = hub.send_model(model_file)
 
+        # Check model sent successfully
+        assert response is True
         # Check that correct request was made
         assert responses.assert_call_count(f"{hub_url}/api/models", 1)
         # Check upload call was made to S3 correctly
         mock_s3_file_upload_in_api_module.assert_called_once_with(
             upload_url=s3_upload_url,
             presigned_fields=s3_upload_fields,
             file_contents=bitfount_model_correct_structure.encode("utf-8"),
@@ -881,15 +884,14 @@
     @responses.activate
     def test_send_model_fails_too_large(
         self, hub: BitfountHub, mocker: MockerFixture
     ) -> None:
         """Tests that send_model() fails if the model file is too large."""
         # Mock out model verification so it will pass
         mocker.patch.object(hub, "_verify_bitfount_model_format")
-        mock_hash = mocker.patch("bitfount.hub.api.hash_file_contents")
 
         # Create mock file path and set the "content" to be slightly too big
         too_long_size: int = _MAX_CUSTOM_MODEL_SIZE_BYTES + 1
         mock_code_path = Mock()
         mock_code_path.read_text.return_value = "a" * too_long_size
 
         with pytest.raises(
@@ -900,32 +902,32 @@
                 f"{_get_mb_from_bytes(too_long_size).fractional} megabytes."
             ),
         ):
             hub.send_model(mock_code_path)
 
         # Check that call to hub didn't occur before error occurred
         assert not responses.calls
-        mock_hash.assert_called_once_with(mock_code_path)
 
     @responses.activate
     def test_send_model_missing_implementations(
         self,
         caplog: LogCaptureFixture,
         hub: BitfountHub,
         model_missing_implementations: str,
         tmp_path: Path,
     ) -> None:
         """Checks send fails if model is still abstract."""
         model_file = tmp_path / "MyModel.py"
         model_file.touch()
         model_file.write_text(model_missing_implementations)
 
-        with pytest.raises(ModelUploadError, match="Model incorrectly structured"):
-            hub.send_model(model_file)
+        response = hub.send_model(model_file)
 
+        # Check model not sent
+        assert response is False
         # Check that no request was made (should have stopped before this point)
         assert len(responses.calls) == 0
         # Assert error message
         error_logs = get_error_logs(caplog)
         assert (
             "Model incorrectly structured. Error: Subclass of `BitfountModel` "
             "not found in file or is still abstract." in error_logs
@@ -940,17 +942,18 @@
         tmp_path: Path,
     ) -> None:
         """Checks send fails if multiple classes."""
         model_file = tmp_path / "MyModel.py"
         model_file.touch()
         model_file.write_text(model_multiple_classes)
 
-        with pytest.raises(ModelUploadError, match="Model incorrectly structured"):
-            hub.send_model(model_file)
+        response = hub.send_model(model_file)
 
+        # Check model not sent
+        assert response is False
         # Check that no request was made (should have stopped before this point)
         assert len(responses.calls) == 0
         # Assert error message
         error_logs = get_error_logs(caplog)
         assert (
             "Model incorrectly structured. Error: Model file contains 2 models. "
             "Must be just 1." in error_logs
@@ -965,683 +968,283 @@
         tmp_path: Path,
     ) -> None:
         """Checks send fails if class name and filename don't match."""
         model_file = tmp_path / "NotaMatchingName.py"
         model_file.touch()
         model_file.write_text(bitfount_model_correct_structure)
 
-        with pytest.raises(ModelUploadError, match="Model incorrectly structured."):
-            hub.send_model(model_file)
+        response = hub.send_model(model_file)
 
+        # Check model not sent
+        assert response is False
         # Check that no request was made (should have stopped before this point)
         assert len(responses.calls) == 0
         # Assert error message
         error_logs = get_error_logs(caplog)
         assert (
             "Model incorrectly structured. Error: MyModel != NotaMatchingName. "
             "Model class name must be the same as the filename" in error_logs
         )
 
     @responses.activate
-    def test_send_model_handles_import_error(
-        self,
-        caplog: LogCaptureFixture,
-        hub: BitfountHub,
-        mocker: MockerFixture,
-    ) -> None:
-        """Checks send fails if unable to import."""
-        mocker.patch.object(
-            hub,
-            "_verify_bitfount_model_format",
-            side_effect=ImportError("IMPORT ERROR"),
-        )
-
-        with pytest.raises(ModelUploadError, match="Unable to import model."):
-            hub.send_model(model_code_path=Mock())
-
-        # Check that no request was made (should have stopped before this point)
-        assert len(responses.calls) == 0
-        # Assert error message
-        error_logs = get_error_logs(caplog)
-        assert "IMPORT ERROR" in error_logs
-
-    @responses.activate
     def test_send_model_failure(
         self,
         bitfount_model_correct_structure: str,
         caplog: LogCaptureFixture,
         hub: BitfountHub,
         hub_url: str,
         json_failure: _HubFailureResponseJSON,
-        mocker: MockerFixture,
+        mock_s3_file_upload_in_api_module: Mock,
         tmp_path: Path,
     ) -> None:
         """Checks send model handles failure to upload."""
         model_name = "MyModel"
         model_file = tmp_path / f"{model_name}.py"
-        mock_hash = mocker.patch("bitfount.hub.api.hash_file_contents")
-        file_hash = "file_hash"
-        mock_hash.return_value = file_hash
         model_file.touch()
         model_file.write_text(bitfount_model_correct_structure)
 
         responses.add(
             responses.POST,
             f"{hub_url}/api/models",
             json=json_failure,
             match=[
                 json_params_matcher(
                     {
                         "modelName": model_name,
                         "modelSize": len(
                             bitfount_model_correct_structure.encode("utf-8")
                         ),
-                        "modelHash": file_hash,
-                        "privateModel": False,
                     }
                 )
             ],
         )
 
-        with pytest.raises(
-            ModelUploadError,
-            match="Failed to upload model details to hub",
-        ):
-            hub.send_model(model_file)
+        response = hub.send_model(model_file)
 
+        # Check response shows failure
+        assert response is False
         # Check that correct request was made (even though it failed))
         assert responses.assert_call_count(f"{hub_url}/api/models", 1)
         # Assert error message
         error_logs = get_error_logs(caplog)
         assert (
             "Could not send model to Bitfount Hub. Failed with message: Error"
             in error_logs
         )
 
     @responses.activate
-    def test_send_model_request_exception(
+    def test_send_model_exception(
         self,
         bitfount_model_correct_structure: str,
         bitfount_model_correct_structure_size: int,
         caplog: LogCaptureFixture,
         hub: BitfountHub,
         hub_url: str,
-        mocker: MockerFixture,
         tmp_path: Path,
     ) -> None:
-        """Checks request exception handled and logged if raised when sending."""
+        """Checks exception handled and logged if raised when sending."""
         model_name = "MyModel"
         model_file = tmp_path / f"{model_name}.py"
         model_file.touch()
         model_file.write_text(bitfount_model_correct_structure)
-        mock_hash = mocker.patch("bitfount.hub.api.hash_file_contents")
-        file_hash = "file_hash"
-        mock_hash.return_value = file_hash
 
         responses.add(
             responses.POST,
             f"{hub_url}/api/models",
             body=RequestException("TEST"),
             match=[
                 json_params_matcher(
                     {
                         "modelName": model_name,
                         "modelSize": bitfount_model_correct_structure_size,
-                        "modelHash": file_hash,
-                        "privateModel": False,
                     }
                 )
             ],
         )
 
-        with pytest.raises(
-            ModelUploadError,
-            match="Request exception occurred when uploading model details to hub",
-        ):
-            hub.send_model(model_file)
+        response = hub.send_model(model_file)
 
+        # Check response shows failure
+        assert response is False
         # Check that correct request was made (even though it failed))
         assert responses.assert_call_count(f"{hub_url}/api/models", 1)
         # Assert error message
         error_logs = get_error_logs(caplog)
         assert "Bitfount Hub connection failed with: TEST" in error_logs
 
-    @pytest.mark.parametrize("s3_exception_cls", (RequestException, HTTPError))
-    @responses.activate
-    def test_send_model_handles_s3_exception(
-        self,
-        bitfount_model_correct_structure: str,
-        bitfount_model_correct_structure_size: int,
-        caplog: LogCaptureFixture,
-        hub: BitfountHub,
-        hub_url: str,
-        mock_s3_file_upload_in_api_module: Mock,
-        mocker: MockerFixture,
-        s3_exception_cls: Type[Exception],
-        s3_upload_fields: _S3PresignedPOSTFields,
-        s3_upload_url: _S3PresignedPOSTURL,
-        tmp_path: Path,
-    ) -> None:
-        """Checks send model handles an exception when uploading to S3."""
-        model_name = "MyModel"
-        model_file = tmp_path / f"{model_name}.py"
-        model_file.touch()
-        model_file.write_text(bitfount_model_correct_structure)
-        mock_hash = mocker.patch("bitfount.hub.api.hash_file_contents")
-        file_hash = "file_hash"
-        mock_hash.return_value = file_hash
-
-        # Create fake hub response
-        model_upload_response: _ModelUploadResponseJSON = {
-            "uploadUrl": s3_upload_url,
-            "uploadFields": s3_upload_fields,
-            "success": True,
-            "errorMessage": "None",
-            "alreadyExisted": False,
-            "version": 1,
-        }
-
-        responses.add(
-            responses.POST,
-            f"{hub_url}/api/models",
-            json=model_upload_response,
-            match=[
-                json_params_matcher(
-                    {
-                        "modelName": model_name,
-                        "modelSize": bitfount_model_correct_structure_size,
-                        "modelHash": file_hash,
-                        "privateModel": False,
-                    }
-                )
-            ],
-        )
-
-        # Add exception to S3 upload
-        mock_s3_file_upload_in_api_module.side_effect = s3_exception_cls("S3 ERROR")
-
-        with pytest.raises(ModelUploadError, match="Failed to upload model to S3"):
-            hub.send_model(model_file)
-
-        # Check that correct request was made
-        assert responses.assert_call_count(f"{hub_url}/api/models", 1)
-        # Check upload call was made to S3 correctly
-        mock_s3_file_upload_in_api_module.assert_called_once_with(
-            upload_url=s3_upload_url,
-            presigned_fields=s3_upload_fields,
-            file_contents=bitfount_model_correct_structure.encode("utf-8"),
-            file_name=model_file.name,
-        )
-        # Check logs
-        error_logs = get_error_logs(caplog)
-        assert "Failed to upload model to S3: S3 ERROR" in error_logs
-
-    @responses.activate
-    def test_send_weights_success(
-        self,
-        hub: BitfountHub,
-        hub_url: str,
-        mock_s3_file_upload_in_api_module: Mock,
-        mocker: MockerFixture,
-        s3_upload_fields: _S3PresignedPOSTFields,
-        s3_upload_url: _S3PresignedPOSTURL,
-        tmp_path: Path,
-    ) -> None:
-        """Checks send weights works correctly."""
-        model_name = "MyModel"
-        weights_file = tmp_path / f"{model_name}_weights.pt"
-        weights_file.touch()
-        weights_contents = b"some bytes"
-        weights_file.write_bytes(weights_contents)
-        model_version = 1
-
-        # Create fake hub response
-        weights_upload_response: _ModelUploadResponseJSON = {
-            "success": True,
-            "uploadUrl": s3_upload_url,
-            "uploadFields": s3_upload_fields,
-            "alreadyExisted": False,
-            "version": 1,
-        }
-
-        responses.add(
-            responses.PUT,
-            f"{hub_url}/api/models",
-            json=weights_upload_response,
-            match=[
-                json_params_matcher(
-                    {
-                        "modelName": model_name,
-                        "modelVersion": model_version,
-                        "weightSize": 10,
-                    }
-                )
-            ],
-        )
-
-        hub.send_weights(model_name, model_version, weights_file)
-
-        # Check that correct request was made
-        assert responses.assert_call_count(f"{hub_url}/api/models", 1)
-        # Check upload call was made to S3 correctly
-        mock_s3_file_upload_in_api_module.assert_called_once_with(
-            upload_url=s3_upload_url,
-            presigned_fields=s3_upload_fields,
-            file_contents=weights_contents,
-            file_name=weights_file.name,
-        )
-
-    @responses.activate
-    def test_send_weights_fails_too_large(self, hub: BitfountHub) -> None:
-        """Tests that send_weights() fails if the weights file is too large."""
-        # Create mock file path and set the "content" to be slightly too big
-        too_long_size: int = _MAX_WEIGHTS_SIZE_BYTES + 1
-        mock_code_path = Mock()
-        mock_code_path.read_bytes.return_value = "a" * too_long_size
-
-        with pytest.raises(
-            ValueError,
-            match=re.escape(
-                f"Model weights are too large to upload: expected max "
-                f"{_MAX_WEIGHTS_SIZE_MEGABYTES} megabytes, got "
-                f"{_get_mb_from_bytes(too_long_size).fractional} megabytes."
-            ),
-        ):
-            hub.send_weights("model_name", 1, mock_code_path)
-
-        # Check that call to hub didn't occur before error occurred
-        assert not responses.calls
-
-    @pytest.mark.parametrize("s3_exception_cls", (RequestException, HTTPError))
-    @responses.activate
-    def test_send_weights_handles_s3_exception(
-        self,
-        caplog: LogCaptureFixture,
-        hub: BitfountHub,
-        hub_url: str,
-        mock_s3_file_upload_in_api_module: Mock,
-        s3_exception_cls: Type[Exception],
-        s3_upload_fields: _S3PresignedPOSTFields,
-        s3_upload_url: _S3PresignedPOSTURL,
-        tmp_path: Path,
-    ) -> None:
-        """Checks send weights handles an exception when uploading to S3."""
-        model_name = "MyModel"
-        weights_file = tmp_path / f"{model_name}_weights.py"
-        weights_file.touch()
-        content = b"bitfount_model_correct_structure"
-        weights_file.write_bytes(content)
-        model_version = 1
-
-        # Create fake hub response
-        weights_upload_response: _ModelUploadResponseJSON = {
-            "success": True,
-            "uploadUrl": s3_upload_url,
-            "uploadFields": s3_upload_fields,
-            "alreadyExisted": False,
-            "version": 1,
-        }
-
-        responses.add(
-            responses.PUT,
-            f"{hub_url}/api/models",
-            json=weights_upload_response,
-            match=[
-                json_params_matcher(
-                    {
-                        "modelName": model_name,
-                        "modelVersion": model_version,
-                        "weightSize": 32,
-                    }
-                )
-            ],
-        )
-
-        # Add exception to S3 upload
-        mock_s3_file_upload_in_api_module.side_effect = s3_exception_cls("S3 ERROR")
-
-        with pytest.raises(
-            ModelUploadError, match="Failed to upload model weights to S3"
-        ):
-            hub.send_weights(model_name, 1, weights_file)
-
-        # Check that correct request was made
-        assert responses.assert_call_count(f"{hub_url}/api/models", 1)
-        # Check upload call was made to S3 correctly
-        mock_s3_file_upload_in_api_module.assert_called_once_with(
-            upload_url=s3_upload_url,
-            presigned_fields=s3_upload_fields,
-            file_contents=content,
-            file_name=weights_file.name,
-        )
-        # Check logs
-        error_logs = get_error_logs(caplog)
-        assert "Failed to upload model weights to S3: S3 ERROR" in error_logs
-
-    @responses.activate
-    def test_get_weights_success(
-        self,
-        good_model_response_w_weights: str,
-        hub: BitfountHub,
-        hub_url: str,
-        mock_s3_file_download_in_api_module: Mock,
-        s3_download_url: _S3PresignedURL,
-    ) -> None:
-        """Checks get weights works correctly."""
-        # Mock S3 download return value
-        model_version = 1
-        mock_s3_file_download_in_api_module.return_value = b"weight file"
-        url = (
-            f"{hub_url}/api/models?modellerName=username&modelName=MyModel"
-            f"&modelVersion={model_version}"
-        )
-        responses.add(
-            responses.GET,
-            url,
-            match_querystring=True,
-            json=good_model_response_w_weights,
-        )
-        weights = hub.get_weights("username", "MyModel", model_version)
-
-        # Returned weights is of correct type
-        assert weights is not None
-        assert isinstance(weights, bytes)
-        # Check correct request was made
-        assert responses.assert_call_count(url, 1)
-        # Check correct S3 download was made
-        mock_s3_file_download_in_api_module.assert_called_once_with(s3_download_url)
-
-    @responses.activate
-    def test_get_weights_none_returned(
-        self,
-        caplog: LogCaptureFixture,
-        hub: BitfountHub,
-        hub_url: str,
-    ) -> None:
-        """Checks None returned if no weights were found."""
-        model_version = 1
-        modeller_name = "username"
-        model_name = "blah"
-        url = (
-            f"{hub_url}/api/models?modellerName={modeller_name}"
-            f"&modelName={model_name}&modelVersion={model_version}"
-        )
-
-        responses.add(
-            responses.GET,
-            url,
-            match_querystring=True,
-            json=[],
-        )
-
-        response = hub.get_weights("username", "blah", model_version)
-
-        # Check that no model was returned
-        assert response is None
-        # Check correct request was made
-        assert responses.assert_call_count(
-            url,
-            1,
-        )
-        # Assert warning message
-        warning_logs = get_warning_logs(caplog)
-        assert (
-            f"No models registered by the name of {model_name} "
-            f"from user {modeller_name}" in warning_logs
-        )
-
-    @responses.activate
-    def test_get_weights_failure(
-        self,
-        caplog: LogCaptureFixture,
-        hub: BitfountHub,
-        hub_url: str,
-    ) -> None:
-        """Checks get model handles RequestException being raised."""
-        model_version = 1
-        url = (
-            f"{hub_url}/api/models?modellerName=username&modelName=MyModel"
-            "&modelVersion=1"
-        )
-
-        responses.add(
-            responses.GET,
-            url,
-            match_querystring=True,
-            body=RequestException("TEST"),
-        )
-
-        response = hub.get_weights("username", "MyModel", model_version)
-
-        # Check that no model was returned
-        assert response is None
-        # Check correct request was made
-        assert responses.assert_call_count(url, 6)
-        # Assert error message
-        error_logs = get_error_logs(caplog)
-        assert "Bitfount Hub connection failed with: TEST" in error_logs
-
-    @pytest.mark.parametrize("model_version", [None, 1])
-    @pytest.mark.parametrize(
-        "model_response",
-        ["good_model_response", "good_model_response_w_weights"],
-    )
     @responses.activate
     def test_get_model_success(
         self,
         bitfount_model_correct_structure: str,
+        good_model_response: _ModelDetailsResponseJSON,
         hub: BitfountHub,
         hub_url: str,
         mock_s3_file_download_in_api_module: Mock,
-        model_response: str,
-        model_version: Optional[int],
-        request: PytestRequest,
         s3_download_url: _S3PresignedURL,
     ) -> None:
         """Checks get model works correctly."""
         # Mock S3 download return value
         mock_s3_file_download_in_api_module.return_value = (
             bitfount_model_correct_structure
         )
-        url = f"{hub_url}/api/models?modellerName=username&modelName=MyModel"
-        if model_version:
-            url = url + f"&modelVersion={model_version}"
 
-        json: _ModelDetailsResponseJSON = request.getfixturevalue(model_response)
         responses.add(
             responses.GET,
-            url,
+            f"{hub_url}/api/models?modellerName=username&modelName=MyModel",
             match_querystring=True,
-            json=json,
+            json=good_model_response,
         )
-        model = hub.get_model("username", "MyModel", model_version)
+
+        response = hub.get_model("username", "MyModel")
 
         # Returned model is of correct type
-        assert model is not None
-        assert issubclass(model, BitfountModel)
+        assert response is not None
+        assert issubclass(response, BitfountModel)
         # Check correct request was made
-        assert responses.assert_call_count(url, 1)
+        assert responses.assert_call_count(
+            f"{hub_url}/api/models?modellerName=username&modelName=MyModel", 1
+        )
         # Check correct S3 download was made
         mock_s3_file_download_in_api_module.assert_called_once_with(
             s3_download_url, encoding="utf-8"
         )
 
-    @pytest.mark.parametrize("model_version", [None, 1])
     @responses.activate
     def test_get_model_none_returned(
-        self,
-        caplog: LogCaptureFixture,
-        hub: BitfountHub,
-        hub_url: str,
-        model_version: Optional[int],
+        self, caplog: LogCaptureFixture, hub: BitfountHub, hub_url: str
     ) -> None:
         """Checks None returned if no model was found."""
         modeller_name = "username"
         model_name = "blah"
-        url = (
-            f"{hub_url}/api/models?modellerName={modeller_name}&modelName={model_name}"
-        )
-        if model_version:
-            url = url + f"&modelVersion={model_version}"
 
         responses.add(
             responses.GET,
-            url,
+            f"{hub_url}/api/models?modellerName={modeller_name}&modelName={model_name}",
             match_querystring=True,
             json=[],
         )
 
-        response = hub.get_model("username", "blah", model_version)
+        response = hub.get_model("username", "blah")
 
         # Check that no model was returned
         assert response is None
         # Check correct request was made
         assert responses.assert_call_count(
-            url,
+            f"{hub_url}/api/models?modellerName={modeller_name}&modelName={model_name}",
             1,
         )
         # Assert warning message
         warning_logs = get_warning_logs(caplog)
         assert (
             f"No models registered by the name of {model_name} "
             f"from user {modeller_name}" in warning_logs
         )
 
-    @pytest.mark.parametrize("model_version", [None, 1])
     @responses.activate
     def test_get_model_wrong_json_dict(
-        self,
-        hub: BitfountHub,
-        hub_url: str,
-        json_failure: _HubFailureResponseJSON,
-        model_version: Optional[int],
+        self, hub: BitfountHub, hub_url: str, json_failure: _HubFailureResponseJSON
     ) -> None:
         """Checks error raised if wrong JSON dict returned.
 
         Expected inner exception is KeyError.
         """
         modeller_name = "username"
         model_name = "blah"
-        url = (
-            f"{hub_url}/api/models?modellerName={modeller_name}&modelName={model_name}"
-        )
-        if model_version:
-            url = url + f"&modelVersion={model_version}"
 
         responses.add(
             responses.GET,
-            url,
+            f"{hub_url}/api/models?modellerName={modeller_name}&modelName={model_name}",
             match_querystring=True,
             json=json_failure,
         )
 
         with pytest.raises(
             InvalidJSONError,
             match=re.escape(
                 f"Cannot retrieve model, no model URL in pod response: {json_failure}"
             ),
         ):
-            hub.get_model(modeller_name, model_name, model_version)
+            hub.get_model(modeller_name, model_name)
 
-    @pytest.mark.parametrize("model_version", [None, 1])
     @responses.activate
     def test_get_model_wrong_json_type(
-        self,
-        hub: BitfountHub,
-        hub_url: str,
-        json_failure: _HubFailureResponseJSON,
-        model_version: Optional[int],
+        self, hub: BitfountHub, hub_url: str, json_failure: _HubFailureResponseJSON
     ) -> None:
         """Checks error raised if wrong JSON type returned.
 
         Expected inner exception is TypeError.
         """
         modeller_name = "username"
         model_name = "blah"
-        url = (
-            f"{hub_url}/api/models?modellerName={modeller_name}&modelName={model_name}"
-        )
-        if model_version:
-            url = url + f"&modelVersion={model_version}"
 
         responses.add(
             responses.GET,
-            url,
+            f"{hub_url}/api/models?modellerName={modeller_name}&modelName={model_name}",
             match_querystring=True,
             json=[json_failure],  # list rather than dict
         )
 
         with pytest.raises(
             InvalidJSONError,
             match=re.escape(
                 f"Cannot retrieve model, no model URL in pod response: {[json_failure]}"
             ),
         ):
-            hub.get_model(modeller_name, model_name, model_version)
+            hub.get_model(modeller_name, model_name)
 
-    @pytest.mark.parametrize("model_version", [None, 1])
     @responses.activate
     def test_get_model_failure(
-        self,
-        caplog: LogCaptureFixture,
-        hub: BitfountHub,
-        hub_url: str,
-        model_version: Optional[int],
+        self, caplog: LogCaptureFixture, hub: BitfountHub, hub_url: str
     ) -> None:
         """Checks get model handles RequestException being raised."""
-        url = f"{hub_url}/api/models?modellerName=username&modelName=MyModel"
-        if model_version:
-            url += f"&modelVersion={model_version}"
-
         responses.add(
             responses.GET,
-            url,
+            f"{hub_url}/api/models?modellerName=username&modelName=MyModel",
             match_querystring=True,
             body=RequestException("TEST"),
         )
 
-        response = hub.get_model("username", "MyModel", model_version)
+        response = hub.get_model("username", "MyModel")
 
         # Check that no model was returned
         assert response is None
         # Check correct request was made
-        assert responses.assert_call_count(url, 6)
+        assert responses.assert_call_count(
+            f"{hub_url}/api/models?modellerName=username&modelName=MyModel", 1
+        )
         # Assert error message
         error_logs = get_error_logs(caplog)
         assert "Bitfount Hub connection failed with: TEST" in error_logs
 
     @pytest.mark.skip("Reintroduce after [BIT-1073] is implemented.")
-    @pytest.mark.parametrize("model_version", [None, 1])
     @responses.activate
     def test_get_model_hash_mismatch_raises_value_error(
         self,
         good_model_response: _ModelDetailsResponseJSON,
         hub: BitfountHub,
         hub_url: str,
-        model_version: Optional[int],
     ) -> None:
         """Checks exception raised when model code hash doesn't match."""
         # add a newline to the end of the model code to change hash
-        # good_model_response["code"] = good_model_response["code"] + "\n"
-        #
-        # responses.add(
-        #     responses.GET,
-        #     f"{hub_url}/api/models?modellerName=username&modelName=MyModel",
-        #     match_querystring=True,
-        #     json=good_model_response,
-        # )
-        #
-        # with pytest.raises(ValueError):
-        #     hub.get_model("username", "MyModel", model_version)
-        #
-        # # Check correct request was made
-        # assert responses.assert_call_count(
-        #     f"{hub_url}/api/models?modellerName=username&modelName=MyModel", 1
-        # )
-        pass
+        good_model_response["code"] = good_model_response["code"] + "\n"  # type: ignore[typeddict-item]  # Reason: TODO: [BIT-1073] Remove ignore and fix issues  # noqa: B950
+
+        responses.add(
+            responses.GET,
+            f"{hub_url}/api/models?modellerName=username&modelName=MyModel",
+            match_querystring=True,
+            json=good_model_response,
+        )
+
+        with pytest.raises(ValueError):
+            hub.get_model("username", "MyModel")
+
+        # Check correct request was made
+        assert responses.assert_call_count(
+            f"{hub_url}/api/models?modellerName=username&modelName=MyModel", 1
+        )
 
     @responses.activate
     def test_register_pod_success(
         self,
         access_manager_public_key: RSAPublicKey,
         authoriser_public_key: RSAPublicKey,
         hub: BitfountHub,
@@ -1650,19 +1253,19 @@
         pod_name: str,
         public_metadata: PodPublicMetadata,
         s3_upload_fields: _S3PresignedPOSTFields,
         s3_upload_url: _S3PresignedPOSTURL,
         schema_size: int,
     ) -> None:
         """Checks pod registration works correctly."""
-        pod_public_key_str: str = _RSAEncryption.serialize_public_key(
-            authoriser_public_key, form="SSH"
+        pod_public_key_str: str = authoriser_public_key.public_bytes(
+            Encoding.OpenSSH, PublicFormat.OpenSSH
         ).decode("utf-8")
-        access_manager_public_key_str: str = _RSAEncryption.serialize_public_key(
-            access_manager_public_key, form="SSH"
+        access_manager_public_key_str: str = access_manager_public_key.public_bytes(
+            Encoding.OpenSSH, PublicFormat.OpenSSH
         ).decode("utf-8")
 
         responses.add(
             responses.POST,
             f"{hub_url}/api/pods",
             json={
                 "success": True,
@@ -1672,14 +1275,15 @@
             match=[
                 json_params_matcher(
                     {
                         "name": pod_name,
                         "podPublicKey": pod_public_key_str,
                         "accessManagerPublicKey": access_manager_public_key_str,
                         "podDisplayName": public_metadata.display_name,
+                        "isPublic": public_metadata.is_public,
                         "description": public_metadata.description,
                         "schemaSize": schema_size,
                     }
                 )
             ],
         )
 
@@ -1759,15 +1363,15 @@
         """Tests that exception raised if schema is too large for storage."""
         # Patch out schema size calculation so we can set it
         too_large_size = _MAX_SCHEMA_SIZE_BYTES + 1
         mock_data_sizer = mocker.patch("bitfount.hub.api._get_packed_data_object_size")
         mock_data_sizer.return_value = too_large_size
 
         with pytest.raises(
-            SchemaUploadError,
+            ValueError,
             match=re.escape(
                 "Schema is too large to upload: "
                 f"expected max {_MAX_SCHEMA_SIZE_MEGABYTES} megabytes, "
                 f"got {_get_mb_from_bytes(too_large_size).fractional} megabytes."
             ),
         ):
             hub.register_pod(
@@ -1782,16 +1386,16 @@
         hub: BitfountHub,
         hub_url: str,
         pod_name: str,
     ) -> None:
         """Checks do_pod_heartbeat correctly."""
         caplog.set_level(logging.DEBUG)
 
-        authoriser_public_key_str = _RSAEncryption.serialize_public_key(
-            authoriser_public_key, form="SSH"
+        authoriser_public_key_str = authoriser_public_key.public_bytes(
+            Encoding.OpenSSH, PublicFormat.OpenSSH
         ).decode("utf-8")
 
         responses.add(
             responses.PATCH,
             f"{hub_url}/api/pods",
             json={"success": True},
             match=[
@@ -1889,20 +1493,16 @@
         self,
         access_manager_public_key: RSAPublicKey,
         authoriser_public_key: RSAPublicKey,
         hub: BitfountHub,
         hub_url: str,
         json_failure: _HubFailureResponseJSON,
         public_metadata: PodPublicMetadata,
-        remove_web_retry_backoff_sleep: Optional[Mock],
     ) -> None:
         """Tests 500 response correctly handled during pod registration."""
-        # Check retry backoff is patched
-        assert remove_web_retry_backoff_sleep is not None
-
         responses.add(
             responses.POST,
             f"{hub_url}/api/pods",
             json=json_failure,
             status=500,
         )
 
@@ -1910,19 +1510,14 @@
             HTTPError,
             match="Bitfount Hub connection failed with: 500 Server Error",
         ):
             hub.register_pod(
                 public_metadata, authoriser_public_key, access_manager_public_key
             )
 
-        # Check retries occurred
-        assert (
-            remove_web_retry_backoff_sleep.call_count == web_utils._DEFAULT_MAX_RETRIES
-        )
-
     @responses.activate
     def test_add_pod_handles_http_non_200_201_error_when_registering(
         self,
         access_manager_public_key: RSAPublicKey,
         authoriser_public_key: RSAPublicKey,
         hub: BitfountHub,
         hub_url: str,
@@ -1948,35 +1543,26 @@
     @responses.activate
     def test_worker_handles_connection_error_when_registering(
         self,
         access_manager_public_key: RSAPublicKey,
         authoriser_public_key: RSAPublicKey,
         hub: BitfountHub,
         public_metadata: PodPublicMetadata,
-        remove_web_retry_backoff_sleep: Optional[Mock],
     ) -> None:
         """Checks connection error raised during failed registering."""
-        # Check retry backoff is patched
-        assert remove_web_retry_backoff_sleep is not None
-
         # No `response` added, so responses raises a connection error. This is
         # wrapped with the RequestException raised by Hub.
         with pytest.raises(
             requests.exceptions.ConnectionError,
             match="Bitfount Hub connection failed with: Connection refused by",
         ):
             hub.register_pod(
                 public_metadata, authoriser_public_key, access_manager_public_key
             )
 
-        # Check retries occurred
-        assert (
-            remove_web_retry_backoff_sleep.call_count == web_utils._DEFAULT_MAX_RETRIES
-        )
-
     @responses.activate
     def test_get_pod_schema(
         self,
         good_pod: _PodDetailsResponseJSON,
         hub: BitfountHub,
         hub_url: str,
         mock_bitfount_schema_load: Mock,
@@ -2076,372 +1662,37 @@
             f"{good_pod}",
         ):
             hub.get_pod_schema(pod_identifier)
 
         # Check URL was called
         assert responses.assert_call_count(f"{hub_url}/api/pods/{pod_identifier}", 1)
 
-    @fixture
-    def monitor_update(self) -> _MonitorPostJSON:
-        """Example monitor update in JSON format."""
-        return {
-            "taskId": "task-id",
-            "senderId": "sender-mailbox-id",
-            "recipientId": "recipient-mailbox-id",
-            "timestamp": datetime.now(timezone.utc).isoformat(),
-            "privacy": "PRIVATE",
-            "type": "TASK_STATUS_UPDATE",
-            "message": "this is a message",
-            "metadata": {"meta": "data"},
-            "progress": {"progress": {"value": 1, "total": 10}},
-            "resourceUsage": {"a_resource": 10},
-        }
-
-    @fixture
-    def ingestion_endpoint_url(self, hub_url: str) -> str:
-        """Monitor service ingestion endpoint url."""
-        return f"{hub_url}/api/ingest"
-
-    @responses.activate
-    def test_send_monitor_update(
-        self,
-        hub: BitfountHub,
-        ingestion_endpoint_url: str,
-        monitor_update: _MonitorPostJSON,
-    ) -> None:
-        """Test send_monitor_update works correctly."""
-        responses.add(
-            responses.POST,
-            url=ingestion_endpoint_url,
-            status=200,
-            match=[json_params_matcher(cast(_JSONDict, monitor_update))],
-        )
-
-        hub.send_monitor_update(monitor_update)
-
-    @pytest.mark.parametrize(
-        argnames="status_code",
-        argvalues=(400, 404, 500),
-    )
-    @responses.activate
-    def test_send_monitor_update_http_error(
-        self,
-        caplog: LogCaptureFixture,
-        hub: BitfountHub,
-        ingestion_endpoint_url: str,
-        monitor_update: _MonitorPostJSON,
-        status_code: int,
-    ) -> None:
-        """Test send_monitor_update handles HTTP errors."""
-        responses.add(
-            responses.POST,
-            url=ingestion_endpoint_url,
-            status=status_code,
-            match=[json_params_matcher(cast(_JSONDict, monitor_update))],
-        )
-
-        with pytest.raises(HTTPError):
-            hub.send_monitor_update(monitor_update)
-
-        # Check error logs
-        error_logs = get_error_logs(caplog)
-        assert str(status_code) in error_logs
-
-    @responses.activate
-    def test_send_monitor_update_request_exception(
-        self,
-        caplog: LogCaptureFixture,
-        hub: BitfountHub,
-        ingestion_endpoint_url: str,
-        monitor_update: _MonitorPostJSON,
-    ) -> None:
-        """Test send_monitor_update handles request errors."""
-        responses.add(
-            responses.POST,
-            url=ingestion_endpoint_url,
-            body=RequestException("test error"),
-            match=[json_params_matcher(cast(_JSONDict, monitor_update))],
-        )
-
-        with pytest.raises(RequestException, match="test error"):
-            hub.send_monitor_update(monitor_update)
-
-        # Check error logs
-        error_logs = get_error_logs(caplog)
-        assert "test error" in error_logs
-
-    @fixture(scope="class")
-    def registration_public_key(self) -> RSAPublicKey:
-        """Public key for hub public key registration tests."""
-        _, public_key = _RSAEncryption.generate_key_pair()
-        return public_key
-
-    @responses.activate
-    def test_register_public_key(
-        self,
-        hub: BitfountHub,
-        hub_url: str,
-        registration_public_key: RSAPublicKey,
-        username: str,
-    ) -> None:
-        """Tests register_public_key() passes through public key correctly."""
-        responses.add(
-            responses.POST,
-            f"{hub_url}/api/{username}/keys",
-            json={"id": "1234"},
-            status=200,
-            match=[
-                json_params_matcher(
-                    {
-                        "key": _RSAEncryption.serialize_public_key(
-                            registration_public_key, form="SSH"
-                        ).decode()
-                    }
-                ),
-                query_param_matcher({"version": 2}),
-            ],
-        )
-
-        hub.register_user_public_key(registration_public_key)
-
-    @pytest.mark.parametrize(
-        argnames="status_code",
-        argvalues=(400, 404, 500),
-    )
-    @responses.activate
-    def test_register_public_key_http_error(
-        self,
-        caplog: LogCaptureFixture,
-        hub: BitfountHub,
-        hub_url: str,
-        registration_public_key: RSAPublicKey,
-        status_code: int,
-        username: str,
-    ) -> None:
-        """Tests register_public_key() handles HTTP error."""
-        responses.add(
-            responses.POST,
-            f"{hub_url}/api/{username}/keys",
-            json={"success": False},
-            status=status_code,
-            match=[
-                json_params_matcher(
-                    {
-                        "key": _RSAEncryption.serialize_public_key(
-                            registration_public_key, form="SSH"
-                        ).decode()
-                    }
-                ),
-                query_param_matcher({"version": 2}),
-            ],
-        )
-
-        with pytest.raises(HTTPError):
-            hub.register_user_public_key(registration_public_key)
-
-        # Check error logs
-        error_logs = get_error_logs(caplog)
-        assert str(status_code) in error_logs
-
-    @responses.activate
-    def test_register_public_key_request_exception(
-        self,
-        caplog: LogCaptureFixture,
-        hub: BitfountHub,
-        hub_url: str,
-        registration_public_key: RSAPublicKey,
-        username: str,
-    ) -> None:
-        """Tests register_public_key() handles request error."""
-        responses.add(
-            responses.POST,
-            f"{hub_url}/api/{username}/keys",
-            body=RequestException("TEST"),
-            match=[
-                json_params_matcher(
-                    {
-                        "key": _RSAEncryption.serialize_public_key(
-                            registration_public_key, form="SSH"
-                        ).decode()
-                    }
-                ),
-                query_param_matcher({"version": 2}),
-            ],
-        )
-
-        with pytest.raises(RequestException, match="TEST"):
-            hub.register_user_public_key(registration_public_key)
-
-        # Check error logs
-        error_logs = get_error_logs(caplog)
-        assert "TEST" in error_logs
-
-    @fixture
-    def user_identity_verifiers_response_json(
-        self,
-        public_key_registered: Union[str, bool, None],
-        registration_public_key: RSAPublicKey,
-        username: str,
-    ) -> _UserRSAPublicKeysResponseJSON:
-        """Mock response from /api/identity-verifiers.
-
-        `public_key_registered` will control whether the public key is included
-        in the response and what form it takes.
-        """
-        if public_key_registered is True:
-            public_key = _RSAEncryption.serialize_public_key(
-                registration_public_key
-            ).decode()
-        elif isinstance(public_key_registered, str):
-            public_key = public_key_registered
-        else:
-            public_key = None
-
-        response_json: _UserRSAPublicKeysResponseJSON = {
-            "maximumOffset": -1,
-            "keys": [],
-        }
-        if public_key and public_key_registered is not False:
-            response_json = {
-                "maximumOffset": 0,
-                "keys": [_PublicKeyJSON(id="4", active=True, public_key=public_key)],
-            }
-
-        return response_json
-
-    @responses.activate
-    def test_check_public_key_registered(
-        self,
-        caplog: LogCaptureFixture,
-        hub: BitfountHub,
-        hub_url: str,
-        registration_public_key: RSAPublicKey,
-        username: str,
-    ) -> None:
-        """Tests register_public_key() handles request error."""
-        expected_key_id = "1337"
-        hub_response = {
-            "active": True,
-            "id": expected_key_id,
-            "public_key": _RSAEncryption.serialize_public_key(
-                registration_public_key, form="SSH"
-            ).decode(),
-        }
-        responses.add(
-            responses.GET,
-            f"{hub_url}/api/{username}/keys/{expected_key_id}",
-            json=hub_response,
-        )
-
-        result = hub.check_public_key_registered_and_active(expected_key_id)
-
-        assert result is not None
-        assert result["active"] == hub_response["active"]
-        assert result["id"] == expected_key_id
-        assert _RSAEncryption.public_keys_equal(
-            result["public_key"], registration_public_key
-        )
-
-    @responses.activate
-    def test_check_public_key_registered_but_inactive(
-        self,
-        caplog: LogCaptureFixture,
-        hub: BitfountHub,
-        hub_url: str,
-        registration_public_key: RSAPublicKey,
-        username: str,
-    ) -> None:
-        """Tests register_public_key() handles request error."""
-        expected_key_id = "1337"
-        expected_key = {"active": False, "id": expected_key_id, "public_key": "someKey"}
-        responses.add(
-            responses.GET,
-            f"{hub_url}/api/{username}/keys/{expected_key_id}",
-            json=expected_key,
-        )
-
-        assert hub.check_public_key_registered_and_active(expected_key_id) is None
-
-    @responses.activate
-    def test_check_public_key_not_found(
-        self,
-        caplog: LogCaptureFixture,
-        hub: BitfountHub,
-        hub_url: str,
-        registration_public_key: RSAPublicKey,
-        username: str,
-    ) -> None:
-        """Tests register_public_key() handles request error."""
-        expected_key_id = "1337"
-        responses.add(
-            responses.GET,
-            f"{hub_url}/api/{username}/keys/{expected_key_id}",
-            status=404,
-        )
-
-        assert hub.check_public_key_registered_and_active(expected_key_id) is None
-
-    @responses.activate
-    def test_check_public_key_registered_hub_returns_error(
-        self,
-        caplog: LogCaptureFixture,
-        hub: BitfountHub,
-        hub_url: str,
-        registration_public_key: RSAPublicKey,
-        username: str,
-    ) -> None:
-        """Tests check_public_key_registered_and_active() handles request error."""
-        expected_key_id = "1337"
-        responses.add(
-            responses.GET,
-            f"{hub_url}/api/{username}/keys/{expected_key_id}",
-            body=RequestException("TEST"),
-        )
-
-        with pytest.raises(RequestException, match="TEST"):
-            hub.check_public_key_registered_and_active(expected_key_id)
-
-        # Check error logs
-        error_logs = get_error_logs(caplog)
-        assert "TEST" in error_logs
-
 
+@unit_test
 class TestAccessManager:
     """Test the BitfountAM class."""
 
     @fixture
     def url(self) -> str:
         """Access Manager URL as a fixture."""
-        return "http://test-am.hub.bitfount.com"
+        return "http://test-am.bitfount.com"
 
     @fixture
-    def session(self, mocker: MockerFixture) -> BitfountSession:
+    def session(self) -> Session:
         """A session with auth."""
-        session = BitfountSession()
+        session = requests.Session()
         # Patching `authenticated` status onto session as it is required
-        mocker.patch.object(
-            type(session), "authenticated", PropertyMock(return_value=True)
-        )
-        mocker.patch.object(
-            type(session),
-            "hub_request_headers",
-            PropertyMock(return_value={"authorization": "Bearer test"}),
-        )
-        mocker.patch.object(
-            type(session),
-            "am_request_headers",
-            PropertyMock(return_value={"authorization": "Bearer test"}),
-        )
+        session.authenticated = True  # type: ignore[attr-defined] # Reason: see comment # noqa: B950
         session.headers.update({"x-Bitfount-unit-test": "Session was used"})
         return session
 
     @fixture
-    def access_manager(self, session: BitfountSession, url: str) -> BitfountAM:
+    def access_manager(self, session: Session, url: str) -> BitfountAM:
         """Access Manager instance as a fixture."""
-        return BitfountAM(session, url)
+        return BitfountAM(cast(BitfountSession, session), url)
 
     @fixture
     def expected_saml_challenge(self) -> str:
         """SAML Challenge string from access manager."""
         return "a legit saml challenge"
 
     @fixture
@@ -2458,96 +1709,73 @@
 
     @fixture
     def modeller_name(self) -> str:
         """Modeller name."""
         return "someModeller"
 
     @fixture
-    def modeller_protocol_request(self) -> SerializedProtocol:
+    def modeller_request(self) -> Tuple[str, str, None, None]:
         """Modeller request."""
-        return {
-            "class_name": "bitfount.FederatedAveraging",
-            "algorithm": {"class_name": "bitfount.FederatedModelTraining"},
-        }
+        return (
+            "bitfount.FederatedAveraging",
+            "bitfount.FederatedModelTraining",
+            None,
+            None,
+        )
 
     @fixture
     def fake_original_saml_request_id(self) -> str:
         """Original SAML Request ID for tests."""
         return "12345"
 
     @fixture
     def expected_saml_post_body(
         self,
         fake_original_saml_request_id: str,
         modeller_name: str,
-        modeller_protocol_request: SerializedProtocol,
+        modeller_request: Tuple[str, str, None, None],
         pod_identifier: str,
-    ) -> _SAMLAdditionalInfoPOSTJSON:
+    ) -> _JSONDict:
         """JSON body Pod sends to AM to check access."""
-        saml_post_body: _SAMLAdditionalInfoPOSTJSON = {
+        additional_info: _SAMLAdditionalInfoPOSTJSON = {
             "originalSAMLRequestID": fake_original_saml_request_id,
             "podIdentifier": pod_identifier,
             "modellerName": modeller_name,
-            "modellerProtocolRequest": modeller_protocol_request,
+            "modellerRequest": list(modeller_request),  # type: ignore[typeddict-item] # Reason: json converts tuples into lists # noqa: B950
             "identityProvider": "SAML",
         }
-        return saml_post_body
+        return dict(
+            {
+                "saml": "response",
+            },
+            **additional_info,
+        )
 
     @fixture
     def fake_access_token(self) -> str:
         """Fake access token."""
         return "fake_access_token"
 
     @fixture
     def expected_oidc_post_body(
         self,
         fake_access_token: str,
         modeller_name: str,
-        modeller_protocol_request: SerializedProtocol,
+        modeller_request: Tuple[str, str, None, None],
         pod_identifier: str,
     ) -> _OIDCAccessCheckPostJSON:
         """JSON body for OIDC Access Check POST request."""
         return {
             "podIdentifier": pod_identifier,
             "modellerName": modeller_name,
-            "modellerProtocolRequest": modeller_protocol_request,
+            "modellerRequest": list(modeller_request),  # type: ignore[typeddict-item] # Reason: json converts tuples into lists # noqa: B950
             "modellerToken": fake_access_token,
             "identityProvider": "OIDC",
         }
 
-    @fixture
-    def unsigned_task(self) -> bytes:
-        """Unsigned task."""
-        return b"unsigned_task"
-
-    @fixture
-    def task_signature(self) -> bytes:
-        """Task signature."""
-        return b"task_signature"
-
-    @fixture
-    def expected_signature_based_access_post_body(
-        self,
-        modeller_name: str,
-        modeller_protocol_request: SerializedProtocol,
-        pod_identifier: str,
-        task_signature: bytes,
-        unsigned_task: bytes,
-    ) -> _SignatureBasedAccessCheckPostJSON:
-        """JSON body for Signature based Access Check POST request."""
-        return {
-            "podIdentifier": pod_identifier,
-            "modellerName": modeller_name,
-            "modellerProtocolRequest": modeller_protocol_request,
-            "unsignedTask": base64.b64encode(unsigned_task).decode("utf-8"),
-            "taskSignature": base64.b64encode(task_signature).decode("utf-8"),
-            "identityProvider": "SIGNATURE",
-        }
-
-    @unit_test
     def test_create_access_manager_without_bitfount_session(
         self, mocker: MockerFixture
     ) -> None:
         """Tests that BitfountSession is created under the hood if not provided."""
         mock_session = Mock(
             spec=BitfountSession,
             authenticated=False,
@@ -2555,26 +1783,25 @@
             username="",
         )
         mocker.patch("bitfount.hub.api.BitfountSession", return_value=mock_session)
         hub = BitfountAM()
         mock_session.authenticate.assert_called_once()
         assert isinstance(hub.session, BitfountSession)
 
-    @unit_test
     @responses.activate
     def test_get_access_manager_key(
         self,
         access_manager: BitfountAM,
         access_manager_public_key: RSAPublicKey,
         url: str,
     ) -> None:
         """Checks AM key getter works."""
         expected_response: _AccessManagerKeyResponseJSON = {
-            "accessManagerPublicKey": _RSAEncryption.serialize_public_key(
-                access_manager_public_key, form="SSH"
+            "accessManagerPublicKey": access_manager_public_key.public_bytes(
+                Encoding.OpenSSH, PublicFormat.OpenSSH
             ).decode("utf-8"),
         }
 
         responses.add(
             responses.GET,
             f"{url}/api/access-manager-key",
             json=expected_response,
@@ -2585,32 +1812,14 @@
 
         # Check key type and value are what we expect
         assert isinstance(key, RSAPublicKey)
         assert _RSAEncryption.serialize_public_key(
             key
         ) == _RSAEncryption.serialize_public_key(access_manager_public_key)
 
-    @integration_test
-    @pytest.mark.timeout(10)
-    def test_get_access_manager_key_speed(self, monkeypatch: MonkeyPatch) -> None:
-        """Checks AM key getter is able to retrieve the key with minimal delay.
-
-        This test is to ensure we don't regress due to IPv4/IPv6 issues either on the
-        requests side or the AM side.
-        """
-        monkeypatch.setenv("BITFOUNT_ENVIRONMENT", "staging")
-        mock_session = Mock(authenticated=True, spec=BitfountSession)
-        am = BitfountAM(session=mock_session)
-        key = am.get_access_manager_key()
-        # Check key type and value are what we expect
-        assert isinstance(key, RSAPublicKey)
-        # Assert that the authenticate method on the session was not called
-        mock_session.authenticate.assert_not_called()
-
-    @unit_test
     @responses.activate
     def test_get_access_manager_key_fails_request_exception(
         self, access_manager: BitfountAM, url: str
     ) -> None:
         """Checks exception raised on if request raises RequestException."""
         responses.add(
             responses.GET,
@@ -2620,47 +1829,28 @@
 
         with pytest.raises(
             RequestException,
             match="Bitfount Access Manager connection failed with: TEST.$",
         ):
             access_manager.get_access_manager_key()
 
-    @unit_test
-    @pytest.mark.parametrize("status_code", (400, 404, 500))
     @responses.activate
     def test_get_access_manager_key_fails_non_200(
-        self,
-        access_manager: BitfountAM,
-        remove_web_retry_backoff_sleep: Optional[Mock],
-        status_code: int,
-        url: str,
+        self, access_manager: BitfountAM, url: str
     ) -> None:
         """Checks exception raised on non-200 response."""
-        # Check retry backoff is patched
-        assert remove_web_retry_backoff_sleep is not None
-
         responses.add(
             responses.GET,
             f"{url}/api/access-manager-key",
-            status=status_code,
+            status=500,
         )
 
         with pytest.raises(HTTPError):
             access_manager.get_access_manager_key()
 
-        # Check retries occurred if expected
-        if status_code in web_utils._RETRY_STATUS_CODES:
-            assert (
-                remove_web_retry_backoff_sleep.call_count
-                == web_utils._DEFAULT_MAX_RETRIES
-            )
-        else:
-            remove_web_retry_backoff_sleep.assert_not_called()
-
-    @unit_test
     @responses.activate
     def test_get_access_manager_key_fails_non_json(
         self, access_manager: BitfountAM, url: str
     ) -> None:
         """Checks exception raised if response is `body`, not `json`."""
         non_json_response = "NOT JSON RESPONSE"
         responses.add(
@@ -2672,15 +1862,14 @@
 
         with pytest.raises(
             InvalidJSONError,
             match=re.escape(f"Invalid JSON response (200): {non_json_response}"),
         ):
             access_manager.get_access_manager_key()
 
-    @unit_test
     @responses.activate
     def test_get_access_manager_key_fails_no_key_in_json(
         self, access_manager: BitfountAM, url: str
     ) -> None:
         """Checks exception raised if key not in JSON."""
         expected_response = {
             "incorrectKey": "incorrectValue",
@@ -2698,15 +1887,14 @@
             match=re.escape(
                 f"Unable to extract public key from access manager response, "
                 f"no key in JSON: {expected_response}"
             ),
         ):
             access_manager.get_access_manager_key()
 
-    @unit_test
     @responses.activate
     def test_get_access_manager_key_fails_bad_json(
         self, access_manager: BitfountAM, url: str
     ) -> None:
         """Checks bad JSON response causes exception."""
         expected_response = "JSON BUT AT WHAT COST"
 
@@ -2722,481 +1910,327 @@
             match=re.escape(
                 f"Unable to extract public key from access manager response, "
                 f"no key in JSON: {expected_response}"
             ),
         ):
             access_manager.get_access_manager_key()
 
-    @unit_test
     @responses.activate
     def test_get_saml_challenge(
         self,
         access_manager: BitfountAM,
         expected_saml_challenge: str,
         expected_saml_request_id: str,
-        mocker: MockerFixture,
         saml_endpoint_result: _SAMLChallengeResponseJSON,
-        session: BitfountSession,
+        session: Session,
         url: str,
     ) -> None:
         """Checks SAML challenge retrieval works."""
-        mocker.patch.object(type(session), "_is_am_url", Mock(return_value=True))
         responses.add(
             responses.GET,
-            f"{url}/api/saml?idp=cli",
+            f"{url}/api/saml",
             json=saml_endpoint_result,
             status=200,
         )
 
         saml_challenge, saml_request_id = access_manager.get_saml_challenge()
 
         assert saml_challenge == expected_saml_challenge
         assert saml_request_id == expected_saml_request_id
-        # Make sure the access token is added as a header to the request
-        first_call: responses.Call = responses.calls[0]
-        assert (
-            first_call.request.headers["Authorization"]
-            == session.hub_request_headers["authorization"]
-        )
+        # Make sure we used the session to make the request!
+        assert responses.calls[0].request.headers == session.headers
 
-    @unit_test
     @responses.activate
     def test_get_saml_challenge_fails_request_exception(
         self, access_manager: BitfountAM, url: str
     ) -> None:
         """Checks exception raised on if request raises RequestException."""
-        responses.add(
-            responses.GET, f"{url}/api/saml?idp=cli", body=RequestException("TEST")
-        )
+        responses.add(responses.GET, f"{url}/api/saml", body=RequestException("TEST"))
 
         with pytest.raises(
             RequestException,
             match="Bitfount Access Manager connection failed with: TEST.$",
         ):
             access_manager.get_saml_challenge()
 
-    @unit_test
-    @pytest.mark.parametrize("status_code", (400, 404, 500))
     @responses.activate
     def test_get_saml_challenge_fails_non_200(
-        self,
-        access_manager: BitfountAM,
-        remove_web_retry_backoff_sleep: Optional[Mock],
-        status_code: int,
-        url: str,
+        self, access_manager: BitfountAM, url: str
     ) -> None:
         """Checks exception raised on non-200 response."""
-        # Check retry backoff is patched
-        assert remove_web_retry_backoff_sleep is not None
-
         responses.add(
-            responses.GET,
-            f"{url}/api/saml?idp=cli",
-            status=status_code,
-            json={"error": "Some error"},
+            responses.GET, f"{url}/api/saml", status=500, json={"error": "Some error"}
         )
 
         with pytest.raises(HTTPError):
             access_manager.get_saml_challenge()
 
-        # Check retries occurred if expected
-        if status_code in web_utils._RETRY_STATUS_CODES:
-            assert (
-                remove_web_retry_backoff_sleep.call_count
-                == web_utils._DEFAULT_MAX_RETRIES
-            )
-        else:
-            remove_web_retry_backoff_sleep.assert_not_called()
-
-    @unit_test
     @responses.activate
     def test_get_saml_challenge_fails_non_json(
         self, access_manager: BitfountAM, url: str
     ) -> None:
         """Checks exception raised if response is `body`, not `json`."""
         non_json_response = "NOT JSON RESPONSE"
         responses.add(
             responses.GET,
-            f"{url}/api/saml?idp=cli",
+            f"{url}/api/saml",
             body=non_json_response,
             status=200,
         )
 
         with pytest.raises(
             InvalidJSONError,
             match=re.escape(f"Invalid JSON response (200): {non_json_response}"),
         ):
             access_manager.get_saml_challenge()
 
-    @unit_test
     @responses.activate
     def test_get_saml_challenge_fails_wrong_json(
         self, access_manager: BitfountAM, url: str
     ) -> None:
         """Checks exception raised if expected keys not in JSON."""
         expected_response = {
             "incorrectKey": "incorrectValue",
         }
 
         responses.add(
             responses.GET,
-            f"{url}/api/saml?idp=cli",
+            f"{url}/api/saml",
             json=expected_response,
             status=200,
         )
 
         with pytest.raises(
             InvalidJSONError,
             match=re.escape(
                 f"Unable to extract SAML Challenge from access manager response, "
                 f"no challenge in JSON: {expected_response}"
             ),
         ):
             access_manager.get_saml_challenge()
 
-    @unit_test
     @responses.activate
     def test_get_saml_challenge_fails_bad_json(
         self, access_manager: BitfountAM, url: str
     ) -> None:
         """Checks bad JSON response causes exception."""
         expected_response = "JSON BUT AT WHAT COST"
 
         responses.add(
             responses.GET,
-            f"{url}/api/saml?idp=cli",
+            f"{url}/api/saml",
             json=expected_response,
             status=200,
         )
 
         with pytest.raises(
             InvalidJSONError,
             match=re.escape(
                 f"Unable to extract SAML Challenge from access manager response, "
                 f"no challenge in JSON: {expected_response}"
             ),
         ):
             access_manager.get_saml_challenge()
 
-    @unit_test
     @responses.activate
     def test_validate_saml_response_success(
         self,
         access_manager: BitfountAM,
-        expected_saml_post_body: _SAMLAdditionalInfoPOSTJSON,
-        session: BitfountSession,
+        expected_saml_post_body: _JSONDict,
+        session: Session,
         url: str,
     ) -> None:
         """Checks accepted SAML & Access check is returned."""
         accept_response: _AMAccessCheckResponseJSON = {"code": "ACCEPT"}
-        saml_response: _SAMLResponse = {"saml": "response"}
+
         responses.add(
             responses.POST,
-            f"{url}/api/access?idp=cli",
+            f"{url}/api/access",
             json=accept_response,
         )
 
         assert (
             access_manager.validate_saml_response(
-                saml_response,
+                {"saml": "response"},
                 expected_saml_post_body["originalSAMLRequestID"],
                 expected_saml_post_body["podIdentifier"],
                 expected_saml_post_body["modellerName"],
-                expected_saml_post_body["modellerProtocolRequest"],
+                _ProtocolDetails(*expected_saml_post_body["modellerRequest"]),
             )
             == _PodResponseType.ACCEPT
         )
 
-        first_call: responses.Call = responses.calls[0]
-        request_body = first_call.request.body
+        request_body = responses.calls[0].request.body
         assert request_body is not None
-        assert json.loads(request_body) == {**saml_response, **expected_saml_post_body}
+        assert json.loads(request_body) == expected_saml_post_body
         # Make sure we used the session to make the request!
         # Some headers are added when we attach the body
         # So we just check that the expected headers are a subset of the attached ones
-        assert session.headers.items() <= first_call.request.headers.items()
+        assert session.headers.items() <= responses.calls[0].request.headers.items()
 
-    @unit_test
     @responses.activate
     def test_validate_saml_response_rejects_access(
         self, access_manager: BitfountAM, expected_saml_post_body: _JSONDict, url: str
     ) -> None:
         """Check validate SAML response when AM denies access."""
-        reject_response: _AMAccessCheckResponseJSON = {"code": "UNAUTHORISED"}
-        saml_response: _SAMLResponse = {"saml": "response"}
+        reject_response: _AMAccessCheckResponseJSON = {"code": "PROTOCOL_NOT_APPROVED"}
         responses.add(
             responses.POST,
-            f"{url}/api/access?idp=cli",
+            f"{url}/api/access",
             json=reject_response,
         )
 
         assert (
             access_manager.validate_saml_response(
-                saml_response,
+                {"saml": "response"},
                 expected_saml_post_body["originalSAMLRequestID"],
                 expected_saml_post_body["podIdentifier"],
                 expected_saml_post_body["modellerName"],
-                expected_saml_post_body["modellerProtocolRequest"],
+                _ProtocolDetails(*expected_saml_post_body["modellerRequest"]),
             )
-            == _PodResponseType.UNAUTHORISED
+            == _PodResponseType.PROTOCOL_NOT_APPROVED
         )
 
-        first_call: responses.Call = responses.calls[0]
-        request_body = first_call.request.body
+        request_body = responses.calls[0].request.body
         assert request_body is not None
-        assert json.loads(request_body) == {**saml_response, **expected_saml_post_body}
+        assert json.loads(request_body) == expected_saml_post_body
 
-    @unit_test
     @responses.activate
     def test_validate_saml_response_fails_request_exception(
         self, access_manager: BitfountAM, expected_saml_post_body: _JSONDict, url: str
     ) -> None:
         """Checks exception raised on request raising RequestException."""
         responses.add(
-            responses.POST, f"{url}/api/access?idp=cli", body=RequestException("TEST")
+            responses.POST, f"{url}/api/access", body=RequestException("TEST")
         )
 
         with pytest.raises(
             RequestException,
             match="Bitfount Access Manager connection failed with: TEST.$",
         ):
             access_manager.validate_saml_response(
                 {"saml": "response"},
                 expected_saml_post_body["originalSAMLRequestID"],
                 expected_saml_post_body["podIdentifier"],
                 expected_saml_post_body["modellerName"],
-                expected_saml_post_body["modellerProtocolRequest"],
+                _ProtocolDetails(*expected_saml_post_body["modellerRequest"]),
             )
 
-    @unit_test
-    @pytest.mark.parametrize("status_code", (400, 404, 500))
     @responses.activate
     def test_validate_saml_response_fails_non_200(
-        self,
-        access_manager: BitfountAM,
-        expected_saml_post_body: _JSONDict,
-        remove_web_retry_backoff_sleep: Optional[Mock],
-        status_code: int,
-        url: str,
+        self, access_manager: BitfountAM, expected_saml_post_body: _JSONDict, url: str
     ) -> None:
         """Checks exception raised on non-200 response."""
-        # Check retry backoff is patched
-        assert remove_web_retry_backoff_sleep is not None
-
         responses.add(
             responses.POST,
-            f"{url}/api/access?idp=cli",
-            status=status_code,
+            f"{url}/api/access",
+            status=500,
             json={"error": "Some error"},
         )
 
         with pytest.raises(HTTPError):
             access_manager.validate_saml_response(
                 {"saml": "response"},
                 expected_saml_post_body["originalSAMLRequestID"],
                 expected_saml_post_body["podIdentifier"],
                 expected_saml_post_body["modellerName"],
-                expected_saml_post_body["modellerProtocolRequest"],
-            )
-
-        # Check retries occurred if expected
-        if status_code in web_utils._RETRY_STATUS_CODES:
-            assert (
-                remove_web_retry_backoff_sleep.call_count
-                == web_utils._DEFAULT_MAX_RETRIES
+                _ProtocolDetails(*expected_saml_post_body["modellerRequest"]),
             )
-        else:
-            remove_web_retry_backoff_sleep.assert_not_called()
 
-    @unit_test
     @responses.activate
     def test_validate_saml_response_fails_json_missing_response_code(
         self, access_manager: BitfountAM, expected_saml_post_body: _JSONDict, url: str
     ) -> None:
         """Checks "ERROR" code returned if expected keys not in JSON."""
         expected_response = {
             "incorrectKey": "incorrectValue",
         }
 
         responses.add(
             responses.POST,
-            f"{url}/api/access?idp=cli",
+            f"{url}/api/access",
             json=expected_response,
             status=200,
         )
 
         assert (
             access_manager.validate_saml_response(
                 {"saml": "response"},
                 expected_saml_post_body["originalSAMLRequestID"],
                 expected_saml_post_body["podIdentifier"],
                 expected_saml_post_body["modellerName"],
-                expected_saml_post_body["modellerProtocolRequest"],
+                _ProtocolDetails(*expected_saml_post_body["modellerRequest"]),
             )
-            == _PodResponseType.NO_ACCESS
+            == _PodResponseType.ERROR_IN_VERIFICATION
         )
 
-    @unit_test
     @responses.activate
     def test_validate_saml_response_fails_wrong_json(
         self, access_manager: BitfountAM, expected_saml_post_body: _JSONDict, url: str
     ) -> None:
         """Checks exception raised if expected keys not in JSON."""
         expected_response = "JSON BUT AT WHAT COST"
 
         responses.add(
             responses.POST,
-            f"{url}/api/access?idp=cli",
+            f"{url}/api/access",
             json=expected_response,
             status=200,
         )
 
         with pytest.raises(
             InvalidJSONError,
             match=re.escape(f'Invalid JSON response (200): "{expected_response}"'),
         ):
             access_manager.validate_saml_response(
                 {"saml": "response"},
                 expected_saml_post_body["originalSAMLRequestID"],
                 expected_saml_post_body["podIdentifier"],
                 expected_saml_post_body["modellerName"],
-                expected_saml_post_body["modellerProtocolRequest"],
+                _ProtocolDetails(*expected_saml_post_body["modellerRequest"]),
             )
 
-    @unit_test
     @responses.activate
     def test_check_oidc_access_request_success(
         self,
         access_manager: BitfountAM,
         expected_oidc_post_body: _OIDCAccessCheckPostJSON,
         fake_access_token: str,
         modeller_name: str,
-        modeller_protocol_request: SerializedProtocol,
+        modeller_request: Tuple[str, str, None, None],
         pod_identifier: str,
-        session: BitfountSession,
+        session: Session,
         url: str,
     ) -> None:
         """Checks OIDC access check works."""
         accept_response: _AMAccessCheckResponseJSON = {"code": "ACCEPT"}
 
         responses.add(
             responses.POST,
             f"{url}/api/access",
             json=accept_response,
         )
 
         assert (
             access_manager.check_oidc_access_request(
                 pod_identifier=pod_identifier,
-                serialized_protocol=modeller_protocol_request,
+                task_protocol_details=_ProtocolDetails(*modeller_request),
                 modeller_name=modeller_name,
                 modeller_access_token=fake_access_token,
             )
             == _PodResponseType.ACCEPT
         )
 
-        first_call: responses.Call = responses.calls[0]
-        request_body = first_call.request.body
+        request_body = responses.calls[0].request.body
         assert request_body is not None
         assert json.loads(request_body) == expected_oidc_post_body
         # Make sure we used the session to make the request!
         # Some headers are added when we attach the body
         # So we just check that the expected headers are a subset of the attached ones
-        assert session.headers.items() <= first_call.request.headers.items()
-
-    @unit_test
-    @responses.activate
-    def test_check_signature_based_access_request_success(
-        self,
-        access_manager: BitfountAM,
-        expected_signature_based_access_post_body: _SignatureBasedAccessCheckPostJSON,
-        modeller_name: str,
-        modeller_protocol_request: SerializedProtocol,
-        pod_identifier: str,
-        session: BitfountSession,
-        task_signature: bytes,
-        unsigned_task: bytes,
-        url: str,
-    ) -> None:
-        """Checks signatured based access check works."""
-        accept_response: _AMAccessCheckResponseJSON = {"code": "ACCEPT"}
-
-        responses.add(
-            responses.POST,
-            f"{url}/api/access",
-            json=accept_response,
-        )
-
-        assert (
-            access_manager.check_signature_based_access_request(
-                unsigned_task=unsigned_task,
-                task_signature=task_signature,
-                pod_identifier=pod_identifier,
-                serialized_protocol=modeller_protocol_request,
-                modeller_name=modeller_name,
-            )
-            == _PodResponseType.ACCEPT
-        )
-
-        first_call: responses.Call = responses.calls[0]
-        request_body = first_call.request.body
-        assert request_body is not None
-
-        # assert json.loads(request_body) == expected_signature_based_access_post_body
-        # Make sure we used the session to make the request!
-        # Some headers are added when we attach the body
-        # So we just check that the expected headers are a subset of the attached ones
-        assert session.headers.items() <= first_call.request.headers.items()
-
-    @unit_test
-    def test_am_url_gets_prod_url(
-        self, mocker: MockerFixture, monkeypatch: MonkeyPatch
-    ) -> None:
-        """Tests that the am is loaded based on production environment."""
-        mock_session = Mock(
-            spec=BitfountSession,
-            authenticated=False,
-            user_storage_path="",
-            username="",
-        )
-        mocker.patch("bitfount.hub.api.BitfountSession", return_value=mock_session)
-        monkeypatch.setenv("BITFOUNT_ENVIRONMENT", "production")
-        am = BitfountAM()
-        assert am.access_manager_url == PRODUCTION_AM_URL
-
-    @unit_test
-    def test_am_url_gets_staging_url(
-        self, mocker: MockerFixture, monkeypatch: MonkeyPatch
-    ) -> None:
-        """Tests that the am url is loaded based on staging environment."""
-        mock_session = Mock(
-            spec=BitfountSession,
-            authenticated=False,
-            user_storage_path="",
-            username="",
-        )
-        mocker.patch("bitfount.hub.api.BitfountSession", return_value=mock_session)
-        monkeypatch.setenv("BITFOUNT_ENVIRONMENT", "staging")
-        am = BitfountAM()
-        assert am.access_manager_url == _STAGING_AM_URL
-
-    @unit_test
-    def test_am_url_gets_dev_url(
-        self, mocker: MockerFixture, monkeypatch: MonkeyPatch
-    ) -> None:
-        """Tests that the idp_url is loaded based on dev environment."""
-        mock_session = Mock(
-            spec=BitfountSession,
-            authenticated=False,
-            username="",
-        )
-        mocker.patch("bitfount.hub.api.BitfountSession", return_value=mock_session)
-        monkeypatch.setenv("BITFOUNT_ENVIRONMENT", "dev")
-        am = BitfountAM()
-        assert am.access_manager_url == _DEV_AM_URL
+        assert session.headers.items() <= responses.calls[0].request.headers.items()
 
 
 @unit_test
 class TestCheckPodDetails:
     """Tests __check_pod_id_details function."""
 
     def test__check_pod_id_details_pod_identifier(self, pod_identifier: str) -> None:
```

### Comparing `bitfount-0.5.86/tests/bitfount/hub/test_authentication_flow.py` & `bitfount-0.5.9/tests/bitfount/test_config.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,251 +1,215 @@
-"""Tests for the authentication mechanism."""
-from typing import Callable, Dict, List, Tuple
-from unittest.mock import PropertyMock, create_autospec
+"""Tests for config.py."""
+import importlib
+import sys
+from typing import Generator, List, Optional, Tuple
 
+import GPUtil
+from _pytest.monkeypatch import MonkeyPatch
 import pytest
-from pytest import MonkeyPatch, fixture
-from pytest_lazyfixture import lazy_fixture
-import responses
+from pytest import fixture
+from pytest_mock import MockerFixture
 
+import bitfount.config
 from bitfount.config import (
     _DEVELOPMENT_ENVIRONMENT,
     _PRODUCTION_ENVIRONMENT,
     _STAGING_ENVIRONMENT,
+    _get_environment,
 )
-from bitfount.hub.authentication_flow import (
-    _DEVELOPMENT_AUTH_DOMAIN,
-    _DEVELOPMENT_CLIENT_ID,
-    _PRODUCTION_AUTH_DOMAIN,
-    _PRODUCTION_CLIENT_ID,
-    _STAGING_AUTH_DOMAIN,
-    _STAGING_CLIENT_ID,
-    BitfountSession,
-    _AuthEnv,
-    _get_auth_environment,
-)
-from bitfount.hub.authentication_handlers import AuthenticationHandler
 from tests.utils.helper import unit_test
 
 
-@fixture
-def username() -> str:
-    """Username."""
-    return "someUsername"
-
-
-@fixture
-def hub_request_metadata() -> Dict:
-    """Access token."""
-    return {"authorization": "someHubAccessToken"}
-
-
-@fixture
-def am_request_metadata() -> Dict:
-    """Access token."""
-    return {"authorization": "someAMAccessToken"}
-
-
-@fixture
-def message_service_metadata() -> List[Tuple[str, str]]:
-    """Auth for message service."""
-    return [("token", "someAccessToken")]
-
-
-@fixture
-def bitfount_session_factory(
-    am_request_metadata: Dict,
-    hub_request_metadata: Dict,
-    message_service_metadata: List[Tuple[str, str]],
-    username: str,
-) -> Callable[[], BitfountSession]:
-    """Factory to create and setup BitfountSession instance for tests."""
-
-    def _factory() -> BitfountSession:
-        handler = create_autospec(AuthenticationHandler)
-        handler.am_request_headers = am_request_metadata
-        handler.hub_request_headers = hub_request_metadata
-        handler.message_service_request_metadata = message_service_metadata
-        handler.authenticated = PropertyMock(return_value=True)
-        session = BitfountSession(handler)
-        return session
-
-    return _factory
-
-
-@fixture
-def bitfount_session(
-    bitfount_session_factory: Callable[[], BitfountSession]
-) -> BitfountSession:
-    """Bitfount session instance as fixture."""
-    return bitfount_session_factory()
-
-
 @unit_test
-class TestBitfountSession:
-    """Tests for the custom BitfountSession."""
+class TestConfigLoading:
+    """Tests for loading variables during config.py import."""
 
-    def test_get_message_service_auth_metadata(
-        self,
-        bitfount_session: BitfountSession,
-        message_service_metadata: List[Tuple[str, str]],
-    ) -> None:
-        """Test metadata for message service is retrieved from handler."""
-        assert bitfount_session.message_service_metadata == message_service_metadata
+    @fixture
+    def config_unimport(self) -> Generator[None, None, None]:
+        """Ensures the config module can be freshly imported in the test."""
+        # Retrieve and store the old reference to config
+        old_ref = sys.modules.pop("bitfount.config")
+
+        yield
+
+        # Restore sys.modules mapping and local namespace
+        sys.modules["bitfount.config"] = old_ref
+        bitfount.config = old_ref
+
+    def test_config_loading_no_env_var(
+        self, config_unimport: None, monkeypatch: MonkeyPatch
+    ) -> None:
+        """Tests default value assignment if no envvar."""
+        monkeypatch.delenv("BITFOUNT_ENGINE", raising=False)
+        config = importlib.import_module("bitfount.config")
+        assert config.BITFOUNT_ENGINE == "pytorch"
+
+    def test_config_loading_invalid_envvar(
+        self, config_unimport: None, monkeypatch: MonkeyPatch
+    ) -> None:
+        """Tests exception raised if invalid envvar for BITFOUNT_ENGINE."""
+        monkeypatch.setenv("BITFOUNT_ENGINE", "not_a_backend")
+        with pytest.raises(ValueError, match=".*(not_a_backend).*"):
+            importlib.import_module("bitfount.config")
+
+    def test_config_loading_valid_envvar(
+        self, config_unimport: None, monkeypatch: MonkeyPatch
+    ) -> None:
+        """Tests BITFOUNT_ENGINE setting from environment with valid value."""
+        monkeypatch.setenv("BITFOUNT_ENGINE", bitfount.config._PYTORCH_ENGINE)
+        config = importlib.import_module("bitfount.config")
+        assert config.BITFOUNT_ENGINE == bitfount.config._PYTORCH_ENGINE
 
-    def test_get_message_service_auth_metadata_without_authentication(
-        self,
-        bitfount_session: BitfountSession,
-        message_service_metadata: List[Tuple[str, str]],
-    ) -> None:
-        """Test message service metadata is retrieved after authenticating."""
-        bitfount_session.authentication_handler.authenticated = False  # type: ignore[misc] # Reason: handler is a mock # noqa: B950
-
-        assert bitfount_session.message_service_metadata == message_service_metadata
-        bitfount_session.authentication_handler.authenticate.assert_called()  # type: ignore[attr-defined] # Reason: handler is a mock # noqa: B950
-
-    def test_get_username(self, bitfount_session: BitfountSession) -> None:
-        """Test username retrieved from auth handler."""
-        expected_username = "someUser"
-        bitfount_session.authentication_handler.username = expected_username  # type: ignore[misc] # Reason: handler is a mock # noqa: B950
 
-        assert bitfount_session.username == expected_username
+@unit_test
+class TestGetGPUMetadata:
+    """Tests retrieving GPU Metadata."""
 
     @pytest.mark.parametrize(
-        "url, expected_output",
+        "gpu_info,expected_result",
         [
-            ("https://hub.bitfount.com", True),
-            ("https://hub.bitfount.com/api/blah", True),
-            ("https://hub.staging.bitfount.com", True),
-            ("https://am.hub.bitfount.com", False),
-            ("http://hub.bitfount.com", False),  # HTTP is not allowed
+            ([], (None, 0)),  # getGPUs returns empty list
+            (
+                [
+                    GPUtil.GPU(
+                        x,
+                        f"uuid-{x}",
+                        0.5,
+                        5.3,
+                        0.2,
+                        5.0,
+                        "NVIDIA xyz",
+                        "BitfountGPU",
+                        f"123{x}",
+                        "None",
+                        False,
+                        36,
+                    )
+                    for x in range(3)
+                ],
+                ("BitfountGPU", 3),
+            ),
         ],
     )
-    def test_is_hub_url(
-        self, bitfount_session: BitfountSession, expected_output: bool, url: str
-    ) -> None:
-        """Tests that the is_hub_url method returns the expected output."""
-        assert bitfount_session._is_hub_url(url) == expected_output
+    def test_get_gpu_metadata_with_basic_engine(
+        self,
+        expected_result: Tuple[Optional[str], int],
+        gpu_info: List[GPUtil.GPU],
+        mocker: MockerFixture,
+        monkeypatch: MonkeyPatch,
+    ) -> None:
+        """Test that GPUUtil can is used to retrieve GPU info."""
+        # Set envvar value
+        monkeypatch.setattr(
+            "bitfount.config.BITFOUNT_ENGINE", bitfount.config._BASIC_ENGINE
+        )
 
-    @pytest.mark.parametrize(
-        "url, expected_output",
-        [
-            ("https://am.hub.bitfount.com", True),
-            ("https://am.hub.bitfount.com/api/blah", True),
-            ("https://am.hub.staging.bitfount.com", True),
-            ("https://hub.bitfount.com", False),
-            ("http://am.hub.bitfount.com", False),  # HTTP is not allowed
-        ],
-    )
-    def test_is_am_url(
-        self, bitfount_session: BitfountSession, expected_output: bool, url: str
-    ) -> None:
-        """Tests that the is_am_url method returns the expected output."""
-        assert bitfount_session._is_am_url(url) == expected_output
+        getGPUs = mocker.patch("bitfount.config.GPUtil.getGPUs")
+        getGPUs.return_value = gpu_info
+
+        gpu_metadata = bitfount.config.get_gpu_metadata()
 
-    def test_authenticate_calls_authentication_handler(
-        self, bitfount_session: BitfountSession
+        assert gpu_metadata == expected_result
+
+    def test_get_gpu_metadata_with_basic_engine_throws_exception(
+        self, mocker: MockerFixture, monkeypatch: MonkeyPatch
     ) -> None:
-        """Tests that `authenticate()` is called on authentication handler."""
-        bitfount_session.authenticate()
+        """Test that GPUUtil is used to retrieve GPU info."""
+        # Set envvar value
+        monkeypatch.setattr(
+            "bitfount.config.BITFOUNT_ENGINE", bitfount.config._BASIC_ENGINE
+        )
 
-        bitfount_session.authentication_handler.authenticate.assert_called()  # type: ignore[attr-defined] # Reason: handler is a mock # noqa: B950
+        getGPUs = mocker.patch("bitfount.config.GPUtil.getGPUs")
+        getGPUs.side_effect = Exception(
+            "Pretend bad thing happened inside imported library"
+        )
+
+        gpu_metadata = bitfount.config.get_gpu_metadata()
+
+        assert gpu_metadata == (None, 0)
 
-    @responses.activate
-    @pytest.mark.parametrize("authenticated", [True, False])
     @pytest.mark.parametrize(
-        "url,expected_auth_header,would_authenticate_be_called",
-        [
-            (
-                "https://hub.bitfount.com",
-                lazy_fixture("hub_request_metadata"),
-                True,
-            ),  # Hub URL
-            (
-                "https://am.hub.bitfount.com",
-                lazy_fixture("am_request_metadata"),
-                True,
-            ),  # AM URL
-            (
-                "https://some.api.url/goes/here",
-                {"authorization": None},
-                False,
-            ),  # Other URL
-        ],
+        "device_name,device_count,expected_result",
+        [(None, 0, (None, 0)), ("BitfountGPU", 3, ("BitfountGPU", 3))],
     )
-    def test_bitfount_session_request_provides_token_to_hub(
+    def test_get_gpu_metadata_with_pytorch_engine(
         self,
-        am_request_metadata: Dict,
-        authenticated: bool,
-        bitfount_session: BitfountSession,
-        expected_auth_header: Dict,
-        hub_request_metadata: Dict,
-        url: str,
-        would_authenticate_be_called: bool,
-    ) -> None:
-        """Checks that `request()` call provides token."""
-        responses.add(
-            responses.POST,
-            url,
-        )
-        bitfount_session.authentication_handler.authenticated = authenticated  # type: ignore[misc] # Reason: handler is a mock # noqa: B950
-
-        # Send request
-        bitfount_session.request("POST", url)
-
-        # Check request had access token
-        first_call: responses.Call = responses.calls[0]
-        assert (
-            first_call.request.headers.get("authorization", None)
-            == expected_auth_header["authorization"]
+        device_count: int,
+        device_name: Optional[str],
+        expected_result: Tuple[Optional[str], int],
+        mocker: MockerFixture,
+        monkeypatch: MonkeyPatch,
+    ) -> None:
+        """Test that CUDA is used to retrieve GPU info."""
+        # Set envvar value
+        monkeypatch.setattr(
+            "bitfount.config.BITFOUNT_ENGINE", bitfount.config._PYTORCH_ENGINE
         )
-        if would_authenticate_be_called and not authenticated:
-            bitfount_session.authentication_handler.authenticate.assert_called()  # type: ignore[attr-defined] # Reason: handler is a mock # noqa: B950
 
+        cuda = mocker.patch("bitfount.config.torch.cuda")
+        cuda.get_device_name.return_value = device_name
+        cuda.device_count.return_value = device_count
 
-@unit_test
-@pytest.mark.parametrize(
-    argnames=(
-        "environment",
-        "expected_name",
-        "expected_auth_domain",
-        "expected_client_id",
-    ),
-    argvalues=(
-        pytest.param(
-            _STAGING_ENVIRONMENT,
-            "staging",
-            _STAGING_AUTH_DOMAIN,
-            _STAGING_CLIENT_ID,
-            id="staging",
-        ),
-        pytest.param(
-            _DEVELOPMENT_ENVIRONMENT,
-            "development",
-            _DEVELOPMENT_AUTH_DOMAIN,
-            _DEVELOPMENT_CLIENT_ID,
-            id="development",
-        ),
-        pytest.param(
-            _PRODUCTION_ENVIRONMENT,
-            "production",
-            _PRODUCTION_AUTH_DOMAIN,
-            _PRODUCTION_CLIENT_ID,
-            id="production",
-        ),
-    ),
-)
-def test_get_auth_environment(
-    environment: str,
-    expected_auth_domain: str,
-    expected_client_id: str,
-    expected_name: str,
-    monkeypatch: MonkeyPatch,
-) -> None:
-    """Tests _get_auth_environment with various envvar values."""
-    # Patch out environment variable
-    monkeypatch.setenv("BITFOUNT_ENVIRONMENT", environment)
-
-    # Check return
-    assert _get_auth_environment() == _AuthEnv(
-        expected_name, expected_auth_domain, expected_client_id
-    )
+        gpu_metadata = bitfount.config.get_gpu_metadata()
+
+        assert gpu_metadata == expected_result
+
+    def test_get_gpu_metadata_with_pytorch_engine_throws_exception(
+        self, mocker: MockerFixture, monkeypatch: MonkeyPatch
+    ) -> None:
+        """Test that GPUUtil is used to retrieve GPU info."""
+        # Set envvar value
+        monkeypatch.setattr(
+            "bitfount.config.BITFOUNT_ENGINE", bitfount.config._PYTORCH_ENGINE
+        )
+
+        cuda = mocker.patch("bitfount.config.torch.cuda")
+        cuda.get_device_name.side_effect = Exception(
+            "Pretend bad thing happened inside imported library"
+        )
+        cuda.device_count.return_value = 0
+
+        gpu_metadata = bitfount.config.get_gpu_metadata()
+
+        assert gpu_metadata == (None, 0)
+
+
+class TestBitfountEnvironment:
+    """Tests that the BITFOUNT_ENVIRONMENT environment variable is read correctly."""
+
+    @unit_test
+    def test_get_environment_dev(self, monkeypatch: MonkeyPatch) -> None:
+        """Tests dev environment is read correctly."""
+        monkeypatch.setenv("BITFOUNT_ENVIRONMENT", "dev")
+        env = _get_environment()
+        assert env == _DEVELOPMENT_ENVIRONMENT
+
+    @unit_test
+    def test_get_environment_staging(self, monkeypatch: MonkeyPatch) -> None:
+        """Tests staging environment is read correctly."""
+        monkeypatch.setenv("BITFOUNT_ENVIRONMENT", "staging")
+        env = _get_environment()
+        assert env == _STAGING_ENVIRONMENT
+
+    @unit_test
+    def test_get_environment_production(self, monkeypatch: MonkeyPatch) -> None:
+        """Tests production environment is read correctly."""
+        monkeypatch.setenv("BITFOUNT_ENVIRONMENT", "production")
+        env = _get_environment()
+        assert env == _PRODUCTION_ENVIRONMENT
+
+    @unit_test
+    def test_get_environment_variable_not_set(self, monkeypatch: MonkeyPatch) -> None:
+        """Tests that environment defaults to production variable has not been set."""
+        monkeypatch.delenv("BITFOUNT_ENVIRONMENT", raising=False)
+        env = _get_environment()
+        assert env == _PRODUCTION_ENVIRONMENT
+
+    @unit_test
+    def test_get_environment_unknown_variable_raises_environment_error(
+        self, monkeypatch: MonkeyPatch
+    ) -> None:
+        """Tests that error is raised if environment is not recognised."""
+        monkeypatch.setenv("BITFOUNT_ENVIRONMENT", "not-a-real-env")
+        with pytest.raises(ValueError):
+            _get_environment()
```

### Comparing `bitfount-0.5.86/tests/bitfount/hub/test_helper.py` & `bitfount-0.5.9/tests/bitfount/hub/test_helper.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,64 +1,52 @@
 """Tests hub helper.py."""
-from datetime import datetime
 import logging
 from pathlib import Path
 from typing import Dict, Optional
 from unittest.mock import MagicMock, Mock, create_autospec
 
+from _pytest.logging import LogCaptureFixture
+from _pytest.monkeypatch import MonkeyPatch
 from cryptography.hazmat.primitives.asymmetric.rsa import RSAPublicKey
 import pytest
-from pytest import LogCaptureFixture, MonkeyPatch, fixture
+from pytest import fixture
 from pytest_lazyfixture import lazy_fixture
 from pytest_mock import MockerFixture
 import yaml
 
 import bitfount
-from bitfount import DeviceCodeFlowHandler
 from bitfount.data.schema import BitfountSchema
 from bitfount.federated.encryption import _RSAEncryption
-from bitfount.hub.api import BitfountAM, BitfountHub
+from bitfount.hub.api import (
+    _DEV_AM_URL,
+    _DEV_HUB_URL,
+    _STAGING_AM_URL,
+    _STAGING_HUB_URL,
+    PRODUCTION_AM_URL,
+    PRODUCTION_HUB_URL,
+    BitfountAM,
+    BitfountHub,
+)
 from bitfount.hub.authentication_flow import (
     _DEVELOPMENT_CLIENT_ID,
     _PRODUCTION_CLIENT_ID,
     _STAGING_CLIENT_ID,
     BitfountSession,
 )
-from bitfount.hub.exceptions import PodDoesNotExistError
 from bitfount.hub.helper import (
     _check_known_pods,
     _create_access_manager,
     _create_bitfount_session,
     _create_bitfounthub,
     _default_bitfounthub,
     _get_pod_public_key,
-    _save_key_to_key_store,
     get_pod_schema,
 )
-from bitfount.hub.types import (
-    _DEV_AM_URL,
-    _DEV_HUB_URL,
-    _STAGING_AM_URL,
-    _STAGING_HUB_URL,
-    PRODUCTION_AM_URL,
-    PRODUCTION_HUB_URL,
-)
-from bitfount.runners.config_schemas import JWT
 from tests.bitfount import TEST_SECURITY_FILES
-from tests.utils.helper import get_info_logs, unit_test
-
-
-def _gen_new_public_key() -> RSAPublicKey:
-    """Creates a new RSA public key."""
-    return _RSAEncryption.generate_key_pair()[1]
-
-
-def _key_to_str(key: RSAPublicKey) -> str:
-    """Converts RSA public key to string."""
-    return _RSAEncryption.serialize_public_key(key).decode()
+from tests.utils.helper import unit_test
 
 
 @unit_test
 class TestHelperFunctions:
     """Test hub helper functions."""
 
     @fixture
@@ -100,57 +88,16 @@
     )
     def test_create_bitfount_session_with_different_urls(
         self, expected_client_id: str, input_url: str, username: str
     ) -> None:
         """Tests private _create_bitfount_session function with different urls."""
         session = _create_bitfount_session(url=input_url, username=username)
         assert isinstance(session, BitfountSession)
-        assert isinstance(session.authentication_handler, DeviceCodeFlowHandler)
-        assert session.authentication_handler._client_id == expected_client_id
-        assert session.authentication_handler.user_storage_path.stem == username
-
-    def test_create_bitfount_session_loads_api_keys_from_environment(
-        self, mocker: MockerFixture, monkeypatch: MonkeyPatch
-    ) -> None:
-        """Tests that the API keys are read from environment variables."""
-        monkeypatch.setenv("BITFOUNT_API_KEY_ID", "someApiKeyId")
-        monkeypatch.setenv("BITFOUNT_API_KEY", "someApiKey")
-        handler = mocker.patch("bitfount.hub.helper.APIKeysHandler", autospec=True)
-        # Ensure session creation is done post-envvar setting
-        _create_bitfount_session(url=PRODUCTION_HUB_URL, username="someUser")
-
-        handler.assert_called_once_with(
-            api_key_id="someApiKeyId", api_key="someApiKey", username="someUser"
-        )
-
-    def test_create_bitfount_session_uses_external_jwt(
-        self, mocker: MockerFixture
-    ) -> None:
-        """Tests that the external JWT is used."""
-        expected_jwt = "someJWT"
-        expected_expiry = datetime.now()
-        expected_get_token = Mock()
-
-        handler = mocker.patch(
-            "bitfount.hub.helper.ExternallyManagedJWTHandler", autospec=True
-        )
-        secrets = JWT(
-            jwt=expected_jwt, expires=expected_expiry, get_token=expected_get_token
-        )
-
-        _create_bitfount_session(
-            url=PRODUCTION_HUB_URL, username="someUser", secrets=secrets
-        )
-
-        handler.assert_called_once_with(
-            jwt=expected_jwt,
-            expires=expected_expiry,
-            get_token=expected_get_token,
-            username="someUser",
-        )
+        assert session.client_id == expected_client_id
+        assert session.user_storage_path.stem == username
 
     @pytest.mark.parametrize(
         "environment, input_url, expected_url",
         [
             ("production", None, PRODUCTION_HUB_URL),
             ("staging", None, _STAGING_HUB_URL),
             ("dev", None, _DEV_HUB_URL),
@@ -197,113 +144,112 @@
 
         am = _create_access_manager(session, input_url)
         assert isinstance(am, BitfountAM)
         assert am.access_manager_url == expected_url
         assert am.session == session
 
     def test__get_pod_public_key(
-        self,
-        caplog: LogCaptureFixture,
-        mock_key_store: Path,
-        mocker: MockerFixture,
-        public_key_path: Path,
+        self, caplog: LogCaptureFixture, mocker: MockerFixture, public_key_path: Path
     ) -> None:
         """Tests _get_pod_public_key helper function."""
         # Mock out the hub and RSA key loading components
         mock_load_public_key = mocker.patch.object(_RSAEncryption, "load_public_key")
         mock_public_key: str = "this-is-a-pod-public-key"
         hub = create_autospec(BitfountHub, instance=True)
         hub.get_pod_key.return_value = mock_public_key
 
         # Mock out the _check_known_pods() function
         mock_check_known_pods = mocker.patch(
-            "bitfount.hub.helper._check_known_pods",
-            autospec=True,
-            side_effect=lambda pod_id, key, store_path: key,
+            "bitfount.hub.helper._check_known_pods", autospec=True
         )
 
-        mock_mkdir = mocker.patch("bitfount.hub.helper.Path.mkdir")
-
         # BITFOUNTHUB KEY
         # Correct return value
         public_key = _get_pod_public_key("blah/worker_1", hub)
         mock_load_public_key.assert_called_once_with(mock_public_key.encode())
-        mock_check_known_pods.assert_called_once()
-        mock_mkdir.assert_called_once()
+        assert public_key == mock_load_public_key()
+
+        # Connection error (BitfountHub.get_pod_key() returns empty string)
+        mock_load_public_key.reset_mock()
+        hub.get_pod_key.return_value = ""
+        public_key = _get_pod_public_key("blah/worker_1", hub)
+        mock_load_public_key.assert_called_once_with(b"")
         assert public_key == mock_load_public_key()
 
         # KEY FROM FILE
         # Key already exists in file, returns it
         mock_load_public_key.reset_mock()
-        mock_check_known_pods.reset_mock()
         # Mock out the scenario of _check_known_pods() returning the existing key
-        mock_check_known_pods.side_effect = lambda _pod_id, key, _mock_key_store: key
+        mock_check_known_pods.side_effect = lambda _pod_id, key: key
         public_key = _get_pod_public_key(
             "blah/worker_2", hub, {"blah/worker_2": public_key_path}
         )
         mock_load_public_key.assert_called_once_with(public_key_path)
         mock_check_known_pods.assert_called_once_with(
-            "blah/worker_2", mock_load_public_key(), mock_key_store
+            "blah/worker_2", mock_load_public_key()
         )
         assert public_key == mock_load_public_key()
-        # mkdir shouldn't be called on second call to _get_pod_public_key
-        mock_mkdir.assert_called_once()
 
         # Key files exist, but not for specific user, retrieves from hub
         with caplog.at_level(logging.DEBUG):
             hub.get_pod_key.return_value = mock_public_key
             mock_load_public_key.reset_mock()
             public_key = _get_pod_public_key(
                 "blah/worker_3", hub, {"blah/worker_2": public_key_path}
             )
             mock_load_public_key.assert_called_once_with(mock_public_key.encode())
             assert public_key == mock_load_public_key()
             assert "No existing public key file for blah/worker_3" in caplog.text
-            # mkdir shouldn't be called on third call to _get_pod_public_key
-            mock_mkdir.assert_called_once()
 
     def test__check_known_pods(
         self, mock_input: MagicMock, mock_key_store: Path
     ) -> None:
         """Tests _check_known_pods helper function.
 
         Checks that various states (no key exists, key already exists, wrong
         input detected) all get the correct key and that the key store is updated
         each time.
         """
+
+        def _gen_new_public_key() -> RSAPublicKey:
+            """Creates a new RSA public key."""
+            return _RSAEncryption.generate_key_pair()[1]
+
+        def _key_to_str(key: RSAPublicKey) -> str:
+            """Converts RSA public key to string."""
+            return _RSAEncryption.serialize_public_key(key).decode()
+
         orig_key: RSAPublicKey = _gen_new_public_key()
         known_workers: Dict[str, str]
         worker_name = "test-worker"
-        mock_key_store.parent.mkdir(parents=True, exist_ok=True)
-        mock_key_store.touch()
 
         # No key in yaml, should not require `input()` call
-        new_key: RSAPublicKey = _check_known_pods(worker_name, orig_key, mock_key_store)
+        new_key: RSAPublicKey = _check_known_pods(worker_name, orig_key)
         mock_input.assert_not_called()
         assert new_key == orig_key
         with open(mock_key_store, "r") as known_workers_file:
             known_workers = yaml.safe_load(known_workers_file)
         assert known_workers[worker_name] == _key_to_str(orig_key)
 
         # Accept new key, input should be "Y"
         mock_input.reset_mock(return_value=True, side_effect=True)
         mock_input.return_value = "Y"
         diff_key: RSAPublicKey = _gen_new_public_key()
-        new_key = _check_known_pods(worker_name, diff_key, mock_key_store)
+        new_key = _check_known_pods(worker_name, diff_key)
         mock_input.assert_called_once()
         assert new_key == diff_key
         with open(mock_key_store, "r") as known_workers_file:
             known_workers = yaml.safe_load(known_workers_file)
         assert known_workers[worker_name] == _key_to_str(diff_key)
 
         # Wrong input (not "Y" or "N") then reject ("N") new key
         mock_input.reset_mock(return_value=True, side_effect=True)
         mock_input.side_effect = ["INCORRECT_INPUT", "N"]
         key_to_reject: RSAPublicKey = _gen_new_public_key()
-        new_key = _check_known_pods(worker_name, key_to_reject, mock_key_store)
+        new_key = _check_known_pods(worker_name, key_to_reject)
         assert mock_input.call_count == 2
         assert new_key != key_to_reject
         # have to do str compare here as direct key compare relies on python id(),
         # but we've actually reloaded the key from file.
         assert _key_to_str(new_key) == _key_to_str(diff_key)  # i.e. hasn't changed
         with open(mock_key_store, "r") as known_workers_file:
             known_workers = yaml.safe_load(known_workers_file)
@@ -311,47 +257,21 @@
         assert (
             known_workers[worker_name] == _key_to_str(new_key) == _key_to_str(diff_key)
         )  # i.e. hasn't changed
 
         # Return to original key, input should be "Y"
         mock_input.reset_mock(return_value=True, side_effect=True)
         mock_input.return_value = "Y"
-        new_key = _check_known_pods(worker_name, orig_key, mock_key_store)
+        new_key = _check_known_pods(worker_name, orig_key)
         mock_input.assert_called_once()
         assert new_key == orig_key
         with open(mock_key_store, "r") as known_workers_file:
             known_workers = yaml.safe_load(known_workers_file)
         assert known_workers[worker_name] == _key_to_str(orig_key)
 
-    def test__check_known_pods_returns_if_key_match(
-        self, caplog: LogCaptureFixture, mock_key_store: Path, mocker: MockerFixture
-    ) -> None:
-        """Tests _check_known_pods doesn't re-save an already existing key."""
-        public_key = _gen_new_public_key()
-        pod_id = "this-is-a-pod-id"
-
-        # Pre-save the key to the key store
-        mock_key_store.parent.mkdir(parents=True, exist_ok=True)
-        mock_key_store.touch()
-        _save_key_to_key_store(mock_key_store, pod_id, _key_to_str(public_key))
-
-        # Mock out _save_key_to_key_store
-        mock_save_keys_to_key_store = mocker.patch(
-            "bitfount.hub.helper._save_key_to_key_store"
-        )
-
-        with caplog.at_level(logging.INFO):
-            returned_public_key = _check_known_pods(pod_id, public_key, mock_key_store)
-
-        # Check same key and saving not called unnecessarily
-        # We use an `is` check here because it should return the _exact_ same key
-        assert returned_public_key is public_key
-        mock_save_keys_to_key_store.assert_not_called()
-        assert f"Found public key for {pod_id} in key store." in get_info_logs(caplog)
-
     def test_default_bitfounthub(self, mocker: MockerFixture) -> None:
         """Tests default bitfounthub calls create only if not None."""
         mock = Mock()
         mocker.patch("bitfount.hub.helper._create_bitfounthub", mock)
         _default_bitfounthub(hub=Mock())
         mock.assert_not_called()
         _default_bitfounthub()
@@ -415,69 +335,7 @@
         mock_create_hub.assert_called_once()
 
         # Check schema download called with constructed pod_identifier
         mock_hub.get_pod_schema.assert_called_once_with(f"{username}/{pod_name}")
 
         # Check return
         assert schema == mock_schema
-
-    def test_get_pod_schema_with_hub_and_username(
-        self, caplog: LogCaptureFixture
-    ) -> None:
-        """Test get_pod_schema with hub and username provided."""
-        username = "username"
-        hub_username = "hub_username"
-
-        # Mock out hub creation, set username
-        mock_hub = create_autospec(BitfountHub, instance=True)
-        mock_hub.username = hub_username
-
-        # Mock out schema download
-        mock_schema = create_autospec(BitfountSchema, instance=True)
-        mock_hub.get_pod_schema.return_value = mock_schema
-
-        # Call get_pod_schema
-        get_pod_schema("pod_name", hub=mock_hub, username=username)
-
-        # Check that provided username is ignored
-        assert "Ignoring username argument as hub was provided." in [
-            i.message for i in caplog.records
-        ]
-        assert mock_hub.username != username
-
-    def test__save_keys_to_key_store(
-        self, mock_key_store: Path, mocker: MockerFixture
-    ) -> None:
-        """Tests saving keys to key store."""
-        pod_identifier = "fake/pod"
-        orig_key: RSAPublicKey = _RSAEncryption.generate_key_pair()[1]
-        serialized_key = _RSAEncryption.serialize_public_key(orig_key).decode()
-        mock_open = mocker.mock_open(read_data="")
-        mocker.patch("builtins.open", mock_open)
-
-        _save_key_to_key_store(mock_key_store, pod_identifier, serialized_key)
-
-        assert mock_open.call_count == 2
-        mock_open.assert_any_call(mock_key_store, "r")
-        mock_open.assert_any_call(mock_key_store, "w")
-
-    def test__error_thrown_no_pod_key(
-        self,
-        mocker: MockerFixture,
-    ) -> None:
-        """Tests _get_pod_public_key helper function."""
-        # Mock out the hub and RSA key loading components
-        hub = create_autospec(BitfountHub, instance=True)
-        hub.get_pod_key.return_value = None
-        pod_id = "pod_id"
-        err_msg = f"No public key found for pod: {pod_id}"
-
-        # Mock out the _save_keys_to_key_store() function
-        mock_save_keys_to_key_store = mocker.patch(
-            "bitfount.hub.helper._save_key_to_key_store", autospec=True
-        )
-
-        with pytest.raises(PodDoesNotExistError) as error:
-            _get_pod_public_key(pod_id, hub)
-            assert str(error.value) == err_msg
-
-        assert mock_save_keys_to_key_store.call_count == 0
```

### Comparing `bitfount-0.5.86/tests/bitfount/models/test_base_models.py` & `bitfount-0.5.9/tests/bitfount/models/test_base_models.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,52 +1,52 @@
 """Tests for base model classes."""
 from __future__ import annotations
 
 from typing import Any, cast
 from unittest.mock import Mock
 
-from marshmallow import fields
+from marshmallow import fields, post_load
 from pytest import fixture
 
 from bitfount.data.datastructure import DataStructure
 from bitfount.models.base_models import (
-    MAIN_MODEL_REGISTRY,
     EarlyStopping,
     FeedForwardModelStructure,
     NeuralNetworkMixIn,
     NeuralNetworkModelStructure,
     Optimizer,
     Scheduler,
 )
-from bitfount.schemas.utils import bf_dump, bf_load
-from tests.utils.helper import create_datastructure, unit_test
+from tests.utils.helper import unit_test
 
 
 class _NeuralNetworkMixIn_(NeuralNetworkMixIn):
     def __init__(self, datastructure: DataStructure, **kwargs: Any):
         self.datastructure = datastructure
-        self.class_name = type(self).__name__
-
         super().__init__(**kwargs)
-        fields_dict = {  # noqa: F841
-            "datastructure": fields.Raw(),
-        }
 
     @staticmethod
     def _get_optimizer(  # type:ignore[override] # reason, used just for testing
         optimizer: Optimizer,
     ) -> None:
         return None
 
     @staticmethod
     def _get_scheduler(  # type:ignore[override] # reason, used just for testing
         scheduler: Scheduler,
     ) -> None:
         return None
 
+    class _Schema(NeuralNetworkMixIn._Schema):
+        datastructure = fields.Raw()
+
+        @post_load
+        def recreate(self, data: Any, **_kwargs: Any) -> _NeuralNetworkMixIn_:
+            return _NeuralNetworkMixIn_(**data)
+
 
 @unit_test
 class TestNeuralNetworkMixIn:
     """Tests for the NeuralNetworkMixIn."""
 
     @fixture
     def model_structure(self) -> NeuralNetworkModelStructure:
@@ -54,14 +54,36 @@
         return NeuralNetworkModelStructure()
 
     @fixture
     def mock_datastructure(self) -> Mock:
         """Mock datastructure fixture."""
         return Mock(multihead_col=None)
 
+    def test_training_needed(
+        self, mock_datastructure: Mock, model_structure: NeuralNetworkModelStructure
+    ) -> None:
+        """Tests the training_needed attribute when epochs > 0."""
+        nn = _NeuralNetworkMixIn_(
+            datastructure=mock_datastructure,
+            model_structure=model_structure,
+            epochs=1,
+        )
+        assert nn.training_needed is True
+
+    def test_training_not_needed(
+        self, mock_datastructure: Mock, model_structure: NeuralNetworkModelStructure
+    ) -> None:
+        """Tests the training_needed attribute when not needed."""
+        nn = _NeuralNetworkMixIn_(
+            datastructure=mock_datastructure,
+            model_structure=model_structure,
+            epochs=0,
+        )
+        assert nn.training_needed is False
+
     def test_scheduler_structure_works_with_params(
         self, mock_datastructure: Mock, model_structure: NeuralNetworkModelStructure
     ) -> None:
         """Tests the scheduler loads parameters correctly."""
         nn = _NeuralNetworkMixIn_(
             datastructure=mock_datastructure,
             model_structure=model_structure,
@@ -151,29 +173,26 @@
         model_optimizer = nn.optimizer
         assert model_optimizer.name == "SGD"
         assert model_optimizer.params == {}
 
     def test_serialization_deserialization(self, mock_datastructure: Mock) -> None:
         """Tests (de)serialization of the NeuralNetworkMixin class."""
         nn = _NeuralNetworkMixIn_(
-            datastructure=create_datastructure(),
+            datastructure=mock_datastructure,
             model_structure=FeedForwardModelStructure(),
             batch_size=1,
             epochs=1,
             steps=None,  # as only one of epochs/steps can be specified
             optimizer=Optimizer(name="blah", params={"blah": 1}),
             scheduler=Scheduler("blah", {"blah": 1}),
             custom_loss_func=None,
             early_stopping=EarlyStopping({"blah": 1}),
         )
 
         # Dump and reload the class variables
-        dumped = bf_dump(nn)
-        loaded = bf_load(dumped, MAIN_MODEL_REGISTRY)
+        schema = _NeuralNetworkMixIn_._Schema()
+        dumped = schema.dump(nn)
+        loaded = schema.load(dumped)
 
         # We compare across __dict__ as NeuralNetworkMixIn does not have
         # an __eq__ method.
-        nn_dict = nn.__dict__
-        # The datastructure `ignore_cols` is not serialised as it can be reconstructed
-        # from `selected_cols`
-        nn_dict["datastructure"].ignore_cols = []
-        assert loaded.__dict__ == nn_dict
+        assert loaded.__dict__ == nn.__dict__
```

### Comparing `bitfount-0.5.86/tests/bitfount/transformations/conftest.py` & `bitfount-0.5.9/tests/bitfount/transformations/conftest.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 """Fixtures and other pytest specific components for the transformation tests."""
-from pytest import MonkeyPatch, fixture
+from _pytest.monkeypatch import MonkeyPatch
+from pytest import fixture
 
 
 @fixture
 def fake_uuid(monkeypatch: MonkeyPatch) -> None:
     """Patch the uuid4().hex calls in base_transformation."""
     import bitfount.transformations.base_transformation as base_transformation
```

### Comparing `bitfount-0.5.86/tests/bitfount/transformations/test_base_transformation.py` & `bitfount-0.5.9/tests/bitfount/transformations/test_base_transformation.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 """Tests for the base transformation classes."""
-from marshmallow import Schema as MarshmallowSchema, ValidationError
+from marshmallow import Schema as MarshmallowSchema
+from marshmallow import ValidationError
 import pytest
 
 from bitfount.transformations.base_transformation import (
     Transformation,
     _TransformationSchema,
 )
 from bitfount.transformations.exceptions import TransformationRegistryError
```

### Comparing `bitfount-0.5.86/tests/bitfount/transformations/test_binary_operations.py` & `bitfount-0.5.9/tests/bitfount/transformations/test_binary_operations.py`

 * *Files identical despite different names*

### Comparing `bitfount-0.5.86/tests/bitfount/transformations/test_dataset_operations.py` & `bitfount-0.5.9/tests/bitfount/transformations/test_dataset_operations.py`

 * *Files identical despite different names*

### Comparing `bitfount-0.5.86/tests/bitfount/transformations/test_parser.py` & `bitfount-0.5.9/tests/bitfount/transformations/test_parser.py`

 * *Files 2% similar despite different names*

```diff
@@ -2,16 +2,15 @@
 from pathlib import Path
 from typing import Dict, Tuple
 
 import pytest
 from pytest import fixture
 import yaml
 
-from bitfount.data.types import DataSplit
-from bitfount.transformations.batch_operations import AlbumentationsImageTransformation
+from bitfount.transformations.batch_operations import ImageTransformation
 from bitfount.transformations.binary_operations import (
     AdditionTransformation,
     DivisionTransformation,
 )
 from bitfount.transformations.exceptions import TransformationParsingError
 from bitfount.transformations.parser import TransformationsParser
 from bitfount.transformations.unary_operations import (
@@ -432,51 +431,76 @@
 
     def test_parsing_image_transformation_successfully(
         self, transformation_parser: TransformationsParser
     ) -> None:
         """Tests parsing of image transformation succesfully."""
         transformations_yaml = """
         Transformations:
-            - albumentations:
+            - Image:
                 arg: col:col_1
                 name: test_col
                 step: train
                 output: True
                 transformations:
                     - ResizeTransformation:
                         arg1: 1
                         arg2: 2
                     - NormalizeTransformation
         """
         tfms, _ = transformation_parser.parse(transformations_yaml)
-        assert tfms[0] == AlbumentationsImageTransformation(
+        assert tfms[0] == ImageTransformation(
             name="test_col",
             output=True,
             arg="col:col_1",
-            step=DataSplit.TRAIN,
+            step="train",
             transformations=[
                 {"ResizeTransformation": {"arg1": 1, "arg2": 2}},
                 "NormalizeTransformation",
             ],
         )
 
     def test_image_transformation_output_must_be_true(
         self, transformation_parser: TransformationsParser
     ) -> None:
         """Tests that output must be True for Image Transformation."""
         transformations_yaml = """
         Transformations:
-            - albumentations:
+            - Image:
                 arg: col:col_1
                 name: test_col
                 step: train
                 output: False
                 transformations:
                     - ResizeTransformation:
                         arg1: 1
                         arg2: 2
                     - NormalizeTransformation
         """
         with pytest.raises(
             ValueError, match="`output` cannot be False for a BatchTimeOperation"
         ):
             transformation_parser.parse(transformations_yaml)
+
+    def test_image_transformation_step_must_be_train_or_validation(
+        self, transformation_parser: TransformationsParser
+    ) -> None:
+        """Tests the `step` argument for Image Transformation.
+
+        Must be one of 'train' or 'validation'.
+        """
+        transformations_yaml = """
+        Transformations:
+            - Image:
+                arg: col:col_1
+                name: test_col
+                step: blah
+                output: True
+                transformations:
+                    - ResizeTransformation:
+                        arg1: 1
+                        arg2: 2
+                    - NormalizeTransformation
+        """
+        with pytest.raises(
+            ValueError, match="step must be one of 'train' or 'validation'. Got 'blah'"
+        ):
+            transformation_parser.parse(transformations_yaml)
```

### Comparing `bitfount-0.5.86/tests/bitfount/transformations/test_references.py` & `bitfount-0.5.9/tests/bitfount/transformations/test_references.py`

 * *Files identical despite different names*

### Comparing `bitfount-0.5.86/tests/bitfount/transformations/test_unary_operations.py` & `bitfount-0.5.9/tests/bitfount/transformations/test_unary_operations.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 """Tests for unary transformations."""
 import copy
 import logging
-from typing import Type
+from typing import Optional, Type
 
+from _pytest.logging import LogCaptureFixture
 from marshmallow import ValidationError
 import pytest
-from pytest import LogCaptureFixture
 
 from bitfount.transformations.base_transformation import (
     MultiColumnOutputTransformation,
     Transformation,
 )
 from bitfount.transformations.unary_operations import (
     InclusionTransformation,
@@ -112,14 +112,16 @@
                 name="ohe_test", arg="test", raw_values=["blah"]
             ),
             MultiColumnOutputTransformation,
         )
 
     def test_init(self) -> None:
         """Tests that OHE is correctly initialized with various arg-types."""
+        t_name: Optional[str]
+
         # With name
         t_name = "ohe_test"
         t_arg = "random_name"
         # With list `raw_values`
         ohe = OneHotEncodingTransformation(name=t_name, arg=t_arg, raw_values=[1, 2, 3])
         assert ohe.values == {i: f"{t_name}_{i}" for i in [1, 2, 3]}
 
@@ -135,32 +137,34 @@
         ohe = OneHotEncodingTransformation(
             name=t_name, arg=t_arg, raw_values={1: "hello", 2: None}
         )
         assert ohe.values == {
             k: f"{t_name}_{v}" for k, v in {1: "hello", 2: "2"}.items()
         }
 
-        # With ref arg but NO provided name
-        del t_name
+        # With ref arg
+        t_name = None
         t_ref = "ref_test"
         t_arg = f"c:{t_ref}"
         # With list `raw_values`
-        ohe = OneHotEncodingTransformation(arg=t_arg, raw_values=[1, 2, 3])
+        ohe = OneHotEncodingTransformation(name=t_name, arg=t_arg, raw_values=[1, 2, 3])
         assert ohe.values == {i: f"{t_ref}_{i}" for i in [1, 2, 3]}
 
         # With dict `raw_values`
         ohe = OneHotEncodingTransformation(
-            arg=t_arg, raw_values={1: "hello", 2: "world"}
+            name=t_name, arg=t_arg, raw_values={1: "hello", 2: "world"}
         )
         assert ohe.values == {
             k: f"{t_ref}_{v}" for k, v in {1: "hello", 2: "world"}.items()
         }
 
         # With dict with Nones `raw_values`
-        ohe = OneHotEncodingTransformation(arg=t_arg, raw_values={1: "hello", 2: None})
+        ohe = OneHotEncodingTransformation(
+            name=t_name, arg=t_arg, raw_values={1: "hello", 2: None}
+        )
         assert ohe.values == {
             k: f"{t_ref}_{v}" for k, v in {1: "hello", 2: "2"}.items()
         }
 
     def test_init_fails_duplicate_values(self) -> None:
         """Tests that initialization fails if duplicate values are provided."""
         # Test list `raw_values`
```

### Comparing `bitfount-0.5.86/tests/bitfount/transformations/transformation_test_helpers.py` & `bitfount-0.5.9/tests/bitfount/transformations/transformation_test_helpers.py`

 * *Files identical despite different names*

### Comparing `bitfount-0.5.86/tests/integration/bitfount_web_interactions.py` & `bitfount-0.5.9/tests/integration/bitfount_web_interactions.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,45 +1,30 @@
-"""Provides methods for interacting with the Bitfount Web services."""
+"""Provides methods for interacting with the Bitfount Web frontend via Selenium."""
 from contextlib import contextmanager
-from datetime import datetime, timedelta, timezone
 import logging
 from pathlib import Path
-import pprint
 import threading
 import time
-from typing import Generator, Tuple
+from typing import Generator
 
 import chromedriver_autoinstaller
 from requests import HTTPError
 from selenium import webdriver
 from selenium.webdriver.common.by import By
 from selenium.webdriver.remote.webdriver import WebDriver
 from selenium.webdriver.support import expected_conditions as EC
 from selenium.webdriver.support.ui import WebDriverWait
 
-from bitfount.config import (
-    _DEVELOPMENT_ENVIRONMENT,
-    _SANDBOX_ENVIRONMENT,
-    _STAGING_ENVIRONMENT,
-    _get_environment,
-)
-from bitfount.hub.api import BitfountAM, BitfountSession
-from bitfount.hub.authentication_flow import _get_auth_environment
-from bitfount.hub.authentication_handlers import (
+from bitfount.hub.api import _STAGING_AM_URL, BitfountAM, BitfountSession
+from bitfount.hub.authentication_flow import (
     _HUB_API_IDENTIFIER,
     _SCOPES,
-    ExternallyManagedJWTHandler,
-)
-from bitfount.hub.types import (
-    _DEV_AM_URL,
-    _SANDBOX_AM_URL,
-    _STAGING_AM_URL,
-    PRODUCTION_AM_URL,
+    _STAGING_AUTH_DOMAIN,
+    _STAGING_CLIENT_ID,
 )
-from bitfount.utils import web_utils
 
 SCREENSHOT_DIRECTORY: Path = Path("selenium-screenshots")
 IMPLICIT_WAIT_TIME = 8  # seconds; high value due to slow GitHub runner
 
 logger = logging.getLogger(__name__)
 
 
@@ -59,51 +44,98 @@
 
 def save_screenshot(file_name: str, driver: WebDriver) -> None:
     """Saves a screenshot of the Selenium driver view."""
     SCREENSHOT_DIRECTORY.mkdir(parents=True, exist_ok=True)
     file_path = SCREENSHOT_DIRECTORY / f"{file_name}_{str(int(time.time()))}.png"
     file_path = file_path.resolve()
     driver.get_screenshot_as_file(str(file_path))
-    logger.error(f"Screenshot saved to {file_path}")
 
 
 def _oauth_sign_in(username: str, password: str, driver: WebDriver) -> None:
     """Sign in to the oauth form using the supplied username and password."""
-    logger.info(f"OAuth sign in for {username} at {driver.current_url}")
     try:
         # Supply login details to Auth0 login panel
-        driver.find_element(
-            by=By.CSS_SELECTOR, value="input[name='username']"
-        ).send_keys(username)
-        driver.find_element(
-            by=By.CSS_SELECTOR, value="input[name='password']"
-        ).send_keys(password)
-
-        # As there may be multiple buttons (okta and auth0 both add one) we need
-        # to find the one that's "clickable" (i.e. is actually visible)
-        for button in driver.find_elements(
-            by=By.CSS_SELECTOR, value="button[name='action']"
-        ):
-            if button.is_displayed():
-                button.click()
-                break
-        else:
-            raise ValueError("Unable to find button to submit OAuth sign-in")
+        driver.find_element_by_css_selector("input[name='username']").send_keys(
+            username
+        )
+        driver.find_element_by_css_selector("input[name='password']").send_keys(
+            password
+        )
+        driver.find_element_by_css_selector("button[aria-label='Log In']").click()
     except Exception as e:
         logger.error("Exception encountered whilst signing in.")
-        logger.exception(e)
         save_screenshot("perform_login", driver)
-        raise
+        raise e
+
+
+class WebdriverBitfountSession(BitfountSession):
+    """A BitfountSession implementation using Selenium Webdriver.
+
+    An implementation of BitfountSession which uses a Selenium Webdriver to approve
+    the session access.
+    """
+
+    def __init__(
+        self,
+        username: str,
+        password: str,
+        # Below are the inherited args
+        user_storage_path: Path,
+        auth_domain: str = _STAGING_AUTH_DOMAIN,
+        client_id: str = _STAGING_CLIENT_ID,
+        scopes: str = _SCOPES,
+        audience: str = _HUB_API_IDENTIFIER,
+    ):
+        super(WebdriverBitfountSession, self).__init__(
+            user_storage_path=user_storage_path,
+            auth_domain=auth_domain,
+            client_id=client_id,
+            scopes=scopes,
+            audience=audience,
+        )
+        self._username = username
+        self.password = password
+
+    def _do_verification(self, user_code: str, verification_uri: str) -> None:
+        with webdriver_factory() as driver:
+            try:
+                # Load Auth0 location
+                driver.get(verification_uri)
+
+                # Extract code
+                secure_code_field = driver.find_element_by_css_selector(
+                    "input[aria-label='Secure code']"
+                )
+                secure_code = secure_code_field.get_attribute("value")
+                if secure_code != user_code:
+                    raise ValueError(
+                        f"Secure codes differ: {user_code} != {secure_code}"
+                    )
+
+                # If code ok, click button
+                driver.find_element_by_css_selector("button[value='confirm']").click()
+                # Perform login
+                _oauth_sign_in(self._username, self.password, driver)
+                # Wait for page to load
+                driver.find_element_by_xpath(
+                    "//p[text()='Your device is now connected.']"
+                )
+            except Exception as e:
+                logger.error(
+                    f"Exception encountered whilst attempting to perform oauth "
+                    f"verification at {verification_uri}."
+                )
+                save_screenshot("do_verification", driver)
+                raise e
 
 
 class ExtendedBitfountAM(BitfountAM):
     """Extends BitfountAM with methods relevant to testing."""
 
-    def __init__(self, session: BitfountSession, access_manager_url: str):
-        logger.info(f"Using {access_manager_url} for access manager")
+    def __init__(self, session: WebdriverBitfountSession, access_manager_url: str):
         super().__init__(session, access_manager_url)
 
     def grant_proactive_access(
         self, pod_id: str, user_to_grant: str, role: str
     ) -> None:
         """Sets a pod to grant proactive access to the username specified."""
         response = self.session.post(
@@ -119,122 +151,71 @@
         if response.status_code not in (200, 201):
             raise HTTPError(
                 f"Unexpected response ({response.status_code}): {response.text}"
             )
 
 
 def get_bitfount_session(
-    username: str,
-    password: str,
-    token_dir: Path,
-) -> BitfountSession:
-    """Creates and returns a BitfountSession that uses Resource Owner Password Flow."""
-    bf_env = _get_auth_environment()
-    logger.info(f"Webdriver is using {bf_env.auth_domain} for user {username}")
-
-    def get_token() -> Tuple[str, datetime]:
-        body = {
-            "grant_type": "password",
-            "username": username,
-            "password": password,
-            "audience": _HUB_API_IDENTIFIER,
-            "scope": _SCOPES,
-            "client_id": bf_env.client_id,
-        }
-
-        response = web_utils.post(
-            f"https://{bf_env.auth_domain}/oauth/token", data=body
-        ).json()
-        logger.debug(f"/oauth/token response:\n{pprint.pformat(response)}")
-        return response["access_token"], datetime.now(timezone.utc) + timedelta(
-            seconds=response["expires_in"]
-        )
-
-    token, expiry = get_token()
-
-    handler = ExternallyManagedJWTHandler(token, expiry, get_token, username)
-
-    handler.user_storage_path = token_dir / username
-    session = BitfountSession(
-        authentication_handler=handler,
+    username: str, password: str, token_dir: Path
+) -> WebdriverBitfountSession:
+    """Creates and returns a WebdriverBitfountSession."""
+    return WebdriverBitfountSession(
+        username=username,
+        user_storage_path=token_dir / username,
+        password=password,
     )
 
-    return session
 
-
-def oidc_flow(
+def saml_flow(
     url: str,
     username: str,
     password: str,
 ) -> None:
-    """Opens provided oidc url and logs in before closing browser.
+    """Opens provided saml url and logs in before closing browser.
 
     This is run in a separate thread so that the Modeller can respond to the challenges
     from the pods. Otherwise this ends up blocking the Modeller.
 
     Args:
-        url (str): the oidc authentication url to open. This is provided at run-time.
+        url (str): the saml authentication url to open. This is provided at run-time.
         username (str): the username of the user to log in as
         password (str): the password of the user
     """
 
-    def execute_oidc_flow() -> None:
+    def execute_saml_flow() -> None:
         with webdriver_factory() as driver:
             try:
-                logger.warning("opening url in browser")
+                logger.warning("opening url in browswer")
                 driver.get(url)
+                _oauth_sign_in(username, password, driver)
 
-                # Wait for a maximum of 30 seconds for the OIDC Confirmation to appear
+                # Wait for a maximum of 30 seconds for the SAML flow to complete
                 # The webdriver won't wait for the full 30 seconds if the element is
                 # located before this time.
                 WebDriverWait(driver, 30).until(
-                    EC.visibility_of_element_located(
-                        (By.XPATH, '//button[@value="confirm"]')
-                    )
-                )
-                driver.find_element_by_css_selector("button[value='confirm']").click()
-
-                _oauth_sign_in(username, password, driver)
-
-                # Wait for a maximum of 30 seconds for the device to connect
-                WebDriverWait(driver, 30).until(
-                    EC.visibility_of_element_located(
-                        (By.XPATH, "//p[text()='Your device is now connected.']")
-                    )
+                    EC.visibility_of_element_located((By.XPATH, "/html/body/pre[1]"))
                 )
-
             except Exception as e:
                 logger.error(
-                    f"Exception encountered whilst attempting to perform oidc auth "
+                    f"Exception encountered whilst attempting to perform saml auth "
                     f"verification at {url}."
                 )
-                logger.exception(e)
-                save_screenshot("do_oidc_verification", driver)
-                raise
+                save_screenshot("do_saml_verification", driver)
+                raise e
 
-    oidc_thread = threading.Thread(target=execute_oidc_flow, name="oidc")
-    oidc_thread.start()
+    saml_thread = threading.Thread(target=execute_saml_flow, name="saml")
+    saml_thread.start()
 
 
 def grant_proactive_access(
     modeller_username: str,
     pod_id: str,
     role: str,
-    pod_session: BitfountSession,
+    pod_session: WebdriverBitfountSession,
 ) -> None:
     """Grants proactive access to a pod for a given modeller."""
-    bf_env = _get_environment()
-    if bf_env == _STAGING_ENVIRONMENT:
-        am_url = _STAGING_AM_URL
-    elif bf_env == _DEVELOPMENT_ENVIRONMENT:
-        am_url = _DEV_AM_URL
-    elif bf_env == _SANDBOX_ENVIRONMENT:
-        am_url = _SANDBOX_AM_URL
-    else:
-        am_url = PRODUCTION_AM_URL
-
-    am = ExtendedBitfountAM(pod_session, am_url)
+    am = ExtendedBitfountAM(pod_session, _STAGING_AM_URL)
     am.grant_proactive_access(
         pod_id=pod_id,
         user_to_grant=modeller_username,
         role=role,
     )
```

### Comparing `bitfount-0.5.86/tests/integration/conftest.py` & `bitfount-0.5.9/tests/integration/conftest.py`

 * *Files 16% similar despite different names*

```diff
@@ -3,31 +3,28 @@
 from logging import handlers
 from multiprocessing import Manager
 from multiprocessing.managers import SyncManager
 from pathlib import Path
 import queue
 from typing import Generator
 
+from _pytest.monkeypatch import MonkeyPatch
+from _pytest.tmpdir import TempPathFactory
 import dotenv
-from pytest import MonkeyPatch, TempPathFactory, fixture
+from pytest import fixture
 import requests
 
 import bitfount.config
 from tests.integration import PRIVATE_DETAILS_ENV_FILE
 
 logger = logging.getLogger(__name__)
 
-# This forces `requests` to make IPv4 connections
-# TODO: [BIT-1443] Remove this once Hub/AM support IPv6
-requests.packages.urllib3.util.connection.HAS_IPV6 = False  # type: ignore[attr-defined] # Reason: see above # noqa: B950
-
 TMP_DIR_BASENAME = "E2E-"
-CENSUS_INCOME_CSV_URL = (
-    "https://bitfount-hosted-downloads.s3.eu-west-2.amazonaws.com/"
-    "bitfount-tutorials/census_income.csv"
+PROSPER_CSV_URL = (
+    "https://bitfount-hosted-downloads.s3.eu-west-2.amazonaws.com/prosper.csv"
 )
 
 
 @fixture(autouse=True)
 def env_fix(monkeypatch: MonkeyPatch) -> None:
     """Fix the environment into a known state for tests."""
     # Overrides the default fixture in tests/conftest.py
@@ -41,24 +38,24 @@
 def load_env() -> None:
     """Load the private environment variables file if it exists."""
     if PRIVATE_DETAILS_ENV_FILE.exists():
         dotenv.load_dotenv(dotenv_path=PRIVATE_DETAILS_ENV_FILE)
 
 
 @fixture(scope="session")
-def census_income_data(tmp_path_factory: TempPathFactory) -> Path:
-    """Load the census income data from S3."""
-    logging.info("Downloading census income data... ")
+def prosper_data(tmp_path_factory: TempPathFactory) -> Path:
+    """Load the prosper data from S3."""
+    logging.info("Downloading prosper data... ")
     tmp_dir = tmp_path_factory.mktemp(TMP_DIR_BASENAME)
-    local_filename = tmp_dir / "census_income.csv"
-    with requests.get(CENSUS_INCOME_CSV_URL) as r:
+    local_filename = tmp_dir / "prosper.csv"
+    with requests.get(PROSPER_CSV_URL) as r:
         r.raise_for_status()
         with open(local_filename, "wb") as f:
             f.write(r.content)
-    logging.info(f"Census income data saved to {local_filename}")
+    logging.info(f"Prosper data saved to {local_filename}")
     return local_filename
 
 
 @fixture(scope="module")
 def manager() -> Generator[SyncManager, None, None]:
     """Creates a multiprocessing manager.
```

### Comparing `bitfount-0.5.86/tests/integration/end_to_end_mock/conftest.py` & `bitfount-0.5.9/tests/integration/end_to_end_mock/conftest.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,66 +1,61 @@
 """Test config settings for the mocked end-to-end tests."""
 from typing import Callable, Dict, Tuple
 
 import flask
-from flask import Flask, jsonify, request as flask_request
+from flask import Flask, jsonify
+from flask import request as flask_request
 from pytest import fixture
 
+from bitfount.federated.transport.message_service import _S3_NETLOC_SUFFIX
 from bitfount.hub.types import (
-    _CreatedResourceResponseJSON,
+    _AccessRequestResponseJSON,
     _HubFailureResponseJSON,
     _HubSuccessResponseJSON,
     _PodDetailsResponseJSON,
     _PodRegistrationFailureJSON,
     _PodRegistrationResponseJSON,
 )
-from bitfount.types import (
-    _JSONDict,
-    _S3PresignedPOSTFields,
-    _S3PresignedPOSTURL,
-    _S3PresignedURL,
-)
+from bitfount.types import _S3PresignedPOSTFields, _S3PresignedPOSTURL, _S3PresignedURL
 from tests.utils import PytestRequest
 
 
 @fixture
-def s3_upload_url_builder(
-    s3_netloc_subdomain: str,
-) -> Callable[[str], _S3PresignedPOSTURL]:
+def s3_upload_url_builder() -> Callable[[str], _S3PresignedPOSTURL]:
     """Builds an S3 upload url with the specified pod name at the end."""
 
     def _create(pod_name: str) -> _S3PresignedPOSTURL:
         return _S3PresignedPOSTURL(
-            f"https://an-s3-upload-url{s3_netloc_subdomain}.s3.com/some-resource/{pod_name}"
+            f"https://an-s3-upload-url{_S3_NETLOC_SUFFIX}/some-resource/{pod_name}"
         )
 
     return _create
 
 
 @fixture
-def s3_download_url_builder(
-    s3_netloc_subdomain: str,
-) -> Callable[[str], _S3PresignedURL]:
+def s3_download_url_builder() -> Callable[[str], _S3PresignedURL]:
     """Builds an S3 download url with the specified pod name at the end."""
 
     def _create(pod_name: str) -> _S3PresignedURL:
         return _S3PresignedURL(
-            f"https://an-s3-download-url{s3_netloc_subdomain}.s3.com/some-resource/{pod_name}"
+            f"https://an-s3-download-url{_S3_NETLOC_SUFFIX}/some-resource/{pod_name}"
         )
 
     return _create
 
 
 @fixture
 def app(
+    access_json: str,
     modeller_public_key_string: str,
     request: PytestRequest,
     s3_download_url_builder: Callable[[str], _S3PresignedURL],
     s3_upload_fields: _S3PresignedPOSTFields,
     s3_upload_url_builder: Callable[[str], _S3PresignedPOSTURL],
+    signed_permission: str,
 ) -> Flask:
     """Creates the Bitfount Hub API."""
     register_pod_fail, access_request_fail, get_pod_fail = (
         False,
         False,
         False,
     )
@@ -145,32 +140,25 @@
             "isOnline": True,
             "providerUserName": "providerUserName",
             "visibility": "public",
             "schemaDownloadUrl": s3_download_url_builder(pod_name),
         }
         return jsonify(pod_details_json), 200
 
-    @api.route("/api/ingest", methods=["POST"])
-    def ingest_monitor_update() -> Tuple[_JSONDict, int]:
-        return {}, 200
-
-    @api.route("/api/<username>/keys/<key_id>", methods=["GET"])
-    def get_modeller_public_keys(
-        username: str, key_id: str
-    ) -> Tuple[flask.Response, int]:
-        json_dict = {
-            # Explicitly DON'T include a key in the response as not needed and
-            # wouldn't be registered in this scenario
-            "message": "Key not registered"
+    @api.route("/api/access/access-request", methods=["GET"])
+    def access_request() -> Tuple[flask.Response, int]:
+        if access_request_fail:
+            return jsonify([]), 200
+        access_request_json: _AccessRequestResponseJSON = {
+            "accessJSON": access_json,
+            "modellerPublicKeyPem": "modellerPublicKey",
+            "status": "APPROVED",
+            "signedPermission": signed_permission,
         }
-        return jsonify(json_dict), 404
-
-    @api.route("/api/<username>/keys", methods=["POST"])
-    def register_modeller_public_key(
-        username: str,
-    ) -> Tuple[flask.Response, int]:
-        json_dict: _CreatedResourceResponseJSON = {"id": "1"}
-        return jsonify(json_dict), 200
+        return (
+            jsonify([access_request_json]),
+            200,
+        )
 
     api.testing = True
 
     return api
```

### Comparing `bitfount-0.5.86/tests/integration/end_to_end_mock/test_end_to_end_mock_bitfount_hub.py` & `bitfount-0.5.9/tests/integration/end_to_end_mock/test_end_to_end_mock_bitfount_hub.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,48 +1,34 @@
 """Test end-to-end but mock out Bitfount Hub API (as well as message service)."""
 import asyncio
 import copy
 import logging
 import multiprocessing
 from multiprocessing import current_process
-from multiprocessing.managers import DictProxy, SyncManager
+from multiprocessing.managers import SyncManager
 import os
 from pathlib import Path
 from queue import Queue
 import time
-from typing import (
-    Any,
-    Callable,
-    Dict,
-    Generator,
-    Iterable,
-    List,
-    Optional,
-    Tuple,
-    Type,
-    Union,
-    cast,
-)
-from unittest.mock import Mock, create_autospec
+from typing import Any, Callable, Dict, Generator, Iterable, Union, cast
+from unittest.mock import Mock
 
+from _pytest.monkeypatch import MonkeyPatch
 from cryptography.hazmat.primitives.asymmetric.rsa import RSAPrivateKey, RSAPublicKey
 from flask import Flask, url_for
 import pytest
-from pytest import MonkeyPatch, fixture
+from pytest import fixture
 from pytest_flask.live_server import LiveServer
-from pytest_lazyfixture import lazy_fixture
 from pytest_mock import MockerFixture
 from pytorch_lightning.loggers import TensorBoardLogger
 
-import bitfount
-from bitfount import AuthenticationHandler, PodRegistrationError
-from bitfount.federated.modeller import _Modeller
+from bitfount.federated.exceptions import PodRegistrationError
+from bitfount.federated.modeller import Modeller
 from bitfount.federated.transport.base_transport import _run_func_and_listen_to_mailbox
 from bitfount.federated.transport.message_service import _LargeObjectRequestHandler
-from bitfount.federated.types import _PodResponseType
 from bitfount.hub.authentication_flow import BitfountSession
 from bitfount.runners.config_schemas import ModellerConfig, PodConfig
 from bitfount.runners.modeller_runner import setup_modeller_from_config
 from bitfount.runners.pod_runner import setup_pod_from_config
 from bitfount.types import (
     _JSONDict,
     _S3PresignedPOSTFields,
@@ -74,17 +60,17 @@
 
     def __init__(self, access_token: str, username: str, **kwargs: Any):
         self._faked_access_token = access_token
         self._username = username
         super().__init__(**kwargs)
 
     @property
-    def message_service_metadata(self) -> List[Tuple[str, str]]:
+    def access_token(self) -> str:
         """The supplied access token to provide to the message service."""
-        return [("token", self._faked_access_token)]
+        return self._faked_access_token
 
     def authenticate(self) -> None:
         """Same as BitfountSession but doesn't authenticate anything."""
         pass
 
     @property
     def username(self) -> str:
@@ -93,68 +79,45 @@
 
 
 @end_to_end_mocks_test
 class TestEndToEnd:
     """Tests end-to-end functionality using mocks."""
 
     @fixture(autouse=True)
-    def patch_get_access_manager_key(
+    def mock_am(
         self, monkeypatch: MonkeyPatch, access_manager_public_key: RSAPublicKey
     ) -> None:
-        """Patch get_access_manager_key."""
+        """Mock get_access_manager_key."""
         import bitfount.hub.api as api_
 
         monkeypatch.setattr(
             api_.BitfountAM,
             "get_access_manager_key",
             lambda x: access_manager_public_key,
         )
 
     @fixture(autouse=True)
-    def patch_am_signature_checking(self, mocker: MockerFixture) -> Mock:
-        """Patch access manager signature-based authorisation checking.
-
-        Sets "ACCEPT" by default but returns the mock so this can be overridden.
-        """
-        from bitfount.hub.api import BitfountAM
-
-        mock_signature_checker: Mock = mocker.patch.object(
-            BitfountAM, "check_signature_based_access_request", autospec=True
-        )
-        mock_signature_checker.return_value = _PodResponseType.ACCEPT
-        return mock_signature_checker
-
-    @fixture(autouse=True)
-    def patch_logger_save_dir(self, monkeypatch: MonkeyPatch, tmp_path: Path) -> None:
-        """Patch pytorch logger save directory."""
+    def mock_logger_save_dir(self, monkeypatch: MonkeyPatch, tmp_path: Path) -> None:
+        """Mock pytorch logger."""
         from bitfount.backends.pytorch.models.base_models import BasePyTorchModel
 
         monkeypatch.setattr(
             BasePyTorchModel,
             "_get_logger_from_config",
             lambda *args, **kwargs: TensorBoardLogger(
                 str(tmp_path),
                 name="mock_e2e_tests_model_name",
             ),
         )
 
     @fixture
-    def pod_config(self, census_income_data: Path) -> PodConfig:
+    def pod_config(self, prosper_data: Path) -> PodConfig:
         """Pod config to use for tests."""
-        pod_config = load_pod_config(CONFIG_DIR / "pod_1.yaml", census_income_data)
-        pod_config.approved_pods = [f"{pod_config.pod_id}2"]
-        pod_config.hub.url = url_for("index", _external=True)
-        return pod_config
-
-    @fixture
-    def multidataset_pod_config(self, census_income_data: Path) -> PodConfig:
-        """Multidataset pod config to use for tests."""
-        pod_config = load_pod_config(
-            CONFIG_DIR / "pod_census_income_multidatasource.yaml", census_income_data
-        )
+        pod_config = load_pod_config(CONFIG_DIR / "pod_1.yaml", prosper_data)
+        pod_config.other_pods = [f"{pod_config.pod_id}2"]
         pod_config.hub.url = url_for("index", _external=True)
         return pod_config
 
     @fixture
     def modeller_config(
         self, modeller_private_key: RSAPrivateKey
     ) -> Generator[ModellerConfig, None, None]:
@@ -191,23 +154,17 @@
         self, tmp_path: Path
     ) -> Callable[[str, str], MockBitfountSession]:
         """Mock bitfount session factory."""
 
         def _create_mock_bitfount_session(
             username: str, access_token: str
         ) -> MockBitfountSession:
-            handler = create_autospec(AuthenticationHandler)
-            handler.user_storage_path = tmp_path / username
-            handler.token_file = handler.user_storage_path / ".token"
-            session = MockBitfountSession(
-                access_token=access_token,
-                username=username,
-                authentication_handler=handler,
+            return MockBitfountSession(
+                access_token=access_token, username=username, user_storage_path=tmp_path
             )
-            return session
 
         return _create_mock_bitfount_session
 
     @fixture
     def apply_mock_large_object_interactions(
         self, mocker: MockerFixture
     ) -> Callable[[SyncManager], None]:
@@ -220,35 +177,35 @@
         def _mock_large_object_interactions(manager: SyncManager) -> None:
             """Mock out LargeObjectRequestHandler methods.
 
             Mocks out LargeObjectRequestHandler.upload_large_object() and
             LargeObjectRequestHandler.get_large_object_from_url() to use local storage.
             """
             # Mock out message service functions
-            large_object_storage: DictProxy[
+            large_object_storage: Dict[
                 Union[_S3PresignedURL, _S3PresignedPOSTURL], bytes
             ] = manager.dict()
 
             # upload_fields is unused below
             # noinspection PyUnusedLocal
-            async def store_large_object(
+            def store_large_object(
                 upload_url: _S3PresignedPOSTURL,
                 upload_fields: _S3PresignedPOSTFields,
                 large_object: bytes,
             ) -> None:
                 """Mocks uploading to the URL.
 
                 Args:
                     upload_url: URL to "upload" to.
                     upload_fields: Unused. Fields to upload with.
                     large_object: Object to store.
                 """
                 large_object_storage[upload_url] = large_object
 
-            async def get_large_object(url: _S3PresignedURL) -> bytes:
+            def get_large_object(url: _S3PresignedURL) -> bytes:
                 """Mock downloading the object from the download URL.
 
                 Args:
                     url: The download URL provided.
 
                 Returns: The retrieved object/message
                 """
@@ -272,101 +229,85 @@
                 new=get_large_object,
             )
 
         return _mock_large_object_interactions
 
     @staticmethod
     def _modeller_send_task_requests(
-        modeller: _Modeller,
+        modeller: Modeller,
         pod_identifiers: Iterable[str],
-        project_id: Optional[str] = None,
     ) -> None:
         """Helper for sending Modeller task requests in a separate process.
 
         Fails tests if there are any errors.
         """
-
         # As this is used in multiprocessing, _modeller_send_task_requests() has
         # to be a def function. So we wrap the desired methods in an async def
         # and use asyncio.run() to simulate async def.
         async def _run_modeller() -> None:
-            """Asyncronous modeller runner to process and run task requests."""
-            modeller_mailbox = await modeller._send_task_requests(
-                pod_identifiers, project_id
-            )
+            modeller_mailbox = await modeller._send_task_requests(pod_identifiers)
             await _run_func_and_listen_to_mailbox(
                 modeller_mailbox.process_task_request_responses(), modeller_mailbox
             )
 
         try:
             asyncio.run(_run_modeller())
         except Exception as e:
             print(f"Caught exception in {current_process().name} process: {e}")
             raise e
 
-    @pytest.mark.parametrize(
-        argnames=("cli_mode", "expected_exc"),
-        argvalues=((True, SystemExit), (False, PodRegistrationError)),
-    )
     @pytest.mark.parametrize("app", [(True, False, False)], indirect=True)
     @pytest.mark.timeout(STD_TIMEOUT)
     def test_pod_setup_raises_pod_registration_error(
         self,
         app: Flask,
         bitfount_session_factory: Callable[[str, str], MockBitfountSession],
-        cli_mode: bool,
-        expected_exc: Type[BaseException],
         live_server: LiveServer,
         mock_bitfount_session: Mock,
-        monkeypatch: MonkeyPatch,
         pod_config: PodConfig,
     ) -> None:
         """Tests handling when pod raises registration error."""
-        # Set _BITFOUNT_CLI_MODE config variable to elicit different error handling
-        monkeypatch.setattr(bitfount.config, "_BITFOUNT_CLI_MODE", cli_mode)
-
         username = pod_config.username
         # Override default return_value with bitfount_session_factory() one
         mock_bitfount_session.return_value = bitfount_session_factory(username, "pod")
 
-        with pytest.raises(expected_exc):
+        with pytest.raises(PodRegistrationError):
             setup_pod_from_config(pod_config)
 
-    @pytest.mark.parametrize(
-        "test_pod_config",
-        [lazy_fixture("pod_config"), lazy_fixture("multidataset_pod_config")],
-    )
     @pytest.mark.timeout(STD_TIMEOUT)
     def test_pod_starts_successfully(
         self,
         bitfount_session_factory: Callable[[str, str], MockBitfountSession],
         caplog_queue: Queue,
         live_server: LiveServer,  # automatically finds app fixture
         mock_bitfount_session: Mock,
         mock_grpc_insecure_channel: Mock,
         mock_message_service_stub: Mock,
         mock_s3_data_upload_in_api_module: Mock,
-        test_pod_config: PodConfig,
+        pod_config: PodConfig,
     ) -> None:
         """Starts Pod listening in a subprocess.
 
         Sends SIGINT to the Pod after 10s to stop listening
         (Otherwise this test will hang indefinitely)
         """
         with multiprocessing.Manager() as manager:
+            # multiprocessing.Manager() actually returns a SyncManager
+            manager = cast(SyncManager, manager)
+
             pod_token = "pod"
-            username = test_pod_config.username
+            username = pod_config.username
             mock_message_service_stub.return_value = GRPCStubMock(
-                {pod_token: test_pod_config.pod_id}, manager
+                {pod_token: pod_config.pod_id}, manager
             )
             # Override default return_value with bitfount_session_factory() one
             mock_bitfount_session.return_value = bitfount_session_factory(
                 username, pod_token
             )
-            pod = setup_pod_from_config(test_pod_config)
+            pod = setup_pod_from_config(pod_config)
 
             p = multiprocessing.Process(
                 target=pod_start, name="Pod_Runner", args=(pod,)
             )
             p.start()
             time.sleep(10)
             p.terminate()
@@ -382,55 +323,44 @@
 
     @backend_test
     @pytest.mark.parametrize("app", [(False, True, False)], indirect=True)
     @pytest.mark.timeout(STD_TIMEOUT)
     async def test_modeller_send_task_request_rejected_by_pod(
         self,
         app: Flask,
-        apply_mock_large_object_interactions: Callable[[SyncManager], None],
         bitfount_session_factory: Callable[[str, str], MockBitfountSession],
         caplog_queue: Queue,
         live_server: LiveServer,
         mock_bitfount_session: Mock,
         mock_grpc_insecure_channel: Mock,
         mock_message_service_stub: Mock,
         mock_s3_data_upload_in_api_module: Mock,
         mocker: MockerFixture,
         modeller_config: ModellerConfig,
-        patch_am_signature_checking: Mock,
         pod_config: PodConfig,
     ) -> None:
         """Tests Modeller handling if task requests are rejected by pods."""
-        # Make access checking return a reject
-        patch_am_signature_checking.return_value = _PodResponseType.UNAUTHORISED
-
         try:
             with multiprocessing.Manager() as manager:
+                # multiprocessing.Manager() actually returns a SyncManager
+                manager = cast(SyncManager, manager)
+
                 modeller_token = "modeller_token"
                 pod_token = "pod_token"
 
                 # Mock out GRPC interactions
                 mock_stub = GRPCStubMock(
                     {
                         modeller_token: modeller_config.modeller.username,
                         pod_token: pod_config.pod_id,
                     },
                     manager,
                 )
                 mock_message_service_stub.return_value = mock_stub
 
-                # Mock out Key ID file handling
-                mocker.patch("bitfount.federated.modeller._store_key_id")
-                mocker.patch(
-                    "bitfount.federated.modeller._get_key_id", return_value="1"
-                )
-
-                # Mock out large object handling in message service
-                apply_mock_large_object_interactions(manager)
-
                 # Ensure only testing against one pod
                 modeller_config.pods.identifiers = [pod_config.pod_id]
 
                 # Mock out the sessions used for authentication
                 pod_username = pod_config.username
                 modeller_username = modeller_config.modeller.username
                 # Override default return_value with side_effect
@@ -438,35 +368,30 @@
                 # over return_value.
                 mock_bitfount_session.side_effect = [
                     bitfount_session_factory(pod_username, pod_token),
                     bitfount_session_factory(modeller_username, modeller_token),
                 ]
                 # Setup pod and modeller
                 pod = setup_pod_from_config(pod_config)
-                ds = pod.datasource
-                assert ds is not None
                 mocker.patch(
                     "bitfount.runners.modeller_runner.get_pod_schema",
-                    return_value=ds.schema,
+                    return_value=pod.schema,
                 )
 
-                modeller, pod_identifiers, _, _, _ = setup_modeller_from_config(
-                    modeller_config
-                )
+                modeller, pod_identifiers = setup_modeller_from_config(modeller_config)
                 print(f"{modeller._identity_verification_method}")
 
                 # Start pod, give time to spin up
                 pod_process = multiprocessing.Process(
                     target=pod_start, name="Pod_Runner", args=(pod,)
                 )
                 pod_process.start()
                 time.sleep(2)
 
                 # Start modeller
-                mocker.patch("bitfount.hub.helper._save_key_to_key_store")
                 modeller_process = multiprocessing.Process(
                     target=self._modeller_send_task_requests,
                     name="Modeller_Runner",
                     args=(modeller, pod_identifiers),
                 )
                 modeller_process.start()
 
@@ -477,22 +402,20 @@
                     proc.join()
 
                 # Check conditions in log messages match rejection of task
                 caplog_records = get_caplog_records(caplog_queue)
                 assert "INFO" in caplog_records
                 assert (
                     "Task request from e2e_modeller rejected. "
-                    "Insufficient permissions for the requested task on this pod."
-                    in caplog_records["INFO"]
+                    "Protocol not approved" in caplog_records["INFO"]
                 )
                 assert "ERROR" in caplog_records
                 assert (
                     f"Received rejection from {pod_config.pod_id}. "
-                    f"Insufficient permissions for the requested task on this pod."
-                    in caplog_records["ERROR"]
+                    f"Protocol not approved" in caplog_records["ERROR"]
                 )
                 assert "CRITICAL" not in caplog_records
 
                 # Check that things shutdown correctly
                 assert (
                     modeller_process.exitcode is not None
                     and modeller_process.exitcode <= 0
@@ -553,38 +476,40 @@
         bitfount_session_factory: Callable[[str, str], MockBitfountSession],
         caplog_queue: Queue,
         live_server: LiveServer,  # automatically finds app fixture
         mock_bitfount_session: Mock,
         mock_grpc_insecure_channel: Mock,
         mock_message_service_stub: Mock,
         mock_schema_upload_download: None,
-        mocker: MockerFixture,
         modeller_config: ModellerConfig,
         pod_config: PodConfig,
         tmp_path: Path,
         use_local_storage: bool,
     ) -> None:
         """Tests that training works end to end.
 
         Parameterised to run by sending messages directly, and then sending messages
         as file paths.
         """
         try:
             with multiprocessing.Manager() as manager:
+                # multiprocessing.Manager() actually returns a SyncManager
+                manager = cast(SyncManager, manager)
+
                 pod1_config = pod_config
 
                 # Determine if we are using file paths for messages and set it up
                 if use_local_storage:
                     pod1_config.message_service.use_local_storage = True
                     modeller_config.message_service.use_local_storage = True
 
                 # Create second pod config from first
                 second_pod_owner = "e2e_provider_2"
                 pod2_config = copy.deepcopy(pod1_config)
-                pod2_config.name += "2"
+                pod2_config.pod_name += "2"
                 pod2_config.username = second_pod_owner
 
                 # Patch out GRPC interactions
                 pod1_token = "pod1"
                 modeller_token = "modeller_token"
                 pod2_token = "pod2"
                 mock_stub = GRPCStubMock(
@@ -594,35 +519,25 @@
                         pod2_token: pod2_config.pod_id,
                     },
                     manager,
                     get_message_timeout=5,
                 )
                 mock_message_service_stub.return_value = mock_stub
 
-                # Mock out Key ID file handling
-                mocker.patch("bitfount.federated.modeller._store_key_id")
-                mocker.patch(
-                    "bitfount.federated.modeller._get_key_id", return_value="1"
-                )
-
                 # Mock out large object handling in message service
                 apply_mock_large_object_interactions(manager)
 
                 # Tie pod configs together
-                pod1_config.approved_pods = [pod2_config.pod_id]
-                pod2_config.approved_pods = [pod1_config.pod_id]
+                pod1_config.other_pods = [pod2_config.pod_id]
+                pod2_config.other_pods = [pod1_config.pod_id]
                 modeller_config.pods.identifiers = [
                     pod1_config.pod_id,
                     pod2_config.pod_id,
                 ]
 
-                modeller_config.task.data_structure.table_config.table = {  # type: ignore[union-attr] # reason: model algorithm has a datastructure # noqa: B950
-                    pod1_config.pod_id: "census-income-demo",
-                    pod2_config.pod_id: "census-income-demo",
-                }
                 # Generate mocked sessions for modeller and pods
                 modeller_username = modeller_config.modeller.username
                 pod1_username = pod1_config.username
                 pod2_username = second_pod_owner
                 # Override default return_value with side_effect
                 # bitfount_session_factory() one. side_effect takes precedence
                 # over return_value.
@@ -634,42 +549,38 @@
                     bitfount_session_factory(modeller_username, modeller_token),
                 ]
 
                 # Setup pods and modeller (Note: order is important)
                 pod1 = setup_pod_from_config(pod1_config)
                 pod2 = setup_pod_from_config(pod2_config)
 
-                modeller, pod_identifiers, _, _, _ = setup_modeller_from_config(
-                    modeller_config
-                )
+                modeller, pod_identifiers = setup_modeller_from_config(modeller_config)
                 model_out = tmp_path / "model.out"
+
                 # Start pods and wait to spin up
                 pod_1_process = multiprocessing.Process(
                     target=pod_start, name="Pod_1_Runner", args=(pod1,)
                 )
                 pod_1_process.start()
                 time.sleep(0.5)
                 pod_2_process = multiprocessing.Process(
                     target=pod_start, name="Pod_2_Runner", args=(pod2,)
                 )
                 pod_2_process.start()
                 time.sleep(0.5)
 
                 # Start modeller
-                mocker.patch("bitfount.hub.helper._save_key_to_key_store")
                 modeller_process = multiprocessing.Process(
                     target=run_modeller_process,
                     name="MM_1",
                     args=(
                         modeller,
                         pod_identifiers,
+                        model_out,
                     ),
-                    kwargs={
-                        "model_out": model_out,
-                    },
                 )
                 modeller_process.start()
 
                 # TODO: [BIT-350] instead of relying on a timeout - replace this
                 #       with a call that waits for the process to finish before
                 #       terminating. multiprocessing join() method currently hangs
                 #       when using tox (but not pytest individually).
```

### Comparing `bitfount-0.5.86/tests/integration/tutorials/notebook_logging.py` & `bitfount-0.5.9/tests/integration/tutorials/notebook_logging.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,49 +1,22 @@
 """Module for handling logging messages out of a Jupyter Notebook."""
-import errno
 import inspect
 import logging
 from logging import Logger, LogRecord
 from multiprocessing.managers import BaseManager
 from queue import Empty, Queue
-import random
 import secrets
-import socket
-import textwrap
 from threading import Event, Thread
-from typing import Final, Generator, List
+from typing import Final, Generator, List, cast
 
 from pytest import fixture
 
 logger = logging.getLogger(__name__)
 
-
-def _get_random_port() -> int:
-    # Max of 10 attempts
-    for _ in range(10):
-        # Recommended ephemeral port range from IANA
-        port = random.randint(49152, 65535)
-        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
-            try:
-                # Attempt to bind to the port. If successful, then return the port
-                # which will close the socket immediately, freeing the port up to be
-                # used elsewhere
-                s.bind(("", port))
-                return port
-            except OSError as e:
-                if e.errno in (errno.EADDRINUSE, errno.EADDRNOTAVAIL):
-                    # Address already unavailable so try again
-                    continue
-                else:
-                    raise e
-
-    raise RuntimeError("Unable to get random port for log queue")
-
-
-_LOG_QUEUE_PORT: Final[int] = _get_random_port()
+_LOG_QUEUE_PORT: Final[int] = 50000
 _LOG_QUEUE_AUTH_KEY: Final[bytes] = secrets.token_hex(10).encode()
 _RECORD_GET_TIMEOUT: Final[int] = 1
 
 # Shared queue for logging messages to and from
 _LOG_QUEUE: Final[Queue] = Queue()
 
 
@@ -81,15 +54,15 @@
 @fixture(scope="module")
 def log_queue_manager() -> Generator[LogQueueManager, None, None]:
     """Hosts and starts a LogQueueManager instance for logging messages."""
     with LogQueueManager(
         address=("", _LOG_QUEUE_PORT), authkey=_LOG_QUEUE_AUTH_KEY
     ) as manager:
         logger.info("LogQueueManager started")
-        yield manager
+        yield cast(LogQueueManager, manager)
 
 
 @fixture
 def clear_log_queue() -> None:
     """Clears the log queue, ready for the next test."""
     # Clear queue, using direct reference as otherwise will hit Manager's proxy
     logger.info("Clearing log queue")
@@ -150,15 +123,15 @@
     # Create queue handler for loggers
     queue_handler = QueueHandler(log_queue)
     queue_handler.setLevel("INFO")
 
     # Create custom filter to append additional LogRecord field
     class LogQueueFilter(Filter):
         def filter(self, record: LogRecord) -> bool:
-            record.notebook = _NOTEBOOK_PLACEHOLDER
+            record.notebook = _NOTEBOOK_PLACEHOLDER  # type: ignore[attr-defined] # Reason: explicitly setting this for later use # noqa: B950
             return True
 
     queue_handler.addFilter(LogQueueFilter())
 
     # Attach to root logger and set others to propagate
     logging.info("Attaching queue handler to root logger")
     logging.root.addHandler(queue_handler)
@@ -181,8 +154,8 @@
     # Replace constants that aren't in scope
     source = source.replace("_LOG_QUEUE_PORT", str(_LOG_QUEUE_PORT))
     source = source.replace(
         "_LOG_QUEUE_AUTH_KEY", str(_LOG_QUEUE_AUTH_KEY)
     )  # will include quote marks
     source = source.replace("_NOTEBOOK_PLACEHOLDER", f'"{notebook_name}"')
 
-    return textwrap.dedent(source) + f"{_add_notebook_queue_logging.__name__}()"
+    return source + f"{_add_notebook_queue_logging.__name__}()"
```

### Comparing `bitfount-0.5.86/tests/utils/fixtures/__init__.py` & `bitfount-0.5.9/tests/utils/fixtures/__init__.py`

 * *Files 14% similar despite different names*

```diff
@@ -4,8 +4,7 @@
 from .encryption_fixtures import *  # noqa: F401, F403
 from .env_fixtures import *  # noqa: F401, F403
 from .helper_fixtures import *  # noqa: F401, F403
 from .hub_and_am_fixtures import *  # noqa: F401, F403
 from .schema_fixtures import *  # noqa: F401, F403
 from .storage_fixtures import *  # noqa: F401, F403
 from .transport_layer_fixtures import *  # noqa: F401, F403
-from .web_utils_fixtures import *  # noqa: F401, F403
```

### Comparing `bitfount-0.5.86/tests/utils/fixtures/authorisation_checker_fixtures.py` & `bitfount-0.5.9/tests/utils/fixtures/authorisation_checker_fixtures.py`

 * *Files identical despite different names*

### Comparing `bitfount-0.5.86/tests/utils/fixtures/encryption_fixtures.py` & `bitfount-0.5.9/tests/utils/fixtures/encryption_fixtures.py`

 * *Files identical despite different names*

### Comparing `bitfount-0.5.86/tests/utils/fixtures/helper_fixtures.py` & `bitfount-0.5.9/tests/utils/fixtures/helper_fixtures.py`

 * *Files 9% similar despite different names*

```diff
@@ -17,16 +17,14 @@
         "bitfount.hub.helper.BitfountSession",
     )
     mock_bitfount_session.return_value = create_autospec(
         BitfountSession,
         instance=True,
         # attribute needed in mock
         user_storage_path=BITFOUNT_STORAGE_PATH / "test_username",
-        username="test_username",
-        authentication_handler=Mock(),
     )
     return mock_bitfount_session
 
 
 @fixture
 def apply_mock_get_pod_public_keys(mocker: MockerFixture) -> Callable[[str], Mock]:
     """Allows mocking out the get_pod_public_keys function.
```

### Comparing `bitfount-0.5.86/tests/utils/fixtures/hub_and_am_fixtures.py` & `bitfount-0.5.9/tests/utils/fixtures/hub_and_am_fixtures.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,19 +1,22 @@
 """Contains pytest fixtures related to hub and access manager interactions."""
+import base64
 import inspect
-from typing import Any, Dict, List, Optional, Tuple, cast
+import json
+from typing import cast
 from unittest.mock import Mock
 
 from cryptography.hazmat.backends import default_backend
 from cryptography.hazmat.primitives import serialization
 from cryptography.hazmat.primitives.asymmetric.rsa import RSAPrivateKey, RSAPublicKey
-from pytest import MonkeyPatch, fixture
+from pytest import fixture
 from pytest_mock import MockerFixture
 
-from bitfount import AuthenticationHandler, BitfountSession
+from bitfount.federated.encryption import _RSAEncryption
+from bitfount.hub.types import _AccessJSON
 
 
 @fixture
 def authoriser_public_key() -> RSAPublicKey:
     """Authoriser public key fixture."""
     serialized = inspect.cleandoc(
         """-----BEGIN PUBLIC KEY-----
@@ -120,14 +123,26 @@
         serialization.load_pem_public_key(
             modeller_public_key_string.encode(), backend=default_backend()
         ),
     )
 
 
 @fixture
+def access_json(modeller_public_key_string: str) -> str:
+    """Example access json."""
+    access_json: _AccessJSON = {
+        "protocol": "FederatedAveraging",
+        "modellerIdPUserID": "modeller_idp_user",
+        "status": "APPROVED",
+        "modellerPublicKey": modeller_public_key_string,
+    }
+    return json.dumps(access_json)
+
+
+@fixture
 def modeller_private_key() -> RSAPrivateKey:
     """Modeller private key fixture."""
     serialized = inspect.cleandoc(
         """
         -----BEGIN RSA PRIVATE KEY-----
         MIIEpAIBAAKCAQEAruIdWcWQKIph+mQd6XVknbs95a0mslUq7Y6ukeCgE1nwqphT
         MDYefbvCXNQ9C1CX / b7ydyoZBSQ92RdsR19tJ6AzpfZco3DflTamqxiqxyy75uMN
@@ -161,29 +176,38 @@
         serialization.load_pem_private_key(
             serialized.encode(), password=None, backend=default_backend()
         ),
     )
 
 
 @fixture
+def signed_permission(
+    access_json: str, access_manager_private_key: RSAPrivateKey
+) -> str:
+    """Signed permission from hub."""
+    signed_message = _RSAEncryption.sign_message(
+        access_manager_private_key, access_json.encode()
+    )
+    return base64.b64encode(signed_message).decode("ascii")
+
+
+@fixture
 def modeller_message() -> str:
     """Example modeller message."""
     return "TEST Message"
 
 
 @fixture
 def bitfount_model_correct_structure() -> str:
     """Example good BitfountModel."""
     return inspect.cleandoc(
         """
-        from typing import Optional
         from bitfount.models.bitfount_model import BitfountModel
-        from bitfount.federated.mixins import _DistributedModelMixIn
 
-        class MyModel(BitfountModel, _DistributedModelMixIn):
+        class MyModel(BitfountModel):
 
             def __init__(self, **kwargs) -> None:
                 super().__init__(**kwargs)
                 self.epochs: Optional[int] = 1
                 self.steps: Optional[int] = None
                 self._total_num_batches_trained: int = 0
 
@@ -199,20 +223,14 @@
 
             def apply_weight_updates(self):
                 ...
 
             def update_params(self):
                 ...
 
-            def deserialize_params(self):
-                ...
-
-            def serialize_params(self):
-                ...
-
             def diff_params(self):
                 ...
 
             def set_model_training_iterations(self):
                 ...
 
             def reset_trainer(self) -> None:
@@ -226,32 +244,24 @@
 
             def initialise_model(self):
                 ...
 
             def evaluate(self):
                 ...
 
-            def predict(self, *args, **kwargs):
-                pass
-
             def serialize(self):
                 ...
 
             def deserialize(self):
                 ...
 
-            def _fit_local(self):
+            @property
+            def training_needed(self):
                 ...
 
-            @staticmethod
-            def _get_import_statements():
-                ...
-
-            def _get_model():
-                ...
         """
     )
 
 
 @fixture
 def bitfount_model_incorrect_structure() -> str:
     """Example BitfountModel which doesn't implement DistributedModelProtocol.
@@ -304,32 +314,24 @@
 
             def initialise_model(self):
                 ...
 
             def evaluate(self):
                 ...
 
-            def predict(self):
-                pass
-
             def serialize(self):
                 ...
 
             def deserialize(self):
                 ...
 
-            def _fit_local(self):
+            @property
+            def training_needed(self):
                 ...
 
-            @staticmethod
-            def _get_import_statements():
-                ...
-
-            def _get_model():
-                ...
         """
     )
 
 
 @fixture
 def bitfount_model_correct_structure_size(bitfount_model_correct_structure: str) -> int:
     """Size of the correct structure BitfountModel fixture in storage."""
@@ -378,62 +380,7 @@
 
     This call is used to download models.
     """
     mock_s3_download: Mock = mocker.patch(
         "bitfount.hub.api._download_file_from_s3", autospec=True
     )
     return mock_s3_download
-
-
-class MockAuthenticationHandler(AuthenticationHandler):
-    """Mock authentication handler that assumes always authenticated."""
-
-    def __init__(self, username: str) -> None:
-        super().__init__(username)
-        self._username = username
-
-    @property
-    def username(self) -> str:
-        """See parent class."""
-        return self._username
-
-    @property
-    def hub_request_headers(self) -> Dict:
-        """See parent class."""
-        return {}
-
-    @property
-    def am_request_headers(self) -> Dict:
-        """See parent class."""
-        return {}
-
-    @property
-    def message_service_request_metadata(self) -> List[Tuple[str, str]]:
-        """See parent class."""
-        return []
-
-    def authenticate(self) -> None:
-        """See parent class."""
-        return
-
-    @property
-    def authenticated(self) -> bool:
-        """See parent class."""
-        return True
-
-
-def _mock__create_bitfount_session(
-    url: str,
-    username: Optional[str] = None,
-    secrets: Optional[Any] = None,
-) -> BitfountSession:
-    """Creates a Bitfount session with a mock auth handler."""
-    username = username if username else "_test_user"
-    return BitfountSession(MockAuthenticationHandler(username))
-
-
-@fixture
-def patch__create_bitfount_session_in_helper(monkeypatch: MonkeyPatch) -> None:
-    """Patches _create_bitfount_session to use the mock creator above."""
-    monkeypatch.setattr(
-        "bitfount.hub.helper._create_bitfount_session", _mock__create_bitfount_session
-    )
```

### Comparing `bitfount-0.5.86/tests/utils/fixtures/schema_fixtures.py` & `bitfount-0.5.9/tests/utils/fixtures/schema_fixtures.py`

 * *Files identical despite different names*

### Comparing `bitfount-0.5.86/tests/utils/helper.py` & `bitfount-0.5.9/tests/utils/helper.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,97 +1,84 @@
 """Testing helper functions."""
 import datetime
 import inspect
 from io import BytesIO
-import logging
-from pathlib import Path
 import random
 import string
-from typing import Any, List, Literal, Optional, Protocol, Sequence, Tuple, Union, cast
-from unittest.mock import Mock, create_autospec
+from typing import (
+    Any,
+    Dict,
+    List,
+    Literal,
+    Optional,
+    Protocol,
+    Sequence,
+    Tuple,
+    Union,
+    cast,
+)
+from unittest.mock import create_autospec
 
 from PIL import Image, ImageOps
-from cryptography.hazmat.primitives.asymmetric.rsa import RSAPrivateKey, RSAPublicKey
+from _pytest.logging import LogCaptureFixture
+from cryptography.hazmat.primitives.asymmetric.rsa import RSAPrivateKey
+import databricks.koalas as ks
 import numpy as np
 import pandas as pd
 import pytest
-from pytest import LogCaptureFixture
 
-from bitfount.data.datasources.base_source import BaseSource
-from bitfount.data.datasources.dataframe_source import DataFrameSource
+from bitfount.data.datasource import DataSource
 from bitfount.data.datastructure import DataStructure
 from bitfount.data.schema import BitfountSchema
-from bitfount.data.types import SchemaOverrideMapping
 from bitfount.federated.aggregators.aggregator import Aggregator
 from bitfount.federated.aggregators.base import _BaseAggregatorFactory
 from bitfount.federated.aggregators.secure import SecureAggregator
 from bitfount.federated.authorisation_checkers import (
     IdentityVerificationMethod,
     _LocalAuthorisation,
 )
 from bitfount.federated.early_stopping import FederatedEarlyStopping
-from bitfount.federated.modeller import _Modeller
+from bitfount.federated.modeller import Modeller
 from bitfount.federated.pod_response_message import _PodResponseMessage
 from bitfount.federated.secure import SecureShare
-from bitfount.federated.transport.worker_transport import (
-    _InterPodWorkerMailbox,
-    _WorkerMailbox,
-)
-from bitfount.federated.types import AlgorithmType, ProtocolType
+from bitfount.federated.task_requests import _ProtocolDetails
+from bitfount.federated.transport.worker_transport import _WorkerMailbox
+from bitfount.federated.types import AggregatorType, AlgorithmType, ProtocolType
 from bitfount.federated.utils import _ALGORITHMS, _MODELS, _PROTOCOLS
 from bitfount.federated.worker import _Worker
 from bitfount.hub.api import BitfountHub
-from bitfount.hub.types import _PublicKeyJSON
 from bitfount.metrics import (
     BINARY_CLASSIFICATION_METRICS,
     MULTICLASS_CLASSIFICATION_METRICS,
     REGRESSION_METRICS,
     MetricCollection,
 )
 from bitfount.models.base_models import ClassifierMixIn, Optimizer, _BaseModel
-from bitfount.types import DistributedModelProtocol, _StrAnyDict
-from bitfount.utils import ExampleSegmentationData, _add_this_to_list, seed_all
+from bitfount.types import DistributedModelProtocol, _DataFrameType, _SeriesType
+from bitfount.utils import _add_this_to_list, seed_all
 from tests.utils.mocks import LocalMessageService, LocalMessageServiceSharedQueues
 
 # PYTEST MARKS
 # tests involving a specific backend
 backend_test = pytest.mark.backend_test
-# tests involving differential privacy which is an optional extra
-dp_test = pytest.mark.dp_test
+
 # tests that run the full end-to-end system in the form of tutorials
 tutorial_test = pytest.mark.tutorial_test
 # tests that run the full end-to-end system, but mock out web calls
 end_to_end_mocks_test = pytest.mark.end_to_end_mocks_test
 # tests that run the full end-to-end system, including web calls
 end_to_end_test = pytest.mark.end_to_end_test
 # slower, integration- or training-like tests
 integration_test = pytest.mark.integration_test
 # faster, unit-test-like tests
 unit_test = pytest.mark.unit_test
-# Individual test markers for tutorial tests
-tutorial_1_test = pytest.mark.tutorial_1_test
-tutorial_2_test = pytest.mark.tutorial_2_test
-tutorial_3_test = pytest.mark.tutorial_3_test
-tutorial_4_test = pytest.mark.tutorial_4_test
-tutorial_5_test = pytest.mark.tutorial_5_test
-tutorial_6_test = pytest.mark.tutorial_6_test
-tutorial_7_test = pytest.mark.tutorial_7_test
-tutorial_8_test = pytest.mark.tutorial_8_test
-tutorial_9_test = pytest.mark.tutorial_9_test
-tutorial_10_test = pytest.mark.tutorial_10_test
-tutorial_11_test = pytest.mark.tutorial_11_test
-tutorial_12_test = pytest.mark.tutorial_12_test
-tutorial_13_test = pytest.mark.tutorial_13_test
-tutorial_14_test = pytest.mark.tutorial_14_test
-tutorial_15_test = pytest.mark.tutorial_15_test
-
 
 DATASET_ROW_COUNT = 4000
 DIMS = (DATASET_ROW_COUNT, 4)
-AUC_THRESHOLD = 0.6
+AUC_THRESHOLD = 0.65
 MAE_THRESHOLD = 0.2
 
 PRIVATE_KEY = inspect.cleandoc(
     """-----BEGIN RSA PRIVATE KEY-----
     MIIEpQIBAAKCAQEAwA0cNMjzEm+LPFolbdOAcIB6hX7QGqkzVC8L8W5X2qFFDrSb
     TI9CSifQ3/1A+xYfiZrR4Za2cCvCtIzHxVctEAcQSW+IDRjh5vvdGTKgJMtaf3YH
     eXMk9NAN4Px6McuqaBE2llYt6okICha8MEUR9lffa761WkOOWnWAXQFUWxWOxuVn
@@ -195,160 +182,76 @@
     "P+24rUb3dN9twqK6lEENG/0XIY1XQ4H+3c4zPs+Lf7hgJhRaFObw9DHZxJOBI"
     "0BGEQrSsrY//IuEpxy6p+0wDDaoWxWjtTh4iycUpQCVgEl5FR4ewg+tzhTrpW"
     "fqrfNWMvhy9bmbCJ0b3px5cUJ42szQCI7IgCkYBiiJ0iZ+KwYvrGDCtPvIQk0"
     "r3nbV7BTnjjonSUJ8FcLh7AmEQ+oUlIw3d7t4015QboQmXCum2pN71Oy2Y/Je"
     "me6L/3NtAnOYTppgP5wVxFbMYt8N4eLbh2Hf0/m20NpuctT/SDw=="
     " test_open_ssh@example.com"
 )
-TABLE_NAME: str = "test_table"
 
 
 def create_dataset(
     classification: bool = True,
     seed: int = 420,
+    koalas: bool = False,
     image: bool = False,
-    file_image: bool = False,
     multiimage: bool = False,
     multihead: bool = False,
     img_size: int = 50,
-    grayscale_image: bool = False,
-    path: Optional[Path] = None,
-    dims: Tuple[int, int] = (DATASET_ROW_COUNT, 4),
-) -> pd.DataFrame:
+) -> _DataFrameType:
     """Creates a random (seeded) dataset for testing."""
     seed_all(seed)
 
     base_date = datetime.date(2019, 1, 1)
     date_list = [
-        base_date - datetime.timedelta(days=x) for x in range(int(dims[0] / 3))
+        base_date - datetime.timedelta(days=x) for x in range(int(DIMS[0] / 3))
     ] * 3
     df_date = pd.DataFrame(date_list, columns=["Date"])
-    df_int = pd.DataFrame(np.random.randint(1, 1000, size=dims), columns=list("ABCD"))
-    df_float = pd.DataFrame(np.random.uniform(0, 1, size=dims), columns=list("EFGH"))
+    df_int = pd.DataFrame(np.random.randint(1, 1000, size=DIMS), columns=list("ABCD"))
+    df_float = pd.DataFrame(np.random.uniform(0, 1, size=DIMS), columns=list("EFGH"))
     df_str = pd.DataFrame(
-        np.random.choice(list(string.ascii_lowercase), size=dims).tolist(),
+        np.random.choice(list(string.ascii_lowercase), size=DIMS).tolist(),
         columns=list("IJKL"),
     )
     df_bool = pd.DataFrame(
-        list(np.random.choice([True, False], size=dims)), columns=list("MNOP")
+        list(np.random.choice([True, False], size=DIMS)), columns=list("MNOP")
     )
-    target = pd.DataFrame(list(np.zeros(shape=dims[0], dtype=int)), columns=["TARGET"])
+    target = pd.DataFrame(list(np.zeros(shape=DIMS[0], dtype=int)), columns=["TARGET"])
 
     data: pd.DataFrame = pd.concat(
         [df_date, df_int, df_float, df_str, df_bool, target], axis=1
     )
 
     if classification:
         data.loc[(data.A > 500) & (data.F < 0.5) & (data.D % 2 == 0), "TARGET"] = 1
     else:
         data["TARGET"] = data.E + data.F - (1 / data.B) / np.random.uniform()
     size = img_size if img_size is not None else 50
     if image or multiimage:
 
-        def create_image(
-            x: pd.DataFrame, size: int = size, path: Optional[Path] = None
-        ) -> Union[BytesIO, str]:
-            def sigmoid(n: pd.Series) -> int:
+        def create_image(x: _DataFrameType, size: int = size) -> BytesIO:
+            def sigmoid(n: _SeriesType) -> int:
                 return int(255 / (1 + np.exp(-n)))
 
             # Use column "M" as defining whether the image is grayscale
             R, G, B = sigmoid(x["A"] / 100), sigmoid(x["F"]), sigmoid(x["D"] / 100)
             image = Image.new("RGB", size=(size, size), color=(R, G, B))
-            if grayscale_image:
+            if x["M"]:
                 image = ImageOps.grayscale(image)
-            if not path:
-                temp = BytesIO()
-                image.save(temp, format="png")
-                return temp
-            else:
-                index = random.randint(0, 10**4)
-                temp_path = path / f"{index}.png"
-                image.save(temp_path, format="png")
-                return str(temp_path)
+            temp = BytesIO()
+            image.save(temp, format="png")
+            return temp
 
         if image:
-            if file_image and path:
-                data["image"] = data.apply(create_image, args=[size, path], axis=1)
-            else:
-                data["image"] = data.apply(create_image, args=[size], axis=1)
-
+            data["image"] = data.apply(create_image, args=[size], axis=1)
         if multiimage:
-            if file_image and path:
-                data["image1"] = data.apply(create_image, args=[size, path], axis=1)
-                data["image2"] = data.apply(create_image, args=[size, path], axis=1)
-            else:
-                data["image1"] = data.apply(create_image, args=[size], axis=1)
-                data["image2"] = data.apply(create_image, args=[size], axis=1)
+            data["image1"] = data.apply(create_image, args=[size], axis=1)
+            data["image2"] = data.apply(create_image, args=[size], axis=1)
     if multihead:
         data = data.assign(category=list(np.random.choice(["A", "B"], size=len(data))))
-    return data
-
-
-def create_dataset_pii(
-    seed: int = 420,
-) -> pd.DataFrame:
-    """Creates a PII dataset with (seeded) random values for testing."""
-    seed_all(seed)
-
-    # Create a list of private identifiers
-    df_names = pd.DataFrame(
-        np.asarray(range(1, 5001)),
-        columns=["name"],
-    )
-    df_int = pd.DataFrame(
-        np.random.randint(1, 1000, size=(5000, 1)), columns=["int_field"]
-    ).astype("int")
-    df_str = pd.DataFrame(
-        np.random.choice(list(string.ascii_lowercase), size=(5000, 1)).tolist(),
-        columns=["str_field"],
-    ).astype("str")
-
-    # mypy_reason: The pandas stubs are overzealous and don't represent the fact that
-    #              astype can take a string that resolves to a numpy type
-    df_obj = pd.DataFrame(
-        np.random.choice(list(string.ascii_lowercase), size=(5000, 1)).tolist(),
-        columns=["obj_field"],
-    ).astype(
-        "object"  # type: ignore[arg-type] # Reason: see comment
-    )
-
-    df_age = pd.DataFrame(np.random.randint(1, 110, size=(5000, 1)), columns=["age"])
-    df_height = pd.DataFrame(
-        np.random.triangular(left=25, mode=168, right=230, size=(5000, 1)),
-        columns=["height"],
-    )
-    df_weight = pd.DataFrame(
-        np.random.triangular(left=1.8, mode=70, right=230, size=(5000, 1)),
-        columns=["weight"],
-    )
-
-    exercise_choices = [
-        "none",
-        "run",
-        "cycle",
-        "swim",
-        "row",
-        "weights",
-        "elliptical",
-        "yoga",
-        "pilates",
-        "sport",
-        "other",
-    ]
-    exercise_priors = [0.2, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08]
-    df_exercise = pd.DataFrame(
-        np.random.choice(exercise_choices, size=(5000, 1), p=exercise_priors),
-        columns=["exercise"],
-    )
-    data: pd.DataFrame = pd.concat(
-        [df_names, df_age, df_weight, df_height, df_exercise, df_int, df_str, df_obj],
-        axis=1,
-    )
-
-    return data
+    return ks.from_pandas(data) if koalas else data
 
 
 def create_datastructure(
     multilabel: bool = False,
     multihead: bool = False,
     multihead_size: int = 1,
     loss_weights: bool = False,
@@ -365,239 +268,146 @@
     if multihead:
         multihead_col = "category"
 
     if multilabel:
         target = [cast(str, target), "TARGET_2"]
 
     return DataStructure(
-        table=TABLE_NAME,
         target=target,
         ignore_cols=ignore_cols,
         multihead_col=multihead_col,
         multihead_size=multihead_size,
         loss_weights_col=loss_weights_col,
     )
 
 
-def create_query_datastructure(
-    multilabel: bool = False,
-    multihead: bool = False,
-    multihead_size: int = 1,
-    loss_weights: bool = False,
-) -> DataStructure:
-    """This method creates and returns a DataStructure object."""
-    target: Union[str, List[str]] = "target"
-    loss_weights_col = None
-    multihead_col = None
-    query = """SELECT dd1."A" as a1, dd1."D" as d1, dd1."M" as m1,
-                dd1."N" as n1, dd1."TARGET" as target,
-                dd2."A" as a2, dd2."D" as d2, dd2."M" as m2, dd2."N" as n2
-                FROM dummy_data dd1, dummy_data_2 dd2
-                WHERE dd1."Date" = dd2."Date"
-    """
-    feature_override: SchemaOverrideMapping = {
-        "continuous": ["a1", "d1", "a2", "d2"],
-        "categorical": [
-            {"m1": {"True": 1, "False": 0}},
-            {"n1": {"True": 1, "False": 0}},
-            {"m2": {"True": 1, "False": 0}},
-            {"n2": {"True": 1, "False": 0}},
-            {"target": {"0": 0, "1": 1}},
-        ],
-    }
-    if loss_weights:
-        loss_weights_col = "weights"
-
-    if multihead:
-        multihead_col = "category"
-
-    if multilabel:
-        target = [cast(str, target), "TARGET_2"]
-
-    return DataStructure(
-        query=query,
-        schema_types_override=feature_override,
-        target=target,
-        multihead_col=multihead_col,
-        multihead_size=multihead_size,
-        loss_weights_col=loss_weights_col,
-    )
-
-
 def create_datasource(
     classification: bool,
     multilabel: bool = False,
     multihead: bool = False,
+    koalas: bool = False,
     loss_weights: bool = False,
     image: bool = False,
     multiimage: bool = False,
-) -> BaseSource:
-    """This method creates and returns a BaseSource object."""
+) -> DataSource:
+    """This method creates and returns a DataSource object."""
     image_col: Optional[List[str]]
     if image:
         data = create_dataset(
-            classification=classification, image=image, multihead=multihead
+            classification=classification,
+            koalas=koalas,
+            image=image,
+            multihead=multihead,
         )
         image_col = ["image"]
     elif multiimage:
         data = create_dataset(
-            classification=classification, multiimage=multiimage, multihead=multihead
+            classification=classification,
+            koalas=koalas,
+            multiimage=multiimage,
+            multihead=multihead,
         )
         image_col = ["image1", "image2"]
     else:
-        data = create_dataset(classification=classification, multihead=multihead)
+        data = create_dataset(
+            classification=classification, koalas=koalas, multihead=multihead
+        )
         image_col = None
 
     if loss_weights:
         data = data.assign(weights=1.0)
 
     if multihead:
         data = data.assign(category=list(np.random.choice(["A", "B"], size=len(data))))
 
     if multilabel:
         data = data.assign(TARGET_2=np.zeros(len(data)))
         data.loc[(data.A < 700) & (data.F < 0.5) & (data.D % 2 == 1), "TARGET_2"] = 1
 
-    dataset = DataFrameSource(data, seed=420, image_col=image_col)
-
-    return dataset
-
-
-def create_datasource_pii() -> BaseSource:
-    """This method creates and returns a PII DataFrameSource dataset."""
-    data = create_dataset_pii()
-
-    dataset = DataFrameSource(data, seed=420, image_col=None)
-
-    return dataset
-
-
-def create_datasource_prompts() -> BaseSource:
-    """This method creates and returns an LLM Prompt DataFrmeSource dataset."""
-    prompts = ["This is a prompt example ", "And another prompt example"]
-    data = pd.DataFrame({"TARGET": prompts})
-
-    dataset = DataFrameSource(data)
+    dataset = DataSource(data, seed=420, koalas=koalas, image_col=image_col)
 
     return dataset
 
 
 def create_schema(
     classification: bool,
     multilabel: bool = False,
     multihead: bool = False,
     loss_weights: bool = False,
+    koalas: bool = False,
 ) -> BitfountSchema:
     """Helper function for creating datastructure and datasource."""
     datastructure = create_datastructure(multilabel, multihead, loss_weights)
-    datasource = create_datasource(classification, multilabel, multihead, loss_weights)
+    datasource = create_datasource(
+        classification,
+        multilabel,
+        multihead,
+        koalas,
+        loss_weights,
+    )
     schema = BitfountSchema()
-    force_stypes = {}
+    force_stype = {}
     force_categorical: List[Optional[str]] = []
     if classification:
         force_categorical = _add_this_to_list(datastructure.target, force_categorical)
         if multihead:
             force_categorical = _add_this_to_list(
                 datastructure.multihead_col, force_categorical
             )
 
     if force_categorical:
-        force_stypes = {TABLE_NAME: {"categorical": force_categorical}}
-    datasource.load_data()
-    schema.add_datasource_tables(
-        datasource=datasource,
-        table_name=TABLE_NAME,
-        force_stypes=force_stypes,  # type: ignore[arg-type] # Reason: this will always have the needed type after the ifs. # noqa: B950
-    )
-
-    return schema
-
-
-def create_schema_pii() -> BitfountSchema:
-    """Helper function for creating datastructure and datasource for a PII dataset."""
-    datasource = create_datasource_pii()
-    schema = BitfountSchema()
-
-    datasource.load_data()
-    schema.add_datasource_tables(
-        datasource=datasource,
-        table_name=TABLE_NAME,
-        force_stypes=None,
+        force_stype = {"categorical": force_categorical}
+    schema.add_datasource_features(
+        datasource,
+        ignore_cols=datastructure.ignore_cols,
+        force_stype=force_stype,  # type: ignore[arg-type] # Reason: this will always have the needed type after the ifs. # noqa: B950
     )
 
     return schema
 
 
 def get_datastructure_and_datasource(
     classification: bool,
     multilabel: bool = False,
     multihead: bool = False,
     multihead_size: int = 1,
     loss_weights: bool = False,
-) -> Tuple[DataStructure, BaseSource]:
+    koalas: bool = False,
+) -> Tuple[DataStructure, DataSource]:
     """Helper function for creating datastructure and datasource."""
     datastructure = create_datastructure(
         multilabel, multihead, multihead_size, loss_weights
     )
-    datasource = create_datasource(classification, multilabel, multihead, loss_weights)
-    return datastructure, datasource
-
-
-def create_segmentation_dataset(
-    seg_dir: Path, height: int = 100, width: int = 100, count: int = 25
-) -> pd.DataFrame:
-    """Create a segmentation dataset."""
-    seg = ExampleSegmentationData()
-    input_images, target_masks = seg.generate_data(height, width, count=count)
-    input_images_rgb = [x.astype(np.uint8) for x in input_images]
-    target_masks_rgb = [seg.masks_to_colorimg(x.astype(np.uint8)) for x in target_masks]
-    img_names_list = []
-    masks_names_list = []
-    # Save images
-    for i in range(count):
-        im2 = Image.fromarray((input_images_rgb[i]).astype(np.uint8))
-        im2.save(f"{seg_dir}/img_{i}.png")
-        img_names_list.append(f"img_{i}.png")
-    # Save masks
-    for i in range(count):
-        im2 = Image.fromarray((target_masks_rgb[i]).astype(np.uint8))
-        im2.save(f"{seg_dir}/masks_{i}.png")
-        masks_names_list.append(f"masks_{i}.png")
-
-    # Create dataframe with image and masks locations
-    df = pd.DataFrame(
-        {
-            "img": [str(seg_dir) + "/" + img_name for img_name in img_names_list],
-            "masks": [str(seg_dir) + "/" + mask_name for mask_name in masks_names_list],
-        },
-        columns=["img", "masks"],
+    datasource = create_datasource(
+        classification,
+        multilabel,
+        multihead,
+        koalas,
+        loss_weights,
     )
-    return df
+    return datastructure, datasource
 
 
 def create_local_modeller_and_workers(
     model_name: str,
     protocol_name: str,
     algorithm_name: str,
     secure_aggregation: bool = False,
     early_stopping: bool = False,
     auto_eval: bool = True,
-) -> Tuple[_Modeller, List[_Worker]]:
+) -> Tuple[Modeller, List[_Worker]]:
     """Creates a modeller and workers to run in tests."""
     pod_ids = ["Alice/AlicePod", "Bob/BobPod"]
-    pod_public_keys = {
-        pod_id: create_autospec(RSAPublicKey, instance=True) for pod_id in pod_ids
-    }
     results_only: bool = bool(protocol_name == "ResultsOnly")
 
     # Seed everything for consistency
     seed_all(43)
 
     # Create default hyperparams
-    hyperparams: _StrAnyDict = {
+    hyperparams: Dict[str, Any] = {
         "epochs": 1,
         "batch_size": 32,
         "optimizer": Optimizer("RAdam", {"lr": 0.001}),
     }
 
     # Add to hyperparams based on model type
     if "RandomForest" in model_name or "LogisticRegression" in model_name:
@@ -608,148 +418,133 @@
             )
         hyperparams = {}
 
     # Create base data variables
     data = create_dataset(classification=True)
     schema = BitfountSchema()
     pod_datasets = []
-    table_mapping = {"Alice/AlicePod": TABLE_NAME, "Bob/BobPod": TABLE_NAME}
+
     if results_only:
         # ResultsOnly only works with one pod
         pod_ids = ["Alice/AlicePod"]
-        table_mapping = {"Alice/AlicePod": TABLE_NAME}
 
     # Create datasets for each pod and combine them in the schema
     for _ in pod_ids:
-        dataset = DataFrameSource(
-            data.sample(n=1000, random_state=random.randint(1, 101))
-        )
+        dataset = DataSource(data.sample(n=1000, random_state=random.randint(1, 101)))
         pod_datasets.append(dataset)
-
         # to make sure modeller schema corresponds to the worker schema
-        schema.add_datasource_tables(
+        schema.add_datasource_features(
             dataset,
-            table_name=TABLE_NAME,
-            force_stypes={TABLE_NAME: {"categorical": ["TARGET"]}},
-            ignore_cols={TABLE_NAME: ["Date"]},
+            force_stype={"categorical": ["TARGET"]},
+            ignore_cols=["Date", "TARGET"],
         )
-    data_structure = DataStructure(
-        table=table_mapping, target="TARGET", ignore_cols=["Date"]
-    )
+    data_structure = DataStructure(target="TARGET", ignore_cols=["Date"])
 
     # Create local message service object
     mailbox_queues = LocalMessageServiceSharedQueues()
     modeller_name = "modeller-name"
     modeller_mailbox_id = "modeller_mailbox_id"
     # Just use the same key for each as we're running local tests
     aes_encryption_key = b"aes_encryption_key"
 
+    if secure_aggregation:
+        task_request = _ProtocolDetails(
+            ProtocolType[protocol_name].value,
+            AlgorithmType[algorithm_name].value,
+            aggregator=AggregatorType["SecureAggregator"].value,
+            model=f"bitfount.{model_name}",
+        )
+    elif protocol_name == "FederatedAveraging":
+        task_request = _ProtocolDetails(
+            ProtocolType[protocol_name].value,
+            AlgorithmType[algorithm_name].value,
+            aggregator=AggregatorType["Aggregator"].value,
+            model=f"bitfount.{model_name}",
+        )
+    else:
+        task_request = _ProtocolDetails(
+            ProtocolType[protocol_name].value,
+            AlgorithmType[algorithm_name].value,
+            model=f"bitfount.{model_name}",
+        )
+    # Create workers directly
+    workers = []
+    for pod_identifier, dataset in zip(pod_ids, pod_datasets):
+        message_service = LocalMessageService(
+            username=pod_identifier.split("/")[0],
+            shared_queues=mailbox_queues,
+            modeller_mailbox_id=modeller_mailbox_id,
+            pod_ids=pod_ids,
+        )
+
+        worker = _Worker(
+            data=dataset,
+            mailbox=_WorkerMailbox(
+                pod_identifier=pod_identifier,
+                modeller_mailbox_id=modeller_mailbox_id,
+                modeller_name=modeller_name,
+                aes_encryption_key=aes_encryption_key,
+                message_service=message_service,
+                pod_mailbox_ids=message_service.worker_mailbox_ids,
+            ),
+            bitfounthub=create_autospec(BitfountHub, instance=True),
+            authorisation=_LocalAuthorisation(
+                _PodResponseMessage(modeller_name, pod_identifier),
+                task_request,
+            ),
+        )
+        workers.append(worker)
+
     # Handle early stopping
     early_stopping_ = None
     if early_stopping:
         early_stopping_ = FederatedEarlyStopping("validation_loss", 1, 0.01)
 
     # Create model
     model_class = _MODELS[model_name]
     model = model_class(datastructure=data_structure, schema=schema, **hyperparams)
-
     # Create algorithm
     algorithm = _ALGORITHMS[AlgorithmType[algorithm_name].name](
         model=model,
         epochs_between_parameter_updates=1,
     )
 
     # Create protocol
-    protocol_kwargs: _StrAnyDict = {}
+    protocol_kwargs: Dict[str, Any] = {}
     if protocol_name == "FederatedAveraging":
         assert isinstance(model, DistributedModelProtocol)
         if secure_aggregation:
-            sec_share = SecureShare()
+            sec_share = SecureShare(tensor_shim=model.backend_tensor_shim())
             aggregator: _BaseAggregatorFactory = SecureAggregator(
-                secure_share=sec_share
+                secure_share=sec_share, tensor_shim=model.backend_tensor_shim()
             )
         else:
-            aggregator = Aggregator()
+            aggregator = Aggregator(tensor_shim=model.backend_tensor_shim())
 
         protocol_kwargs = {
             "epochs_between_parameter_updates": 1,
             "aggregator": aggregator,
             "auto_eval": auto_eval,
         }
     protocol = _PROTOCOLS[ProtocolType[protocol_name].name](
         early_stopping=early_stopping_, algorithm=algorithm, **protocol_kwargs
     )
 
-    # Create workers directly
-    workers = []
-    serialized_protocol = protocol.dump()
-    for pod_identifier, dataset in zip(pod_ids, pod_datasets):
-        message_service = LocalMessageService(
-            username=pod_identifier.split("/")[0],
-            shared_queues=mailbox_queues,
-            modeller_mailbox_id=modeller_mailbox_id,
-            pod_ids=pod_ids,
-        )
-
-        worker_mailbox: _WorkerMailbox
-        if secure_aggregation:
-            worker_mailbox = _InterPodWorkerMailbox(
-                pod_public_keys=pod_public_keys,
-                private_key=create_autospec(RSAPrivateKey, instance=True),
-                pod_identifier=pod_identifier,
-                modeller_mailbox_id=modeller_mailbox_id,
-                modeller_name=modeller_name,
-                aes_encryption_key=aes_encryption_key,
-                message_service=message_service,
-                pod_mailbox_ids=message_service.worker_mailbox_ids,
-                task_id="this-is-a-task-id",
-            )
-        else:
-            worker_mailbox = _WorkerMailbox(
-                pod_identifier=pod_identifier,
-                modeller_mailbox_id=modeller_mailbox_id,
-                modeller_name=modeller_name,
-                aes_encryption_key=aes_encryption_key,
-                message_service=message_service,
-                pod_mailbox_ids=message_service.worker_mailbox_ids,
-                task_id="this-is-a-task-id",
-            )
-
-        worker = _Worker(
-            datasource=dataset,
-            schema=schema,
-            mailbox=worker_mailbox,
-            bitfounthub=create_autospec(BitfountHub, instance=True),
-            authorisation=_LocalAuthorisation(
-                _PodResponseMessage(modeller_name, pod_identifier),
-                serialized_protocol,
-            ),
-            parent_pod_identifier=pod_identifier,
-            serialized_protocol=serialized_protocol,
-        )
-        workers.append(worker)
-
     # Create modeller
-    mock_hub: Mock = create_autospec(BitfountHub, instance=True)
-    mock_hub.username = modeller_name
-    mock_modeller_private_key: Mock = create_autospec(RSAPrivateKey, instance=True)
-    mock_hub.check_public_key_registered_and_active.return_value = _PublicKeyJSON(
-        public_key=mock_modeller_private_key.public_key(), id="1", active=True
-    )
-    modeller = _Modeller(
+    modeller = Modeller(
         protocol=protocol,
         message_service=LocalMessageService(
             username=modeller_name,
             shared_queues=mailbox_queues,
             modeller_mailbox_id=modeller_mailbox_id,
             pod_ids=pod_ids,
         ),
-        bitfounthub=mock_hub,
+        bitfounthub=create_autospec(BitfountHub, instance=True),
         identity_verification_method=IdentityVerificationMethod.KEYS,
-        private_key=mock_modeller_private_key,
+        private_key=create_autospec(RSAPrivateKey, instance=True),
     )
 
     return modeller, workers
 
 
 def assert_results(
     model: _BaseModel,
@@ -757,19 +552,18 @@
     test_target: Optional[Union[Sequence[float], Sequence[int]]] = None,
 ) -> None:
     """Assert results of a fitted model.
 
     This method takes a fitted model, evaluates it and performs some checks
     on the output.
     """
+    preds: np.ndarray
+    target: np.ndarray
     if test_preds is None or test_target is None:
         preds, target = model.evaluate()
-        # TODO: [BIT-1604] Remove these assert statements once they become superfluous.
-        assert isinstance(preds, np.ndarray)
-        assert isinstance(target, np.ndarray)
     else:
         preds = test_preds
         target = np.asarray(test_target)
     metrics = MetricCollection.create_from_model(model)
     results = metrics.compute(target, preds)
     assert results is not None
     assert isinstance(results, dict)
@@ -786,15 +580,15 @@
         assert mae < MAE_THRESHOLD
 
 
 class CallArgsLike(Protocol):
     """A protocol that defines a unittest.mock.call_args-like interface."""
 
     args: List[Any]
-    kwargs: _StrAnyDict
+    kwargs: Dict[str, Any]
 
 
 def get_arg_from_args_or_kwargs(
     call_args: CallArgsLike, args_idx: int, kwarg_name: str
 ) -> Any:
     """Retrieves a mock function call argument regardless of how it was passed.
 
@@ -832,115 +626,74 @@
     Literal["WARNING"],
     Literal["ERROR"],
     Literal["CRITICAL"],
 ]
 
 
 def get_logs_at_level(
-    caplog_fixture: LogCaptureFixture,
-    levelname: _LOG_LEVEL_NAMES,
-    full_details: bool = False,
-    and_higher: bool = False,
+    caplog_fixture: LogCaptureFixture, levelname: _LOG_LEVEL_NAMES
 ) -> str:
     """Returns all logs at a given level as a single string.
 
     Args:
         caplog_fixture: The caplog fixture to extract from.
         levelname: The name of the log level to retrieve messages of.
-        full_details: Whether to extract only the log record message or the full details
-                      (timestamp, exception info, etc.). Uses the default formatter
-                      to get the full log record output.
-        and_higher: Whether to get logs only at target level or at any level greater
-                    than or equal to the target level.
 
     Returns:
-        All the log messages at the target level (or higher) as a newline-joined
-        string.
+         All the log messages at the target level as a newline-joined string.
     """
-    if not and_higher:
-        level_records = [
-            record for record in caplog_fixture.records if record.levelname == levelname
-        ]
-    else:
-        level_records = [
-            record
-            for record in caplog_fixture.records
-            if record.levelno >= logging.getLevelName(levelname)
-        ]
-
+    level_records = [
+        record for record in caplog_fixture.records if record.levelname == levelname
+    ]
     # Need to interpolate/format the messages
-    if full_details:
-        formatter = logging.Formatter()
-        level_messages = [formatter.format(r) for r in level_records]
-    else:
-        level_messages = [r.getMessage() for r in level_records]
+    level_messages = [r.getMessage() for r in level_records]
     return "\n".join(level_messages)
 
 
-def get_debug_logs(
-    caplog_fixture: LogCaptureFixture,
-    full_details: bool = False,
-    and_higher: bool = False,
-) -> str:
+def get_debug_logs(caplog_fixture: LogCaptureFixture) -> str:
     """Returns all logs at debug level as a single string.
 
     All the log messages are combined as a newline-joined string.
 
     See `get_logs_at_level` for more information.
     """
-    return get_logs_at_level(caplog_fixture, "DEBUG", full_details, and_higher)
+    return get_logs_at_level(caplog_fixture, "DEBUG")
 
 
-def get_info_logs(
-    caplog_fixture: LogCaptureFixture,
-    full_details: bool = False,
-    and_higher: bool = False,
-) -> str:
+def get_info_logs(caplog_fixture: LogCaptureFixture) -> str:
     """Returns all logs at info level as a single string.
 
     All the log messages are combined as a newline-joined string.
 
     See `get_logs_at_level` for more information.
     """
-    return get_logs_at_level(caplog_fixture, "INFO", full_details, and_higher)
+    return get_logs_at_level(caplog_fixture, "INFO")
 
 
-def get_warning_logs(
-    caplog_fixture: LogCaptureFixture,
-    full_details: bool = False,
-    and_higher: bool = False,
-) -> str:
+def get_warning_logs(caplog_fixture: LogCaptureFixture) -> str:
     """Returns all logs at warning level as a single string.
 
     All the log messages are combined as a newline-joined string.
 
     See `get_logs_at_level` for more information.
     """
-    return get_logs_at_level(caplog_fixture, "WARNING", full_details, and_higher)
+    return get_logs_at_level(caplog_fixture, "WARNING")
 
 
-def get_error_logs(
-    caplog_fixture: LogCaptureFixture,
-    full_details: bool = False,
-    and_higher: bool = False,
-) -> str:
+def get_error_logs(caplog_fixture: LogCaptureFixture) -> str:
     """Returns all logs at error level as a single string.
 
     All the log messages are combined as a newline-joined string.
 
     See `get_logs_at_level` for more information.
     """
-    return get_logs_at_level(caplog_fixture, "ERROR", full_details, and_higher)
+    return get_logs_at_level(caplog_fixture, "ERROR")
 
 
-def get_critical_logs(
-    caplog_fixture: LogCaptureFixture,
-    full_details: bool = False,
-    and_higher: bool = False,
-) -> str:
+def get_critical_logs(caplog_fixture: LogCaptureFixture) -> str:
     """Returns all logs at critical level as a single string.
 
     All the log messages are combined as a newline-joined string.
 
     See `get_logs_at_level` for more information.
     """
-    return get_logs_at_level(caplog_fixture, "CRITICAL", full_details, and_higher)
+    return get_logs_at_level(caplog_fixture, "CRITICAL")
```

### Comparing `bitfount-0.5.86/tests/utils/mocks.py` & `bitfount-0.5.9/bitfount/federated/transport/message_service.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,713 +1,677 @@
-"""File containing mock objects for the purposes of testing."""
+"""Interface classes and methods for the message service.
+
+Provides functionality to wrap generated GRPC code and python-friendly versions
+of GRPC classes and methods.
+"""
 from __future__ import annotations
 
 import asyncio
-from collections.abc import Awaitable
-import dataclasses
-import inspect
-from inspect import Parameter
+from dataclasses import dataclass, field
+from datetime import datetime
+from enum import Enum
 import logging
-import multiprocessing
-from multiprocessing.managers import DictProxy, ListProxy, SyncManager
-import queue
-import threading
-import time
+import os
+import platform
+import tempfile
 from typing import (
+    TYPE_CHECKING,
     Any,
     AsyncGenerator,
-    AsyncIterator,
     Dict,
-    Generator,
-    Generic,
-    Iterable,
-    Iterator,
-    List,
+    Final,
     Mapping,
     Optional,
-    Sequence,
     Tuple,
-    Type,
-    TypeVar,
     Union,
     cast,
 )
-from unittest.mock import AsyncMockMixin, NonCallableMock, create_autospec
-import uuid
+from urllib.parse import urlparse
 
 from grpc import RpcError, StatusCode
-from typing_extensions import TypeAlias
-
-from bitfount.federated.transport.message_service import (
-    _BitfountMessage,
-    _BitfountMessageType,
-    _MessageService,
-)
+import msgpack
+import psutil
+from requests.exceptions import RequestException
+
+from bitfount.config import get_gpu_metadata
+from bitfount.federated.encryption import _AESEncryption
+from bitfount.federated.exceptions import PodConnectFailedError
+from bitfount.federated.logging import _get_federated_logger
+from bitfount.federated.transport.config import MessageServiceConfig
 from bitfount.federated.transport.protos.messages_pb2 import (
-    Acknowledgement,
     BitfountMessage as GrpcBitfountMessage,
+)
+from bitfount.federated.transport.protos.messages_pb2 import (
+    BitfountTask,
     BitfountTasks,
-    BlobStorageData,
-    CommunicationDetails as GrpcCommunicationDetails,
+    CommunicationDetails,
     LargeStorageRequest,
     PodData,
     SuccessResponse,
-    TaskMetadata,
-    TaskTransferMetadata,
-    TaskTransferRequests,
 )
+from bitfount.federated.transport.protos.messages_pb2 import Acknowledgement
 from bitfount.federated.transport.protos.messages_pb2_grpc import MessageServiceStub
-from bitfount.federated.transport.types import CommunicationDetails
-from bitfount.utils.concurrency_utils import await_threading_event
+from bitfount.storage import (
+    _download_data_from_s3,
+    _get_packed_data_object_size,
+    _upload_data_to_s3,
+)
+from bitfount.types import _S3PresignedPOSTFields, _S3PresignedPOSTURL, _S3PresignedURL
+from bitfount.utils import _get_mb_from_bytes
 
-logger = logging.getLogger(__name__)
+if TYPE_CHECKING:
+    from bitfount.hub.authentication_flow import BitfountSession
 
+logger = _get_federated_logger(__name__)
 
-class LocalMessageServiceSharedQueues:
-    """Class that simulates message mailboxes with asyncio.Queues."""
+# This is used for sanity checking/detecting when the message body is an S3 URL.
+# However, it doesn't give us any security guarantees as this suffix may not be
+# unique to us.
+_S3_NETLOC_SUFFIX: str = "-message-service-external.s3.eu-west-2.amazonaws.com"
 
-    def __init__(self) -> None:
-        """Create new LocalMessageServiceSharedQueues instance."""
-        self._queues: Dict[str, queue.SimpleQueue[_BitfountMessage]] = {}
-        self._lock = threading.RLock()
-
-    def send_message(self, mailbox_id: str, message: _BitfountMessage) -> None:
-        """Sends a message to the queue for mailbox_id."""
-        with self._lock:
-            mailbox = self._queues[mailbox_id]
-        mailbox.put_nowait(message)
-
-    async def get_message(self, mailbox_id: str) -> _BitfountMessage:
-        """Awaits on a message to pull from queue for mailbox_id."""
-        with self._lock:
-            mailbox = self._queues[mailbox_id]
+# This limit is lower than the SQS message limit intentionally.
+# When messages are packaged on the message service we attach some metadata,
+# the metadata will make them larger, so the python code needs a lower limit.
+# So this limit ensures that our message service will accept the raw messages.
+_SMALL_MESSAGE_UPPER_LIMIT_SIZE_BYTES: int = 100 * 1024  # 100kb
 
-        while True:
-            try:
-                message = mailbox.get_nowait()
-                logger.debug(
-                    f"Retrieved message ({message.message_type})"
-                    f" from {message.sender}"
-                    f" from mailbox {mailbox_id}"
-                )
-                return message
-            except queue.Empty:
-                await asyncio.sleep(0)
+# Maximum storage request size in message service: 3gb. This should match the value
+# in bitfount/message-service:src/messaging/queue.service.ts
+_MAX_STORAGE_SIZE_BYTES: Final[int] = 3 * 1024 * 1024 * 1024
+_MAX_STORAGE_SIZE_MEGABYTES: Final[int] = _get_mb_from_bytes(
+    _MAX_STORAGE_SIZE_BYTES
+).whole
+
+# Create fibonacci backoffs to avoid constant querying for messages
+_POLLING_BACKOFFS: Tuple[int, ...] = (1, 2, 3, 5, 8, 13, 21)
+
+
+def _current_time() -> str:
+    """Gets current time as factory for dataclass timestamps."""
+    return datetime.now().isoformat()
+
+
+class _BitfountMessageType(Enum):
+    """Used for providing information on the expected shape of a Message.
+
+    These can be used to quickly identify the expected type of an unstructured message
+    body from the message service.
+
+    NB: These message types are also defined in the message service and the proto file
+    so if you are making a change, you need to reflect the change in those other
+    locations too.
+    """
+
+    SAML_REQUEST = GrpcBitfountMessage.SAML_REQUEST
+    SAML_RESPONSE = GrpcBitfountMessage.SAML_RESPONSE
+    ALGORITHM_EXCHANGE = GrpcBitfountMessage.ALGORITHM_EXCHANGE
+    # TODO: [BIT-1049] Should these be combined into JOB_RESPONSE?
+    #       The message body itself indicates whether it was an accept or reject.
+    JOB_ACCEPT = GrpcBitfountMessage.JOB_ACCEPT
+    JOB_REJECT = GrpcBitfountMessage.JOB_REJECT
+    JOB_REQUEST = GrpcBitfountMessage.JOB_REQUEST
+    EVALUATION_RESULTS = GrpcBitfountMessage.EVALUATION_RESULTS
+    KEY_EXCHANGE = GrpcBitfountMessage.KEY_EXCHANGE
+    LOG_MESSAGE = GrpcBitfountMessage.LOG_MESSAGE
+    # weight updates sent modeller->worker
+    MODEL_PARAMETERS = GrpcBitfountMessage.MODEL_PARAMETERS
+    SECURE_SHARE = GrpcBitfountMessage.SECURE_SHARE
+    TRAINING_ABORT = GrpcBitfountMessage.TRAINING_ABORT  # Not yet used
+    TRAINING_COMPLETE = GrpcBitfountMessage.TRAINING_COMPLETE
+    # task complete message modeller->worker
+    TASK_COMPLETE = GrpcBitfountMessage.TASK_COMPLETE
+    # weight updates send worker->modeller
+    TRAINING_UPDATE = GrpcBitfountMessage.TRAINING_UPDATE
+    # validation metrics sent worker->modeller
+    TRAINING_METRICS = GrpcBitfountMessage.TRAINING_METRICS
+    # Default value for Proto
+    UNDEFINED = GrpcBitfountMessage.UNDEFINED
+    # OIDC identity verification related types
+    OIDC_CHALLENGE = GrpcBitfountMessage.OIDC_CHALLENGE
+    # Authorisation Flow Code with PKCE Response
+    OIDC_AFC_PKCE_RESPONSE = GrpcBitfountMessage.OIDC_AFC_PKCE_RESPONSE
+    # Device Code Response
+    OIDC_DEVICE_CODE_RESPONSE = GrpcBitfountMessage.OIDC_DEVICE_CODE_RESPONSE
 
-    def add_mailbox(self, mailbox_id: str) -> None:
-        """Creates a new queue for mailbox_id.
 
-        If queue already exists, does nothing.
+class _LargeObjectRequestHandler:
+    """Slim wrapper for handling requests to blob storage."""
+
+    @staticmethod
+    def upload_large_object(
+        upload_url: _S3PresignedPOSTURL,
+        upload_fields: _S3PresignedPOSTFields,
+        large_object: bytes,
+    ) -> None:
+        """Uploads the object to the upload URL.
+
+        Args:
+            upload_url: The URL to upload to.
+            upload_fields: Additional data fields needed to upload the object.
+            large_object: The object to upload as bytes.
+
+        Raises:
+            RequestException: If the upload fails.
         """
-        with self._lock:
-            if mailbox_id not in self._queues:
-                self._queues[mailbox_id] = queue.SimpleQueue()
+        try:
+            _upload_data_to_s3(upload_url, upload_fields, large_object)
+        except RequestException as ex:
+            raise RequestException(
+                f"Failed to upload message to large message storage. Cause: {ex}."
+            ) from ex
+
+    @staticmethod
+    def get_large_object_from_url(url: _S3PresignedURL) -> bytes:
+        """Retrieves a larger message from the provided download URL.
 
+        Args:
+            url: The URL provided on the message service.
 
-class LocalMessageService(_MessageService):
-    """A class that implements the MessageService using asyncio.Queues.
+        Returns:
+            The message that was uploaded.
 
-    Has the same interface as MessageService but is designed for local testing.
+        Raises:
+            RequestException: If the download fails
+        """
+        try:
+            # We can guarantee the return type is bytes as `upload_large_object`
+            # takes in bytes.
+            large_object: bytes = _download_data_from_s3(url)
+            return large_object
+        except RequestException as ex:
+            raise RequestException(
+                f"Failed to retrieve message from large message storage. Cause: {ex}."
+            ) from ex
+
+
+@dataclass
+class _BitfountMessage:
+    """A common message structure for all messages from the message service.
+
+    This is a pythonic wrapper on the generated gRPC Proto,
+    allowing us to pre-emptively build in support for upcoming features
+    as well as backwards compatibility where needed.
     """
 
-    # noinspection PyMissingConstructor
-    def __init__(
-        self,
-        username: str,
-        shared_queues: LocalMessageServiceSharedQueues,
-        modeller_mailbox_id: str,
-        pod_ids: List[str],
-        task_id: Optional[str] = None,
-    ):
-        """Create new LocalMessageService.
-
-        Args:
-            username: The username of the account "running" this message service.
-            shared_queues: The shared queues to act as the mailboxes.
-            modeller_mailbox_id: The modeller's mailbox ID to simulate with.
-            pod_ids: The group of pod_ids to simulate with.
-            task_id: Optional. The task ID to simulate with. If not provided, one
-                will be generated.
-        """
-        # We don't have a BitfountSession to keep track of username with, so we
-        # need it manually.
-        self._username = username
-        self._shared_queues = shared_queues
-        self._modeller_mailbox_id = modeller_mailbox_id
-        self._pod_ids = pod_ids
-
-        self._task_id: str
-        if task_id is None:
-            self._task_id = uuid.uuid4().hex
-        else:
-            self._task_id = task_id
+    message_type: _BitfountMessageType
+    body: bytes
+    recipient: str
+    recipient_mailbox_id: str
+    sender: str
+    sender_mailbox_id: str
+    # This is provided to pods for secure aggregation
+    pod_mailbox_ids: Dict[str, str] = field(default_factory=dict)
+    # default_factory is used here to ensure current time is set for each instantiation
+    timestamp: str = field(default_factory=_current_time)
+    # Provided by the message service
+    receipt_handle: Optional[str] = None
 
-        # Pre-create queues
-        # NOTE: In the actual message service these won't be created until
-        #       connect_pod() or setup_communication_with_pods() are called. For
-        #       the mock use case, because we often have the pods/workers/modellers
-        #       running in parallel, we need to ensure the queues are already created
-        #       before they might be accessed.
-        # Create modeller queue
-        self._shared_queues.add_mailbox(self.modeller_mailbox_id)
-
-        # Create pod mailboxes
-        for pod_id in self._pod_ids:
-            self._shared_queues.add_mailbox(self._get_pod_mailbox_id(pod_id))
-
-        # Create worker mailboxes
-        for worker_mailbox_id in self.worker_mailbox_ids.values():
-            self._shared_queues.add_mailbox(worker_mailbox_id)
+    @staticmethod
+    def from_rpc(message: GrpcBitfountMessage) -> _BitfountMessage:
+        """Converts Proto messages into a BitfountMessage.
 
-    @property
-    def username(self) -> str:
-        """Username running this message service."""
-        return self._username
+        Currently requires some additional logic due to the differing protos,
+        FromModeller & FromPod, but hopefully those will soon be one proto.
 
-    @property
-    def modeller_mailbox_id(self) -> str:
-        """The modeller_mailbox_id to use for this message service."""
-        if self._modeller_mailbox_id:
-            return self._modeller_mailbox_id
-        else:
-            raise ValueError("No modeller_mailbox_id set.")
+        Args:
+            message: A message from a pod or modeller.
 
-    @property
-    def worker_mailbox_ids(self) -> Dict[str, str]:
-        """A mapping of pod identifiers to worker_mailbox_ids.
+        Returns: A BitfountMessage containing the data from the original message
 
-        Deterministically generated so that they can be accessed consistently
-        across all LocalMessageService instances using the same queue backend.
         """
-        if not self._pod_ids:
-            raise ValueError("No worker_mailbox_ids set.")
-        return {
-            pod_identifier: self._get_worker_mailbox_id(pod_identifier)
-            for pod_identifier in self._pod_ids
-        }
-
-    async def connect_pod(
-        self, pod_name: str, dataset_names: Optional[List[str]] = None
-    ) -> str:
-        """See parent for more information."""
-        # NOTE: In the actual message service the pod queue will be created at this
-        #       point (by the message service itself). For the mock we pre-create
-        #       it as the way the mock is often used will require them to already
-        #       exist.
-        return self._get_pod_mailbox_id(pod_name)
+        message_body: bytes = message.body
+        try:
+            # We attempt to unpack this, as we need to know if it is:
+            #   bytes  - Encrypted
+            #   string - A URL to download the message body from
+            unpacked_body_to_inspect: Union[str, bytes] = msgpack.loads(message.body)
+
+            # Checks if message body is an S3 file reference or local file reference
+            if isinstance(unpacked_body_to_inspect, str):
+                parsed_url = urlparse(unpacked_body_to_inspect)
+                # If it is an S3 file reference, retrieve the actual message from S3
+                if parsed_url.scheme == "https" and parsed_url.netloc.endswith(
+                    _S3_NETLOC_SUFFIX
+                ):
+                    logger.debug("Large message, retrieving from large object storage")
+                    storage_url = cast(_S3PresignedURL, unpacked_body_to_inspect)
+                    message_body = _LargeObjectRequestHandler.get_large_object_from_url(
+                        storage_url
+                    )
+                # If it is a local file reference,
+                # retrieve the actual message and re-parse
+                # the message with the file contents
+                elif (
+                    not parsed_url.scheme and not parsed_url.netloc and parsed_url.path
+                ):
+                    logger.debug("Local message, retrieving from local storage")
+                    with open(unpacked_body_to_inspect, "rb") as f:
+                        message_body = f.read()
+                    os.remove(
+                        unpacked_body_to_inspect
+                    )  # deletes the tempfile after contents have been read
+        except (msgpack.exceptions.UnpackException, ValueError):
+            logger.debug(
+                "Couldn't unpack message - "
+                "The message is most likely an encrypted message "
+                "which is not on blob storage"
+            )
 
-    async def setup_task(
-        self,
-        tasks_per_pod: Dict[str, bytes],
-        task_metadata: TaskMetadata,
-        project_id: Optional[str] = None,
-    ) -> CommunicationDetails:
-        """See parent for more information."""
-        # NOTE: In the actual message service the modeller and message queues will
-        #       be created at this point (by the message service itself). For the
-        #       mock we pre-create them as the mock is often used as though the
-        #       Pod had already approved the tasks and the workers already spun
-        #       up which means the worker queues need to already exist.
-        return CommunicationDetails(
-            self.modeller_mailbox_id, self.worker_mailbox_ids, self._task_id
+        return _BitfountMessage(
+            message_type=_BitfountMessageType(message.messageType),
+            body=message_body,
+            recipient=message.recipient,
+            recipient_mailbox_id=message.recipientMailboxId,
+            sender=message.sender,
+            sender_mailbox_id=message.senderMailboxId,
+            pod_mailbox_ids=message.podMailboxIds,
+            receipt_handle=message.receiptHandle,
+            timestamp=message.timestamp,
         )
 
-    async def setup_communication_with_pods(
-        self, tasks_per_pod: Mapping[str, bytes]
-    ) -> CommunicationDetails:
-        """See parent for more information."""
-        # NOTE: In the actual message service the modeller and message queues will
-        #       be created at this point (by the message service itself). For the
-        #       mock we pre-create them as the mock is often used as though the
-        #       Pod had already approved the tasks and the workers already spun
-        #       up which means the worker queues need to already exist.
-        return CommunicationDetails(
-            self.modeller_mailbox_id, self.worker_mailbox_ids, self._task_id
-        )
+    def decrypt(self, aes_key: bytes) -> _DecryptedBitfountMessage:
+        """Decrypt message body using AES.
 
-    async def poll_for_messages(
-        self,
-        mailbox_id: str,
-        stop_event: threading.Event,
-    ) -> AsyncGenerator[_BitfountMessage, None]:
-        """See parent for more information."""
-        # Create this at the beginning so it is only created once rather than per
-        # message yielded. Can set larger polling timeout as should only be a single
-        # instance of this and should be `set()` in event of failure or polling end
-        # (see `_BaseMailbox.listen()`).
-        stop_event_wait_task = asyncio.create_task(
-            await_threading_event(
-                stop_event,
-                event_name=f"LocalMessageService_poll_for_messages_stop_{mailbox_id}",
-                polling_timeout=30,
-            )
+        Args:
+            aes_key: AES Key to use for decryption
+        """
+        decrypted_message = msgpack.loads(
+            _MessageEncryption.decrypt_incoming_message(self.body, aes_key)
+        )
+        return _DecryptedBitfountMessage(
+            message_type=self.message_type,
+            body=decrypted_message,
+            recipient=self.recipient,
+            recipient_mailbox_id=self.recipient_mailbox_id,
+            sender=self.sender,
+            sender_mailbox_id=self.sender_mailbox_id,
+            pod_mailbox_ids=self.pod_mailbox_ids,
+            timestamp=self.timestamp,
         )
 
-        # Check stop condition at start of loop
-        while not stop_event.is_set():
-            get_message_task = asyncio.create_task(
-                self._shared_queues.get_message(mailbox_id)
-            )
 
-            # Check stop condition whilst waiting for message
-            awaitables: List[Awaitable] = [get_message_task, stop_event_wait_task]
-            done, pending = await asyncio.wait(
-                awaitables, return_when=asyncio.FIRST_COMPLETED
-            )
+@dataclass
+class _DecryptedBitfountMessage:
+    """BitfountMessage where the body has been decrypted.
+
+    This gives us more confidence that we have a decrypted body,
+    and allows us to pass around the decrypted message body and
+    metadata together easily.
+    """
 
-            if get_message_task in done:
-                yield get_message_task.result()
-            else:  # stop_event is set
-                get_message_task.cancel()
-                return
+    message_type: _BitfountMessageType
+    body: Any
+    recipient: str
+    recipient_mailbox_id: str
+    sender: str
+    sender_mailbox_id: str
+    # This is provided to pods for secure aggregation
+    pod_mailbox_ids: Dict[str, str] = field(default_factory=dict)
+    # default_factory is used here to ensure current time is set for each instantiation
+    timestamp: str = field(default_factory=_current_time)
+
+
+class _MessageService:
+    """Slim wrapper around the GRPC Functions provided by the message service.
+
+    It stops the GRPC objects which aren't very pleasant to use from being everywhere
+    as well as making it easier to replace these in future if needed.
+
+    Args:
+        session: Used for authentication.
+        config: The configuration for the gRPC message service.
+    """
 
-    async def send_message(
-        self, message: _BitfountMessage, already_packed: bool = False
-    ) -> SuccessResponse:
-        """See parent for more information."""
-        logger.debug(
-            f"Sending message {message.message_type} to {message.recipient_mailbox_id}"
-        )
-        self._shared_queues.send_message(message.recipient_mailbox_id, message)
-        return cast(SuccessResponse, create_autospec(SuccessResponse, instance=True))
+    def __init__(self, session: BitfountSession, config: MessageServiceConfig):
+        self.session = session
+        self.config = config
+        self.use_local_storage: bool = config.use_local_storage
+        self._stub: Optional[MessageServiceStub] = None
 
-    @staticmethod
-    def _get_worker_mailbox_id(pod_identifier: str) -> str:
-        """Deterministically generates a worker mailbox ID from a pod identifier."""
-        return f"{pod_identifier}_worker"
+    # @cached_property can't be used because it returns the _exact_ same Awaitable
+    # which cannot be awaited more than once.
+    @property
+    async def stub(self) -> MessageServiceStub:
+        """The gRPC stub generated for this instance."""
+        if self._stub is None:
+            self._stub = await self.config.stub
+        return self._stub
 
-    @staticmethod
-    def _get_pod_mailbox_id(pod_identifier_or_name: str) -> str:
-        """Deterministically generate pod mailbox ID from a pod identifier or name."""
-        if "/" in pod_identifier_or_name:
-            _, pod_name = pod_identifier_or_name.split("/", maxsplit=1)
-        else:
-            pod_name = pod_identifier_or_name
+    @property
+    def username(self) -> str:
+        """Get the authenticated username."""
+        return self.session.username
+
+    async def connect_pod(self, pod_name: str) -> str:
+        """Make the message service aware of the Pod.
+
+        This ensures that the correct queue is set up for this pod,
+        in future it may contain more metadata.
+
+        Args:
+            pod_name: The name of the pod (the part after 'username/')
+
+        Returns:
+            The created mailbox ID for this pod.
+
+        Raises:
+            RpcError: if an exception occurs
+        """
+        processor = platform.processor()
+        pod_os = platform.system()
+        logical_cpu_count = psutil.cpu_count()
+        total_physical_memory_bytes = psutil.virtual_memory().total
+        gpu_name, available_gpus = get_gpu_metadata()
+
+        response = await (await self.stub).PodConnect(
+            PodData(
+                podName=pod_name,
+                processor=processor,
+                podOS=pod_os,
+                cpuCount=logical_cpu_count,
+                totalMemoryBytes=total_physical_memory_bytes,
+                gpuCount=available_gpus,
+                gpuName=gpu_name,
+            ),
+            metadata=[("token", self.session.access_token)],
+        )
+
+        # If we've not received a successful response (but no other RPC error
+        # happened) then raise it here.
+        if not isinstance(response, SuccessResponse):
+            raise PodConnectFailedError("Unable to connect pod to message service.")
 
         # TODO: [BIT-960] Currently this is just hardcoded to return the pod_name
         #       (which is what the mailbox ID will actually be). [BIT-960] will have
         #       the PodConnect method actually return the generated mailbox ID so
         #       that if the approach changes in future it only needs to change on
         #       the message service side. At that point this should return the
         #       generated mailbox ID instead.
-        pod_mailbox_id = pod_name
-        return pod_mailbox_id
+        return pod_name
 
+    async def setup_communication_with_pods(
+        self, tasks_per_pod: Mapping[str, bytes]
+    ) -> Tuple[str, Dict[str, str]]:
+        """Called by the modeller to set up communication channels with specified pods.
+
+        Args:
+            tasks_per_pod: Mapping of pod identifiers to AES Encrypted messages.
 
-class GRPCStubMock(MessageServiceStub):
-    """Multiprocess friendly GRPC Stub Fake.
+        Returns:
+            A tuple of mailbox IDs used for communications:
+                - mailbox_id (str): The modeller's mailbox ID
+                - worker_mailbox_ids (dict): Mapping from Pod ID to Worker Mailbox ID
 
-    This mocks the behaviour of the message service.
-    It can handle a single training job.
-    It assumes the happy path is followed, the only 'error' it handles is
-    when no messages are available.
-    """
+        """
+        logger.debug(f"Sending task request to: '{tasks_per_pod.keys()}'")
+        communication_details = await (await self.stub).SetupTaskMailboxes(
+            BitfountTasks(
+                tasks=[
+                    BitfountTask(
+                        podIdentifier=pod_identifier, encryptedTask=aes_encrypted_task
+                    )
+                    for pod_identifier, aes_encrypted_task in tasks_per_pod.items()
+                ]
+            ),
+            metadata=[("token", self.session.access_token)],
+        )
 
-    # noinspection PyMissingConstructor
-    def __init__(
+        # communication_details.podMailboxIds is actually the _worker_ mailbox_ids
+        return communication_details.mailboxId, communication_details.podMailboxIds
+
+    async def poll_for_messages(
         self,
-        tokens_to_usernames: Mapping[str, str],
-        manager: SyncManager,
-        get_message_timeout: float = 0.1,
-    ):
-        self.tokens_to_usernames = tokens_to_usernames
-        self.manager = manager
-        self.get_message_timeout = get_message_timeout
-        self.queue_id_counter = manager.Value("i", 1)
-        self.large_object_id_counter = manager.Value("i", 1)
+        mailbox_id: str,
+    ) -> AsyncGenerator[_BitfountMessage, None]:
+        """Polls target mailbox for message.
 
-        self.queues: DictProxy[str, ListProxy[GrpcBitfountMessage]] = manager.dict()
+        Will keep retrying up to `self.minutes_before_timeout_no_messages`
+        or until a message is received, whichever is sooner.
 
-        self.lock = multiprocessing.Lock()
+        It has the same behaviour as `get_message` regarding exceptions:
+        if exceptions are repeatedly raised then it will throw the exception.
 
-    @staticmethod
-    def _get_token(metadata: Sequence[Tuple[str, str]]) -> str:
-        """Retrieve token from metadata."""
-        return metadata[0][1]
+        Args:
+            mailbox_id: The mailbox to poll for messages
 
-    async def PodConnect(
-        self,
-        data: PodData,
-        /,
-        *,
-        metadata: Sequence[Tuple[str, Any]],
-        timeout: Optional[float] = None,
-    ) -> SuccessResponse:
-        """Fakes PodConnect behaviour."""
-        pod_id = self.tokens_to_usernames[self._get_token(metadata)]
-        pod_queue_name = pod_id.replace("/", "-")
-        with self.lock:
-            self.queues[pod_queue_name] = self.manager.list()
+        Yields:
+            BitfountMessage: the next message received
+        """
+        counter = 0
 
-        return SuccessResponse()
+        while True:
+            message = await self._get_message(mailbox_id)
 
-    async def SetupTask(
-        self,
-        data: TaskTransferRequests,
-        /,
-        *,
-        metadata: Sequence[Tuple[str, Any]],
-        timeout: Optional[float] = None,
-    ) -> TaskTransferMetadata:
-        """Fakes SetupTask behaviour."""
-        task_id: str = uuid.uuid4().hex
-
-        storage = []
-        # Locking just to ensure that we have a unique counter value
-        with self.lock:
-            for task in data.podTasks:
-                # Tests using this stub will also want to use
-                # `apply_mock_large_object_interactions`
-                # as this stub does not handle large object storage
-                print(
-                    f"Large storage {self.large_object_id_counter.value} for task "
-                    f"taken by: {self.tokens_to_usernames[self._get_token(metadata)]} "
-                    f"(pod: {task.podIdentifier})"
-                )
-                result = BlobStorageData(
-                    uploadUrl=f"https://test-message-service-"
-                    f"external.s3.eu-west-2.amazonaws.com/"
-                    f"upload?large-object-id="
-                    f"{self.large_object_id_counter.value}",
-                    downloadUrl=f"https://test-message-service-"
-                    f"external.s3.eu-west-2.amazonaws.com/"
-                    f"download?large-object-id="
-                    f"{self.large_object_id_counter.value}",
-                    uploadFields={"key": "some/key", "bucket": "some-bucket"},
-                    podIdentifier=task.podIdentifier,
-                )
-                storage.append(result)
-                self.large_object_id_counter.value += 1
-            return TaskTransferMetadata(taskId=task_id, taskStorage=storage)
+            if message:
+                # Reset counter
+                counter = 0
+                # Yield as soon as we've retrieved a message
+                yield message
+            else:
+                counter += 1
+                # Cap counter at the maximum `backoffs` entry
+                counter = min(counter, len(_POLLING_BACKOFFS) - 1)
 
-    async def InitiateTask(
+            # Sleep for backoff time until trying again
+            await asyncio.sleep(_POLLING_BACKOFFS[counter])
+
+    async def _get_message(
         self,
-        data: BitfountTasks,
-        /,
-        *,
-        metadata: Sequence[Tuple[str, Any]],
-        timeout: Optional[float] = None,
-    ) -> GrpcCommunicationDetails:
-        """Fakes InitiateTask behaviour."""
-        modeller_name = self.tokens_to_usernames[self._get_token(metadata)]
-
-        with self.lock:
-            modeller_queue_id = self.queue_id_counter.value
-            self.queues[
-                f"{modeller_name}-{self.queue_id_counter.value}"
-            ] = self.manager.list()
-            self.queue_id_counter.value += 1
-
-            pod_mailbox_ids = {}
-
-            # Create worker/task queues for all involved in task
-            for task in data.tasks:
-                print(f"Queues are: {self.queues}")
-                pod_namespace, pod_name = task.podIdentifier.split("/")
-                # Pod queue creation
-                print(f"making queue: '{pod_namespace}-{self.queue_id_counter.value}'")
-                self.queues[
-                    f"{pod_namespace}-{self.queue_id_counter.value}"
-                ] = self.manager.list()
-                pod_mailbox_ids[task.podIdentifier] = str(self.queue_id_counter.value)
-                self.queue_id_counter.value += 1
-
-            # Put task request message on pod queue for all involved in task
-            for task in data.tasks:
-                pod_namespace, pod_name = task.podIdentifier.split("/")
-                general_mailbox_name = f"{pod_namespace}-{pod_name}"
-                print(f"MOCK MAILBOXES: {pod_mailbox_ids}")
-                self.queues[general_mailbox_name].append(
-                    GrpcBitfountMessage(
-                        messageType=_BitfountMessageType.JOB_REQUEST.value,
-                        body=task.taskURL,
-                        sender=modeller_name,
-                        senderMailboxId=str(modeller_queue_id),
-                        recipient=task.podIdentifier,
-                        recipientMailboxId=pod_name,
-                        podMailboxIds=pod_mailbox_ids,
-                        taskId=data.taskId,
-                    )
-                )
+        mailbox_id: str,
+        max_attempts: int = 3,
+        wait_between_errors: int = 30,
+    ) -> Optional[_BitfountMessage]:
+        """Retrieve message from mailbox if available.
 
-        print(f"POD MAILBOXES: {pod_mailbox_ids}")
-        return GrpcCommunicationDetails(
-            mailboxId=str(modeller_queue_id),
-            podMailboxIds=pod_mailbox_ids,
-            taskId=data.taskId,
-        )
+        If none are available then it returns None.
 
-    async def SetupTaskMailboxes(
-        self,
-        data: BitfountTasks,
-        /,
-        *,
-        metadata: Sequence[Tuple[str, Any]],
-        timeout: Optional[float] = None,
-    ) -> GrpcCommunicationDetails:
-        """Fakes SetupTaskMailboxes behaviour."""
-        modeller_name = self.tokens_to_usernames[self._get_token(metadata)]
-
-        task_id: str = uuid.uuid4().hex
-
-        with self.lock:
-            modeller_queue_id = self.queue_id_counter.value
-            self.queues[
-                f"{modeller_name}-{self.queue_id_counter.value}"
-            ] = self.manager.list()
-            self.queue_id_counter.value += 1
-
-            pod_mailbox_ids = {}
-
-            # Create worker/task queues for all involved in task
-            for task in data.tasks:
-                print(f"Queues are: {self.queues}")
-                pod_namespace, pod_name = task.podIdentifier.split("/")
-                # Pod queue creation
-                print(f"making queue: '{pod_namespace}-{self.queue_id_counter.value}'")
-                self.queues[
-                    f"{pod_namespace}-{self.queue_id_counter.value}"
-                ] = self.manager.list()
-                pod_mailbox_ids[task.podIdentifier] = str(self.queue_id_counter.value)
-                self.queue_id_counter.value += 1
-
-            # Put task request message on pod queue for all involved in task
-            for task in data.tasks:
-                pod_namespace, pod_name = task.podIdentifier.split("/")
-                general_mailbox_name = f"{pod_namespace}-{pod_name}"
-                print(f"MOCK MAILBOXES: {pod_mailbox_ids}")
-                self.queues[general_mailbox_name].append(
-                    GrpcBitfountMessage(
-                        messageType=_BitfountMessageType.JOB_REQUEST.value,
-                        body=task.encryptedTask,
-                        sender=modeller_name,
-                        senderMailboxId=str(modeller_queue_id),
-                        recipient=task.podIdentifier,
-                        recipientMailboxId=pod_name,
-                        podMailboxIds=pod_mailbox_ids,
-                        taskId=task_id,
-                    )
+        Other errors are raised if they occur repeatedly.
+
+        Args:
+            mailbox_id: Mailbox to check for messages.
+            max_attempts: Number of times that we will retry if an error occurs.
+            wait_between_errors: Time to wait between errors in seconds.
+
+        Returns:
+            An BitfountMessage if one is available, else None.
+
+        Raises:
+            (RpcError, RequestException): If an error occurs multiple times then it
+                will be raised.
+        """
+        errors_encountered = 0
+        delete_mailbox = False
+
+        while errors_encountered < max_attempts:
+            try:
+                message = await (await self.stub).GetBitfountMessage(
+                    CommunicationDetails(mailboxId=mailbox_id),
+                    metadata=[("token", self.session.access_token)],
                 )
 
-        print(f"POD MAILBOXES: {pod_mailbox_ids}")
-        return GrpcCommunicationDetails(
-            mailboxId=str(modeller_queue_id),
-            podMailboxIds=pod_mailbox_ids,
-            taskId=task_id,
-        )
+                if message.messageType == _BitfountMessageType.TASK_COMPLETE.value:
+                    logging.debug(
+                        "Received a TASK_COMPLETE message - "
+                        "asking message service to tidy up."
+                    )
+                    delete_mailbox = True
 
-    async def SendBitfountMessage(
-        self,
-        data: GrpcBitfountMessage,
-        /,
-        *,
-        metadata: Sequence[Tuple[str, Any]],
-        timeout: Optional[float] = None,
-    ) -> SuccessResponse:
-        """Fakes SendBitfountMessage behaviour."""
-        sender_name = self.tokens_to_usernames[self._get_token(metadata)]
-        recipient_username = data.recipient
-
-        if "/" in data.recipient:
-            # It's for a pod
-            recipient_username, pod_name = data.recipient.split("/")
-
-        with self.lock:
-            self.queues[f"{recipient_username}-{data.recipientMailboxId}"].append(
-                GrpcBitfountMessage(
-                    messageType=data.messageType,
-                    body=data.body,
-                    recipient=recipient_username,
-                    recipientMailboxId=data.recipientMailboxId,
-                    sender=sender_name,
-                    senderMailboxId=data.senderMailboxId,
-                    taskId=data.taskId,
+                await (await self.stub).AcknowledgeMessage(
+                    Acknowledgement(
+                        mailboxId=mailbox_id,
+                        receiptHandle=message.receiptHandle,
+                        deleteMailbox=delete_mailbox,
+                    ),
+                    metadata=[("token", self.session.access_token)],
                 )
-            )
-        return SuccessResponse()
 
-    async def GetBitfountMessage(
-        self,
-        data: GrpcCommunicationDetails,
-        /,
-        *,
-        metadata: Sequence[Tuple[str, Any]],
-        timeout: Optional[float] = None,
-    ) -> GrpcBitfountMessage:
-        """Fakes GetBitfountMessage behaviour."""
-        user_name = self.tokens_to_usernames[self._get_token(metadata)]
-
-        if "/" in user_name:
-            # It's a pod, we need to split out the pod name from the username
-            user_name = user_name.split("/")[0]
-
-        self.lock.acquire()
-        mailbox = self.queues[f"{user_name}-{data.mailboxId}"]
-
-        if len(mailbox) == 0:
-            self.lock.release()
-            time.sleep(self.get_message_timeout)
-            self.lock.acquire()
-
-            if len(mailbox) == 0:
-                self.lock.release()
-
-                # Create an RpcError, manually setting the code property as this
-                # class is normally constructed in C code.
-                error = RpcError()
-                error.code = lambda: StatusCode.NOT_FOUND  # type: ignore[method-assign] # Reason: see comment # noqa: B950
+                return _BitfountMessage.from_rpc(message)
+            except (RpcError, RequestException) as ex:
+                # This indicates that no message was waiting
+                if isinstance(ex, RpcError) and ex.code() == StatusCode.NOT_FOUND:
+                    return None
+
+                errors_encountered += 1
+                logger.warning(
+                    f"Error retrieving message from mailbox: {mailbox_id}. "
+                    f"Error was: {ex}"
+                )
 
-                raise error
+                if errors_encountered == max_attempts:
+                    logging.error(
+                        f"Exceeded {max_attempts} when retrieving message "
+                        f"from {mailbox_id}"
+                    )
+                    raise ex
 
-        message = mailbox[0]
-        self.lock.release()
+                # The message service already has a 'wait' built in
+                # to allow time for messages to arrive
+                # but in these other error cases we should wait before retrying
+                await asyncio.sleep(wait_between_errors)
 
-        return message
+        # This shouldn't be reached but is included to make mypy happy
+        return None
 
-    async def AcknowledgeMessage(
+    async def send_message(
         self,
-        data: Acknowledgement,
-        /,
-        *,
-        metadata: Sequence[Tuple[str, Any]],
-        timeout: Optional[float] = None,
+        message: _BitfountMessage,
+        already_packed: bool = False,
     ) -> SuccessResponse:
-        """Fakes AcknowledgeMessage behaviour."""
-        user_name = self.tokens_to_usernames[self._get_token(metadata)]
-        if "/" in user_name:
-            # It's a pod, we need to split out the pod name from the username
-            user_name = user_name.split("/")[0]
+        """Send a message.
 
-        with self.lock:
-            self.queues[f"{user_name}-{data.mailboxId}"].pop(0)
+        Args:
+            message (BitfountMessage): The message to send
+            already_packed (bool): Whether or not the message is already in required
+                bytes format
 
-        return SuccessResponse()
+        Returns:
+            SuccessResponse if the message was successfully sent
+        """
+        if not already_packed:
+            message.body = msgpack.dumps(message.body)
 
-    async def GetLargeObjectStorage(
-        self,
-        data: LargeStorageRequest,
-        /,
-        *,
-        metadata: Sequence[Tuple[str, Any]],
-        timeout: Optional[float] = None,
-    ) -> BlobStorageData:
-        """Get URLs for upload/download of a large message."""
-        # Tests using this stub will also want to use
-        # `apply_mock_large_object_interactions`
-        # as this stub does not handle large object storage
-        # Locking just to ensure that we have a unique counter value
-        with self.lock:
-            print(
-                f"Large storage {self.large_object_id_counter.value} "
-                f"taken by: {self.tokens_to_usernames[self._get_token(metadata)]} "
-                f"(pod: {data.podName})"
-            )
-            result = BlobStorageData(
-                uploadUrl=f"https://test-message-service-"
-                f"external.s3.eu-west-2.amazonaws.com/"
-                f"upload?large-object-id="
-                f"{self.large_object_id_counter.value}",
-                downloadUrl=f"https://test-message-service-"
-                f"external.s3.eu-west-2.amazonaws.com/"
-                f"download?large-object-id="
-                f"{self.large_object_id_counter.value}",
-                uploadFields={"key": "some/key", "bucket": "some-bucket"},
-            )
-            self.large_object_id_counter.value += 1
-            return result
+        if self.use_local_storage:
+            message.body = self._save_object_to_local_storage(message)
+        else:
+            message.body = await self._maybe_upload_to_large_object_storage(message)
 
+        return await (await self.stub).SendBitfountMessage(
+            GrpcBitfountMessage(
+                messageType=message.message_type.value,
+                body=message.body,
+                recipient=message.recipient,
+                recipientMailboxId=message.recipient_mailbox_id,
+                sender=message.sender,
+                senderMailboxId=message.sender_mailbox_id,
+                timestamp=message.timestamp,
+                podMailboxIds=message.pod_mailbox_ids,
+            ),
+            metadata=[("token", self.session.access_token)],
+        )
 
-# Type var for AsyncIteratorMock
-T = TypeVar("T")
+    async def _maybe_upload_to_large_object_storage(
+        self, message: _BitfountMessage
+    ) -> bytes:
+        """Uploads the message body to large object storage if it's too big.
 
+        Args:
+            message: the message containing the body to upload
 
-class AsyncIteratorMock(AsyncIterator, Generic[T]):
-    """A mocked async iterator that will iterate a sync iterable."""
+        Returns:
+            Either the original message body, or a URL that can be used to download it
+        """
+        # Larger messages are sent as references to large object storage
+        if len(message.body) > _SMALL_MESSAGE_UPPER_LIMIT_SIZE_BYTES:
+            # Check that message body isn't above the maximum storage size
+            message_body_size = _get_packed_data_object_size(message.body)
+            if message_body_size > _MAX_STORAGE_SIZE_BYTES:
+                raise ValueError(
+                    f"Message body is too large to upload: "
+                    f"expected max {_MAX_STORAGE_SIZE_MEGABYTES} megabytes, "
+                    f"got {_get_mb_from_bytes(message_body_size).fractional} megabytes."
+                )
 
-    def __init__(self, iterable: Iterable[T]) -> None:
-        self.iterator: Iterator[T] = iter(iterable)
+            pod_name = None
+            if message.sender and "/" in message.sender:
+                pod_name = message.sender.split("/")[1]
+            (
+                upload_url,
+                upload_fields,
+            ), download_url = await self._get_large_object_storage_details(
+                message.sender_mailbox_id, message_body_size, pod_name
+            )
+            _LargeObjectRequestHandler.upload_large_object(
+                upload_url, upload_fields, message.body
+            )
 
-    def __aiter__(self) -> AsyncIteratorMock[T]:
-        return self
+            # Currently necessary, as the message service is expecting bytes
+            return msgpack.dumps(download_url)
+        # Small messages can just go on the message service directly
+        return message.body
+
+    async def _get_large_object_storage_details(
+        self, mailbox_id: str, message_size: int, pod_name: Optional[str] = None
+    ) -> Tuple[Tuple[_S3PresignedPOSTURL, _S3PresignedPOSTFields], _S3PresignedURL]:
+        """Get an upload and download URL for larger message bodies.
 
-    async def __anext__(self) -> T:
-        # Iterate through the iterable, raising exceptions if they are present
-        try:
-            result = next(self.iterator)
-            if self._is_exception(result):
-                raise cast(BaseException, result)
-            return result
-        except StopIteration:
-            pass
-        raise StopAsyncIteration
+        Args:
+            mailbox_id: Your mailbox id for the current task.
+            message_size: The size of the message to upload (an upper limit).
+            pod_name: Name of the pod that you're acting as, if relevant.
 
-    @staticmethod
-    def _is_exception(obj: Any) -> bool:
+        Returns:
+            Tuple of upload handler, download URL.
+        """
+        blob_storage_data = await (await self.stub).GetLargeObjectStorage(
+            LargeStorageRequest(
+                senderMailboxId=mailbox_id, podName=pod_name, contentSize=message_size
+            ),
+            metadata=[("token", self.session.access_token)],
+        )
         return (
-            isinstance(obj, BaseException)
-            or isinstance(obj, type)
-            and issubclass(obj, BaseException)
+            (blob_storage_data.uploadUrl, blob_storage_data.uploadFields),
+            blob_storage_data.downloadUrl,
         )
 
+    @staticmethod
+    def _save_object_to_local_storage(message: _BitfountMessage) -> bytes:
+        """Saves object to local storage and returns filename.
+
+        Args:
+            message: the message containing the body to upload
 
-DataclassMock: TypeAlias = NonCallableMock
+        Returns:
+            (bytes): A path to the file where the local object has been stored.
+        """
+        # we use mkstemp here to ensure we have complete control over when the file gets
+        # deleted on every OS - unlike e.g. NamedTemporaryFile
+        handle, local_tempfile = tempfile.mkstemp()
+        # write to the open file handler rather than finding the file by name
+        with os.fdopen(handle, "wb") as f:
+            f.write(message.body)
+        return msgpack.dumps(local_tempfile)
 
 
-def create_dataclass_mock(
-    dataclass_cls_or_instance: Union[Any, Type[Any]]
-) -> DataclassMock:
-    """Creates a mock whose spec matches the attributes of target dataclass.
+class _MessageEncryption:
+    """Shared functions for handling encryption/decryption of messages.
 
-    The actual mock is a NonCallableMock instance as it makes no sense to call
-    a dataclass instance.
+    These handle encrypting or decrypting a single or multiple messages
+    at the same time using different keys (for different pods)
     """
-    if not dataclasses.is_dataclass(dataclass_cls_or_instance):
-        raise TypeError("create_dataclass_mock should only be used on dataclasses.")
 
-    # To force create_autospec to find all the attributes (including those that
-    # don't have default values) we need to instantiate a dataclass instance to
-    # get everything set. If we already have an instance this is fine, otherwise
-    # we need to work out (and pass) the number of expected args.
-    dataclass_instance: Any
-    if inspect.isclass(dataclass_cls_or_instance):
-        # Work out and create instance from class
-        dataclass_cls: Type[Any] = dataclass_cls_or_instance
-        sig = inspect.signature(dataclass_cls)
-        params = sig.parameters.values()
-        # Find params that _can_ be supplied positionally
-        args = [
-            p
-            for p in params
-            if p.kind in (Parameter.POSITIONAL_ONLY, Parameter.POSITIONAL_OR_KEYWORD)
-        ]
-        # Find params that don't have default values (Parameter.empty denotes no
-        # default value)
-        needed_args = [p for p in args if p.default == Parameter.empty]
-        false_args = [None] * len(needed_args)
-        dataclass_instance = dataclass_cls(*false_args)
-    else:
-        # Use instance directly
-        dataclass_instance = cast(Any, dataclass_cls_or_instance)
-
-    dataclass_mock: DataclassMock = create_autospec(
-        spec=dataclass_instance, instance=True
-    )
-
-    return dataclass_mock
-
-
-class AwaitableMock(AsyncMockMixin, NonCallableMock, Awaitable):
-    """A non-callable async mock which is also directly awaitable.
-
-    Useful for mocking out things like Task or Future instances.
-
-    It implements __await__ to yield and return `None` (which gets around
-    compatibility issues with asyncio not being able to await a Mock subclass
-    directly) and to record the call to __await__.
-    """
+    @staticmethod
+    def encrypt_outgoing_message(body: bytes, encryption_key: bytes) -> bytes:
+        """Encrypt `body` using `encryption_key`."""
+        body, nonce = _AESEncryption.encrypt(encryption_key, body)
+        body += nonce
+        return body
 
-    def __init__(self, *args: Any, **kwargs: Any):
-        super().__init__(*args, **kwargs)
+    @staticmethod
+    def decrypt_incoming_message(
+        body: bytes,
+        encryption_key: bytes,
+    ) -> bytes:
+        """Decrypt incoming message."""
+        body, nonce = body[:-12], body[-12:]
+        body = _AESEncryption.decrypt(encryption_key, nonce, body)
 
-    def __await__(self) -> Generator[None, None, None]:
-        self.await_count += 1
-        yield None
+        return body
```

