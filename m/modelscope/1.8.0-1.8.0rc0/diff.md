# Comparing `tmp/modelscope-1.8.0.tar.gz` & `tmp/modelscope-1.8.0rc0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "dist/modelscope-1.8.0.tar", last modified: Thu Aug  3 04:26:21 2023, max compression
+gzip compressed data, was "dist/modelscope-1.8.0rc0.tar", last modified: Mon Jul 31 12:41:38 2023, max compression
```

## Comparing `modelscope-1.8.0.tar` & `modelscope-1.8.0rc0.tar`

### file list

```diff
@@ -1,2931 +1,2906 @@
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      105 2023-08-03 04:26:19.000000 modelscope-1.8.0/MANIFEST.in
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    17074 2023-08-03 04:26:21.000000 modelscope-1.8.0/PKG-INFO
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    16077 2023-08-03 04:26:19.000000 modelscope-1.8.0/README.md
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4020 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/cli/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/cli/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      404 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/cli/base.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      829 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/cli/cli.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1230 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/cli/download.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6242 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/cli/modelcard.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4371 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/cli/pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3304 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/cli/plugins.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/cli/template/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      523 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/cli/template/readme.tpl
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4912 2023-04-20 06:52:22.000000 modelscope-1.8.0/modelscope/cli/template/template.tpl
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/configs/
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/configs/examples/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       36 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/configs/examples/configuration.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/exporters/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1360 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/exporters/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/exporters/audio/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      507 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/exporters/audio/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2272 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/exporters/audio/ans_dfsmn_exporter.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2659 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/exporters/base.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      732 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/exporters/builder.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/exporters/cv/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      869 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/exporters/cv/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2585 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/exporters/cv/cartoon_translation_exporter.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3619 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/exporters/cv/face_detection_scrfd_exporter.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1358 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/exporters/cv/object_detection_damoyolo_exporter.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/exporters/multi_modal/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      531 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/exporters/multi_modal/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11238 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/exporters/multi_modal/stable_diffusion_exporter.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/exporters/nlp/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1082 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/exporters/nlp/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7333 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/exporters/nlp/csanmt_for_translation_exporter.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4579 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/exporters/nlp/model_for_token_classification_exporter.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3409 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/exporters/nlp/sbert_for_sequence_classification_exporter.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2316 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/exporters/nlp/sbert_for_zero_shot_classification_exporter.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4809 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/exporters/tf_model_exporter.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    15145 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/exporters/torch_model_exporter.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/fileio/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      122 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/fileio/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10241 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/fileio/file.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/fileio/format/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      143 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/fileio/format/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      454 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/fileio/format/base.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1050 2023-04-20 06:25:03.000000 modelscope-1.8.0/modelscope/fileio/format/json.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    13853 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/fileio/format/jsonplus.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      669 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/fileio/format/yaml.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4254 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/fileio/io.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/hub/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/hub/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    42689 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/hub/api.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3927 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/hub/check_model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1628 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/hub/constants.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11358 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/hub/deploy.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4066 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/hub/errors.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12706 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/hub/file_download.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8958 2023-04-20 06:52:22.000000 modelscope-1.8.0/modelscope/hub/git.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7272 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/hub/push_to_hub.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12489 2023-04-20 06:52:22.000000 modelscope-1.8.0/modelscope/hub/repository.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7178 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/hub/snapshot_download.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/hub/utils/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/hub/utils/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9916 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/hub/utils/caching.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2978 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/hub/utils/utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    56473 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/metainfo.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/metrics/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3978 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/metrics/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2510 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/metrics/accuracy_metric.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7530 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/metrics/action_detection_evaluator.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1925 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/metrics/audio_noise_metric.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1454 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/metrics/base.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1726 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/metrics/bleu_metric.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3715 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/metrics/builder.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/metrics/ciderD/
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)       21 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/metrics/ciderD/__init__.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     1926 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/metrics/ciderD/ciderD.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     8931 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/metrics/ciderD/ciderD_scorer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8664 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/metrics/image_color_enhance_metric.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    13387 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/metrics/image_colorization_metric.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10011 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/metrics/image_denoise_metric.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7612 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/metrics/image_inpainting_metric.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    13318 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/metrics/image_instance_segmentation_metric.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1739 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/metrics/image_portrait_enhancement_metric.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2536 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/metrics/image_quality_assessment_degradation_metric.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1632 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/metrics/image_quality_assessment_mos_metric.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2231 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/metrics/inbatch_recall_metric.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1358 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/metrics/loss_metric.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3002 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/metrics/map_metric.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2032 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/metrics/movie_scene_segmentation_metric.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3197 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/metrics/ned_metric.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2310 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/metrics/ocr_recognition_metric.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2082 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/metrics/ppl_metric.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1163 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/metrics/prediction_saving_wrapper.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4453 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/metrics/referring_video_object_segmentation_metric.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3112 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/metrics/sequence_classification_metric.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3694 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/metrics/text_generation_metric.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3197 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/metrics/text_ranking_metric.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5765 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/metrics/token_classification_metric.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5703 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/metrics/translation_evaluation_metric.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5435 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/metrics/video_frame_interpolation_metric.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8655 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/metrics/video_stabilization_metric.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3092 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/metrics/video_summarization_metric.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/metrics/video_super_resolution_metric/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/metrics/video_super_resolution_metric/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7030 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/metrics/video_super_resolution_metric/matlab_functions.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4771 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/metrics/video_super_resolution_metric/metric_util.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8932 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/metrics/video_super_resolution_metric/niqe.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1710 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/metrics/video_super_resolution_metric/video_super_resolution_metric.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      519 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/audio/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       93 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/audio/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/audio/aec/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/audio/aec/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/audio/aec/layers/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/audio/aec/layers/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1434 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/audio/aec/layers/activations.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2700 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/audio/aec/layers/affine_transform.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5630 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/audio/aec/layers/deep_fsmn.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1322 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/audio/aec/layers/layer_base.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    14384 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/audio/aec/layers/uni_deep_fsmn.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/audio/aec/network/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/audio/aec/network/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    14438 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/audio/aec/network/loss.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9382 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/audio/aec/network/modulation_loss.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    15396 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/audio/aec/network/se_net.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/audio/ans/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      515 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/audio/ans/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6475 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/audio/ans/complex_nn.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3688 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/audio/ans/conv_stft.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2513 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/audio/ans/denoise_net.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10241 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/audio/ans/frcrn.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/audio/ans/layers/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/audio/ans/layers/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1468 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/audio/ans/layers/activations.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2414 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/audio/ans/layers/affine_transform.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      661 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/audio/ans/layers/layer_base.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5284 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/audio/ans/layers/uni_deep_fsmn.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1038 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/audio/ans/se_module_complex.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9833 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/audio/ans/unet.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/audio/asr/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      585 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/audio/asr/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1780 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/audio/asr/generic_automatic_speech_recognition.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1204 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/audio/asr/wenet_automatic_speech_recognition.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/audio/itn/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      557 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/audio/itn/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1462 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/audio/itn/generic_inverse_text_processing.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/audio/kws/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      735 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/audio/kws/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/audio/kws/farfield/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/audio/kws/farfield/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    14349 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/audio/kws/farfield/fsmn.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7462 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/audio/kws/farfield/fsmn_sele_v2.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7861 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/audio/kws/farfield/fsmn_sele_v3.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3521 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/audio/kws/farfield/model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2608 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/audio/kws/farfield/model_def.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      908 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/audio/kws/generic_key_word_spotting.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/audio/kws/nearfield/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/audio/kws/nearfield/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3300 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/audio/kws/nearfield/cmvn.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    17496 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/audio/kws/nearfield/fsmn.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6079 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/audio/kws/nearfield/model.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/audio/punc/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/audio/punc/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1466 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/audio/punc/generic_punctuation.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/audio/separation/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/audio/separation/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2240 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/audio/separation/layer_norm.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    13963 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/audio/separation/mossformer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9142 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/audio/separation/mossformer_block.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9004 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/audio/separation/mossformer_conv_module.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/audio/sv/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7326 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/models/audio/sv/DTDNN.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8425 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/audio/sv/DTDNN_layers.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11692 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/audio/sv/ERes2Net.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11358 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/models/audio/sv/ERes2Net_aug.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      498 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/audio/sv/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6397 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/audio/sv/cluster_backend.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    14980 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/models/audio/sv/ecapa_tdnn.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      904 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/models/audio/sv/fusion.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1488 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/audio/sv/generic_speaker_verification.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3743 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/audio/sv/lanuage_recognition_model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3630 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/models/audio/sv/pooling_layers.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    16895 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/audio/sv/rdino.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12338 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/models/audio/sv/speaker_change_locator.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3519 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/audio/sv/speaker_diarization_dialogue_detection.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5170 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/audio/sv/speaker_diarization_semantic_speaker_turn_detection.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/audio/tts/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      474 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/audio/tts/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11845 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/audio/tts/sambert_hifi.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    26390 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/audio/tts/voice.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/base/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      303 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/base/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1091 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/base/base_head.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7308 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/base/base_model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      605 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/base/base_torch_head.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5679 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/base/base_torch_model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3647 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/builder.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1812 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/cv/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/abnormal_object_detection/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      490 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/abnormal_object_detection/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3806 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/abnormal_object_detection/mmdet_model.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/abnormal_object_detection/mmdet_ms/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      113 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/abnormal_object_detection/mmdet_ms/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      211 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6414 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/mask_scoring_roi_head.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/roi_extractors/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      145 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/roi_extractors/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6729 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/roi_extractors/single_level_roi_extractor.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/action_detection/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      493 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/action_detection/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7307 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/action_detection/action_detection_onnx.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/action_detection/modules/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/action_detection/modules/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9364 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/action_detection/modules/action_detection_pytorch.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12132 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/action_detection/modules/resnet.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/action_recognition/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      709 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/action_recognition/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4304 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/action_recognition/models.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10276 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/action_recognition/s3dg.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    18618 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/action_recognition/tada_convnext.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    45442 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/cv/action_recognition/temporal_patch_shift_transformer.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/animal_recognition/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      558 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/animal_recognition/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    14141 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/animal_recognition/resnet.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3955 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/animal_recognition/splat.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/bad_image_detecting/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      496 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/bad_image_detecting/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2589 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/bad_image_detecting/bad_image_detecting.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/body_2d_keypoints/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      502 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/body_2d_keypoints/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12660 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/body_2d_keypoints/hrnet_basic_modules.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8773 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/cv/body_2d_keypoints/hrnet_v2.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1707 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/body_2d_keypoints/w48.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/body_3d_keypoints/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      601 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/body_3d_keypoints/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/body_3d_keypoints/cannonical_pose/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       51 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/body_3d_keypoints/cannonical_pose/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8957 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/cv/body_3d_keypoints/cannonical_pose/body_3d_pose.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8196 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/body_3d_keypoints/cannonical_pose/canonical_pose_modules.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/body_3d_keypoints/hdformer/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       98 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/body_3d_keypoints/hdformer/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11552 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/body_3d_keypoints/hdformer/backbone.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12870 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/body_3d_keypoints/hdformer/block.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8080 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/cv/body_3d_keypoints/hdformer/directed_graph.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2516 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/body_3d_keypoints/hdformer/hdformer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7719 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/body_3d_keypoints/hdformer/hdformer_detector.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3401 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/body_3d_keypoints/hdformer/skeleton.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/cartoon/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1216 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/cartoon/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/cartoon/facelib/
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/cartoon/facelib/LK/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/cartoon/facelib/LK/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3488 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/cartoon/facelib/LK/lk.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/cartoon/facelib/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      713 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/cartoon/facelib/config.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3512 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/cartoon/facelib/face_detector.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5158 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/cv/cartoon/facelib/face_landmark.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4563 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/cv/cartoon/facelib/facer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9075 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/cartoon/loss.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2174 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/cartoon/model_tf.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/cartoon/mtcnn_pytorch/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/cartoon/mtcnn_pytorch/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/cartoon/mtcnn_pytorch/src/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/cartoon/mtcnn_pytorch/src/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6072 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/cartoon/mtcnn_pytorch/src/align_trans.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8519 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/cartoon/mtcnn_pytorch/src/matlab_cp2tform.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4404 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/cartoon/network.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5265 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/cartoon/utils.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/cmdssl_video_embedding/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      647 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/cmdssl_video_embedding/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3876 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/cmdssl_video_embedding/c3d.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10785 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/cmdssl_video_embedding/resnet2p1d.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8949 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/cmdssl_video_embedding/resnet3d.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      414 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/annotator/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/annotator/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    13516 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/annotator/annotator.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/annotator/midas/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/annotator/midas/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4960 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/annotator/midas/api.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      504 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/base_model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10152 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/blocks.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3307 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/dpt_depth.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2746 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/midas_net.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5829 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/midas_net_custom.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8045 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/transforms.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    15438 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/vit.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4799 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/annotator/midas/utils.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/annotator/mlsd/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/annotator/mlsd/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9896 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/annotator/mlsd/mbv2_mlsd_large.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    25158 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/annotator/mlsd/utils.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/annotator/openpose/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/annotator/openpose/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12595 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/annotator/openpose/body.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3894 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/annotator/openpose/hand.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9024 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/annotator/openpose/model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8031 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/annotator/openpose/util.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8546 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/controlnet.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/crowd_counting/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      491 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/crowd_counting/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1100 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/crowd_counting/cc_model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    22875 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/cv/crowd_counting/hrnet_aspp_relu.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/face_attribute_recognition/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      490 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_attribute_recognition/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/face_attribute_recognition/fair_face/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      115 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_attribute_recognition/fair_face/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2559 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_attribute_recognition/fair_face/face_attribute_recognition.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      930 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/mogface/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       96 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/mogface/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/mogface/models/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/mogface/models/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3032 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/mogface/models/detectors.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4675 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/mogface/models/mogface.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5578 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/mogface/models/mogprednet.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6264 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/mogface/models/resnet.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     7072 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/mogface/models/utils.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/mtcnn/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       97 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/mtcnn/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/mtcnn/models/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/mtcnn/models/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7061 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/mtcnn/models/box_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5415 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/mtcnn/models/detector.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3171 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/mtcnn/models/first_stage.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5232 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/mtcnn/models/get_nets.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/peppa_pig_face/
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/peppa_pig_face/LK/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/peppa_pig_face/LK/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3444 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/peppa_pig_face/LK/lk.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/peppa_pig_face/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3578 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/peppa_pig_face/face_detector.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5129 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/peppa_pig_face/face_landmark.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4210 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/peppa_pig_face/facer.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/retinaface/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       93 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/retinaface/__init__.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     4832 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/retinaface/detection.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/retinaface/models/
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/retinaface/models/__init__.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     4766 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/retinaface/models/net.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     4590 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/retinaface/models/retinaface.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     4120 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/retinaface/utils.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      174 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      955 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/damofd_detect.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)      192 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/bbox/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      325 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/bbox/__init__.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     2945 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/bbox/transforms.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/post_processing/
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)      292 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/post_processing/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3423 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/post_processing/bbox_nms.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      276 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)      471 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11707 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/auto_augment.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4169 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/formating.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7981 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/loading.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)    30670 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/transforms.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     5543 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/retinaface.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)      289 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)      361 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4202 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/master_net.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3590 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/mobilenet.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    16146 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/resnet.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/dense_heads/
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)      270 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/dense_heads/__init__.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)    46999 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/dense_heads/scrfd_head.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)      295 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11062 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/base.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     6092 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/scrfd.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6395 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/single_stage.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     5970 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/tinymog.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3451 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/preprocessor.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3869 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/scrfd_detect.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      960 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/tinymog_detect.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/ulfd_slim/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       90 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/ulfd_slim/__init__.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     1477 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/ulfd_slim/detection.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/ulfd_slim/vision/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/ulfd_slim/vision/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4126 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/ulfd_slim/vision/box_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2058 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/ulfd_slim/vision/mb_tiny.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      571 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/data_preprocessing.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1641 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/fd_config.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3719 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/mb_tiny_fd.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2845 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/predictor.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4621 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/ssd.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1497 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_detection/ulfd_slim/vision/transforms.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/face_emotion/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      540 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_emotion/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/face_emotion/efficient/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      327 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_emotion/efficient/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    15911 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_emotion/efficient/model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    20077 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_emotion/efficient/utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2007 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_emotion/emotion_infer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3171 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_emotion/emotion_model.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/face_emotion/face_alignment/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_emotion/face_alignment/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2844 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_emotion/face_alignment/face.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1674 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_emotion/face_alignment/face_align.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/face_generation/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      475 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_generation/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/face_generation/op/
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)       89 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_generation/op/__init__.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     6497 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_generation/op/conv2d_gradfix.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     3104 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_generation/op/fused_act.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     5922 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_generation/op/upfirdn2d.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)    19998 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_generation/stylegan2.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/face_human_hand_detection/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      544 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_human_hand_detection/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4109 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_human_hand_detection/det_infer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12584 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_human_hand_detection/ghost_pan.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    16846 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_human_hand_detection/nanodet_plus_head.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1833 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_human_hand_detection/one_stage_detector.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5791 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_human_hand_detection/shufflenetv2.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8874 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_human_hand_detection/utils.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/face_recognition/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_recognition/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1572 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_recognition/align_face.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/face_recognition/torchkit/
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)      473 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_recognition/torchkit/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/face_recognition/torchkit/backbone/
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     1080 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_recognition/torchkit/backbone/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6507 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_recognition/torchkit/backbone/arcface_backbone.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     1977 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_recognition/torchkit/backbone/common.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7572 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_recognition/torchkit/backbone/facemask_backbone.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     8551 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_recognition/torchkit/backbone/model_irse.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     4744 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/face_recognition/torchkit/backbone/model_resnet.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7186 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_recognition/torchkit/rts_backbone.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/face_reconstruction/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_reconstruction/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/face_reconstruction/models/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_reconstruction/models/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    31337 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/cv/face_reconstruction/models/bfm.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1428 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/cv/face_reconstruction/models/de_retouching_module.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/face_reconstruction/models/facelandmark/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_reconstruction/models/facelandmark/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2583 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_reconstruction/models/facelandmark/large_base_lmks_infer.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/face_reconstruction/models/facelandmark/nets/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_reconstruction/models/facelandmark/nets/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6561 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_reconstruction/models/facelandmark/nets/large_base_lmks_net.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5124 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_reconstruction/models/facelandmark/nets/large_eyeball_net.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    30403 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/cv/face_reconstruction/models/facerecon_model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    13171 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/cv/face_reconstruction/models/losses.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    21235 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/face_reconstruction/models/networks.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8821 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/cv/face_reconstruction/models/nv_diffrast.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      277 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/cv/face_reconstruction/models/opt.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/face_reconstruction/models/pix2pix/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/cv/face_reconstruction/models/pix2pix/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    31957 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/cv/face_reconstruction/models/pix2pix/networks.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5908 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/cv/face_reconstruction/models/pix2pix/pix2pix_model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      779 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/cv/face_reconstruction/models/pix2pix/pix2pix_options.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)    13410 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/cv/face_reconstruction/models/renderer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4933 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/cv/face_reconstruction/models/unet.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    31247 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/cv/face_reconstruction/utils.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/facial_expression_recognition/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      484 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/facial_expression_recognition/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/facial_expression_recognition/fer/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      121 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/facial_expression_recognition/fer/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2414 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/facial_expression_recognition/fer/facial_expression_recognition.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3277 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/facial_expression_recognition/fer/transforms.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1311 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/facial_expression_recognition/fer/vgg.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/facial_landmark_confidence/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      478 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/facial_landmark_confidence/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/facial_landmark_confidence/flc/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      115 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/facial_landmark_confidence/flc/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3197 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/facial_landmark_confidence/flc/facial_landmark_confidence.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4932 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/facial_landmark_confidence/flc/manual_landmark_net.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/hand_static/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      502 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/hand_static/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2480 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/hand_static/hand_model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10891 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/hand_static/networks.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/human_reconstruction/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5936 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/human_reconstruction/Reconstruction.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/human_reconstruction/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/human_reconstruction/models/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1065 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/human_reconstruction/models/Embedding.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4542 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/human_reconstruction/models/PixToMesh.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11301 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/human_reconstruction/models/Res_backbone.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2418 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/human_reconstruction/models/Surface_head.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/human_reconstruction/models/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2716 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/human_reconstruction/models/detectors.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1982 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/human_reconstruction/models/geometry.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2248 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/cv/human_reconstruction/models/human_segmenter.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11865 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/human_reconstruction/models/networks.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6105 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/cv/human_reconstruction/utils.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_binary_quant_classification/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      573 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_binary_quant_classification/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2859 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_binary_quant_classification/binary_quant_model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    19111 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_binary_quant_classification/bnext.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_body_reshaping/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      500 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_body_reshaping/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3920 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_body_reshaping/image_body_reshaping.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6541 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_body_reshaping/model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    13616 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_body_reshaping/person_info.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_body_reshaping/pose_estimator/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_body_reshaping/pose_estimator/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12027 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_body_reshaping/pose_estimator/body.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5673 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_body_reshaping/pose_estimator/model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1311 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_body_reshaping/pose_estimator/util.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    15760 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_body_reshaping/slim_utils.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_classification/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      598 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_classification/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_classification/backbones/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      107 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_classification/backbones/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    20295 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_classification/backbones/beit_v2.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    17261 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_classification/backbones/nextvit.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2185 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_classification/mmcls_model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1554 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_classification/resnet50_cc.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5255 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_classification/utils.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_color_enhance/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      704 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_color_enhance/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_color_enhance/adaint/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       44 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_color_enhance/adaint/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    14338 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_color_enhance/adaint/adaint.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3753 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_color_enhance/csrnet.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_color_enhance/deeplpf/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       66 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_color_enhance/deeplpf/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2575 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_color_enhance/deeplpf/deeplpf_image_color_enhance.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    31764 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_color_enhance/deeplpf/deeplpfnet.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2821 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_color_enhance/image_color_enhance.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_colorization/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      640 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_colorization/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_colorization/ddcolor/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      122 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_colorization/ddcolor/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9385 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_colorization/ddcolor/ddcolor.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6437 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_colorization/ddcolor/ddcolor_for_image_colorization.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9576 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_colorization/ddcolor/loss.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_colorization/ddcolor/utils/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_colorization/ddcolor/utils/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6857 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_colorization/ddcolor/utils/convnext.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2196 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_colorization/ddcolor/utils/position_encoding.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7796 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_colorization/ddcolor/utils/transformer_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6465 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_colorization/ddcolor/utils/unet.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6510 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_colorization/ddcolor/utils/vgg.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_colorization/unet/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      129 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_colorization/unet/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10574 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_colorization/unet/unet.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10647 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_colorization/unet/utils.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_debanding/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      483 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_debanding/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_debanding/rrdb/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       53 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_debanding/rrdb/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3005 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_debanding/rrdb/rrdb_image_debanding.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_deblur/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      510 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_deblur/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4241 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_deblur/nafnet_for_image_deblur.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_defrcn_fewshot/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      492 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_defrcn_fewshot/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4329 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_defrcn_fewshot/defrcn_for_fewshot.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_defrcn_fewshot/evaluation/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_defrcn_fewshot/evaluation/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12474 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_defrcn_fewshot/evaluation/coco_evaluation.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3141 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_defrcn_fewshot/evaluation/evaluator.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4354 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_defrcn_fewshot/evaluation/pascal_voc_evaluation.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_defrcn_fewshot/models/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      448 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_defrcn_fewshot/models/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6827 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_defrcn_fewshot/models/calibration_layer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7064 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_defrcn_fewshot/models/defrcn.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10779 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_defrcn_fewshot/models/fast_rcnn.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1220 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_defrcn_fewshot/models/gdl.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1732 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_defrcn_fewshot/models/resnet.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4770 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_defrcn_fewshot/models/roi_heads.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_defrcn_fewshot/utils/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_defrcn_fewshot/utils/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    23599 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_defrcn_fewshot/utils/coco_register.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4803 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_defrcn_fewshot/utils/configuration_mapper.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3595 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_defrcn_fewshot/utils/model_surgery_op.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      583 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_defrcn_fewshot/utils/register_data.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2617 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_defrcn_fewshot/utils/requirements_check.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11013 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/cv/image_defrcn_fewshot/utils/voc_register.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_denoise/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      514 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_denoise/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_denoise/nafnet/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6664 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_denoise/nafnet/NAFNet_arch.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_denoise/nafnet/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1627 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_denoise/nafnet/arch_util.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2475 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_denoise/nafnet_for_image_denoise.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_depth_estimation/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       50 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_depth_estimation/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_depth_estimation/networks/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       50 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_depth_estimation/networks/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6757 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_depth_estimation/networks/newcrf_depth.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    18233 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_depth_estimation/networks/newcrf_layers.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10106 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_depth_estimation/networks/newcrf_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    26006 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_depth_estimation/networks/swin_transformer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    13560 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_depth_estimation/networks/uper_crf_head.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1600 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_depth_estimation/newcrfs_model.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_depth_estimation_bts/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      536 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_depth_estimation_bts/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2606 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_depth_estimation_bts/depth_estimation_bts_model.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_depth_estimation_bts/networks/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_depth_estimation_bts/networks/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1485 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_depth_estimation_bts/networks/bts_model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2819 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_depth_estimation_bts/networks/decoder.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2506 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_depth_estimation_bts/networks/encoder.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8545 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_depth_estimation_bts/networks/utils.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_driving_perception/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      999 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_driving_perception/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2022 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_driving_perception/image_driving_percetion_model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4352 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_driving_perception/preprocessor.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7475 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_driving_perception/utils.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_face_fusion/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      488 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_face_fusion/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_face_fusion/facegan/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_face_fusion/facegan/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3124 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_face_fusion/facegan/gan_wrap.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    21432 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_face_fusion/facegan/model.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_face_fusion/facegan/op/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      242 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_face_fusion/facegan/op/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6497 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_face_fusion/facegan/op/conv2d_gradfix.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3094 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_face_fusion/facegan/op/fused_act.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5912 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_face_fusion/facegan/op/upfirdn2d.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_face_fusion/facelib/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_face_fusion/facelib/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10514 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_face_fusion/facelib/align_trans.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5955 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_face_fusion/facelib/matlab_cp2tform.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9077 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_face_fusion/image_face_fusion.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_face_fusion/network/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_face_fusion/network/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3045 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_face_fusion/network/aad_layer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8555 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_face_fusion/network/aei_flow_net.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9366 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_face_fusion/network/bfm.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12449 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_face_fusion/network/dense_motion.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    20604 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_face_fusion/network/facerecon_model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7433 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_face_fusion/network/model_irse.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7790 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_face_fusion/network/ops.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_human_parsing/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      575 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_human_parsing/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_human_parsing/backbone/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      525 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_human_parsing/backbone/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11736 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_human_parsing/backbone/deeplab_resnet.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_human_parsing/m2fp/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      640 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_human_parsing/m2fp/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8257 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_human_parsing/m2fp/m2fp_decoder.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8361 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_human_parsing/m2fp/m2fp_encoder.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    15097 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_human_parsing/m2fp_net.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5634 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/models/cv/image_human_parsing/parsing_utils.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_inpainting/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      475 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_inpainting/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2690 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_inpainting/base.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7605 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_inpainting/default.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1253 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_inpainting/model.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_inpainting/modules/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_inpainting/modules/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_inpainting/modules/ade20k/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       81 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_inpainting/modules/ade20k/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12202 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_inpainting/modules/ade20k/base.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5348 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_inpainting/modules/ade20k/resnet.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7509 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_inpainting/modules/adversarial.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1564 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_inpainting/modules/feature_matching.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    19052 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_inpainting/modules/ffc.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11842 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_inpainting/modules/inception.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1511 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_inpainting/modules/perceptual.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1965 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_inpainting/modules/pix2pixhd.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    13350 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_inpainting/refinement.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1065 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/backbones/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      664 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/backbones/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3826 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/backbones/resnet.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    27174 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/backbones/swin_transformer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9305 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/cascade_mask_rcnn_swin.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/datasets/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      101 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/datasets/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3992 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/datasets/transforms.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/fastinst/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       50 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/fastinst/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    14867 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/fastinst/fastinst_decoder.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6266 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/fastinst/fastinst_encoder.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8692 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/fastinst_model.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/maskdino/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      600 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/maskdino/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10072 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/maskdino/dino_decoder.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    15407 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/maskdino/maskdino_decoder.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    18188 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/maskdino/maskdino_encoder.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7274 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/maskdino/ms_deform_attn.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2720 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/maskdino/position_encoding.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6712 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/maskdino/utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1429 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/maskdino_model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11454 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/maskdino_swin.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1549 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7886 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/postprocess_utils.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_matching/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      553 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_matching/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_matching/config/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_matching/config/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6991 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_matching/config/default.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_matching/loftr_quadtree/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      171 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_matching/loftr_quadtree/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_matching/loftr_quadtree/backbone/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      588 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_matching/loftr_quadtree/backbone/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7244 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_matching/loftr_quadtree/backbone/resnet_fpn.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3784 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_matching/loftr_quadtree/loftr.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      239 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2968 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/fine_preprocess.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3012 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/linear_attention.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2994 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/quadtree_attention.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9677 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/transformer.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_matching/loftr_quadtree/utils/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_matching/loftr_quadtree/utils/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10531 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_matching/loftr_quadtree/utils/coarse_matching.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3048 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_matching/loftr_quadtree/utils/fine_matching.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2132 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_matching/loftr_quadtree/utils/position_encoding.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2735 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_matching/quadtree_attention_model.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_matching/utils/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_matching/utils/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      344 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_matching/utils/misc.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_mvs_depth_estimation/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      521 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_mvs_depth_estimation/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8546 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_mvs_depth_estimation/cas_mvsnet.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6274 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_mvs_depth_estimation/casmvs_model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    18175 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/cv/image_mvs_depth_estimation/colmap2mvsnet.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9861 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/cv/image_mvs_depth_estimation/depth_filter.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9404 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_mvs_depth_estimation/general_eval_dataset.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    21735 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_mvs_depth_estimation/module.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3367 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_mvs_depth_estimation/utils.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_paintbyexample/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      507 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_paintbyexample/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1567 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_paintbyexample/model.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_panoptic_segmentation/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      513 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/cv/image_panoptic_segmentation/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1694 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_panoptic_segmentation/panseg_model.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_portrait_enhancement/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      538 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_portrait_enhancement/__init__.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     8563 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_portrait_enhancement/align_faces.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_portrait_enhancement/eqface/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_portrait_enhancement/eqface/__init__.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     1836 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_portrait_enhancement/eqface/fqa.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4710 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_portrait_enhancement/eqface/model_resnet.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)    22869 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_portrait_enhancement/gpen.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7114 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_portrait_enhancement/image_portrait_enhancement.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_portrait_enhancement/losses/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_portrait_enhancement/losses/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4336 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_portrait_enhancement/losses/helpers.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3286 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_portrait_enhancement/losses/losses.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3155 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_portrait_enhancement/losses/model_irse.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_portrait_enhancement/retinaface/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_portrait_enhancement/retinaface/__init__.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     7313 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_portrait_enhancement/retinaface/detection.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_portrait_enhancement/retinaface/models/
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_portrait_enhancement/retinaface/models/__init__.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     4845 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_portrait_enhancement/retinaface/models/net.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     4676 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_portrait_enhancement/retinaface/models/retinaface.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     4120 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_portrait_enhancement/retinaface/utils.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_probing_model/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      533 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_probing_model/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10076 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_probing_model/backbone.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3319 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_probing_model/model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5206 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_probing_model/utils.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_quality_assessment_degradation/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      584 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_quality_assessment_degradation/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5064 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_quality_assessment_degradation/degradation_model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4654 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_quality_assessment_degradation/image_quality_assessment_degradation.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_quality_assessment_man/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      544 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_quality_assessment_man/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2600 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_quality_assessment_man/image_quality_assessment_man.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5163 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_quality_assessment_man/maniqa.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    23230 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_quality_assessment_man/swin.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_quality_assessment_mos/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      544 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_quality_assessment_mos/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_quality_assessment_mos/backbones/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      272 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_quality_assessment_mos/backbones/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    16222 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_quality_assessment_mos/backbones/resnet.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1060 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_quality_assessment_mos/censeo_ivqa_model.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_quality_assessment_mos/heads/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       36 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_quality_assessment_mos/heads/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      529 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_quality_assessment_mos/heads/simple_head.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2651 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_quality_assessment_mos/image_quality_assessment_mos.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_reid_person/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      467 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_reid_person/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5444 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_reid_person/pass_model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    14108 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_reid_person/transreid_model.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_restoration/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      527 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_restoration/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_restoration/demoire_models/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       69 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_restoration/demoire_models/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10243 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_restoration/demoire_models/nets.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2640 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_restoration/image_restoration_model.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      712 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7896 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/data_util.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4661 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/feature_extractors.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5155 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/pixel_classifier.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2273 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3208 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/ddpm_segmentation_model.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/pan_merge/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      111 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/pan_merge/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1516 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/pan_merge/base_panoptic_fusion_head.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2122 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/pan_merge/maskformer_semantic_head.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2566 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/semantic_seg_model.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      303 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      290 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      249 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    17484 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/adapter_modules.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/base/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      196 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/base/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    18241 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/base/beit.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6379 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/beit_adapter.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      251 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10785 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/base_decode_head.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    26514 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/mask2former_head_from_mmseg.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      253 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12370 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/base_segmentor.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11715 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/encoder_decoder_mask2former.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/utils/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      368 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/utils/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      398 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/utils/builder.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1938 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/utils/data_process_func.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1788 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/utils/seg_func.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_skychange/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      650 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_skychange/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9312 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_skychange/preprocessor.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_skychange/ptsemseg/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3740 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_skychange/ptsemseg/BlockModules.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_skychange/ptsemseg/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    21275 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/cv/image_skychange/ptsemseg/hrnet_backnone.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    18023 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/cv/image_skychange/ptsemseg/hrnet_super_and_ocr.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8159 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_skychange/ptsemseg/unet.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11210 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_skychange/skychange.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7874 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_skychange/skychange_model.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_to_image_generation/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      120 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_to_image_generation/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_to_image_generation/data/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      561 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_to_image_generation/data/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3309 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_to_image_generation/data/transforms.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10476 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_to_image_generation/model.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_to_image_generation/models/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      603 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_to_image_generation/models/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12670 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_to_image_generation/models/autoencoder.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12594 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_to_image_generation/models/clip.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_to_image_generation/ops/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      561 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_to_image_generation/ops/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    24469 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_to_image_generation/ops/diffusion.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1320 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_to_image_generation/ops/losses.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_to_image_translation/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      536 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_to_image_translation/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_to_image_translation/data/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      127 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_to_image_translation/data/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3309 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_to_image_translation/data/transforms.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10527 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_to_image_translation/model_translation.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_to_image_translation/models/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      161 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_to_image_translation/models/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12670 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_to_image_translation/models/autoencoder.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12594 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_to_image_translation/models/clip.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_to_image_translation/ops/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      384 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_to_image_translation/ops/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    21828 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_to_image_translation/ops/apps.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    36549 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_to_image_translation/ops/degradation.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    24615 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/image_to_image_translation/ops/diffusion.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1320 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_to_image_translation/ops/losses.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4389 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_to_image_translation/ops/metrics.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6736 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_to_image_translation/ops/random_color.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2745 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_to_image_translation/ops/random_mask.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3777 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_to_image_translation/ops/svd.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6728 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/image_to_image_translation/ops/utils.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/image_try_on/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      518 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/image_try_on/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    15834 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/image_try_on/generator.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    16386 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/image_try_on/landmark.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9109 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/image_try_on/try_on_infer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    46223 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/image_try_on/warping.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/indoor_layout_estimation/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      486 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/indoor_layout_estimation/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/indoor_layout_estimation/networks/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       50 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/indoor_layout_estimation/networks/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/indoor_layout_estimation/networks/backbone/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      136 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/indoor_layout_estimation/networks/backbone/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4099 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/indoor_layout_estimation/networks/backbone/resnet_DA.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6931 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/indoor_layout_estimation/networks/backbone/vit_horizon_pry_image.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/indoor_layout_estimation/networks/misc/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       50 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/indoor_layout_estimation/networks/misc/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2510 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/indoor_layout_estimation/networks/misc/fourier.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3251 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/indoor_layout_estimation/networks/misc/panostretch.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    14887 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/indoor_layout_estimation/networks/misc/post_proc.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/indoor_layout_estimation/networks/modality/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       86 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/indoor_layout_estimation/networks/modality/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5518 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/indoor_layout_estimation/networks/modality/layout.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3406 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/indoor_layout_estimation/networks/panovit.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4808 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/indoor_layout_estimation/networks/utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1834 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/indoor_layout_estimation/panovit.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/language_guided_video_summarization/
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)      542 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/language_guided_video_summarization/__init__.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     6441 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/language_guided_video_summarization/summarizer.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/language_guided_video_summarization/transformer/
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)      508 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/language_guided_video_summarization/transformer/__init__.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     1900 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/language_guided_video_summarization/transformer/layers.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     7275 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/language_guided_video_summarization/transformer/models.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)      826 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/language_guided_video_summarization/transformer/modules.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     2690 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/language_guided_video_summarization/transformer/sub_layers.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/motion_generation/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      639 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/motion_generation/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2151 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/motion_generation/model.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/motion_generation/modules/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/motion_generation/modules/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1209 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/motion_generation/modules/cfg_sampler.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    26783 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/motion_generation/modules/gaussian_diffusion.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    13594 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/motion_generation/modules/mdm.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5412 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/motion_generation/modules/respace.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3914 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/motion_generation/modules/rotation2xyz.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3189 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/motion_generation/modules/smpl.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/movie_scene_segmentation/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      615 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/movie_scene_segmentation/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1567 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/movie_scene_segmentation/get_model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8465 2023-04-20 06:52:22.000000 modelscope-1.8.0/modelscope/models/cv/movie_scene_segmentation/model.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/movie_scene_segmentation/utils/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      181 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/movie_scene_segmentation/utils/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      798 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/movie_scene_segmentation/utils/head.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4291 2023-04-20 06:52:22.000000 modelscope-1.8.0/modelscope/models/cv/movie_scene_segmentation/utils/save_op.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10325 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/movie_scene_segmentation/utils/shot_encoder.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5057 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/movie_scene_segmentation/utils/trn.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_4k/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      598 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_4k/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_4k/dataloader/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_4k/dataloader/__init__.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     2934 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_4k/dataloader/load_blender.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     4301 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_4k/dataloader/load_data.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)    17159 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_4k/dataloader/load_llff.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     2498 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_4k/dataloader/load_tankstemple.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    20567 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_4k/dataloader/read_write_model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7297 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_4k/nerf_preprocess.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)    10526 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_4k/nerf_recon_4k.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_4k/network/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_4k/network/__init__.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)    77434 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_4k/network/dvgo.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5720 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_4k/network/utils.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_acc/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      602 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_acc/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_acc/dataloader/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_acc/dataloader/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    18782 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_acc/dataloader/nerf_dataset.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    20567 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_acc/dataloader/read_write_model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7297 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_acc/nerf_preprocess.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7876 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_acc/nerf_recon_acc.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_acc/network/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_acc/network/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    13317 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_acc/network/nerf.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1509 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_acc/network/segmenter.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5720 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_acc/network/utils.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_vq_compression/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      662 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_vq_compression/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_vq_compression/dataloader/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      315 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_vq_compression/dataloader/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6055 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_vq_compression/dataloader/blender.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10222 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_vq_compression/dataloader/llff.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7105 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_vq_compression/dataloader/nsvf.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10361 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_vq_compression/dataloader/ray_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9228 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_vq_compression/dataloader/tankstemple.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3890 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_vq_compression/nerf_recon_vq_compression.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_vq_compression/network/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       61 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_vq_compression/network/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    23489 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_vq_compression/network/tensoRF.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11430 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_vq_compression/network/tensoRF_VQ.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    19228 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_vq_compression/network/tensorBase.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    16555 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_vq_compression/network/weighted_vq.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7142 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_vq_compression/renderer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7854 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/nerf_recon_vq_compression/utils.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/object_detection/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      606 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3432 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/object_detection/mmdet_model.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/object_detection/mmdet_ms/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      321 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection/mmdet_ms/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/object_detection/mmdet_ms/backbones/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      211 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection/mmdet_ms/backbones/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    21306 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/object_detection/mmdet_ms/backbones/vit.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/object_detection/mmdet_ms/dense_heads/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      278 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection/mmdet_ms/dense_heads/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2220 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection/mmdet_ms/dense_heads/anchor_head.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11529 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection/mmdet_ms/dense_heads/rpn_head.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/object_detection/mmdet_ms/necks/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      213 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection/mmdet_ms/necks/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9021 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection/mmdet_ms/necks/fpn.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      426 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/bbox_heads/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      375 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/bbox_heads/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8542 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/bbox_heads/convfc_bbox_head.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/mask_heads/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      239 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/mask_heads/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    17739 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/mask_heads/fcn_mask_head.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/object_detection/mmdet_ms/utils/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      306 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection/mmdet_ms/utils/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    21926 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection/mmdet_ms/utils/checkpoint.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1204 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection/mmdet_ms/utils/convModule_norm.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      467 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       86 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2657 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/depe_detect.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      605 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/assigners/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      299 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/assigners/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6670 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/assigners/hungarian_assigner_3d.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/coders/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      282 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/coders/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4476 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/coders/nms_free_coder.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/match_costs/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      276 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/match_costs/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1064 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/match_costs/match_cost.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2041 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/util.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      294 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3095 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/nuscenes_dataset.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      522 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7936 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/loading.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8689 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/transform_3d.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/backbones/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      255 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/backbones/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12874 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/backbones/vovnet.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      282 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7442 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/depth_net.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    59197 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/petrv2_dednhead.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/detectors/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      255 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/detectors/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9533 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/detectors/petr3d.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/necks/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      256 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/necks/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9032 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/necks/cp_fpn.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      562 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    18126 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/petr_transformer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5175 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/positional_encoding.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11047 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/result_vis.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/ocr_detection/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      473 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/ocr_detection/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2969 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/models/cv/ocr_detection/model.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/ocr_detection/modules/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/ocr_detection/modules/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    26142 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/models/cv/ocr_detection/modules/dbnet.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    28715 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/models/cv/ocr_detection/modules/layers.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    31084 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/models/cv/ocr_detection/modules/mix_ops.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5974 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/models/cv/ocr_detection/modules/proxyless.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8217 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/ocr_detection/modules/seg_detector_loss.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2647 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/cv/ocr_detection/preprocessor.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7972 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/ocr_detection/utils.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/ocr_recognition/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      477 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/ocr_recognition/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6378 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/models/cv/ocr_recognition/model.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/ocr_recognition/modules/
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/ocr_recognition/modules/CRNN/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/models/cv/ocr_recognition/modules/CRNN/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3724 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/models/cv/ocr_recognition/modules/CRNN/main_model.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/ocr_recognition/modules/ConvNextViT/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/models/cv/ocr_recognition/modules/ConvNextViT/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6209 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/models/cv/ocr_recognition/modules/ConvNextViT/convnext.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      753 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/models/cv/ocr_recognition/modules/ConvNextViT/main_model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11644 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/models/cv/ocr_recognition/modules/ConvNextViT/timm_tinyc.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1955 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/models/cv/ocr_recognition/modules/ConvNextViT/vitstr.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/ocr_recognition/modules/LightweightEdge/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/models/cv/ocr_recognition/modules/LightweightEdge/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1458 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/models/cv/ocr_recognition/modules/LightweightEdge/main_model.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/ocr_recognition/modules/LightweightEdge/nas_block/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       43 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/models/cv/ocr_recognition/modules/LightweightEdge/nas_block/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    25097 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/models/cv/ocr_recognition/modules/LightweightEdge/nas_block/layers.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10662 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/models/cv/ocr_recognition/modules/LightweightEdge/nas_block/mix_ops.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5031 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/models/cv/ocr_recognition/modules/LightweightEdge/nas_block/proxyless.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/ocr_recognition/modules/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3710 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/models/cv/ocr_recognition/preprocessor.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/open_vocabulary_detection_vild/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      501 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/open_vocabulary_detection_vild/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    14225 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/cv/open_vocabulary_detection_vild/vild.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/panorama_depth_estimation/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      511 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/panorama_depth_estimation/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/panorama_depth_estimation/networks/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      102 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/panorama_depth_estimation/networks/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5077 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/panorama_depth_estimation/networks/equi.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7500 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/cv/panorama_depth_estimation/networks/layers.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7839 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/panorama_depth_estimation/networks/mobilenet.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    14421 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/panorama_depth_estimation/networks/resnet.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9175 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/panorama_depth_estimation/networks/unifuse.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3962 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/panorama_depth_estimation/networks/util.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3307 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/panorama_depth_estimation/unifuse_model.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/pedestrian_attribute_recognition/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      488 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/cv/pedestrian_attribute_recognition/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3468 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/cv/pedestrian_attribute_recognition/model.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/pointcloud_sceneflow_estimation/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      495 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/pointcloud_sceneflow_estimation/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    16147 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/pointcloud_sceneflow_estimation/common.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11898 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/pointcloud_sceneflow_estimation/pointnet2_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1797 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/pointcloud_sceneflow_estimation/rcp_model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    18618 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/pointcloud_sceneflow_estimation/sf_rcp.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/product_retrieval_embedding/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      548 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/product_retrieval_embedding/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    20974 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/product_retrieval_embedding/item_detection.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4509 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/product_retrieval_embedding/item_embedding.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4340 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/product_retrieval_embedding/item_model.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/product_segmentation/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      528 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/product_segmentation/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7978 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/product_segmentation/net.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2194 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/product_segmentation/seg_infer.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/referring_video_object_segmentation/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      514 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/referring_video_object_segmentation/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5832 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/referring_video_object_segmentation/model.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/referring_video_object_segmentation/utils/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      318 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/referring_video_object_segmentation/utils/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8504 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/referring_video_object_segmentation/utils/backbone.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9206 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/referring_video_object_segmentation/utils/criterion.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7299 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/referring_video_object_segmentation/utils/matcher.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7935 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/referring_video_object_segmentation/utils/misc.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6242 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/referring_video_object_segmentation/utils/mttr.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    16610 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/referring_video_object_segmentation/utils/multimodal_transformer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2091 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/referring_video_object_segmentation/utils/position_encoding_2d.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5629 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/referring_video_object_segmentation/utils/postprocessing.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5180 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/referring_video_object_segmentation/utils/segmentation.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    27941 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/referring_video_object_segmentation/utils/swin_transformer.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/robust_image_classification/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      501 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/robust_image_classification/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2902 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/robust_image_classification/easyrobust_model.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/s2net_panorama_depth_estimation/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      507 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/s2net_panorama_depth_estimation/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/s2net_panorama_depth_estimation/networks/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      219 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/s2net_panorama_depth_estimation/networks/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5374 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/s2net_panorama_depth_estimation/networks/config.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    15565 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/s2net_panorama_depth_estimation/networks/decoder.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5737 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/s2net_panorama_depth_estimation/networks/model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    15735 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/s2net_panorama_depth_estimation/networks/resnet.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    26071 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/s2net_panorama_depth_estimation/networks/swin_transformer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    17279 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/s2net_panorama_depth_estimation/networks/util_helper.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3582 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/s2net_panorama_depth_estimation/s2net_model.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/salient_detection/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      497 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/salient_detection/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/salient_detection/models/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      213 2023-03-15 14:02:28.000000 modelscope-1.8.0/modelscope/models/cv/salient_detection/models/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/salient_detection/models/backbone/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6558 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/salient_detection/models/backbone/Res2Net_v1b.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      256 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/salient_detection/models/backbone/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6525 2023-03-15 14:02:28.000000 modelscope-1.8.0/modelscope/models/cv/salient_detection/models/modules.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3047 2023-03-15 14:02:28.000000 modelscope-1.8.0/modelscope/models/cv/salient_detection/models/senet.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12007 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/salient_detection/models/u2net.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3524 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/salient_detection/models/utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2704 2023-03-15 14:02:28.000000 modelscope-1.8.0/modelscope/models/cv/salient_detection/salient_model.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/shop_segmentation/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      464 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/shop_segmentation/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2203 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/shop_segmentation/common.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4385 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/shop_segmentation/head_fpn.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    32989 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/shop_segmentation/models.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9319 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/shop_segmentation/neck_fpn.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5632 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/shop_segmentation/shop_seg_base.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4095 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/shop_segmentation/shop_seg_model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6699 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/shop_segmentation/utils.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/skin_retouching/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/skin_retouching/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/skin_retouching/detection_model/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/skin_retouching/detection_model/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1780 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/skin_retouching/detection_model/detection_module.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2532 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/skin_retouching/detection_model/detection_unet_in.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/skin_retouching/inpainting_model/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/skin_retouching/inpainting_model/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5759 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/skin_retouching/inpainting_model/gconv.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3233 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/skin_retouching/inpainting_model/inpainting_unet.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/skin_retouching/retinaface/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/skin_retouching/retinaface/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10898 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/skin_retouching/retinaface/box_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3911 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/skin_retouching/retinaface/net.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4799 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/skin_retouching/retinaface/network.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5003 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/skin_retouching/retinaface/predict_single.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1035 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/skin_retouching/retinaface/prior_box.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2110 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/skin_retouching/retinaface/utils.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     3890 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/skin_retouching/unet_deploy.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9917 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/skin_retouching/utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1169 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/skin_retouching/weights_init.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/stream_yolo/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      526 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/stream_yolo/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/stream_yolo/data/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/stream_yolo/data/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2094 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/stream_yolo/data/data_augment.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/stream_yolo/exp/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      165 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/stream_yolo/exp/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      274 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/stream_yolo/exp/base_exp.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      392 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/stream_yolo/exp/build.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/stream_yolo/exp/default/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      137 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/stream_yolo/exp/default/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1209 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/stream_yolo/exp/default/streamyolo.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1845 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/stream_yolo/exp/yolox_base.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/stream_yolo/models/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      238 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/stream_yolo/models/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6274 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/stream_yolo/models/darknet.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10911 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/stream_yolo/models/dfp_pafpn.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6156 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/stream_yolo/models/network_blocks.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1314 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/stream_yolo/models/streamyolo.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5669 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/stream_yolo/models/tal_head.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4352 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/stream_yolo/realtime_video_detector.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/stream_yolo/utils/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      270 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/stream_yolo/utils/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3681 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/stream_yolo/utils/boxes.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      209 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/stream_yolo/utils/format.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/super_resolution/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      555 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/super_resolution/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7652 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/super_resolution/arch_util.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10508 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/super_resolution/ecb.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3525 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/super_resolution/ecbsr_model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4945 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/super_resolution/rrdbnet_arch.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/table_recognition/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      462 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/table_recognition/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    16198 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/models/cv/table_recognition/lineless_table_process.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3360 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/table_recognition/model_lore.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/table_recognition/modules/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/table_recognition/modules/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12614 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/table_recognition/modules/lore_detector.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    15090 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/table_recognition/modules/lore_processor.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/text_driven_segmentation/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       96 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/text_driven_segmentation/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5488 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/text_driven_segmentation/clip.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      777 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/text_driven_segmentation/lseg_base.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7587 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/text_driven_segmentation/lseg_blocks.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4143 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/text_driven_segmentation/lseg_model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6145 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/text_driven_segmentation/lseg_net.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    18951 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/text_driven_segmentation/lseg_vit.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    16418 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/text_driven_segmentation/model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5165 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/text_driven_segmentation/simple_tokenizer.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/text_to_360panorama_image/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      680 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/text_to_360panorama_image/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    39007 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/text_to_360panorama_image/pipeline_base.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    56816 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/cv/text_to_360panorama_image/pipeline_sr.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_classfication/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      594 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_classfication/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    43681 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_classfication/basic_blocks.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2174 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_classfication/global_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2902 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_classfication/master_net.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      766 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_classfication/model_zoo.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2892 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_classfication/plain_net_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7405 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_classfication/super_blocks.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    14983 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_classfication/super_res_idwexkx.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8709 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_classfication/super_res_k1kxk1.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6923 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_classfication/super_res_kxkx.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      699 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       50 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/apis/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       50 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/apis/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2067 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/apis/detector_evaluater.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4000 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/apis/detector_inference.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/augmentations/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       49 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/augmentations/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       49 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3460 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/box_level_augs.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8123 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/color_augs.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2345 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/gaussian_maps.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5911 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/geometric_augs.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2842 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/augmentations/scale_aware_aug.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      148 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      621 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3701 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/darknet.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9356 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/tinynas_csp.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7465 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/tinynas_res.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       50 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    14261 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/base_ops.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10559 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/neck_ops.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    13101 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/ops.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)    18626 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/ota_assigner.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7526 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/repvgg_block.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2572 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      561 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/weight_init.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/heads/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      409 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/heads/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12602 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/heads/gfocal_v2_tiny.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    19162 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/heads/zero_head.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/losses/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       50 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/losses/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5588 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/losses/distill_loss.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12284 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/losses/gfocal_loss.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/necks/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      421 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/necks/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7347 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/necks/giraffe_config.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    24842 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/necks/giraffe_fpn.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3499 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/necks/giraffe_fpn_btn.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/detectors/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       50 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/detectors/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2809 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/detectors/detector.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/structures/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       49 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/structures/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9197 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/structures/bounding_box.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2627 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/structures/boxlist_ops.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2774 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/structures/image_list.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/utils/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      189 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/utils/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11594 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/utils/boxes.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5838 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/utils/model_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1164 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/utils/scheduler.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6400 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/detector.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      627 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/tinynas_damoyolo.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      643 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/tinynas_detector.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1026 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/tinynas_detection/utils.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_deinterlace/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2896 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_deinterlace/UNet_for_video_deinterlace.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      494 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_deinterlace/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      773 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_deinterlace/deinterlace_arch.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_deinterlace/models/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_deinterlace/models/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3159 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_deinterlace/models/archs.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1441 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_deinterlace/models/deep_fourier_upsampling.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2972 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_deinterlace/models/enh.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3572 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_deinterlace/models/fre.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3716 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_deinterlace/models/utils.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      483 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/configs/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       50 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/configs/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12753 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/configs/default_config.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6731 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/dro_model.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/geometry/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       50 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/geometry/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5494 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/geometry/camera.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2285 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/geometry/camera_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3737 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/geometry/pose.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3685 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/geometry/pose_utils.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/models/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       50 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/models/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5277 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/models/model_checkpoint.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2356 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/models/model_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10098 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/models/model_wrapper.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7204 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/models/sfm_model_mf.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4387 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/models/sup_model_mf.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/networks/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       50 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/networks/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/networks/depth_pose/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       50 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/networks/depth_pose/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10532 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/networks/depth_pose/depth_pose_net.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/networks/layers/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       50 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/networks/layers/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       50 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8902 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/depth_decoder.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1607 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/layers.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1915 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/pose_decoder.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4110 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/resnet_encoder.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/networks/optim/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       50 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/networks/optim/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    15781 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/networks/optim/extractor.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8039 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/networks/optim/update.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/utils/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       50 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/utils/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6815 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/utils/augmentations.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10248 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/utils/config.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    15068 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/utils/depth.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1153 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/utils/horovod.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10965 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/utils/image.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9407 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/utils/image_gt.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6499 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/utils/load.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2726 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/utils/misc.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1348 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/utils/types.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_frame_interpolation/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1894 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_frame_interpolation/VFINet_arch.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3334 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_frame_interpolation/VFINet_for_video_frame_interpolation.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      458 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_frame_interpolation/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_frame_interpolation/flow_model/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_frame_interpolation/flow_model/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3212 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_frame_interpolation/flow_model/corr.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9228 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_frame_interpolation/flow_model/extractor.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5346 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_frame_interpolation/flow_model/raft.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5540 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_frame_interpolation/flow_model/update.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_frame_interpolation/interp_model/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    13745 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_frame_interpolation/interp_model/IFNet_swin.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4049 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_frame_interpolation/interp_model/UNet.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_frame_interpolation/interp_model/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3823 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_frame_interpolation/interp_model/flow_reversal.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    16050 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_frame_interpolation/interp_model/refinenet_arch.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    36415 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_frame_interpolation/interp_model/transformer_layers.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_frame_interpolation/utils/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_frame_interpolation/utils/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3314 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/cv/video_frame_interpolation/utils/scene_change_detection.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2907 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_frame_interpolation/utils/utils.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_human_matting/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      558 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/video_human_matting/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1324 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/video_human_matting/model.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_human_matting/models/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       36 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/video_human_matting/models/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10465 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/video_human_matting/models/decoder.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2662 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/video_human_matting/models/deep_guided_filter.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5506 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/video_human_matting/models/effv2.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2904 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/video_human_matting/models/lraspp.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2234 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/video_human_matting/models/matting.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_inpainting/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      524 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/video_inpainting/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11056 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_inpainting/inpainting.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    13727 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_inpainting/inpainting_model.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_instance_segmentation/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      582 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_instance_segmentation/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_instance_segmentation/head/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       50 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_instance_segmentation/head/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    18236 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_instance_segmentation/head/kernel_frame_iter_head.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    21551 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_instance_segmentation/head/kernel_head.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    16955 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_instance_segmentation/head/kernel_iter_head.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    20983 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_instance_segmentation/head/kernel_update_head.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4136 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_instance_segmentation/head/kernel_updator.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_instance_segmentation/neck/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      525 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_instance_segmentation/neck/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11733 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_instance_segmentation/neck/msdeformattn_decoder.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_instance_segmentation/track/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       50 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_instance_segmentation/track/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    26843 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_instance_segmentation/track/kernel_update_head.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10771 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_instance_segmentation/track/mask_hungarian_assigner.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4163 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_instance_segmentation/utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    18059 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_instance_segmentation/video_knet.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_multi_object_tracking/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      541 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_multi_object_tracking/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_multi_object_tracking/models/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_multi_object_tracking/models/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2970 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_multi_object_tracking/models/common.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2420 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_multi_object_tracking/models/decode.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1890 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_multi_object_tracking/models/model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4921 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_multi_object_tracking/models/yolo.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_multi_object_tracking/tracker/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_multi_object_tracking/tracker/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1084 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_multi_object_tracking/tracker/basetrack.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3117 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/cv/video_multi_object_tracking/tracker/matching.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    14995 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/cv/video_multi_object_tracking/tracker/multitracker.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_multi_object_tracking/utils/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_multi_object_tracking/utils/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2230 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_multi_object_tracking/utils/image.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9570 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_multi_object_tracking/utils/kalman_filter.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7296 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_multi_object_tracking/utils/utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2888 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_multi_object_tracking/utils/visualization.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_object_segmentation/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      497 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_object_segmentation/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      818 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_object_segmentation/aggregate.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3582 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_object_segmentation/cbam.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1979 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_object_segmentation/eval_network.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3980 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_object_segmentation/inference_core.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8255 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_object_segmentation/inference_memory_bank.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7037 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_object_segmentation/mod_resnet.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      974 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_object_segmentation/model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    15309 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_object_segmentation/modules.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5593 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_object_segmentation/network.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_panoptic_segmentation/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      477 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_panoptic_segmentation/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_panoptic_segmentation/backbone/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       50 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_panoptic_segmentation/backbone/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10040 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_panoptic_segmentation/backbone/swin_checkpoint.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    26628 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_panoptic_segmentation/backbone/swin_transformer.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_panoptic_segmentation/head/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      515 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_panoptic_segmentation/head/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8349 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_panoptic_segmentation/head/kernel_head.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    20677 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_panoptic_segmentation/head/kernel_iter_head.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    28653 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_panoptic_segmentation/head/kernel_update_head.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4081 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_panoptic_segmentation/head/kernel_updator.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6696 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_panoptic_segmentation/head/mask.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8627 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_panoptic_segmentation/head/semantic_fpn_wrapper.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5885 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_panoptic_segmentation/head/track_heads.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_panoptic_segmentation/neck/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       50 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_panoptic_segmentation/neck/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6255 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_panoptic_segmentation/neck/fpn.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_panoptic_segmentation/track/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       50 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_panoptic_segmentation/track/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8508 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_panoptic_segmentation/track/quasi_dense_embed_tracker.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    17155 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_panoptic_segmentation/video_k_net.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5440 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_panoptic_segmentation/visualizer.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/config/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/config/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1024 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/config/ostrack.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/models/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/models/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/models/layers/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/models/layers/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1641 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/models/layers/attn.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5004 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/models/layers/attn_blocks.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4805 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/models/layers/head.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1234 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/models/layers/patch_embed.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/models/ostrack/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/models/ostrack/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3380 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/models/ostrack/base_backbone.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3393 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/models/ostrack/ostrack.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      653 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/models/ostrack/utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12278 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/models/ostrack/vit_ce.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/models/procontext/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/models/procontext/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3497 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/models/procontext/procontext.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      943 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/models/procontext/utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4473 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/models/procontext/vit_ce.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/tracker/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      114 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/tracker/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5507 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/tracker/ostrack.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7123 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/tracker/procontext.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/utils/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/utils/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8488 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/utils/utils.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_stabilization/
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_stabilization/DUT/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    14702 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_stabilization/DUT/DUT_raft.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4715 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_stabilization/DUT/MotionPro.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_stabilization/DUT/RAFT/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_stabilization/DUT/RAFT/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3290 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_stabilization/DUT/RAFT/corr.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9214 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_stabilization/DUT/RAFT/extractor.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5275 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_stabilization/DUT/RAFT/raft.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5526 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_stabilization/DUT/RAFT/update.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3922 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_stabilization/DUT/Smoother.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_stabilization/DUT/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1447 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_stabilization/DUT/config.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7292 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_stabilization/DUT/rf_det_module.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7981 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_stabilization/DUT/rf_det_so.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3452 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_stabilization/DUTRAFTStabilizer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      492 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_stabilization/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_stabilization/utils/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4684 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_stabilization/utils/IterativeSmooth.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    14212 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_stabilization/utils/MedianFilter.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    22544 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_stabilization/utils/ProjectionUtils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2893 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_stabilization/utils/RAFTUtils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2817 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_stabilization/utils/WarpUtils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_stabilization/utils/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    14474 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_stabilization/utils/image_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4977 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_stabilization/utils/math_utils.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_streaming_perception/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_streaming_perception/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_streaming_perception/longshortnet/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      487 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_streaming_perception/longshortnet/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_streaming_perception/longshortnet/exp/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_streaming_perception/longshortnet/exp/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2488 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_streaming_perception/longshortnet/exp/longshortnet_base.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7097 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_streaming_perception/longshortnet/longshortnet.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_streaming_perception/longshortnet/models/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_streaming_perception/longshortnet/models/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5774 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_streaming_perception/longshortnet/models/dfp_pafpn_long.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4848 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_streaming_perception/longshortnet/models/dfp_pafpn_short.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9990 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_streaming_perception/longshortnet/models/longshort.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3905 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_streaming_perception/longshortnet/models/longshort_backbone_neck.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_summarization/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      536 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/video_summarization/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4959 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/video_summarization/base_model.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_summarization/kts/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/video_summarization/kts/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1221 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/video_summarization/kts/cpd_auto.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3238 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/video_summarization/kts/cpd_nonlin.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    13098 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/video_summarization/pgl_sum.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8812 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_summarization/summarizer.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/video_super_resolution/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      689 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_super_resolution/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    14118 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_super_resolution/basicvsr_net.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4727 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_super_resolution/common.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4720 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_super_resolution/msrresnet_lite_model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3636 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_super_resolution/real_basicvsr_for_video_super_resolution.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3637 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/video_super_resolution/real_basicvsr_net.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/vidt/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      464 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/vidt/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    40074 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/vidt/backbone.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    25588 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/vidt/deformable_transformer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7436 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/vidt/fpn_fusion.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    16697 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/vidt/head.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3651 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/vidt/model.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/virual_tryon/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      464 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/virual_tryon/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    17690 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/cv/virual_tryon/sdafnet.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/vision_efficient_tuning/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      502 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/vision_efficient_tuning/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    16152 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/models/cv/vision_efficient_tuning/backbone.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      797 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/vision_efficient_tuning/head.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1786 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/vision_efficient_tuning/model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9340 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/vision_efficient_tuning/petl.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5021 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/vision_efficient_tuning/timm_helpers.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    28715 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/vision_efficient_tuning/timm_vision_transformer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5040 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/vision_efficient_tuning/timm_weight_init.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5614 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/vision_efficient_tuning/vision_efficient_tuning.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/vision_middleware/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      492 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/vision_middleware/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5749 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/vision_middleware/backbone.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    28089 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/vision_middleware/head.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6494 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/vision_middleware/model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6192 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/vision_middleware/vim.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/cv/vop_retrieval/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      919 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/vop_retrieval/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12238 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/vop_retrieval/backbone.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5188 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/vop_retrieval/basic_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    13520 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/vop_retrieval/model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5299 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/vop_retrieval/model_se.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5177 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/cv/vop_retrieval/tokenization_clip.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/multi_modal/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2182 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/multi_modal/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/multi_modal/clip/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       97 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/multi_modal/clip/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    14496 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/multi_modal/clip/bert_tokenizer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3898 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/multi_modal/clip/configuration_bert.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    22351 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/multi_modal/clip/model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    20998 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/clip/modeling_bert.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/multi_modal/clip_interrogator/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       37 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/multi_modal/clip_interrogator/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    24293 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/multi_modal/clip_interrogator/model.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/multi_modal/diffusion/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      140 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/multi_modal/diffusion/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    26652 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/diffusion/diffusion.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    14447 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/models/multi_modal/diffusion/model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    39910 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/diffusion/structbert.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11507 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/diffusion/tokenizer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11834 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/diffusion/unet_generator.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8566 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/diffusion/unet_upsampler_1024.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12361 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/diffusion/unet_upsampler_256.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    44602 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/dpm_solver_pytorch.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/multi_modal/efficient_diffusion_tuning/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      540 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/multi_modal/efficient_diffusion_tuning/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12939 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/multi_modal/efficient_diffusion_tuning/efficient_stable_diffusion.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/multi_modal/gemm/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      139 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/multi_modal/gemm/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    21694 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/multi_modal/gemm/gemm_base.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3718 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/multi_modal/gemm/gemm_model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6982 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/multi_modal/gemm/tokenizer.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/multi_modal/guided_diffusion/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      548 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/guided_diffusion/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    36265 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/guided_diffusion/gaussian_diffusion.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2969 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/guided_diffusion/respace.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1365 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/guided_diffusion/script.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    36898 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/guided_diffusion/unet.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/multi_modal/mgeo/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1444 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/mgeo/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)   101942 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/mgeo/backbone.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8651 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/mgeo/text_classification.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3268 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/mgeo/text_ranking.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10331 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/mgeo/token_classification.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/multi_modal/mmr/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      141 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/multi_modal/mmr/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/multi_modal/mmr/dataloaders/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/multi_modal/mmr/dataloaders/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3789 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/multi_modal/mmr/dataloaders/rawvideo_util.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/multi_modal/mmr/models/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      162 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/multi_modal/mmr/models/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9790 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/multi_modal/mmr/models/clip_for_mm_video_embedding.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1442 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/multi_modal/mmr/models/dynamic_inverted_softmax.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    21257 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/multi_modal/mmr/models/modeling.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    18790 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/multi_modal/mmr/models/module_clip.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3577 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/multi_modal/mmr/models/module_cross.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5581 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/multi_modal/mmr/models/tokenization_clip.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4116 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/multi_modal/mmr/models/until_module.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/multi_modal/mplug/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      739 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/mplug/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/multi_modal/mplug/clip/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       86 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/multi_modal/mplug/clip/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    16605 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/multi_modal/mplug/clip/clip.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6030 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/mplug/configuration_mplug.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)   120753 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/mplug/modeling_mplug.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    34049 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/mplug/mvit.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)    21332 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/multi_modal/mplug/predictor.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5536 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/mplug_for_all_tasks.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/multi_modal/mplug_owl/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      835 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/multi_modal/mplug_owl/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10465 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/multi_modal/mplug_owl/configuration_mplug_owl.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    70240 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/multi_modal/mplug_owl/modeling_mplug_owl.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/multi_modal/multi_stage_diffusion/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      150 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/multi_modal/multi_stage_diffusion/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10208 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/multi_stage_diffusion/clip.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11503 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/multi_modal/multi_stage_diffusion/decoder.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    28436 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/multi_stage_diffusion/gaussian_diffusion.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    13487 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/multi_stage_diffusion/model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5276 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/multi_modal/multi_stage_diffusion/prior.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6898 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/multi_stage_diffusion/tokenizer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    16675 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/multi_modal/multi_stage_diffusion/upsampler.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6565 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/multi_stage_diffusion/xglm.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/multi_modal/ofa/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      306 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/multi_modal/ofa/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/multi_modal/ofa/adaptor/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/multi_modal/ofa/adaptor/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    18720 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/ofa/configuration_mmspeech.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    16552 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/ofa/configuration_ofa.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/multi_modal/ofa/generate/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/multi_modal/ofa/generate/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1785 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/multi_modal/ofa/generate/incremental_decoding_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    21049 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/multi_modal/ofa/generate/multihead_attention.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5658 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/multi_modal/ofa/generate/ngram_repeat_block.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    43439 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/ofa/generate/search.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    41976 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/multi_modal/ofa/generate/sequence_generator.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    16690 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/multi_modal/ofa/generate/token_generation_constraints.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4883 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/ofa/generate/utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    44296 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/ofa/modeling_mmspeech.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)   100933 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/ofa/modeling_ofa.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    13226 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/ofa/resnet.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    15028 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/multi_modal/ofa/tokenization_ofa.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8004 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/multi_modal/ofa/tokenization_ofa_fast.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/multi_modal/ofa/utils/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       50 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/multi_modal/ofa/utils/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      676 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/ofa/utils/constant.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1877 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/ofa/utils/utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8435 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/ofa/vit.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    24779 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/ofa_for_all_tasks.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12983 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/ofa_for_text_to_image_synthesis_model.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/multi_modal/rleg/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      538 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/rleg/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4865 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/rleg/model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3410 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/rleg/rleg.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/multi_modal/soonet/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      678 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/soonet/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9838 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/soonet/blocks.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11896 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/soonet/clip.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6165 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/soonet/model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    22584 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/soonet/swin_transformer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4885 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/soonet/tokenizer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1597 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/soonet/utils.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/multi_modal/stable_diffusion/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       96 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/models/multi_modal/stable_diffusion/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6903 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/models/multi_modal/stable_diffusion/stable_diffusion.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/multi_modal/team/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      140 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/multi_modal/team/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5181 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/multi_modal/team/team_model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11502 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/multi_modal/team/utils.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/multi_modal/video_synthesis/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      538 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/video_synthesis/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    18477 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/video_synthesis/autoencoder.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9264 2023-03-23 13:29:10.000000 modelscope-1.8.0/modelscope/models/multi_modal/video_synthesis/diffusion.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9564 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/multi_modal/video_synthesis/text_to_video_synthesis_model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    38498 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/video_synthesis/unet_sd.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/multi_modal/vldoc/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       93 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/vldoc/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11339 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/vldoc/conv_fpn_trans.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6422 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/vldoc/convnext.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    16687 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/vldoc/model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    47348 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/vldoc/modeling_layout_roberta.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    20956 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/vldoc/processing.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4680 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/vldoc/tokenization.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7333 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/multi_modal/vldoc/transformer_local.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/T5/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      599 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/T5/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    67374 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/nlp/T5/backbone.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7541 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/T5/configuration.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    21517 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/models/nlp/T5/text2text_generation.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7349 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/nlp/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/bart/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      112 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/bart/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3058 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/bart/text_error_correction.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/bert/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1519 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/bert/__init__.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)    40458 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/bert/backbone.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7308 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/bert/configuration.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4113 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/bert/document_segmentation.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      512 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/bert/fill_mask.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6074 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/bert/sentence_embedding.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7153 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/bert/siamese_uie.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1478 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/bert/text_classification.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1138 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/bert/text_ranking.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2140 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/bert/token_classification.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6904 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/bert/word_alignment.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/bloom/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      472 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/bloom/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      505 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/bloom/backbone.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/canmt/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      102 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/nlp/canmt/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    52235 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/nlp/canmt/canmt_model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2940 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/nlp/canmt/canmt_translation.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    35688 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/nlp/canmt/sequence_generator.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/chatglm/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1578 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/models/nlp/chatglm/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4403 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/nlp/chatglm/configuration.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    15789 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/nlp/chatglm/quantization.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    60869 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/nlp/chatglm/text_generation.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    17422 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/nlp/chatglm/tokenization.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/chatglm2/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1584 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/models/nlp/chatglm2/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2576 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/nlp/chatglm2/configuration.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    15206 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/nlp/chatglm2/quantization.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    55824 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/nlp/chatglm2/text_generation.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10173 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/models/nlp/chatglm2/tokenization.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/codegeex/
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)      730 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/codegeex/__init__.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)    35473 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/codegeex/codegeex.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     3995 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/codegeex/codegeex_for_code_generation.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     4008 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/codegeex/codegeex_for_code_translation.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     9997 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/codegeex/inference.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     6111 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/codegeex/tokenizer.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/csanmt/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       96 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/csanmt/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    61868 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/csanmt/translation.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/deberta_v2/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1771 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/deberta_v2/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    47998 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/deberta_v2/backbone.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6771 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/deberta_v2/configuration.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10364 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/deberta_v2/fill_mask.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    21421 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/deberta_v2/tokenization.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10698 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/deberta_v2/tokenization_fast.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/dgds/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      939 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/nlp/dgds/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7092 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/nlp/dgds/backbone.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1656 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/dgds/document_grounded_dialog_generate.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1059 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/dgds/document_grounded_dialog_rerank.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2180 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/dgds/document_grounded_dialog_retrieval.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/fid_T5/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1082 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/nlp/fid_T5/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8108 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/nlp/fid_T5/text_generation.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/fid_plug/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1181 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/fid_plug/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    45112 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/nlp/fid_plug/backbone.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5045 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/nlp/fid_plug/configuration.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6709 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/nlp/fid_plug/text_generation.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/glm_130b/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      540 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/nlp/glm_130b/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/glm_130b/generation/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       87 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/nlp/glm_130b/generation/__init__.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)    10112 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/nlp/glm_130b/generation/strategies.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     5152 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/nlp/glm_130b/initialize.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/glm_130b/kernels/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3263 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/nlp/glm_130b/kernels/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/glm_130b/quantization/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2635 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/nlp/glm_130b/quantization/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1189 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/nlp/glm_130b/quantization/functional.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4510 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/nlp/glm_130b/quantization/layers.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)    13504 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/nlp/glm_130b/text_generation.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/gpt2/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      470 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/gpt2/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      498 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/gpt2/backbone.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/gpt3/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      852 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/gpt3/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    16094 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/models/nlp/gpt3/backbone.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8971 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/gpt3/configuration.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    52139 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/nlp/gpt3/distributed_gpt3.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3207 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/nlp/gpt3/text_generation.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2586 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/gpt3/tokenizer.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/gpt_moe/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      874 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/gpt_moe/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    13129 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/gpt_moe/backbone.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5198 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/gpt_moe/checkpointing.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4930 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/gpt_moe/configuration.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    49316 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/gpt_moe/distributed_gpt_moe.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/gpt_moe/moe/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/gpt_moe/moe/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1194 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/gpt_moe/moe/experts.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3504 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/gpt_moe/moe/layer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2553 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/gpt_moe/moe/mappings.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    23756 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/gpt_moe/moe/sharded_moe.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4110 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/gpt_moe/moe/utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2717 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/gpt_moe/text_generation.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2277 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/gpt_moe/tokenizer.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/gpt_neo/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      474 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/gpt_neo/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      518 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/gpt_neo/backbone.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/heads/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      661 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/heads/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    25248 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/heads/crf_head.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6947 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/nlp/heads/fill_mask_head.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5145 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/heads/infromation_extraction_head.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2056 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/heads/text_classification_head.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      964 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/heads/text_generation_head.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2029 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/heads/text_ranking_head.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2596 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/heads/token_classification_head.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      948 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/heads/torch_pretrain_head.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/hf_transformers/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      488 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/hf_transformers/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4897 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/hf_transformers/backbone.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/llama/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      866 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/nlp/llama/__init__.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)    29189 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/nlp/llama/backbone.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4695 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/nlp/llama/configuration.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11349 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/nlp/llama/convert_llama_weights_to_hf.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7557 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/nlp/llama/text_generation.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10325 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/nlp/llama/tokenization.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4712 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/nlp/llama/tokenization_fast.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/llama2/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      876 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/models/nlp/llama2/__init__.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)    32097 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/nlp/llama2/backbone.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8226 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/nlp/llama2/configuration.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10447 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/nlp/llama2/text_generation.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    16964 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/nlp/llama2/tokenization.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10110 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/nlp/llama2/tokenization_fast.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/lstm/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      610 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/lstm/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1312 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/lstm/backbone.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2187 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/lstm/token_classification.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/megatron_bert/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      688 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/megatron_bert/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    39864 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/megatron_bert/backbone.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6599 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/megatron_bert/configuration.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12452 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/megatron_bert/fill_mask.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/mglm/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      572 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/mglm/__init__.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)    28115 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/mglm/arguments.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    28162 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/nlp/mglm/blocklm_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    17995 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/mglm/configure_data.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/mglm/data_utils/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    13656 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/mglm/data_utils/__init__.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)    20350 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/mglm/data_utils/corpora.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    45765 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/nlp/mglm/data_utils/datasets.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3342 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/mglm/data_utils/extraction.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     8459 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/mglm/data_utils/file_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9797 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/mglm/data_utils/lazy_loader.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7221 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/mglm/data_utils/samplers.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4661 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/mglm/data_utils/sp_tokenizer.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)    53304 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/mglm/data_utils/tokenization.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    14551 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/mglm/data_utils/tokenization_gpt2.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)    15773 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/mglm/data_utils/wordpiece.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    21669 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/mglm/generation_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    16247 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/mglm/mglm_for_text_summarization.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/mglm/model/
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)      984 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/mglm/model/__init__.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     5443 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/mglm/model/distributed.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9967 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/mglm/model/downstream.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    70249 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/mglm/model/modeling_bert.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8743 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/mglm/model/modeling_glm.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2531 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/mglm/model/prompt.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    48886 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/mglm/model/transformer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1955 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/mglm/process_grid.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      203 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/mglm/run_test.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/mglm/test/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/mglm/test/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      950 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/nlp/mglm/test/test_block.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      704 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/nlp/mglm/test/test_rel_shift.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    17815 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/mglm/train_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    19029 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/mglm/utils.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/palm_v2/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1268 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/palm_v2/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5532 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/palm_v2/configuration.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    29453 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/palm_v2/dureader_eval.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    55450 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/palm_v2/text_generation.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/peer/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1194 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/peer/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    55772 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/nlp/peer/backbone.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11708 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/nlp/peer/configuration.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6166 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/peer/sas_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4933 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/peer/text_classification.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/plug/
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     3321 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/plug/AnnealingLR.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      660 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/plug/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    41209 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/plug/backbone.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11797 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/plug/configuration.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10983 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/plug/distributed_plug.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8427 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/plug/generator.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/plug_mental/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1359 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/plug_mental/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7671 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/plug_mental/adv_utils.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)    47486 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/plug_mental/backbone.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7956 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/plug_mental/configuration.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10881 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/plug_mental/text_classification.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/polylm/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      499 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/nlp/polylm/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1947 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/nlp/polylm/text_generation.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/ponet/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1490 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/ponet/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    37918 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/ponet/backbone.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6366 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/ponet/configuration.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4176 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/ponet/document_segmentation.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11011 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/ponet/fill_mask.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7147 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/ponet/tokenization.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/qwen/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      855 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/nlp/qwen/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    28451 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/nlp/qwen/backbone.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2393 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/nlp/qwen/configuration.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    14469 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/nlp/qwen/qwen_generation_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9448 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/nlp/qwen/text_generation.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8447 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/models/nlp/qwen/tokenization.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/space/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      987 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/space/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1208 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/space/configuration.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3832 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/space/dialog_intent_prediction.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4354 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/space/dialog_modeling.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    16648 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/space/dialog_state_tracking.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/space/model/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      421 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/space/model/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10281 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/space/model/gen_unified_transformer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10757 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/space/model/generator.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7505 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/space/model/intent_unified_transformer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3032 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/space/model/model_base.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1189 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/space/model/tokenization_space.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11941 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/space/model/unified_transformer.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/space/modules/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/space/modules/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2398 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/space/modules/embedder.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      956 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/space/modules/feedforward.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1721 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/space/modules/functions.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3397 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/space/modules/multihead_attention.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1922 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/space/modules/transformer_block.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/space_T_cn/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      529 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/space_T_cn/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    44438 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/space_T_cn/backbone.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5194 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/space_T_cn/configuration.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    30117 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/space_T_cn/table_question_answering.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/space_T_en/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      492 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/space_T_en/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3715 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/space_T_en/text_to_sql.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/structbert/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1674 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/structbert/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7673 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/structbert/adv_utils.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)    40848 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/structbert/backbone.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7686 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/structbert/configuration.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    27203 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/structbert/faq_question_answering.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12581 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/structbert/fill_mask.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11862 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/structbert/text_classification.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11074 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/structbert/token_classification.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/task_models/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1453 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/task_models/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5323 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/task_models/feature_extraction.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2698 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/task_models/fill_mask.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      766 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/task_models/information_extraction.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    28991 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/nlp/task_models/task_model.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1912 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/task_models/text_classification.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8678 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/models/nlp/task_models/text_generation.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2092 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/task_models/text_ranking.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5116 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/task_models/token_classification.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/unite/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      626 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/nlp/unite/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      409 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/nlp/unite/configuration.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    18320 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/nlp/unite/translation_evaluation.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/use/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      545 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/use/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5133 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/use/transformer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5832 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/models/nlp/use/user_satisfaction_estimation.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/veco/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1479 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/veco/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4031 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/veco/backbone.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1192 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/veco/configuration.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4278 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/veco/fill_mask.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6643 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/veco/text_classification.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4129 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/nlp/veco/token_classification.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/nlp/xlm_roberta/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1156 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/xlm_roberta/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    42932 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/xlm_roberta/backbone.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7697 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/models/nlp/xlm_roberta/configuration.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/science/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      491 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/science/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/science/unifold/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       46 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/science/unifold/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    24057 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/science/unifold/config.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/science/unifold/data/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      634 2022-12-08 09:39:02.000000 modelscope-1.8.0/modelscope/models/science/unifold/data/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    49139 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/models/science/unifold/data/data_ops.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    19691 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/science/unifold/data/msa_pairing.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8941 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/models/science/unifold/data/process.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    14792 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/models/science/unifold/data/process_multimer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11422 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/models/science/unifold/data/protein.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    42023 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/models/science/unifold/data/residue_constants.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4628 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/models/science/unifold/data/utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    19250 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/models/science/unifold/dataset.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2307 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/models/science/unifold/model.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/science/unifold/modules/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      187 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/models/science/unifold/modules/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    17066 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/models/science/unifold/modules/alphafold.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12323 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/models/science/unifold/modules/attentions.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5403 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/models/science/unifold/modules/auxillary_heads.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10676 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/models/science/unifold/modules/common.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5814 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/models/science/unifold/modules/confidence.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8771 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/models/science/unifold/modules/embedders.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11078 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/models/science/unifold/modules/evoformer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6820 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/models/science/unifold/modules/featurization.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    18025 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/models/science/unifold/modules/frame.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    18476 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/models/science/unifold/modules/structure_module.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9771 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/models/science/unifold/modules/template.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5623 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/models/science/unifold/modules/triangle_multiplication.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/science/unifold/msa/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       46 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/models/science/unifold/msa/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    17940 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/models/science/unifold/msa/mmcif.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3128 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/models/science/unifold/msa/msa_identifiers.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    23503 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/models/science/unifold/msa/parsers.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11644 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/models/science/unifold/msa/pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    45468 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/models/science/unifold/msa/templates.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/models/science/unifold/msa/tools/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      639 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/models/science/unifold/msa/tools/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6218 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/models/science/unifold/msa/tools/hhblits.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3991 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/models/science/unifold/msa/tools/hhsearch.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5098 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/models/science/unifold/msa/tools/hmmbuild.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5013 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/models/science/unifold/msa/tools/hmmsearch.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8662 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/models/science/unifold/msa/tools/jackhmmer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3752 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/models/science/unifold/msa/tools/kalign.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1255 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/models/science/unifold/msa/tools/utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2892 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/models/science/unifold/msa/utils.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/msdatasets/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       84 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/msdatasets/audio/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       50 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/audio/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      346 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/audio/asr_dataset.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/msdatasets/auth/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       50 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/auth/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1326 2023-04-20 06:52:22.000000 modelscope-1.8.0/modelscope/msdatasets/auth/auth_config.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/msdatasets/context/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       50 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/context/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3444 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/msdatasets/context/dataset_context_config.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/msdatasets/data_files/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       50 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/data_files/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5270 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/msdatasets/data_files/data_files_manager.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/msdatasets/data_loader/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       50 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/data_loader/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12636 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/msdatasets/data_loader/data_loader.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5764 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/msdatasets/data_loader/data_loader_manager.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      111 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4111 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      730 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1919 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/asr_dataset.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8512 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/kws_farfield_dataset.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7723 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/kws_nearfield_dataset.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11689 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/kws_nearfield_processor.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/bad_image_detecting/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      541 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/bad_image_detecting/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1264 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/bad_image_detecting/bad_image_detecting_dataset.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      791 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/builder.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      134 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4708 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/build.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      945 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/collate_batch.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      177 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3815 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/coco.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    16180 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/mosaic_wrapper.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      934 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/coco/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      544 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/coco/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11572 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/coco/coco_eval.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      312 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2622 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/distributed.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4858 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/grouped_batch_sampler.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1469 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/iteration_based_batch_sampler.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      196 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1086 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/build.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2509 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/transforms.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1511 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/easycv_base.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2141 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/gopro_image_deblurring_dataset.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_colorization/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      539 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_colorization/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2196 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_colorization/image_colorization_dataset.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      529 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2968 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/aug.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12152 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/image_inpainting_dataset.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12574 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_instance_segmentation_coco_dataset.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      577 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1091 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/data_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1629 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/image_portrait_enhancement_dataset.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assessment_degradation/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      615 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assessment_degradation/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1487 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assessment_degradation/image_quality_assessment_degradation_dataset.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assmessment_mos/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      583 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assmessment_mos/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1194 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assmessment_mos/image_quality_assessment_mos_dataset.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4849 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/language_guided_video_summarization_dataset.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6225 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/mgeo_ranking_dataset.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      559 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6171 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/movie_scene_segmentation_dataset.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4013 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/sampler.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      161 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1589 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/augmenter.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4945 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/data_loader.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5437 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/image_dataset.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       40 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7547 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/iou_evaluator.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3416 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/quad_measurer.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      309 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3172 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/augment_data.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      971 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/data_process.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5818 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_border_map.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2000 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_icdar_data.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3512 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_seg_detection_data.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      707 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/normalize_image.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4912 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/random_crop_data.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2402 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_recognition_dataset.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2017 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/reds_image_deblurring_dataset.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      599 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    16767 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/referring_video_object_segmentation_dataset.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8725 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/transformers.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      545 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1475 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/data_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1984 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/sidd_image_denoising_dataset.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2684 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/transforms.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5114 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/text_ranking_dataset.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1635 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/torch_custom_dataset.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2664 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/veco_dataset.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      573 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1390 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/data_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1780 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/video_frame_interpolation_dataset.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_stabilization/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      543 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_stabilization/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      809 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_stabilization/video_stabilization_dataset.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2624 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_summarization_dataset.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_super_resolution/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      553 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_super_resolution/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2370 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_super_resolution/video_super_resolution_dataset.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11485 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/msdatasets/dataset_cls/dataset.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/msdatasets/download/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       50 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/download/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    21287 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/msdatasets/download/dataset_builder.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      640 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/msdatasets/download/download_config.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2487 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/download/download_manager.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/msdatasets/meta/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       50 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/meta/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1772 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/meta/data_meta_config.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8679 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/msdatasets/meta/data_meta_manager.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    36737 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/msdatasets/ms_dataset.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/msdatasets/task_datasets/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1072 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/task_datasets/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      408 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/task_datasets/gopro_image_deblurring_dataset.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      401 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/task_datasets/reds_image_deblurring_dataset.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      404 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/task_datasets/sidd_image_denoising.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      410 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/task_datasets/torch_base_dataset.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      404 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/task_datasets/video_summarization_dataset.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/msdatasets/utils/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/msdatasets/utils/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8124 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/msdatasets/utils/dataset_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1026 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/msdatasets/utils/delete_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5540 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/msdatasets/utils/maxcompute_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6200 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/msdatasets/utils/oss_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2496 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/msdatasets/utils/upload_utils.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/ops/
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/ops/4knerf/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/ops/4knerf/__init__.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     2395 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/ops/4knerf/adam_upd.cpp
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     4462 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/ops/4knerf/adam_upd_kernel.cu
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     7716 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/ops/4knerf/render_utils.cpp
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)    24806 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/ops/4knerf/render_utils_kernel.cu
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)      777 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/ops/4knerf/total_variation.cpp
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     2432 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/ops/4knerf/total_variation_kernel.cu
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)      600 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/ops/4knerf/ub360_utils.cpp
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     1329 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/ops/4knerf/ub360_utils_kernel.cu
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/ops/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/ops/ailut/
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/ops/ailut/Ailut/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/ops/ailut/Ailut/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/ops/ailut/Ailut/csrc/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/ops/ailut/Ailut/csrc/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6065 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/ops/ailut/Ailut/csrc/ailut_transform.cpp
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    27053 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/ops/ailut/Ailut/csrc/ailut_transform_cpu.cpp
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    31866 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/ops/ailut/Ailut/csrc/ailut_transform_cuda.cu
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      105 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/ops/ailut/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4314 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/ops/ailut/pyinterfaces.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/ops/quadtree_attention/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      106 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/ops/quadtree_attention/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/ops/quadtree_attention/functions/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/ops/quadtree_attention/functions/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2925 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/ops/quadtree_attention/functions/quadtree_attention.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/ops/quadtree_attention/modules/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/ops/quadtree_attention/modules/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    13956 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/ops/quadtree_attention/modules/quadtree_attention.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/ops/quadtree_attention/src/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/ops/quadtree_attention/src/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1425 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/ops/quadtree_attention/src/score_computation.cpp
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1008 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/ops/quadtree_attention/src/score_computation.h
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6033 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/ops/quadtree_attention/src/score_computation_kernal.cu
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      612 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/ops/quadtree_attention/src/utils.h
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2369 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/ops/quadtree_attention/src/value_aggregation.cpp
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      815 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/ops/quadtree_attention/src/value_aggregation.h
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3600 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/ops/quadtree_attention/src/value_aggregation_kernel.cu
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/outputs/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      132 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/outputs/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      908 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/outputs/cv_outputs.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    19855 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/outputs/nlp_outputs.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    52143 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/outputs/outputs.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11586 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/pipeline_inputs.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/pipelines/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      150 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/pipelines/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/pipelines/audio/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1567 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/pipelines/audio/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7519 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/pipelines/audio/ans_dfsmn_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5016 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/audio/ans_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    24110 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/pipelines/audio/asr_inference_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3189 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/audio/asr_wenet_inference_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4357 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/audio/inverse_text_processing_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3830 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/audio/kws_farfield_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7276 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/audio/kws_kwsbp_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5698 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/pipelines/audio/language_recognition_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5642 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/pipelines/audio/linear_aec_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8068 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/pipelines/audio/lm_infer_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6481 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/pipelines/audio/punctuation_processing_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12545 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/pipelines/audio/segmentation_clustering_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2560 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/audio/separation_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4829 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/pipelines/audio/speaker_change_locating_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6038 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/pipelines/audio/speaker_diarization_dialogue_detection_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11421 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/pipelines/audio/speaker_diarization_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4246 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/pipelines/audio/speaker_diarization_semantic_speaker_turn_detection_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4078 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/pipelines/audio/speaker_verification_eres2net_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6089 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/pipelines/audio/speaker_verification_light_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10817 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/pipelines/audio/speaker_verification_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4072 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/pipelines/audio/speaker_verification_rdino_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1789 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/audio/text_to_speech_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11930 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/pipelines/audio/timestamp_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9696 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/pipelines/audio/voice_activity_detection_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    23185 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/pipelines/base.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7702 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/pipelines/builder.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/pipelines/cv/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    16754 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/pipelines/cv/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2545 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/action_detection_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4852 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/action_recognition_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4024 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/animal_recognition_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2576 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/pipelines/cv/arc_face_recognition_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2614 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/bad_image_detecting_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9581 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/body_2d_keypoints_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    14219 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/body_3d_keypoints_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4753 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/pipelines/cv/card_detection_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5307 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/cmdssl_video_embedding_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2528 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/content_check_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5055 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/controllable_image_generation_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5942 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/crowd_counting_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5136 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/ddcolor_image_colorization_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1965 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/ddpm_semantic_segmentation_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2498 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/pipelines/cv/face_attribute_recognition_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3993 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/pipelines/cv/face_detection_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1531 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/face_emotion_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1648 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/face_human_hand_detection_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2894 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/face_image_generation_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3312 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/pipelines/cv/face_liveness_ir_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3573 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/pipelines/cv/face_liveness_xc_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7748 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/pipelines/cv/face_processing_base_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3872 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/pipelines/cv/face_quality_assessment_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3528 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/pipelines/cv/face_recognition_onnx_fm_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3493 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/pipelines/cv/face_recognition_onnx_ir_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2878 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/pipelines/cv/face_recognition_ood_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2498 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/face_recognition_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    19634 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/pipelines/cv/face_reconstruction_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2400 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/pipelines/cv/facial_expression_recognition_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2681 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/pipelines/cv/facial_landmark_confidence_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4442 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/pipelines/cv/fast_instance_segmentation_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4030 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/general_recognition_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1360 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/hand_static_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2982 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/hicossl_video_embedding_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4372 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/human_reconstruction_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1399 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/image_body_reshaping_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2919 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/image_bts_depth_estimation_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5251 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/image_cartoon_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5857 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/image_classification_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3296 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/image_color_enhance_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4441 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/image_colorization_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2648 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/image_debanding_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4624 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/image_deblur_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3517 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/image_defrcn_fewshot_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4160 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/image_denoise_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1943 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/image_depth_estimation_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1854 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/pipelines/cv/image_detection_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4134 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/image_driving_perception_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2317 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/image_face_fusion_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4866 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/image_human_parsing_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5854 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/image_inpainting_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4673 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/image_inpainting_sdv2_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3903 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/image_instance_segmentation_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5929 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/image_matching_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2607 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/pipelines/cv/image_matting_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2796 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/image_mvs_depth_estimation_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2770 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/image_open_vocabulary_detection_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6113 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/image_paintbyexample_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3622 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/pipelines/cv/image_panoptic_segmentation_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8567 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/image_portrait_enhancement_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3549 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/image_quality_assessment_degradation_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2835 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/image_quality_assessment_man_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2801 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/image_quality_assessment_mos_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2026 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/image_reid_person_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1702 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/image_restoration_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1644 2023-03-15 14:02:28.000000 modelscope-1.8.0/modelscope/pipelines/cv/image_salient_detection_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3207 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/image_semantic_segmentation_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2237 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/image_skychange_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2839 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/image_structured_model_probing_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4615 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/pipelines/cv/image_style_transfer_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3138 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/image_super_resolution_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9718 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/image_to_image_generate_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12737 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/image_to_image_translation_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2319 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/pipelines/cv/image_try_on_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1883 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/indoor_layout_estimation_pipeline.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     9872 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/language_guided_video_summarization_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4570 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/license_plate_detection_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4182 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/lineless_table_recognition_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5276 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/live_category_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2757 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/mask_face_recognition_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2787 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/maskdino_instance_segmentation_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3670 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/mobile_image_super_resolution_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1857 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/mog_face_detection_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4844 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/motion_generation_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2628 2023-04-20 06:52:22.000000 modelscope-1.8.0/modelscope/pipelines/cv/movie_scene_segmentation_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1953 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/mtcnn_face_detection_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3245 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/pipelines/cv/nerf_recon_4k_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3328 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/nerf_recon_acc_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3592 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/pipelines/cv/nerf_recon_vq_compression_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6018 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/object_detection_3d_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11033 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/ocr_detection_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2509 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/pipelines/cv/ocr_recognition_pipeline.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/pipelines/cv/ocr_utils/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      966 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/ocr_utils/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      720 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/ocr_utils/model_convnext_transformer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    19821 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/ocr_utils/model_dla34.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8768 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/ocr_utils/model_resnet18_half.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5286 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/ocr_utils/model_resnet_mutex_v4_linewithchar.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    15421 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/ocr_utils/model_vlpt.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/pipelines/cv/ocr_utils/ocr_modules/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      550 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/ocr_utils/ocr_modules/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6209 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/ocr_utils/ocr_modules/convnext.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11644 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/ocr_utils/ocr_modules/timm_tinyc.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1560 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/ocr_utils/ocr_modules/vitstr.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    38278 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/ocr_utils/ops.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    19004 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/ocr_utils/resnet18_v1.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11371 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/ocr_utils/resnet_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11168 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/ocr_utils/table_process.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7971 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/ocr_utils/utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3029 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/panorama_depth_estimation_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2791 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/pipelines/cv/panorama_depth_estimation_s2net_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9781 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/pipelines/cv/pedestrian_attribute_recognition_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4199 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/pointcloud_sceneflow_estimation_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1457 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/product_retrieval_embedding_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1451 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/product_segmentation_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2020 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/realtime_video_object_detection_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9253 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/pipelines/cv/referring_video_object_segmentation_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1956 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/retina_face_detection_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1760 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/shop_segmentation_pipleline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12129 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/pipelines/cv/skin_retouching_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4446 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/table_recognition_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4899 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/pipelines/cv/tbs_detection_pipeline.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/pipelines/cv/tbs_detection_utils/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       10 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/tbs_detection_utils/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    15542 2023-03-23 13:29:10.000000 modelscope-1.8.0/modelscope/pipelines/cv/tbs_detection_utils/utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1841 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/text_driven_segmentation_pipleline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8865 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/pipelines/cv/text_to_360panorama_image_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3246 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/tinynas_classification_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3244 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/tinynas_detection_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1888 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/ulfd_face_detection_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    13776 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/video_category_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6097 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/video_colorization_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7757 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/video_deinterlace_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1565 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/video_depth_estimation_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    24271 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/video_frame_interpolation_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3635 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/video_human_matting_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1719 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/video_inpainting_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9652 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/video_instance_segmentation_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3294 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/video_multi_object_tracking_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4724 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/video_object_segmentation_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4683 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/video_panoptic_segmentation_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3436 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/video_single_object_tracking_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4647 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/video_stabilization_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4289 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/video_summarization_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7078 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/video_super_resolution_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7564 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/vidt_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5240 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/cv/virtual_try_on_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3986 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/vision_efficient_tuning_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2180 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/vision_middleware_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4762 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/vop_retrieval_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5787 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/cv/vop_retrieval_se_pipeline.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/pipelines/multi_modal/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3057 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/pipelines/multi_modal/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2366 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/multi_modal/asr_pipeline.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/pipelines/multi_modal/diffusers_wrapped/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      622 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/pipelines/multi_modal/diffusers_wrapped/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2026 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/pipelines/multi_modal/diffusers_wrapped/diffusers_pipeline.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      704 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12129 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/chinese_stable_diffusion_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8750 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/stable_diffusion_pipeline.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      585 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    15631 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/disco_guided_diffusion.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    18987 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2311 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/multi_modal/document_vl_embedding_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3086 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/pipelines/multi_modal/efficient_diffusion_tuning_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1093 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/multi_modal/generative_multi_modal_embedding_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9352 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/multi_modal/gridvlp_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3219 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/pipelines/multi_modal/image_captioning_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1612 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/multi_modal/image_text_retrieval_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7869 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/multi_modal/mgeo_ranking_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1648 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/multi_modal/multi_modal_embedding_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3254 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/pipelines/multi_modal/multimodal_dialogue_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1780 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/multi_modal/ocr_recognition_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8561 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/multi_modal/soonet_video_temporal_grounding_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1848 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/multi_modal/sudoku_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1059 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/multi_modal/team_multi_modal_similarity_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1789 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/multi_modal/text2sql_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2064 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/multi_modal/text_to_image_synthesis_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3796 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/pipelines/multi_modal/text_to_video_synthesis_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2150 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/multi_modal/video_captioning_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1329 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/multi_modal/video_multi_modal_embedding_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1956 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/multi_modal/video_question_answering_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1780 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/multi_modal/visual_entailment_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1781 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/multi_modal/visual_grounding_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2396 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/multi_modal/visual_question_answering_pipeline.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/pipelines/nlp/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6953 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/pipelines/nlp/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6939 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/nlp/automatic_post_editing_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3696 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/pipelines/nlp/canmt_translation_pipeline.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     2202 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/nlp/codegeex_code_generation_pipeline.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     2885 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/nlp/codegeex_code_translation_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2251 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/nlp/conversational_text_to_sql_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2577 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/pipelines/nlp/dialog_intent_prediction_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2308 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/nlp/dialog_modeling_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7663 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/pipelines/nlp/dialog_state_tracking_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3882 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/pipelines/nlp/distributed_gpt3_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1851 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/nlp/distributed_gpt_moe_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4425 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/nlp/distributed_plug_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2754 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/pipelines/nlp/document_grounded_dialog_generate_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    25997 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/pipelines/nlp/document_grounded_dialog_rerank_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5480 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/pipelines/nlp/document_grounded_dialog_retrieval_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9797 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/pipelines/nlp/document_segmentation_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6259 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/pipelines/nlp/extractive_summarization_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3348 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/nlp/faq_question_answering_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2437 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/nlp/fasttext_text_classification_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3258 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/pipelines/nlp/feature_extraction_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10171 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/pipelines/nlp/fid_dialogue_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4655 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/pipelines/nlp/fill_mask_pipeline.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)      999 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/pipelines/nlp/glm130b_text_generation_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2383 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/nlp/information_extraction_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6363 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/nlp/interactive_translation_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9928 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/nlp/language_identification_pipline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4079 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/pipelines/nlp/llama2_text_generation_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1776 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/nlp/mglm_text_summarization_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2955 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/pipelines/nlp/named_entity_recognition_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2671 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/pipelines/nlp/polylm_text_generation_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3184 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/pipelines/nlp/sentence_embedding_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    14334 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/pipelines/nlp/siamese_uie_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2566 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/nlp/summarization_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    16315 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/pipelines/nlp/table_question_answering_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6787 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/pipelines/nlp/text_classification_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3490 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/nlp/text_error_correction_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    16178 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/pipelines/nlp/text_generation_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2877 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/pipelines/nlp/text_ranking_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6399 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/pipelines/nlp/token_classification_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4372 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/pipelines/nlp/translation_evaluation_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5586 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/nlp/translation_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2339 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/nlp/translation_quality_estimation_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4613 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/pipelines/nlp/user_satisfaction_estimation_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2713 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/nlp/word_alignment_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4523 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/nlp/word_segmentation_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5875 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/pipelines/nlp/zero_shot_classification_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2816 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/pipelines/pipeline_template.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/pipelines/science/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      538 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/pipelines/science/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8255 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/science/protein_structure_pipeline.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3492 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/pipelines/util.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/preprocessors/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5792 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/preprocessors/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10063 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/preprocessors/asr.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8680 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/audio.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    15531 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/preprocessors/base.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      812 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/preprocessors/builder.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6982 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/common.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/preprocessors/cv/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1896 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/preprocessors/cv/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7446 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/cv/action_detection_mapper.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1126 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/cv/bad_image_detecting_preprocessor.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7789 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/cv/controllable_image_generation.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    20909 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/cv/cv2_transforms.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12826 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/cv/image_classification_preprocessor.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1250 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/cv/image_quality_assessment_man.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2113 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/cv/image_quality_assessment_mos.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2790 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/cv/image_restoration_preprocessor.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2625 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/cv/mmcls_preprocessor.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3050 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/cv/timer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3606 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/cv/util.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1501 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/cv/video_stabilization.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8579 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/cv/video_super_resolution.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    13406 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/image.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4884 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/preprocessors/kws.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/preprocessors/movie_scene_segmentation/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      483 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/preprocessors/movie_scene_segmentation/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10809 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/movie_scene_segmentation/transforms.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    30257 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/preprocessors/multi_modal.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5966 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      762 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/bert_seq_cls_tokenizer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3929 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/canmt_translation.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2572 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/dialog_classification_use_preprocessor.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4102 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/document_grounded_dialog_generate_preprocessor.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4391 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/document_grounded_dialog_rerank_preprocessor.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4086 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/document_grounded_dialog_retrieval_preprocessor.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11004 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/document_segmentation_preprocessor.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6844 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/faq_question_answering_preprocessor.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3304 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/feature_extraction_preprocessor.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12086 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/fill_mask_preprocessor.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8199 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/mgeo_ranking_preprocessor.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1168 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/mglm_summarization_preprocessor.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1799 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/relation_extraction_preprocessor.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4023 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/sentence_embedding_preprocessor.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1424 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/siamese_uie_preprocessor.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/space/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1219 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/space/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1908 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/space/args.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1553 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/space/batch.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3629 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/space/data_loader.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2701 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/space/dialog_intent_prediction_preprocessor.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2789 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/space/dialog_modeling_preprocessor.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5457 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/space/dialog_state_tracking_preprocessor.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    56779 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/space/dst_processors.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/space/fields/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      592 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/space/fields/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    33884 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/space/fields/gen_field.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    42467 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/space/fields/intent_field.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1147 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/space/lazy_dataset.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1935 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/space/preprocess.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1610 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/space/sampler.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2084 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/space/tensorlistdataset.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    24803 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/space/tokenizer.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/space_T_cn/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      654 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/space_T_cn/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/space_T_cn/fields/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/space_T_cn/fields/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4936 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/space_T_cn/fields/database.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    15867 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/space_T_cn/fields/schema_link.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4888 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/space_T_cn/fields/struct.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4175 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/space_T_cn/table_question_answering_preprocessor.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/space_T_en/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      846 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/space_T_en/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4902 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/space_T_en/conversational_text_to_sql_preprocessor.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/space_T_en/fields/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      818 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/space_T_en/fields/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    21521 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/space_T_en/fields/common_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12500 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/space_T_en/fields/parse.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1380 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/space_T_en/fields/preprocess_dataset.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2111 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/space_T_en/fields/process_dataset.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6422 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/text_classification_preprocessor.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1641 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/text_clean.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2298 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/text_error_correction.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    14124 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/text_generation_preprocessor.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3834 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/text_ranking_preprocessor.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    19629 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/token_classification_preprocessor.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1640 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/token_classification_thai_preprocessor.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1238 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/token_classification_viet_preprocessor.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4482 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/transformers_tokenizer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7304 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/translation_evaluation_preprocessor.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3622 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4962 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/word_alignment_preprocessor.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2584 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/preprocessors/nlp/zero_shot_classification_preprocessor.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/preprocessors/ofa/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      762 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/ofa/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4839 2023-01-13 02:33:56.000000 modelscope-1.8.0/modelscope/preprocessors/ofa/asr.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11909 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/ofa/base.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4172 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/ofa/image_captioning.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6461 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/ofa/image_classification.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5740 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/ofa/ocr_recognition.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4603 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/ofa/sudoku.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5210 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/ofa/summarization.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    16449 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/ofa/text2sql.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5368 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/ofa/text_classification.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2164 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/ofa/text_to_image_synthesis.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/preprocessors/ofa/utils/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/preprocessors/ofa/utils/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3221 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/preprocessors/ofa/utils/audio_helper.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8936 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/ofa/utils/bridge_content_encoder.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6809 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/ofa/utils/collate.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      660 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/ofa/utils/constant.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3421 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/ofa/utils/get_tables.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1302 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/ofa/utils/random_help.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5459 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/preprocessors/ofa/utils/text2phone.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    19116 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/preprocessors/ofa/utils/transforms.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9581 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/preprocessors/ofa/utils/vision_helper.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7664 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/ofa/visual_entailment.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8484 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/ofa/visual_grounding.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6796 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/ofa/visual_question_answering.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/preprocessors/science/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      478 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/preprocessors/science/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    21698 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/science/uni_fold.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    13735 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/preprocessors/speaker.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2101 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/tts.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    13168 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/preprocessors/video.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/swift/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1394 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/swift/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7462 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/swift/adapter.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      841 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/swift/base.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    38973 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/swift/control_sd_lora.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    27160 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/swift/lora.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/swift/optimizers/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/swift/optimizers/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6931 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/swift/optimizers/child_tuning_adamw_optimizer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9560 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/swift/prompt.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8811 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/swift/sd_lora.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/tools/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       49 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/tools/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      772 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/tools/eval.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5359 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/tools/speech_tts_autolabel.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      607 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/tools/train.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/trainers/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1799 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/trainers/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/trainers/audio/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      826 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/trainers/audio/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1873 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/trainers/audio/ans_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6969 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/trainers/audio/asr_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12816 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/trainers/audio/kws_farfield_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    22065 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/trainers/audio/kws_nearfield_trainer.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/trainers/audio/kws_utils/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1768 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/trainers/audio/kws_utils/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    22114 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/trainers/audio/kws_utils/batch_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11499 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/trainers/audio/kws_utils/det_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7022 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/trainers/audio/kws_utils/file_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4509 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/trainers/audio/kws_utils/model_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2854 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/trainers/audio/kws_utils/runtime_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    21962 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/trainers/audio/separation_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10858 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/trainers/audio/tts_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4389 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/trainers/base.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1808 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/trainers/builder.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5553 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/trainers/cli_argument_parser.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/trainers/cv/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1974 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/trainers/cv/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7494 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/trainers/cv/action_detection_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      668 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/trainers/cv/card_detection_scrfd_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10044 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/trainers/cv/cartoon_translation_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6737 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/trainers/cv/face_detection_scrfd_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    20323 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/trainers/cv/image_classifition_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11655 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/trainers/cv/image_defrcn_fewshot_detection_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    22965 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/trainers/cv/image_detection_damoyolo_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4405 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/trainers/cv/image_inpainting_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      816 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/trainers/cv/image_instance_segmentation_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5520 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/trainers/cv/image_portrait_enhancement_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      680 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/trainers/cv/movie_scene_segmentation_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    20149 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/trainers/cv/nerf_recon_acc_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    16911 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/trainers/cv/ocr_detection_db_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3153 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/trainers/cv/ocr_recognition_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2248 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/trainers/cv/referring_video_object_segmentation_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4602 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/trainers/cv/vision_efficient_tuning_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2607 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/trainers/default_config.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/trainers/hooks/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2064 2023-06-26 10:20:06.000000 modelscope-1.8.0/modelscope/trainers/hooks/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      296 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/trainers/hooks/builder.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/trainers/hooks/checkpoint/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      116 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/trainers/hooks/checkpoint/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    19610 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/trainers/hooks/checkpoint/checkpoint_hook.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11015 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/trainers/hooks/checkpoint/checkpoint_processor.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5541 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/trainers/hooks/checkpoint/load_checkpoint_hook.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      764 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/trainers/hooks/clip_clamp_logit_scale_hook.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/trainers/hooks/compression/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      610 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/trainers/hooks/compression/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4812 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/trainers/hooks/compression/sparsity_hook.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6915 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/trainers/hooks/compression/utils.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/trainers/hooks/distributed/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-06-26 10:20:07.000000 modelscope-1.8.0/modelscope/trainers/hooks/distributed/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1458 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/trainers/hooks/distributed/ddp_hook.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    17119 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/trainers/hooks/distributed/deepspeed_hook.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7030 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/trainers/hooks/distributed/megatron_hook.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4398 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/trainers/hooks/early_stop_hook.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3825 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/trainers/hooks/evaluation_hook.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6507 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/trainers/hooks/hook.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      731 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/trainers/hooks/iter_timer_hook.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/trainers/hooks/logger/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      718 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/trainers/hooks/logger/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4498 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/trainers/hooks/logger/base.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4507 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/trainers/hooks/logger/tensorboard_hook.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7443 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/trainers/hooks/logger/text_logger_hook.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6567 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/trainers/hooks/lr_scheduler_hook.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/trainers/hooks/optimizer/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      743 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/trainers/hooks/optimizer/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3139 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/trainers/hooks/optimizer/apex_optimizer_hook.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3786 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/trainers/hooks/optimizer/base.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3729 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/trainers/hooks/optimizer/torch_optimizer_hook.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1707 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/trainers/hooks/priority.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/trainers/lrscheduler/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      699 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/trainers/lrscheduler/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2040 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/trainers/lrscheduler/builder.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/trainers/lrscheduler/warmup/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      614 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/trainers/lrscheduler/warmup/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2547 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/trainers/lrscheduler/warmup/base.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2934 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/trainers/lrscheduler/warmup/warmup.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/trainers/multi_modal/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      682 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/trainers/multi_modal/__init__.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/trainers/multi_modal/clip/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       89 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/trainers/multi_modal/clip/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9702 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/trainers/multi_modal/clip/clip_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4388 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/trainers/multi_modal/clip/clip_trainer_utils.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/trainers/multi_modal/custom_diffusion/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      110 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/trainers/multi_modal/custom_diffusion/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    32625 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/trainers/multi_modal/custom_diffusion/custom_diffusion_trainer.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/trainers/multi_modal/dreambooth_diffusion/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      118 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/trainers/multi_modal/dreambooth_diffusion/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    15264 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/trainers/multi_modal/dreambooth_diffusion/dreambooth_diffusion_trainer.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/trainers/multi_modal/efficient_diffusion_tuning/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/trainers/multi_modal/efficient_diffusion_tuning/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3047 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/trainers/multi_modal/efficient_diffusion_tuning/efficient_diffusion_tuning_trainer.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/trainers/multi_modal/lora_diffusion/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      106 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/trainers/multi_modal/lora_diffusion/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3503 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/trainers/multi_modal/lora_diffusion/lora_diffusion_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8086 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/trainers/multi_modal/mgeo_ranking_trainer.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/trainers/multi_modal/mplug/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       91 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/trainers/multi_modal/mplug/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1642 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/trainers/multi_modal/mplug/mplug_trainer.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/trainers/multi_modal/ofa/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       87 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/trainers/multi_modal/ofa/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    10186 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/trainers/multi_modal/ofa/ofa_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    17647 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/trainers/multi_modal/ofa/ofa_trainer_utils.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/trainers/multi_modal/stable_diffusion/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      112 2023-06-26 10:20:07.000000 modelscope-1.8.0/modelscope/trainers/multi_modal/stable_diffusion/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1144 2023-06-26 10:20:07.000000 modelscope-1.8.0/modelscope/trainers/multi_modal/stable_diffusion/stable_diffusion_trainer.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/trainers/multi_modal/team/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       95 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/trainers/multi_modal/team/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5429 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/trainers/multi_modal/team/team_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2443 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/trainers/multi_modal/team/team_trainer_utils.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/trainers/nlp/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1332 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/trainers/nlp/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    14266 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/trainers/nlp/csanmt_translation_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     9999 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/trainers/nlp/document_grounded_dialog_generate_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    24284 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/trainers/nlp/document_grounded_dialog_rerank_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8159 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/trainers/nlp/document_grounded_dialog_retrieval_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12769 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/trainers/nlp/faq_question_answering_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3064 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/trainers/nlp/gpt3_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2097 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/trainers/nlp/gpt_moe_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8171 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/trainers/nlp/plug_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3869 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/trainers/nlp/sentence_embedding_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     8378 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/trainers/nlp/sequence_classification_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    17110 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/trainers/nlp/siamese_uie_trainer.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/trainers/nlp/space/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/trainers/nlp/space/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5324 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/trainers/nlp/space/dialog_intent_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4229 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/trainers/nlp/space/dialog_modeling_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    36354 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/trainers/nlp/space/eval.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/trainers/nlp/space/metrics/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/trainers/nlp/space/metrics/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2447 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/trainers/nlp/space/metrics/metrics_tracker.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/trainers/nlp/space/trainer/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/trainers/nlp/space/trainer/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    30567 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/trainers/nlp/space/trainer/gen_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    29570 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/trainers/nlp/space/trainer/intent_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    20453 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/trainers/nlp/table_question_answering_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1367 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/trainers/nlp/text_generation_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7514 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/trainers/nlp/text_ranking_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    15326 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/trainers/nlp/translation_evaluation_trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7129 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/trainers/nlp_trainer.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/trainers/optimizer/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      210 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/trainers/optimizer/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1753 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/trainers/optimizer/builder.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/trainers/parallel/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       80 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/trainers/parallel/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      681 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/trainers/parallel/builder.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      754 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/trainers/parallel/utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    59372 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/trainers/trainer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    17846 2023-06-26 10:20:07.000000 modelscope-1.8.0/modelscope/trainers/training_args.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/trainers/utils/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/trainers/utils/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11090 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/trainers/utils/inference.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1294 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/trainers/utils/log_buffer.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/utils/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       56 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/utils/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)   547685 2023-08-03 04:26:18.000000 modelscope-1.8.0/modelscope/utils/ast_index_file.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    28920 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/utils/ast_utils.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/utils/audio/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/utils/audio/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11795 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/utils/audio/audio_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2428 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/utils/audio/tts_exceptions.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    26390 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/utils/checkpoint.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2573 2023-03-23 13:31:58.000000 modelscope-1.8.0/modelscope/utils/chinese_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    27433 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/utils/config.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1027 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/utils/config_ds.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    19245 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/utils/constant.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/utils/cv/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/utils/cv/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    23195 2023-04-20 06:25:07.000000 modelscope-1.8.0/modelscope/utils/cv/image_utils.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/utils/cv/motion_utils/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/utils/cv/motion_utils/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2307 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/utils/cv/motion_utils/motion_process.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3847 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/utils/cv/motion_utils/plot_script.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4290 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/utils/cv/motion_utils/rotation_conversions.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3145 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/utils/data_collators.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1279 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/utils/data_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     3744 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/utils/device.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6522 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/utils/error.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1128 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/utils/file_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4501 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/utils/hf_util.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5939 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/utils/hub.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    15900 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/utils/import_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    25766 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/utils/input_output.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      296 2023-06-26 10:20:07.000000 modelscope-1.8.0/modelscope/utils/input_output_typing.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      478 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/utils/json_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2817 2023-07-19 09:56:21.000000 modelscope-1.8.0/modelscope/utils/logger.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7612 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/utils/megatron_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2335 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/utils/metric.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     5099 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/utils/model_tag.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/utils/nlp/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      499 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/utils/nlp/__init__.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     4424 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/utils/nlp/distributed.py
--rwxr-xr-x   0 wenmeng.zwm (122285) users      (100)     4526 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/utils/nlp/load_checkpoint.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/utils/nlp/space/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/utils/nlp/space/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1909 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/utils/nlp/space/args.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11818 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/utils/nlp/space/clean_dataset.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1439 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/utils/nlp/space/criterions.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11252 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/utils/nlp/space/db_ops.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6174 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/utils/nlp/space/ontology.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      197 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/utils/nlp/space/scores.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6356 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/utils/nlp/space/utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1054 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/utils/nlp/space/utils_dst.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope/utils/nlp/space_T_en/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        0 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/utils/nlp/space_T_en/__init__.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      859 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/utils/nlp/space_T_en/utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2554 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/utils/nlp/utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    42024 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/utils/plugins.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      697 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/utils/pre_compile.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     7833 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/utils/registry.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    30195 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/utils/regress_test_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     6127 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/utils/service_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    25333 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/utils/streaming_output.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     2500 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/utils/task_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1593 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/utils/tensor_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    12532 2023-05-16 08:00:35.000000 modelscope-1.8.0/modelscope/utils/test_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1209 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/utils/timer.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    11129 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/utils/torch_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      597 2022-12-08 09:39:03.000000 modelscope-1.8.0/modelscope/utils/trie.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     1696 2023-03-23 13:26:52.000000 modelscope-1.8.0/modelscope/utils/type_assert.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      783 2023-04-20 06:52:22.000000 modelscope-1.8.0/modelscope/utils/url_utils.py
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)      272 2023-08-03 04:25:41.000000 modelscope-1.8.0/modelscope/version.py
-drwxr-xr-x   0 wenmeng.zwm (122285) users      (100)        0 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope.egg-info/
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)    17074 2023-08-03 04:26:20.000000 modelscope-1.8.0/modelscope.egg-info/PKG-INFO
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)   139320 2023-08-03 04:26:21.000000 modelscope-1.8.0/modelscope.egg-info/SOURCES.txt
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        1 2023-08-03 04:26:20.000000 modelscope-1.8.0/modelscope.egg-info/dependency_links.txt
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       58 2023-08-03 04:26:20.000000 modelscope-1.8.0/modelscope.egg-info/entry_points.txt
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)        1 2023-08-03 04:26:20.000000 modelscope-1.8.0/modelscope.egg-info/not-zip-safe
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)     4799 2023-08-03 04:26:20.000000 modelscope-1.8.0/modelscope.egg-info/requires.txt
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       11 2023-08-03 04:26:20.000000 modelscope-1.8.0/modelscope.egg-info/top_level.txt
--rw-r--r--   0 wenmeng.zwm (122285) users      (100)       38 2023-08-03 04:26:21.000000 modelscope-1.8.0/setup.cfg
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/
+-rw-r--r--   0 runner    (1001) docker     (122)      105 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/MANIFEST.in
+-rw-r--r--   0 runner    (1001) docker     (122)    19508 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (122)    16077 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/README.md
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/
+-rw-r--r--   0 runner    (1001) docker     (122)     4020 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/cli/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/cli/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      404 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/cli/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)      829 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/cli/cli.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1230 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/cli/download.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6242 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/cli/modelcard.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4371 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/cli/pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3304 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/cli/plugins.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/cli/template/
+-rw-r--r--   0 runner    (1001) docker     (122)      523 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/cli/template/readme.tpl
+-rw-r--r--   0 runner    (1001) docker     (122)     4912 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/cli/template/template.tpl
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/configs/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/configs/examples/
+-rw-r--r--   0 runner    (1001) docker     (122)       36 2023-07-31 12:40:31.000000 modelscope-1.8.0rc0/modelscope/configs/examples/configuration.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/exporters/
+-rw-r--r--   0 runner    (1001) docker     (122)     1360 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/exporters/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/exporters/audio/
+-rw-r--r--   0 runner    (1001) docker     (122)      507 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/exporters/audio/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2272 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/exporters/audio/ans_dfsmn_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2659 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/exporters/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)      732 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/exporters/builder.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/exporters/cv/
+-rw-r--r--   0 runner    (1001) docker     (122)      869 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/exporters/cv/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2585 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/exporters/cv/cartoon_translation_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3619 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/exporters/cv/face_detection_scrfd_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1358 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/exporters/cv/object_detection_damoyolo_exporter.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/exporters/multi_modal/
+-rw-r--r--   0 runner    (1001) docker     (122)      531 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/exporters/multi_modal/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11238 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/exporters/multi_modal/stable_diffusion_exporter.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/exporters/nlp/
+-rw-r--r--   0 runner    (1001) docker     (122)     1082 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/exporters/nlp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7333 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/exporters/nlp/csanmt_for_translation_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4579 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/exporters/nlp/model_for_token_classification_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3409 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/exporters/nlp/sbert_for_sequence_classification_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2316 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/exporters/nlp/sbert_for_zero_shot_classification_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4809 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/exporters/tf_model_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15145 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/exporters/torch_model_exporter.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/fileio/
+-rw-r--r--   0 runner    (1001) docker     (122)      122 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/fileio/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10241 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/fileio/file.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/fileio/format/
+-rw-r--r--   0 runner    (1001) docker     (122)      143 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/fileio/format/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      454 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/fileio/format/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1050 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/fileio/format/json.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13853 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/fileio/format/jsonplus.py
+-rw-r--r--   0 runner    (1001) docker     (122)      669 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/fileio/format/yaml.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4254 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/fileio/io.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/hub/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/hub/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    42689 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/hub/api.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3927 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/hub/check_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1628 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/hub/constants.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11358 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/hub/deploy.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4066 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/hub/errors.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12706 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/hub/file_download.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8958 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/hub/git.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7272 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/hub/push_to_hub.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12489 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/hub/repository.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7178 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/hub/snapshot_download.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/hub/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/hub/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9916 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/hub/utils/caching.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2978 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/hub/utils/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    56449 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/metainfo.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/metrics/
+-rw-r--r--   0 runner    (1001) docker     (122)     3978 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/metrics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2510 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/metrics/accuracy_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7530 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/metrics/action_detection_evaluator.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1925 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/metrics/audio_noise_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1454 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/metrics/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1726 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/metrics/bleu_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3715 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/metrics/builder.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/metrics/ciderD/
+-rwxr-xr-x   0 runner    (1001) docker     (122)       21 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/metrics/ciderD/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     1926 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/metrics/ciderD/ciderD.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     8931 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/metrics/ciderD/ciderD_scorer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8664 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/metrics/image_color_enhance_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13387 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/metrics/image_colorization_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10011 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/metrics/image_denoise_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7612 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/metrics/image_inpainting_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13318 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/metrics/image_instance_segmentation_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1739 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/metrics/image_portrait_enhancement_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2536 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/metrics/image_quality_assessment_degradation_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1632 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/metrics/image_quality_assessment_mos_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2231 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/metrics/inbatch_recall_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1358 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/metrics/loss_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3002 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/metrics/map_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2032 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/metrics/movie_scene_segmentation_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3197 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/metrics/ned_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2310 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/metrics/ocr_recognition_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2082 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/metrics/ppl_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1163 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/metrics/prediction_saving_wrapper.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4453 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/metrics/referring_video_object_segmentation_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3112 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/metrics/sequence_classification_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3694 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/metrics/text_generation_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3197 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/metrics/text_ranking_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5765 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/metrics/token_classification_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5703 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/metrics/translation_evaluation_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5435 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/metrics/video_frame_interpolation_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8655 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/metrics/video_stabilization_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3092 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/metrics/video_summarization_metric.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/metrics/video_super_resolution_metric/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/metrics/video_super_resolution_metric/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7030 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/metrics/video_super_resolution_metric/matlab_functions.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4771 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/metrics/video_super_resolution_metric/metric_util.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8932 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/metrics/video_super_resolution_metric/niqe.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1710 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/metrics/video_super_resolution_metric/video_super_resolution_metric.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/
+-rw-r--r--   0 runner    (1001) docker     (122)      519 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/audio/
+-rw-r--r--   0 runner    (1001) docker     (122)       93 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/audio/aec/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/aec/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/audio/aec/layers/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/aec/layers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1434 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/aec/layers/activations.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2700 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/aec/layers/affine_transform.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5630 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/aec/layers/deep_fsmn.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1322 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/aec/layers/layer_base.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14384 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/aec/layers/uni_deep_fsmn.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/audio/aec/network/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/aec/network/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14438 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/aec/network/loss.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9382 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/aec/network/modulation_loss.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15396 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/aec/network/se_net.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/audio/ans/
+-rw-r--r--   0 runner    (1001) docker     (122)      515 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/ans/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6475 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/ans/complex_nn.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3688 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/ans/conv_stft.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2513 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/ans/denoise_net.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10241 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/ans/frcrn.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/audio/ans/layers/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/ans/layers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1468 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/ans/layers/activations.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2414 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/ans/layers/affine_transform.py
+-rw-r--r--   0 runner    (1001) docker     (122)      661 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/ans/layers/layer_base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5284 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/ans/layers/uni_deep_fsmn.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1038 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/ans/se_module_complex.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9833 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/ans/unet.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/audio/asr/
+-rw-r--r--   0 runner    (1001) docker     (122)      585 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/asr/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1780 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/asr/generic_automatic_speech_recognition.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1204 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/asr/wenet_automatic_speech_recognition.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/audio/itn/
+-rw-r--r--   0 runner    (1001) docker     (122)      557 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/itn/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1462 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/itn/generic_inverse_text_processing.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/audio/kws/
+-rw-r--r--   0 runner    (1001) docker     (122)      735 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/kws/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/audio/kws/farfield/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/kws/farfield/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14349 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/kws/farfield/fsmn.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7462 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/kws/farfield/fsmn_sele_v2.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7861 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/kws/farfield/fsmn_sele_v3.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3521 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/kws/farfield/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2608 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/kws/farfield/model_def.py
+-rw-r--r--   0 runner    (1001) docker     (122)      908 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/kws/generic_key_word_spotting.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/audio/kws/nearfield/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/kws/nearfield/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3300 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/kws/nearfield/cmvn.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17496 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/kws/nearfield/fsmn.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6079 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/kws/nearfield/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/audio/punc/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/punc/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1466 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/punc/generic_punctuation.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/audio/separation/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/separation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2240 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/separation/layer_norm.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13963 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/separation/mossformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9142 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/separation/mossformer_block.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9004 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/separation/mossformer_conv_module.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/audio/sv/
+-rw-r--r--   0 runner    (1001) docker     (122)     7326 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/sv/DTDNN.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8425 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/sv/DTDNN_layers.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11692 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/sv/ERes2Net.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11358 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/sv/ERes2Net_aug.py
+-rw-r--r--   0 runner    (1001) docker     (122)      498 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/sv/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6397 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/sv/cluster_backend.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14980 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/sv/ecapa_tdnn.py
+-rw-r--r--   0 runner    (1001) docker     (122)      904 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/sv/fusion.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1488 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/sv/generic_speaker_verification.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3743 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/sv/lanuage_recognition_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3630 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/sv/pooling_layers.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16895 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/sv/rdino.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12338 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/sv/speaker_change_locator.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3519 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/sv/speaker_diarization_dialogue_detection.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5170 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/sv/speaker_diarization_semantic_speaker_turn_detection.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/audio/tts/
+-rw-r--r--   0 runner    (1001) docker     (122)      474 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/tts/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11845 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/tts/sambert_hifi.py
+-rw-r--r--   0 runner    (1001) docker     (122)    26390 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/audio/tts/voice.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/base/
+-rw-r--r--   0 runner    (1001) docker     (122)      303 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/base/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1091 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/base/base_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7308 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/base/base_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)      605 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/base/base_torch_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5679 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/base/base_torch_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3647 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/builder.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/
+-rw-r--r--   0 runner    (1001) docker     (122)     1812 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/abnormal_object_detection/
+-rw-r--r--   0 runner    (1001) docker     (122)      490 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/abnormal_object_detection/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3806 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/abnormal_object_detection/mmdet_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/abnormal_object_detection/mmdet_ms/
+-rw-r--r--   0 runner    (1001) docker     (122)      113 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/abnormal_object_detection/mmdet_ms/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/
+-rw-r--r--   0 runner    (1001) docker     (122)      211 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6414 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/mask_scoring_roi_head.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/roi_extractors/
+-rw-r--r--   0 runner    (1001) docker     (122)      145 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/roi_extractors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6729 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/roi_extractors/single_level_roi_extractor.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/action_detection/
+-rw-r--r--   0 runner    (1001) docker     (122)      493 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/action_detection/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7307 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/action_detection/action_detection_onnx.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/action_detection/modules/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/action_detection/modules/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9364 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/action_detection/modules/action_detection_pytorch.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12132 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/action_detection/modules/resnet.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/action_recognition/
+-rw-r--r--   0 runner    (1001) docker     (122)      709 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/action_recognition/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4304 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/action_recognition/models.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10276 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/action_recognition/s3dg.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18618 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/action_recognition/tada_convnext.py
+-rw-r--r--   0 runner    (1001) docker     (122)    45442 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/action_recognition/temporal_patch_shift_transformer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/animal_recognition/
+-rw-r--r--   0 runner    (1001) docker     (122)      558 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/animal_recognition/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14141 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/animal_recognition/resnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3955 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/animal_recognition/splat.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/bad_image_detecting/
+-rw-r--r--   0 runner    (1001) docker     (122)      496 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/bad_image_detecting/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2589 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/bad_image_detecting/bad_image_detecting.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/body_2d_keypoints/
+-rw-r--r--   0 runner    (1001) docker     (122)      502 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/body_2d_keypoints/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12660 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/body_2d_keypoints/hrnet_basic_modules.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8773 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/body_2d_keypoints/hrnet_v2.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1707 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/body_2d_keypoints/w48.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/body_3d_keypoints/
+-rw-r--r--   0 runner    (1001) docker     (122)      601 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/body_3d_keypoints/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/body_3d_keypoints/cannonical_pose/
+-rw-r--r--   0 runner    (1001) docker     (122)       51 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/body_3d_keypoints/cannonical_pose/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8957 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/body_3d_keypoints/cannonical_pose/body_3d_pose.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8196 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/body_3d_keypoints/cannonical_pose/canonical_pose_modules.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/body_3d_keypoints/hdformer/
+-rw-r--r--   0 runner    (1001) docker     (122)       98 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/body_3d_keypoints/hdformer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11552 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/body_3d_keypoints/hdformer/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12870 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/body_3d_keypoints/hdformer/block.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8080 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/body_3d_keypoints/hdformer/directed_graph.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2516 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/body_3d_keypoints/hdformer/hdformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7719 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/body_3d_keypoints/hdformer/hdformer_detector.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3401 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/body_3d_keypoints/hdformer/skeleton.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/cartoon/
+-rw-r--r--   0 runner    (1001) docker     (122)     1216 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/cartoon/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/cartoon/facelib/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/cartoon/facelib/LK/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/cartoon/facelib/LK/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3488 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/cartoon/facelib/LK/lk.py
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/cartoon/facelib/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      713 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/cartoon/facelib/config.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3512 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/cartoon/facelib/face_detector.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5158 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/cartoon/facelib/face_landmark.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4563 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/cartoon/facelib/facer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9075 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/cartoon/loss.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2174 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/cartoon/model_tf.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/cartoon/mtcnn_pytorch/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/cartoon/mtcnn_pytorch/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/cartoon/mtcnn_pytorch/src/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/cartoon/mtcnn_pytorch/src/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6072 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/cartoon/mtcnn_pytorch/src/align_trans.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8519 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/cartoon/mtcnn_pytorch/src/matlab_cp2tform.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4404 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/cartoon/network.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5265 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/cartoon/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/cmdssl_video_embedding/
+-rw-r--r--   0 runner    (1001) docker     (122)      647 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/cmdssl_video_embedding/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3876 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/cmdssl_video_embedding/c3d.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10785 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/cmdssl_video_embedding/resnet2p1d.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8949 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/cmdssl_video_embedding/resnet3d.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/
+-rw-r--r--   0 runner    (1001) docker     (122)      414 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/annotator/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/annotator/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13516 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/annotator/annotator.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/annotator/midas/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/annotator/midas/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4960 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/annotator/midas/api.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      504 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/base_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10152 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/blocks.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3307 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/dpt_depth.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2746 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/midas_net.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5829 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/midas_net_custom.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8045 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/transforms.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15438 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/vit.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4799 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/annotator/midas/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/annotator/mlsd/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/annotator/mlsd/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9896 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/annotator/mlsd/mbv2_mlsd_large.py
+-rw-r--r--   0 runner    (1001) docker     (122)    25158 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/annotator/mlsd/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/annotator/openpose/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/annotator/openpose/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12595 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/annotator/openpose/body.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3894 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/annotator/openpose/hand.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9024 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/annotator/openpose/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8031 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/annotator/openpose/util.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8546 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/controlnet.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/crowd_counting/
+-rw-r--r--   0 runner    (1001) docker     (122)      491 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/crowd_counting/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1100 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/crowd_counting/cc_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    22875 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/crowd_counting/hrnet_aspp_relu.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_attribute_recognition/
+-rw-r--r--   0 runner    (1001) docker     (122)      490 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_attribute_recognition/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_attribute_recognition/fair_face/
+-rw-r--r--   0 runner    (1001) docker     (122)      115 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_attribute_recognition/fair_face/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2559 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_attribute_recognition/fair_face/face_attribute_recognition.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/
+-rw-r--r--   0 runner    (1001) docker     (122)      930 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/mogface/
+-rw-r--r--   0 runner    (1001) docker     (122)       96 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/mogface/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/mogface/models/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/mogface/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3032 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/mogface/models/detectors.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4675 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/mogface/models/mogface.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5578 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/mogface/models/mogprednet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6264 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/mogface/models/resnet.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     7072 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/mogface/models/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/mtcnn/
+-rw-r--r--   0 runner    (1001) docker     (122)       97 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/mtcnn/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/mtcnn/models/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/mtcnn/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7061 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/mtcnn/models/box_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5415 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/mtcnn/models/detector.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3171 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/mtcnn/models/first_stage.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5232 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/mtcnn/models/get_nets.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/peppa_pig_face/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/peppa_pig_face/LK/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/peppa_pig_face/LK/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3444 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/peppa_pig_face/LK/lk.py
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/peppa_pig_face/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3578 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/peppa_pig_face/face_detector.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5129 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/peppa_pig_face/face_landmark.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4210 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/peppa_pig_face/facer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/retinaface/
+-rw-r--r--   0 runner    (1001) docker     (122)       93 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/retinaface/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     4832 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/retinaface/detection.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/retinaface/models/
+-rwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/retinaface/models/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     4766 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/retinaface/models/net.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     4590 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/retinaface/models/retinaface.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     4120 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/retinaface/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/
+-rw-r--r--   0 runner    (1001) docker     (122)      174 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      955 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/damofd_detect.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      192 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/bbox/
+-rw-r--r--   0 runner    (1001) docker     (122)      325 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/bbox/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     2945 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/bbox/transforms.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/post_processing/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      292 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/post_processing/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3423 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/post_processing/bbox_nms.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/
+-rw-r--r--   0 runner    (1001) docker     (122)      276 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      471 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11707 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/auto_augment.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4169 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/formating.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7981 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/loading.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    30670 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/transforms.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     5543 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/retinaface.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      289 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      361 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4202 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/master_net.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3590 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/mobilenet.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16146 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/resnet.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/dense_heads/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      270 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/dense_heads/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    46999 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/dense_heads/scrfd_head.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      295 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11062 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/base.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     6092 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/scrfd.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6395 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/single_stage.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     5970 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/tinymog.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3451 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3869 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/scrfd_detect.py
+-rw-r--r--   0 runner    (1001) docker     (122)      960 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/tinymog_detect.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/ulfd_slim/
+-rw-r--r--   0 runner    (1001) docker     (122)       90 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/ulfd_slim/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     1477 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/ulfd_slim/detection.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/ulfd_slim/vision/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/ulfd_slim/vision/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4126 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/ulfd_slim/vision/box_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2058 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/ulfd_slim/vision/mb_tiny.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      571 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/data_preprocessing.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1641 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/fd_config.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3719 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/mb_tiny_fd.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2845 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/predictor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4621 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/ssd.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1497 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_detection/ulfd_slim/vision/transforms.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_emotion/
+-rw-r--r--   0 runner    (1001) docker     (122)      540 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_emotion/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_emotion/efficient/
+-rw-r--r--   0 runner    (1001) docker     (122)      327 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_emotion/efficient/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15911 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_emotion/efficient/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20077 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_emotion/efficient/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2007 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_emotion/emotion_infer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3171 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_emotion/emotion_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_emotion/face_alignment/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_emotion/face_alignment/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2844 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_emotion/face_alignment/face.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1674 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_emotion/face_alignment/face_align.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_generation/
+-rw-r--r--   0 runner    (1001) docker     (122)      475 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_generation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_generation/op/
+-rwxr-xr-x   0 runner    (1001) docker     (122)       89 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_generation/op/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     6497 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_generation/op/conv2d_gradfix.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     3104 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_generation/op/fused_act.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     5922 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_generation/op/upfirdn2d.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    19998 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_generation/stylegan2.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_human_hand_detection/
+-rw-r--r--   0 runner    (1001) docker     (122)      544 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_human_hand_detection/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4109 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_human_hand_detection/det_infer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12584 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_human_hand_detection/ghost_pan.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16846 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_human_hand_detection/nanodet_plus_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1833 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_human_hand_detection/one_stage_detector.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5791 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_human_hand_detection/shufflenetv2.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8874 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_human_hand_detection/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_recognition/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_recognition/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1572 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_recognition/align_face.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_recognition/torchkit/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      473 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_recognition/torchkit/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_recognition/torchkit/backbone/
+-rwxr-xr-x   0 runner    (1001) docker     (122)     1080 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_recognition/torchkit/backbone/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6507 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_recognition/torchkit/backbone/arcface_backbone.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     1977 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_recognition/torchkit/backbone/common.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7572 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_recognition/torchkit/backbone/facemask_backbone.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     8551 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_recognition/torchkit/backbone/model_irse.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     4744 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_recognition/torchkit/backbone/model_resnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7186 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_recognition/torchkit/rts_backbone.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_reconstruction/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_reconstruction/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_reconstruction/models/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_reconstruction/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    31337 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_reconstruction/models/bfm.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1428 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_reconstruction/models/de_retouching_module.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_reconstruction/models/facelandmark/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_reconstruction/models/facelandmark/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2583 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_reconstruction/models/facelandmark/large_base_lmks_infer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_reconstruction/models/facelandmark/nets/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_reconstruction/models/facelandmark/nets/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6561 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_reconstruction/models/facelandmark/nets/large_base_lmks_net.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5124 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_reconstruction/models/facelandmark/nets/large_eyeball_net.py
+-rw-r--r--   0 runner    (1001) docker     (122)    30403 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_reconstruction/models/facerecon_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13171 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_reconstruction/models/losses.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21235 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_reconstruction/models/networks.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8821 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_reconstruction/models/nv_diffrast.py
+-rw-r--r--   0 runner    (1001) docker     (122)      277 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_reconstruction/models/opt.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_reconstruction/models/pix2pix/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_reconstruction/models/pix2pix/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    31957 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_reconstruction/models/pix2pix/networks.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5908 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_reconstruction/models/pix2pix/pix2pix_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)      779 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_reconstruction/models/pix2pix/pix2pix_options.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    13410 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_reconstruction/models/renderer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4933 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_reconstruction/models/unet.py
+-rw-r--r--   0 runner    (1001) docker     (122)    31247 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/face_reconstruction/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/facial_expression_recognition/
+-rw-r--r--   0 runner    (1001) docker     (122)      484 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/facial_expression_recognition/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/facial_expression_recognition/fer/
+-rw-r--r--   0 runner    (1001) docker     (122)      121 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/facial_expression_recognition/fer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2414 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/facial_expression_recognition/fer/facial_expression_recognition.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3277 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/facial_expression_recognition/fer/transforms.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1311 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/facial_expression_recognition/fer/vgg.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/facial_landmark_confidence/
+-rw-r--r--   0 runner    (1001) docker     (122)      478 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/facial_landmark_confidence/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/facial_landmark_confidence/flc/
+-rw-r--r--   0 runner    (1001) docker     (122)      115 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/facial_landmark_confidence/flc/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3197 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/facial_landmark_confidence/flc/facial_landmark_confidence.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4932 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/facial_landmark_confidence/flc/manual_landmark_net.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/hand_static/
+-rw-r--r--   0 runner    (1001) docker     (122)      502 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/hand_static/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2480 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/hand_static/hand_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10891 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/hand_static/networks.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/human_reconstruction/
+-rw-r--r--   0 runner    (1001) docker     (122)     5936 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/human_reconstruction/Reconstruction.py
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/human_reconstruction/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/human_reconstruction/models/
+-rw-r--r--   0 runner    (1001) docker     (122)     1065 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/human_reconstruction/models/Embedding.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4542 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/human_reconstruction/models/PixToMesh.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11301 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/human_reconstruction/models/Res_backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2418 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/human_reconstruction/models/Surface_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/human_reconstruction/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2716 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/human_reconstruction/models/detectors.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1982 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/human_reconstruction/models/geometry.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2248 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/human_reconstruction/models/human_segmenter.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11865 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/human_reconstruction/models/networks.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6105 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/human_reconstruction/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_binary_quant_classification/
+-rw-r--r--   0 runner    (1001) docker     (122)      573 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_binary_quant_classification/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2859 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_binary_quant_classification/binary_quant_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19111 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_binary_quant_classification/bnext.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_body_reshaping/
+-rw-r--r--   0 runner    (1001) docker     (122)      500 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_body_reshaping/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3920 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_body_reshaping/image_body_reshaping.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6541 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_body_reshaping/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13616 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_body_reshaping/person_info.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_body_reshaping/pose_estimator/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_body_reshaping/pose_estimator/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12027 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_body_reshaping/pose_estimator/body.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5673 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_body_reshaping/pose_estimator/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1311 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_body_reshaping/pose_estimator/util.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15760 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_body_reshaping/slim_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_classification/
+-rw-r--r--   0 runner    (1001) docker     (122)      598 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_classification/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_classification/backbones/
+-rw-r--r--   0 runner    (1001) docker     (122)      107 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_classification/backbones/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20295 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_classification/backbones/beit_v2.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17261 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_classification/backbones/nextvit.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2185 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_classification/mmcls_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1554 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_classification/resnet50_cc.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5255 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_classification/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_color_enhance/
+-rw-r--r--   0 runner    (1001) docker     (122)      704 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_color_enhance/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_color_enhance/adaint/
+-rw-r--r--   0 runner    (1001) docker     (122)       44 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_color_enhance/adaint/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14338 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_color_enhance/adaint/adaint.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3753 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_color_enhance/csrnet.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_color_enhance/deeplpf/
+-rw-r--r--   0 runner    (1001) docker     (122)       66 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_color_enhance/deeplpf/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2575 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_color_enhance/deeplpf/deeplpf_image_color_enhance.py
+-rw-r--r--   0 runner    (1001) docker     (122)    31764 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_color_enhance/deeplpf/deeplpfnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2821 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_color_enhance/image_color_enhance.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_colorization/
+-rw-r--r--   0 runner    (1001) docker     (122)      640 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_colorization/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_colorization/ddcolor/
+-rw-r--r--   0 runner    (1001) docker     (122)      122 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_colorization/ddcolor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9385 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_colorization/ddcolor/ddcolor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6437 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_colorization/ddcolor/ddcolor_for_image_colorization.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9576 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_colorization/ddcolor/loss.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_colorization/ddcolor/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_colorization/ddcolor/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6857 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_colorization/ddcolor/utils/convnext.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2196 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_colorization/ddcolor/utils/position_encoding.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7796 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_colorization/ddcolor/utils/transformer_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6465 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_colorization/ddcolor/utils/unet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6510 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_colorization/ddcolor/utils/vgg.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_colorization/unet/
+-rw-r--r--   0 runner    (1001) docker     (122)      129 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_colorization/unet/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10574 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_colorization/unet/unet.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10647 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_colorization/unet/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_debanding/
+-rw-r--r--   0 runner    (1001) docker     (122)      483 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_debanding/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_debanding/rrdb/
+-rw-r--r--   0 runner    (1001) docker     (122)       53 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_debanding/rrdb/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3005 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_debanding/rrdb/rrdb_image_debanding.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_deblur/
+-rw-r--r--   0 runner    (1001) docker     (122)      510 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_deblur/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4241 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_deblur/nafnet_for_image_deblur.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_defrcn_fewshot/
+-rw-r--r--   0 runner    (1001) docker     (122)      492 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_defrcn_fewshot/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4329 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_defrcn_fewshot/defrcn_for_fewshot.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_defrcn_fewshot/evaluation/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_defrcn_fewshot/evaluation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12474 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_defrcn_fewshot/evaluation/coco_evaluation.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3141 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_defrcn_fewshot/evaluation/evaluator.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4354 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_defrcn_fewshot/evaluation/pascal_voc_evaluation.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_defrcn_fewshot/models/
+-rw-r--r--   0 runner    (1001) docker     (122)      448 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_defrcn_fewshot/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6827 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_defrcn_fewshot/models/calibration_layer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7064 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_defrcn_fewshot/models/defrcn.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10779 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_defrcn_fewshot/models/fast_rcnn.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1220 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_defrcn_fewshot/models/gdl.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1732 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_defrcn_fewshot/models/resnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4770 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_defrcn_fewshot/models/roi_heads.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_defrcn_fewshot/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_defrcn_fewshot/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    23599 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_defrcn_fewshot/utils/coco_register.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4803 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_defrcn_fewshot/utils/configuration_mapper.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3595 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_defrcn_fewshot/utils/model_surgery_op.py
+-rw-r--r--   0 runner    (1001) docker     (122)      583 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_defrcn_fewshot/utils/register_data.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2617 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_defrcn_fewshot/utils/requirements_check.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11013 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_defrcn_fewshot/utils/voc_register.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_denoise/
+-rw-r--r--   0 runner    (1001) docker     (122)      514 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_denoise/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_denoise/nafnet/
+-rw-r--r--   0 runner    (1001) docker     (122)     6664 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_denoise/nafnet/NAFNet_arch.py
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_denoise/nafnet/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1627 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_denoise/nafnet/arch_util.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2475 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_denoise/nafnet_for_image_denoise.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_depth_estimation/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_depth_estimation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_depth_estimation/networks/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_depth_estimation/networks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6757 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_depth_estimation/networks/newcrf_depth.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18233 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_depth_estimation/networks/newcrf_layers.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10106 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_depth_estimation/networks/newcrf_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    26006 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_depth_estimation/networks/swin_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13560 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_depth_estimation/networks/uper_crf_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1600 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_depth_estimation/newcrfs_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_depth_estimation_bts/
+-rw-r--r--   0 runner    (1001) docker     (122)      536 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_depth_estimation_bts/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2606 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_depth_estimation_bts/depth_estimation_bts_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_depth_estimation_bts/networks/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_depth_estimation_bts/networks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1485 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_depth_estimation_bts/networks/bts_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2819 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_depth_estimation_bts/networks/decoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2506 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_depth_estimation_bts/networks/encoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8545 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_depth_estimation_bts/networks/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_driving_perception/
+-rw-r--r--   0 runner    (1001) docker     (122)      999 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_driving_perception/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2022 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_driving_perception/image_driving_percetion_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4352 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_driving_perception/preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7475 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_driving_perception/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_face_fusion/
+-rw-r--r--   0 runner    (1001) docker     (122)      488 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_face_fusion/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_face_fusion/facegan/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_face_fusion/facegan/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3124 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_face_fusion/facegan/gan_wrap.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21432 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_face_fusion/facegan/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_face_fusion/facegan/op/
+-rw-r--r--   0 runner    (1001) docker     (122)      242 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_face_fusion/facegan/op/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6497 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_face_fusion/facegan/op/conv2d_gradfix.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3094 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_face_fusion/facegan/op/fused_act.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5912 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_face_fusion/facegan/op/upfirdn2d.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_face_fusion/facelib/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_face_fusion/facelib/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10514 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_face_fusion/facelib/align_trans.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5955 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_face_fusion/facelib/matlab_cp2tform.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9077 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_face_fusion/image_face_fusion.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_face_fusion/network/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_face_fusion/network/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3045 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_face_fusion/network/aad_layer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8555 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_face_fusion/network/aei_flow_net.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9366 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_face_fusion/network/bfm.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12449 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_face_fusion/network/dense_motion.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20604 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_face_fusion/network/facerecon_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7433 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_face_fusion/network/model_irse.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7790 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_face_fusion/network/ops.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_human_parsing/
+-rw-r--r--   0 runner    (1001) docker     (122)      575 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_human_parsing/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_human_parsing/backbone/
+-rw-r--r--   0 runner    (1001) docker     (122)      525 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_human_parsing/backbone/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11736 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_human_parsing/backbone/deeplab_resnet.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_human_parsing/m2fp/
+-rw-r--r--   0 runner    (1001) docker     (122)      640 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_human_parsing/m2fp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8257 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_human_parsing/m2fp/m2fp_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8361 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_human_parsing/m2fp/m2fp_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15097 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_human_parsing/m2fp_net.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5634 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_human_parsing/parsing_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_inpainting/
+-rw-r--r--   0 runner    (1001) docker     (122)      475 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_inpainting/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2690 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_inpainting/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7605 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_inpainting/default.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1253 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_inpainting/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_inpainting/modules/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_inpainting/modules/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_inpainting/modules/ade20k/
+-rw-r--r--   0 runner    (1001) docker     (122)       81 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_inpainting/modules/ade20k/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12202 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_inpainting/modules/ade20k/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5348 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_inpainting/modules/ade20k/resnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7509 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_inpainting/modules/adversarial.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1564 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_inpainting/modules/feature_matching.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19052 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_inpainting/modules/ffc.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11842 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_inpainting/modules/inception.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1511 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_inpainting/modules/perceptual.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1965 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_inpainting/modules/pix2pixhd.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13350 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_inpainting/refinement.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)     1065 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/backbones/
+-rw-r--r--   0 runner    (1001) docker     (122)      664 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/backbones/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3826 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/backbones/resnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)    27174 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/backbones/swin_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9305 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/cascade_mask_rcnn_swin.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/datasets/
+-rw-r--r--   0 runner    (1001) docker     (122)      101 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/datasets/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3992 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/datasets/transforms.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/fastinst/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/fastinst/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14867 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/fastinst/fastinst_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6266 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/fastinst/fastinst_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8692 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/fastinst_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/maskdino/
+-rw-r--r--   0 runner    (1001) docker     (122)      600 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/maskdino/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10072 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/maskdino/dino_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15407 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/maskdino/maskdino_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18188 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/maskdino/maskdino_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7274 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/maskdino/ms_deform_attn.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2720 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/maskdino/position_encoding.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6712 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/maskdino/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1429 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/maskdino_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11454 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/maskdino_swin.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1549 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7886 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/postprocess_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_matching/
+-rw-r--r--   0 runner    (1001) docker     (122)      553 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_matching/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_matching/config/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_matching/config/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6991 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_matching/config/default.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_matching/loftr_quadtree/
+-rw-r--r--   0 runner    (1001) docker     (122)      171 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_matching/loftr_quadtree/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_matching/loftr_quadtree/backbone/
+-rw-r--r--   0 runner    (1001) docker     (122)      588 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_matching/loftr_quadtree/backbone/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7244 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_matching/loftr_quadtree/backbone/resnet_fpn.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3784 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_matching/loftr_quadtree/loftr.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/
+-rw-r--r--   0 runner    (1001) docker     (122)      239 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2968 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/fine_preprocess.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3012 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/linear_attention.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2994 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/quadtree_attention.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9677 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/transformer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_matching/loftr_quadtree/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_matching/loftr_quadtree/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10531 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_matching/loftr_quadtree/utils/coarse_matching.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3048 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_matching/loftr_quadtree/utils/fine_matching.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2132 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_matching/loftr_quadtree/utils/position_encoding.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2735 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_matching/quadtree_attention_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_matching/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_matching/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      344 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_matching/utils/misc.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_mvs_depth_estimation/
+-rw-r--r--   0 runner    (1001) docker     (122)      521 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_mvs_depth_estimation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8546 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_mvs_depth_estimation/cas_mvsnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6274 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_mvs_depth_estimation/casmvs_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18175 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_mvs_depth_estimation/colmap2mvsnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9861 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_mvs_depth_estimation/depth_filter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9404 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_mvs_depth_estimation/general_eval_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21735 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_mvs_depth_estimation/module.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3367 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_mvs_depth_estimation/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_paintbyexample/
+-rw-r--r--   0 runner    (1001) docker     (122)      507 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_paintbyexample/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1567 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_paintbyexample/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_panoptic_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)      513 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_panoptic_segmentation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1694 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_panoptic_segmentation/panseg_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_portrait_enhancement/
+-rw-r--r--   0 runner    (1001) docker     (122)      538 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_portrait_enhancement/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     8563 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_portrait_enhancement/align_faces.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_portrait_enhancement/eqface/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_portrait_enhancement/eqface/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     1836 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_portrait_enhancement/eqface/fqa.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4710 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_portrait_enhancement/eqface/model_resnet.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    22869 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_portrait_enhancement/gpen.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7114 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_portrait_enhancement/image_portrait_enhancement.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_portrait_enhancement/losses/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_portrait_enhancement/losses/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4336 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_portrait_enhancement/losses/helpers.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3286 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_portrait_enhancement/losses/losses.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3155 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_portrait_enhancement/losses/model_irse.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_portrait_enhancement/retinaface/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_portrait_enhancement/retinaface/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     7313 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_portrait_enhancement/retinaface/detection.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_portrait_enhancement/retinaface/models/
+-rwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_portrait_enhancement/retinaface/models/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     4845 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_portrait_enhancement/retinaface/models/net.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     4676 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_portrait_enhancement/retinaface/models/retinaface.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     4120 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_portrait_enhancement/retinaface/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_probing_model/
+-rw-r--r--   0 runner    (1001) docker     (122)      533 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_probing_model/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10076 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_probing_model/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3319 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_probing_model/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5206 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_probing_model/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_quality_assessment_degradation/
+-rw-r--r--   0 runner    (1001) docker     (122)      584 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_quality_assessment_degradation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5064 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_quality_assessment_degradation/degradation_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4654 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_quality_assessment_degradation/image_quality_assessment_degradation.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_quality_assessment_man/
+-rw-r--r--   0 runner    (1001) docker     (122)      544 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_quality_assessment_man/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2600 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_quality_assessment_man/image_quality_assessment_man.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5163 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_quality_assessment_man/maniqa.py
+-rw-r--r--   0 runner    (1001) docker     (122)    23230 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_quality_assessment_man/swin.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_quality_assessment_mos/
+-rw-r--r--   0 runner    (1001) docker     (122)      544 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_quality_assessment_mos/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_quality_assessment_mos/backbones/
+-rw-r--r--   0 runner    (1001) docker     (122)      272 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_quality_assessment_mos/backbones/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16222 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_quality_assessment_mos/backbones/resnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1060 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_quality_assessment_mos/censeo_ivqa_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_quality_assessment_mos/heads/
+-rw-r--r--   0 runner    (1001) docker     (122)       36 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_quality_assessment_mos/heads/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      529 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_quality_assessment_mos/heads/simple_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2651 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_quality_assessment_mos/image_quality_assessment_mos.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_reid_person/
+-rw-r--r--   0 runner    (1001) docker     (122)      467 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_reid_person/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5444 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_reid_person/pass_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14108 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_reid_person/transreid_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_restoration/
+-rw-r--r--   0 runner    (1001) docker     (122)      527 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_restoration/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_restoration/demoire_models/
+-rw-r--r--   0 runner    (1001) docker     (122)       69 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_restoration/demoire_models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10243 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_restoration/demoire_models/nets.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2640 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_restoration/image_restoration_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)      712 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7896 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/data_util.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4661 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/feature_extractors.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5155 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/pixel_classifier.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2273 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3208 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/ddpm_segmentation_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/pan_merge/
+-rw-r--r--   0 runner    (1001) docker     (122)      111 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/pan_merge/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1516 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/pan_merge/base_panoptic_fusion_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2122 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/pan_merge/maskformer_semantic_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2566 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/semantic_seg_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/
+-rw-r--r--   0 runner    (1001) docker     (122)      303 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/
+-rw-r--r--   0 runner    (1001) docker     (122)      290 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/
+-rw-r--r--   0 runner    (1001) docker     (122)      249 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17484 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/adapter_modules.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/base/
+-rw-r--r--   0 runner    (1001) docker     (122)      196 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/base/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18241 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/base/beit.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6379 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/beit_adapter.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/
+-rw-r--r--   0 runner    (1001) docker     (122)      251 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10785 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/base_decode_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)    26514 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/mask2former_head_from_mmseg.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/
+-rw-r--r--   0 runner    (1001) docker     (122)      253 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12370 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/base_segmentor.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11715 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/encoder_decoder_mask2former.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)      368 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      398 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/utils/builder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1938 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/utils/data_process_func.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1788 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/utils/seg_func.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_skychange/
+-rw-r--r--   0 runner    (1001) docker     (122)      650 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_skychange/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9312 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_skychange/preprocessor.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_skychange/ptsemseg/
+-rw-r--r--   0 runner    (1001) docker     (122)     3740 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_skychange/ptsemseg/BlockModules.py
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_skychange/ptsemseg/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21275 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_skychange/ptsemseg/hrnet_backnone.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18023 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_skychange/ptsemseg/hrnet_super_and_ocr.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8159 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_skychange/ptsemseg/unet.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11210 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_skychange/skychange.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7874 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_skychange/skychange_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_generation/
+-rw-r--r--   0 runner    (1001) docker     (122)      120 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_generation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_generation/data/
+-rw-r--r--   0 runner    (1001) docker     (122)      561 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_generation/data/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3309 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_generation/data/transforms.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10476 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_generation/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_generation/models/
+-rw-r--r--   0 runner    (1001) docker     (122)      603 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_generation/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12670 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_generation/models/autoencoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12594 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_generation/models/clip.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_generation/ops/
+-rw-r--r--   0 runner    (1001) docker     (122)      561 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_generation/ops/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24469 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_generation/ops/diffusion.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1320 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_generation/ops/losses.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_translation/
+-rw-r--r--   0 runner    (1001) docker     (122)      536 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_translation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_translation/data/
+-rw-r--r--   0 runner    (1001) docker     (122)      127 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_translation/data/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3309 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_translation/data/transforms.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10527 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_translation/model_translation.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_translation/models/
+-rw-r--r--   0 runner    (1001) docker     (122)      161 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_translation/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12670 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_translation/models/autoencoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12594 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_translation/models/clip.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_translation/ops/
+-rw-r--r--   0 runner    (1001) docker     (122)      384 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_translation/ops/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21828 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_translation/ops/apps.py
+-rw-r--r--   0 runner    (1001) docker     (122)    36549 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_translation/ops/degradation.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24615 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_translation/ops/diffusion.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1320 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_translation/ops/losses.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4389 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_translation/ops/metrics.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6736 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_translation/ops/random_color.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2745 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_translation/ops/random_mask.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3777 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_translation/ops/svd.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6728 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_translation/ops/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_try_on/
+-rw-r--r--   0 runner    (1001) docker     (122)      518 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_try_on/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15834 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_try_on/generator.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16386 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_try_on/landmark.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9109 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_try_on/try_on_infer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    46223 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/image_try_on/warping.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/indoor_layout_estimation/
+-rw-r--r--   0 runner    (1001) docker     (122)      486 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/indoor_layout_estimation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/indoor_layout_estimation/networks/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/indoor_layout_estimation/networks/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/indoor_layout_estimation/networks/backbone/
+-rw-r--r--   0 runner    (1001) docker     (122)      136 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/indoor_layout_estimation/networks/backbone/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4099 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/indoor_layout_estimation/networks/backbone/resnet_DA.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6931 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/indoor_layout_estimation/networks/backbone/vit_horizon_pry_image.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/indoor_layout_estimation/networks/misc/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/indoor_layout_estimation/networks/misc/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2510 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/indoor_layout_estimation/networks/misc/fourier.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3251 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/indoor_layout_estimation/networks/misc/panostretch.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14887 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/indoor_layout_estimation/networks/misc/post_proc.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/indoor_layout_estimation/networks/modality/
+-rw-r--r--   0 runner    (1001) docker     (122)       86 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/indoor_layout_estimation/networks/modality/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5518 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/indoor_layout_estimation/networks/modality/layout.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3406 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/indoor_layout_estimation/networks/panovit.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4808 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/indoor_layout_estimation/networks/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1834 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/indoor_layout_estimation/panovit.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/language_guided_video_summarization/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      542 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/language_guided_video_summarization/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     6441 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/language_guided_video_summarization/summarizer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/language_guided_video_summarization/transformer/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      508 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/language_guided_video_summarization/transformer/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     1900 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/language_guided_video_summarization/transformer/layers.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     7275 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/language_guided_video_summarization/transformer/models.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)      826 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/language_guided_video_summarization/transformer/modules.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     2690 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/language_guided_video_summarization/transformer/sub_layers.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/motion_generation/
+-rw-r--r--   0 runner    (1001) docker     (122)      639 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/motion_generation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2151 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/motion_generation/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/motion_generation/modules/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/motion_generation/modules/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1209 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/motion_generation/modules/cfg_sampler.py
+-rw-r--r--   0 runner    (1001) docker     (122)    26783 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/motion_generation/modules/gaussian_diffusion.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13594 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/motion_generation/modules/mdm.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5412 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/motion_generation/modules/respace.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3914 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/motion_generation/modules/rotation2xyz.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3189 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/motion_generation/modules/smpl.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/movie_scene_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)      615 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/movie_scene_segmentation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1567 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/movie_scene_segmentation/get_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8465 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/movie_scene_segmentation/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/movie_scene_segmentation/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)      181 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/movie_scene_segmentation/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      798 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/movie_scene_segmentation/utils/head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4291 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/movie_scene_segmentation/utils/save_op.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10325 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/movie_scene_segmentation/utils/shot_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5057 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/movie_scene_segmentation/utils/trn.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_4k/
+-rw-r--r--   0 runner    (1001) docker     (122)      598 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_4k/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_4k/dataloader/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_4k/dataloader/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     2934 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_4k/dataloader/load_blender.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     4301 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_4k/dataloader/load_data.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    17159 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_4k/dataloader/load_llff.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     2498 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_4k/dataloader/load_tankstemple.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20567 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_4k/dataloader/read_write_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7297 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_4k/nerf_preprocess.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    10526 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_4k/nerf_recon_4k.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_4k/network/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_4k/network/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    77434 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_4k/network/dvgo.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5720 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_4k/network/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_acc/
+-rw-r--r--   0 runner    (1001) docker     (122)      602 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_acc/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_acc/dataloader/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_acc/dataloader/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18782 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_acc/dataloader/nerf_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20567 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_acc/dataloader/read_write_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7297 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_acc/nerf_preprocess.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7876 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_acc/nerf_recon_acc.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_acc/network/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_acc/network/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13317 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_acc/network/nerf.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1509 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_acc/network/segmenter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5720 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_acc/network/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_vq_compression/
+-rw-r--r--   0 runner    (1001) docker     (122)      662 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_vq_compression/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_vq_compression/dataloader/
+-rw-r--r--   0 runner    (1001) docker     (122)      315 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_vq_compression/dataloader/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6055 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_vq_compression/dataloader/blender.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10222 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_vq_compression/dataloader/llff.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7105 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_vq_compression/dataloader/nsvf.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10361 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_vq_compression/dataloader/ray_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9228 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_vq_compression/dataloader/tankstemple.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3890 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_vq_compression/nerf_recon_vq_compression.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_vq_compression/network/
+-rw-r--r--   0 runner    (1001) docker     (122)       61 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_vq_compression/network/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    23489 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_vq_compression/network/tensoRF.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11430 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_vq_compression/network/tensoRF_VQ.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19228 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_vq_compression/network/tensorBase.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16555 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_vq_compression/network/weighted_vq.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7142 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_vq_compression/renderer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7854 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_vq_compression/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection/
+-rw-r--r--   0 runner    (1001) docker     (122)      606 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3432 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection/mmdet_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection/mmdet_ms/
+-rw-r--r--   0 runner    (1001) docker     (122)      321 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection/mmdet_ms/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection/mmdet_ms/backbones/
+-rw-r--r--   0 runner    (1001) docker     (122)      211 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection/mmdet_ms/backbones/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21306 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection/mmdet_ms/backbones/vit.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection/mmdet_ms/dense_heads/
+-rw-r--r--   0 runner    (1001) docker     (122)      278 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection/mmdet_ms/dense_heads/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2220 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection/mmdet_ms/dense_heads/anchor_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11529 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection/mmdet_ms/dense_heads/rpn_head.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection/mmdet_ms/necks/
+-rw-r--r--   0 runner    (1001) docker     (122)      213 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection/mmdet_ms/necks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9021 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection/mmdet_ms/necks/fpn.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/
+-rw-r--r--   0 runner    (1001) docker     (122)      426 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/bbox_heads/
+-rw-r--r--   0 runner    (1001) docker     (122)      375 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/bbox_heads/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8542 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/bbox_heads/convfc_bbox_head.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/mask_heads/
+-rw-r--r--   0 runner    (1001) docker     (122)      239 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/mask_heads/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17739 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/mask_heads/fcn_mask_head.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection/mmdet_ms/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)      306 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection/mmdet_ms/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21926 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection/mmdet_ms/utils/checkpoint.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1204 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection/mmdet_ms/utils/convModule_norm.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/
+-rw-r--r--   0 runner    (1001) docker     (122)      467 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/
+-rw-r--r--   0 runner    (1001) docker     (122)       86 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2657 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/depe_detect.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/
+-rw-r--r--   0 runner    (1001) docker     (122)      605 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/assigners/
+-rw-r--r--   0 runner    (1001) docker     (122)      299 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/assigners/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6670 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/assigners/hungarian_assigner_3d.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/coders/
+-rw-r--r--   0 runner    (1001) docker     (122)      282 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/coders/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4476 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/coders/nms_free_coder.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/match_costs/
+-rw-r--r--   0 runner    (1001) docker     (122)      276 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/match_costs/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1064 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/match_costs/match_cost.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2041 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/util.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/
+-rw-r--r--   0 runner    (1001) docker     (122)      294 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3095 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/nuscenes_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/
+-rw-r--r--   0 runner    (1001) docker     (122)      522 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7936 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/loading.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8689 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/transform_3d.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/backbones/
+-rw-r--r--   0 runner    (1001) docker     (122)      255 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/backbones/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12874 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/backbones/vovnet.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/
+-rw-r--r--   0 runner    (1001) docker     (122)      282 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7442 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/depth_net.py
+-rw-r--r--   0 runner    (1001) docker     (122)    59197 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/petrv2_dednhead.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/detectors/
+-rw-r--r--   0 runner    (1001) docker     (122)      255 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/detectors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9533 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/detectors/petr3d.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/necks/
+-rw-r--r--   0 runner    (1001) docker     (122)      256 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/necks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9032 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/necks/cp_fpn.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)      562 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18126 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/petr_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5175 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/positional_encoding.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11047 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/result_vis.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/ocr_detection/
+-rw-r--r--   0 runner    (1001) docker     (122)      473 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/ocr_detection/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2969 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/ocr_detection/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/ocr_detection/modules/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/ocr_detection/modules/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    26142 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/ocr_detection/modules/dbnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)    28715 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/ocr_detection/modules/layers.py
+-rw-r--r--   0 runner    (1001) docker     (122)    31084 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/ocr_detection/modules/mix_ops.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5974 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/ocr_detection/modules/proxyless.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8217 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/ocr_detection/modules/seg_detector_loss.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2647 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/ocr_detection/preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7972 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/ocr_detection/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/ocr_recognition/
+-rw-r--r--   0 runner    (1001) docker     (122)      477 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/ocr_recognition/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6378 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/ocr_recognition/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/ocr_recognition/modules/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/ocr_recognition/modules/CRNN/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/ocr_recognition/modules/CRNN/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3724 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/ocr_recognition/modules/CRNN/main_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/ocr_recognition/modules/ConvNextViT/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/ocr_recognition/modules/ConvNextViT/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6209 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/ocr_recognition/modules/ConvNextViT/convnext.py
+-rw-r--r--   0 runner    (1001) docker     (122)      753 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/ocr_recognition/modules/ConvNextViT/main_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11644 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/ocr_recognition/modules/ConvNextViT/timm_tinyc.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1955 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/ocr_recognition/modules/ConvNextViT/vitstr.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/ocr_recognition/modules/LightweightEdge/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/ocr_recognition/modules/LightweightEdge/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1458 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/ocr_recognition/modules/LightweightEdge/main_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/ocr_recognition/modules/LightweightEdge/nas_block/
+-rw-r--r--   0 runner    (1001) docker     (122)       43 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/ocr_recognition/modules/LightweightEdge/nas_block/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    25097 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/ocr_recognition/modules/LightweightEdge/nas_block/layers.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10662 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/ocr_recognition/modules/LightweightEdge/nas_block/mix_ops.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5031 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/ocr_recognition/modules/LightweightEdge/nas_block/proxyless.py
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/ocr_recognition/modules/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3710 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/ocr_recognition/preprocessor.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/open_vocabulary_detection_vild/
+-rw-r--r--   0 runner    (1001) docker     (122)      501 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/open_vocabulary_detection_vild/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14225 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/open_vocabulary_detection_vild/vild.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/panorama_depth_estimation/
+-rw-r--r--   0 runner    (1001) docker     (122)      511 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/panorama_depth_estimation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/panorama_depth_estimation/networks/
+-rw-r--r--   0 runner    (1001) docker     (122)      102 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/panorama_depth_estimation/networks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5077 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/panorama_depth_estimation/networks/equi.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7500 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/panorama_depth_estimation/networks/layers.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7839 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/panorama_depth_estimation/networks/mobilenet.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14421 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/panorama_depth_estimation/networks/resnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9175 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/panorama_depth_estimation/networks/unifuse.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3962 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/panorama_depth_estimation/networks/util.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3307 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/panorama_depth_estimation/unifuse_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/pedestrian_attribute_recognition/
+-rw-r--r--   0 runner    (1001) docker     (122)      488 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/pedestrian_attribute_recognition/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3468 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/pedestrian_attribute_recognition/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/pointcloud_sceneflow_estimation/
+-rw-r--r--   0 runner    (1001) docker     (122)      495 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/pointcloud_sceneflow_estimation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16147 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/pointcloud_sceneflow_estimation/common.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11898 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/pointcloud_sceneflow_estimation/pointnet2_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1797 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/pointcloud_sceneflow_estimation/rcp_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18618 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/pointcloud_sceneflow_estimation/sf_rcp.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/product_retrieval_embedding/
+-rw-r--r--   0 runner    (1001) docker     (122)      548 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/product_retrieval_embedding/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20974 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/product_retrieval_embedding/item_detection.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4509 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/product_retrieval_embedding/item_embedding.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4340 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/product_retrieval_embedding/item_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/product_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)      528 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/product_segmentation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7978 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/product_segmentation/net.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2194 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/product_segmentation/seg_infer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/referring_video_object_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)      514 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/referring_video_object_segmentation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5832 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/referring_video_object_segmentation/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/referring_video_object_segmentation/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)      318 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/referring_video_object_segmentation/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8504 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/referring_video_object_segmentation/utils/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9206 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/referring_video_object_segmentation/utils/criterion.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7299 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/referring_video_object_segmentation/utils/matcher.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7935 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/referring_video_object_segmentation/utils/misc.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6242 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/referring_video_object_segmentation/utils/mttr.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16610 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/referring_video_object_segmentation/utils/multimodal_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2091 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/referring_video_object_segmentation/utils/position_encoding_2d.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5629 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/referring_video_object_segmentation/utils/postprocessing.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5180 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/referring_video_object_segmentation/utils/segmentation.py
+-rw-r--r--   0 runner    (1001) docker     (122)    27941 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/referring_video_object_segmentation/utils/swin_transformer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/robust_image_classification/
+-rw-r--r--   0 runner    (1001) docker     (122)      501 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/robust_image_classification/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2902 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/robust_image_classification/easyrobust_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/s2net_panorama_depth_estimation/
+-rw-r--r--   0 runner    (1001) docker     (122)      507 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/s2net_panorama_depth_estimation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/s2net_panorama_depth_estimation/networks/
+-rw-r--r--   0 runner    (1001) docker     (122)      219 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/s2net_panorama_depth_estimation/networks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5374 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/s2net_panorama_depth_estimation/networks/config.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15565 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/s2net_panorama_depth_estimation/networks/decoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5737 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/s2net_panorama_depth_estimation/networks/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15735 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/s2net_panorama_depth_estimation/networks/resnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)    26071 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/s2net_panorama_depth_estimation/networks/swin_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17279 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/s2net_panorama_depth_estimation/networks/util_helper.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3582 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/s2net_panorama_depth_estimation/s2net_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/salient_detection/
+-rw-r--r--   0 runner    (1001) docker     (122)      497 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/salient_detection/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/salient_detection/models/
+-rw-r--r--   0 runner    (1001) docker     (122)      213 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/salient_detection/models/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/salient_detection/models/backbone/
+-rw-r--r--   0 runner    (1001) docker     (122)     6558 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/salient_detection/models/backbone/Res2Net_v1b.py
+-rw-r--r--   0 runner    (1001) docker     (122)      256 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/salient_detection/models/backbone/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6525 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/salient_detection/models/modules.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3047 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/salient_detection/models/senet.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12007 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/salient_detection/models/u2net.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3524 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/salient_detection/models/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2704 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/salient_detection/salient_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/shop_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)      464 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/shop_segmentation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2203 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/shop_segmentation/common.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4385 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/shop_segmentation/head_fpn.py
+-rw-r--r--   0 runner    (1001) docker     (122)    32989 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/shop_segmentation/models.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9319 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/shop_segmentation/neck_fpn.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5632 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/shop_segmentation/shop_seg_base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4095 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/shop_segmentation/shop_seg_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6699 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/shop_segmentation/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/skin_retouching/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/skin_retouching/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/skin_retouching/detection_model/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/skin_retouching/detection_model/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1780 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/skin_retouching/detection_model/detection_module.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2532 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/skin_retouching/detection_model/detection_unet_in.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/skin_retouching/inpainting_model/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/skin_retouching/inpainting_model/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5759 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/skin_retouching/inpainting_model/gconv.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3233 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/skin_retouching/inpainting_model/inpainting_unet.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/skin_retouching/retinaface/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/skin_retouching/retinaface/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10898 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/skin_retouching/retinaface/box_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3911 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/skin_retouching/retinaface/net.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4799 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/skin_retouching/retinaface/network.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5003 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/skin_retouching/retinaface/predict_single.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1035 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/skin_retouching/retinaface/prior_box.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2110 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/skin_retouching/retinaface/utils.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     3890 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/skin_retouching/unet_deploy.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9917 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/skin_retouching/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1169 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/skin_retouching/weights_init.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/stream_yolo/
+-rw-r--r--   0 runner    (1001) docker     (122)      526 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/stream_yolo/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/stream_yolo/data/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/stream_yolo/data/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2094 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/stream_yolo/data/data_augment.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/stream_yolo/exp/
+-rw-r--r--   0 runner    (1001) docker     (122)      165 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/stream_yolo/exp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      274 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/stream_yolo/exp/base_exp.py
+-rw-r--r--   0 runner    (1001) docker     (122)      392 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/stream_yolo/exp/build.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/stream_yolo/exp/default/
+-rw-r--r--   0 runner    (1001) docker     (122)      137 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/stream_yolo/exp/default/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1209 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/stream_yolo/exp/default/streamyolo.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1845 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/stream_yolo/exp/yolox_base.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/stream_yolo/models/
+-rw-r--r--   0 runner    (1001) docker     (122)      238 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/stream_yolo/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6274 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/stream_yolo/models/darknet.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10911 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/stream_yolo/models/dfp_pafpn.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6156 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/stream_yolo/models/network_blocks.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1314 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/stream_yolo/models/streamyolo.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5669 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/stream_yolo/models/tal_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4352 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/stream_yolo/realtime_video_detector.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/stream_yolo/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)      270 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/stream_yolo/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3681 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/stream_yolo/utils/boxes.py
+-rw-r--r--   0 runner    (1001) docker     (122)      209 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/stream_yolo/utils/format.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/super_resolution/
+-rw-r--r--   0 runner    (1001) docker     (122)      555 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/super_resolution/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7652 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/super_resolution/arch_util.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10508 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/super_resolution/ecb.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3525 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/super_resolution/ecbsr_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4945 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/super_resolution/rrdbnet_arch.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/table_recognition/
+-rw-r--r--   0 runner    (1001) docker     (122)      462 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/table_recognition/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16198 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/table_recognition/lineless_table_process.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3360 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/table_recognition/model_lore.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/table_recognition/modules/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/table_recognition/modules/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12614 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/table_recognition/modules/lore_detector.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15090 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/table_recognition/modules/lore_processor.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/text_driven_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)       96 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/text_driven_segmentation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5488 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/text_driven_segmentation/clip.py
+-rw-r--r--   0 runner    (1001) docker     (122)      777 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/text_driven_segmentation/lseg_base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7587 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/text_driven_segmentation/lseg_blocks.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4143 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/text_driven_segmentation/lseg_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6145 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/text_driven_segmentation/lseg_net.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18951 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/text_driven_segmentation/lseg_vit.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16418 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/text_driven_segmentation/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5165 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/text_driven_segmentation/simple_tokenizer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/text_to_360panorama_image/
+-rw-r--r--   0 runner    (1001) docker     (122)      680 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/text_to_360panorama_image/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    39007 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/text_to_360panorama_image/pipeline_base.py
+-rw-r--r--   0 runner    (1001) docker     (122)    56816 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/text_to_360panorama_image/pipeline_sr.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_classfication/
+-rw-r--r--   0 runner    (1001) docker     (122)      594 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_classfication/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    43681 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_classfication/basic_blocks.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2174 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_classfication/global_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2902 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_classfication/master_net.py
+-rw-r--r--   0 runner    (1001) docker     (122)      766 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_classfication/model_zoo.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2892 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_classfication/plain_net_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7405 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_classfication/super_blocks.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14983 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_classfication/super_res_idwexkx.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8709 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_classfication/super_res_k1kxk1.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6923 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_classfication/super_res_kxkx.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/
+-rw-r--r--   0 runner    (1001) docker     (122)      699 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/apis/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/apis/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2067 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/apis/detector_evaluater.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4000 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/apis/detector_inference.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/augmentations/
+-rw-r--r--   0 runner    (1001) docker     (122)       49 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/augmentations/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/
+-rw-r--r--   0 runner    (1001) docker     (122)       49 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3460 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/box_level_augs.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8123 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/color_augs.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2345 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/gaussian_maps.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5911 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/geometric_augs.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2842 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/augmentations/scale_aware_aug.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/
+-rw-r--r--   0 runner    (1001) docker     (122)      148 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/
+-rw-r--r--   0 runner    (1001) docker     (122)      621 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3701 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/darknet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9356 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/tinynas_csp.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7465 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/tinynas_res.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/core/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/core/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14261 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/core/base_ops.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10559 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/core/neck_ops.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13101 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/core/ops.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    18626 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/core/ota_assigner.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7526 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/core/repvgg_block.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2572 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/core/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)      561 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/core/weight_init.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/heads/
+-rw-r--r--   0 runner    (1001) docker     (122)      409 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/heads/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12602 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/heads/gfocal_v2_tiny.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19162 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/heads/zero_head.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/losses/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/losses/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5588 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/losses/distill_loss.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12284 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/losses/gfocal_loss.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/necks/
+-rw-r--r--   0 runner    (1001) docker     (122)      421 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/necks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7347 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/necks/giraffe_config.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24842 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/necks/giraffe_fpn.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3499 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/necks/giraffe_fpn_btn.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/detectors/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/detectors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2809 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/detectors/detector.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/structures/
+-rw-r--r--   0 runner    (1001) docker     (122)       49 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/structures/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9197 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/structures/bounding_box.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2627 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/structures/boxlist_ops.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2774 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/structures/image_list.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)      189 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11594 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/utils/boxes.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5838 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/utils/model_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1164 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/utils/scheduler.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6400 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/detector.py
+-rw-r--r--   0 runner    (1001) docker     (122)      627 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/tinynas_damoyolo.py
+-rw-r--r--   0 runner    (1001) docker     (122)      643 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/tinynas_detector.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1026 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_deinterlace/
+-rw-r--r--   0 runner    (1001) docker     (122)     2896 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_deinterlace/UNet_for_video_deinterlace.py
+-rw-r--r--   0 runner    (1001) docker     (122)      494 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_deinterlace/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      773 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_deinterlace/deinterlace_arch.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_deinterlace/models/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_deinterlace/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3159 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_deinterlace/models/archs.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1441 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_deinterlace/models/deep_fourier_upsampling.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2972 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_deinterlace/models/enh.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3572 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_deinterlace/models/fre.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3716 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_deinterlace/models/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/
+-rw-r--r--   0 runner    (1001) docker     (122)      483 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/configs/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/configs/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12753 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/configs/default_config.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6731 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/dro_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/geometry/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/geometry/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5494 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/geometry/camera.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2285 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/geometry/camera_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3737 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/geometry/pose.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3685 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/geometry/pose_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/models/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5277 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/models/model_checkpoint.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2356 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/models/model_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10098 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/models/model_wrapper.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7204 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/models/sfm_model_mf.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4387 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/models/sup_model_mf.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/networks/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/networks/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/networks/depth_pose/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/networks/depth_pose/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10532 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/networks/depth_pose/depth_pose_net.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/networks/layers/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/networks/layers/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8902 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/depth_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1607 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/layers.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1915 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/pose_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4110 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/resnet_encoder.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/networks/optim/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/networks/optim/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15781 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/networks/optim/extractor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8039 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/networks/optim/update.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6815 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/utils/augmentations.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10248 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/utils/config.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15068 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/utils/depth.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1153 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/utils/horovod.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10965 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/utils/image.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9407 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/utils/image_gt.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6499 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/utils/load.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2726 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/utils/misc.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1348 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/utils/types.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_frame_interpolation/
+-rw-r--r--   0 runner    (1001) docker     (122)     1894 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_frame_interpolation/VFINet_arch.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3334 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_frame_interpolation/VFINet_for_video_frame_interpolation.py
+-rw-r--r--   0 runner    (1001) docker     (122)      458 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_frame_interpolation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_frame_interpolation/flow_model/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_frame_interpolation/flow_model/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3212 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_frame_interpolation/flow_model/corr.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9228 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_frame_interpolation/flow_model/extractor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5346 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_frame_interpolation/flow_model/raft.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5540 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_frame_interpolation/flow_model/update.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_frame_interpolation/interp_model/
+-rw-r--r--   0 runner    (1001) docker     (122)    13745 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_frame_interpolation/interp_model/IFNet_swin.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4049 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_frame_interpolation/interp_model/UNet.py
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_frame_interpolation/interp_model/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3823 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_frame_interpolation/interp_model/flow_reversal.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16050 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_frame_interpolation/interp_model/refinenet_arch.py
+-rw-r--r--   0 runner    (1001) docker     (122)    36415 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_frame_interpolation/interp_model/transformer_layers.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_frame_interpolation/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_frame_interpolation/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3314 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_frame_interpolation/utils/scene_change_detection.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2907 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_frame_interpolation/utils/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_human_matting/
+-rw-r--r--   0 runner    (1001) docker     (122)      558 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_human_matting/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1324 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_human_matting/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_human_matting/models/
+-rw-r--r--   0 runner    (1001) docker     (122)       36 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_human_matting/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10465 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_human_matting/models/decoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2662 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_human_matting/models/deep_guided_filter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5506 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_human_matting/models/effv2.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2904 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_human_matting/models/lraspp.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2234 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_human_matting/models/matting.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_inpainting/
+-rw-r--r--   0 runner    (1001) docker     (122)      524 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_inpainting/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11056 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_inpainting/inpainting.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13727 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_inpainting/inpainting_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_instance_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)      582 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_instance_segmentation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_instance_segmentation/head/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_instance_segmentation/head/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18236 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_instance_segmentation/head/kernel_frame_iter_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21551 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_instance_segmentation/head/kernel_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16955 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_instance_segmentation/head/kernel_iter_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20983 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_instance_segmentation/head/kernel_update_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4136 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_instance_segmentation/head/kernel_updator.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_instance_segmentation/neck/
+-rw-r--r--   0 runner    (1001) docker     (122)      525 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_instance_segmentation/neck/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11733 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_instance_segmentation/neck/msdeformattn_decoder.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_instance_segmentation/track/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_instance_segmentation/track/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    26843 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_instance_segmentation/track/kernel_update_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10771 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_instance_segmentation/track/mask_hungarian_assigner.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4163 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_instance_segmentation/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18059 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_instance_segmentation/video_knet.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_multi_object_tracking/
+-rw-r--r--   0 runner    (1001) docker     (122)      541 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_multi_object_tracking/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_multi_object_tracking/models/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_multi_object_tracking/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2970 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_multi_object_tracking/models/common.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2420 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_multi_object_tracking/models/decode.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1890 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_multi_object_tracking/models/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4921 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_multi_object_tracking/models/yolo.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_multi_object_tracking/tracker/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_multi_object_tracking/tracker/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1084 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_multi_object_tracking/tracker/basetrack.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3117 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_multi_object_tracking/tracker/matching.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14995 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_multi_object_tracking/tracker/multitracker.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_multi_object_tracking/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_multi_object_tracking/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2230 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_multi_object_tracking/utils/image.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9570 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_multi_object_tracking/utils/kalman_filter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7296 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_multi_object_tracking/utils/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2888 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_multi_object_tracking/utils/visualization.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_object_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)      497 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_object_segmentation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      818 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_object_segmentation/aggregate.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3582 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_object_segmentation/cbam.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1979 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_object_segmentation/eval_network.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3980 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_object_segmentation/inference_core.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8255 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_object_segmentation/inference_memory_bank.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7037 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_object_segmentation/mod_resnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)      974 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_object_segmentation/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15309 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_object_segmentation/modules.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5593 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_object_segmentation/network.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_panoptic_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)      477 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_panoptic_segmentation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_panoptic_segmentation/backbone/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_panoptic_segmentation/backbone/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10040 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_panoptic_segmentation/backbone/swin_checkpoint.py
+-rw-r--r--   0 runner    (1001) docker     (122)    26628 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_panoptic_segmentation/backbone/swin_transformer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_panoptic_segmentation/head/
+-rw-r--r--   0 runner    (1001) docker     (122)      515 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_panoptic_segmentation/head/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8349 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_panoptic_segmentation/head/kernel_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20677 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_panoptic_segmentation/head/kernel_iter_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)    28653 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_panoptic_segmentation/head/kernel_update_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4081 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_panoptic_segmentation/head/kernel_updator.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6696 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_panoptic_segmentation/head/mask.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8627 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_panoptic_segmentation/head/semantic_fpn_wrapper.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5885 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_panoptic_segmentation/head/track_heads.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_panoptic_segmentation/neck/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_panoptic_segmentation/neck/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6255 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_panoptic_segmentation/neck/fpn.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_panoptic_segmentation/track/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_panoptic_segmentation/track/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8508 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_panoptic_segmentation/track/quasi_dense_embed_tracker.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17155 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_panoptic_segmentation/video_k_net.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5440 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_panoptic_segmentation/visualizer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/config/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/config/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1024 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/config/ostrack.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/models/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/models/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/models/layers/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/models/layers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1641 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/models/layers/attn.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5004 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/models/layers/attn_blocks.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4805 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/models/layers/head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1234 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/models/layers/patch_embed.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/models/ostrack/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/models/ostrack/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3380 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/models/ostrack/base_backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3393 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/models/ostrack/ostrack.py
+-rw-r--r--   0 runner    (1001) docker     (122)      653 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/models/ostrack/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12278 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/models/ostrack/vit_ce.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/models/procontext/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/models/procontext/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3497 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/models/procontext/procontext.py
+-rw-r--r--   0 runner    (1001) docker     (122)      943 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/models/procontext/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4473 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/models/procontext/vit_ce.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/tracker/
+-rw-r--r--   0 runner    (1001) docker     (122)      114 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/tracker/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5507 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/tracker/ostrack.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7123 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/tracker/procontext.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8488 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/utils/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/DUT/
+-rw-r--r--   0 runner    (1001) docker     (122)    14702 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/DUT/DUT_raft.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4715 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/DUT/MotionPro.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/DUT/RAFT/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/DUT/RAFT/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3290 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/DUT/RAFT/corr.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9214 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/DUT/RAFT/extractor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5275 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/DUT/RAFT/raft.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5526 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/DUT/RAFT/update.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3922 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/DUT/Smoother.py
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/DUT/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1447 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/DUT/config.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7292 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/DUT/rf_det_module.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7981 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/DUT/rf_det_so.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3452 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/DUTRAFTStabilizer.py
+-rw-r--r--   0 runner    (1001) docker     (122)      492 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)     4684 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/utils/IterativeSmooth.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14212 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/utils/MedianFilter.py
+-rw-r--r--   0 runner    (1001) docker     (122)    22544 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/utils/ProjectionUtils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2893 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/utils/RAFTUtils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2817 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/utils/WarpUtils.py
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14474 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/utils/image_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4977 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/utils/math_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_streaming_perception/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_streaming_perception/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_streaming_perception/longshortnet/
+-rw-r--r--   0 runner    (1001) docker     (122)      487 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_streaming_perception/longshortnet/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_streaming_perception/longshortnet/exp/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_streaming_perception/longshortnet/exp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2488 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_streaming_perception/longshortnet/exp/longshortnet_base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7097 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_streaming_perception/longshortnet/longshortnet.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_streaming_perception/longshortnet/models/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_streaming_perception/longshortnet/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5774 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_streaming_perception/longshortnet/models/dfp_pafpn_long.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4848 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_streaming_perception/longshortnet/models/dfp_pafpn_short.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9990 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_streaming_perception/longshortnet/models/longshort.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3905 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_streaming_perception/longshortnet/models/longshort_backbone_neck.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_summarization/
+-rw-r--r--   0 runner    (1001) docker     (122)      536 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_summarization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4959 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_summarization/base_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_summarization/kts/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_summarization/kts/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1221 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_summarization/kts/cpd_auto.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3238 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_summarization/kts/cpd_nonlin.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13098 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_summarization/pgl_sum.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8812 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_summarization/summarizer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_super_resolution/
+-rw-r--r--   0 runner    (1001) docker     (122)      689 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_super_resolution/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14118 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_super_resolution/basicvsr_net.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4727 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_super_resolution/common.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4720 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_super_resolution/msrresnet_lite_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3636 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_super_resolution/real_basicvsr_for_video_super_resolution.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3637 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/video_super_resolution/real_basicvsr_net.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/vidt/
+-rw-r--r--   0 runner    (1001) docker     (122)      464 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/vidt/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    40074 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/vidt/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)    25588 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/vidt/deformable_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7436 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/vidt/fpn_fusion.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16697 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/vidt/head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3651 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/vidt/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/virual_tryon/
+-rw-r--r--   0 runner    (1001) docker     (122)      464 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/virual_tryon/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17690 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/virual_tryon/sdafnet.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/vision_efficient_tuning/
+-rw-r--r--   0 runner    (1001) docker     (122)      502 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/vision_efficient_tuning/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16152 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/vision_efficient_tuning/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)      797 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/vision_efficient_tuning/head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1786 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/vision_efficient_tuning/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9340 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/vision_efficient_tuning/petl.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5021 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/vision_efficient_tuning/timm_helpers.py
+-rw-r--r--   0 runner    (1001) docker     (122)    28715 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/vision_efficient_tuning/timm_vision_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5040 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/vision_efficient_tuning/timm_weight_init.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5614 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/vision_efficient_tuning/vision_efficient_tuning.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/vision_middleware/
+-rw-r--r--   0 runner    (1001) docker     (122)      492 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/vision_middleware/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5749 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/vision_middleware/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)    28089 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/vision_middleware/head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6494 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/vision_middleware/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6192 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/vision_middleware/vim.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/cv/vop_retrieval/
+-rw-r--r--   0 runner    (1001) docker     (122)      919 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/vop_retrieval/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12238 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/vop_retrieval/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5188 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/vop_retrieval/basic_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13520 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/vop_retrieval/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5299 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/vop_retrieval/model_se.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5177 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/cv/vop_retrieval/tokenization_clip.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/
+-rw-r--r--   0 runner    (1001) docker     (122)     2182 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/clip/
+-rw-r--r--   0 runner    (1001) docker     (122)       97 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/clip/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14496 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/clip/bert_tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3898 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/clip/configuration_bert.py
+-rw-r--r--   0 runner    (1001) docker     (122)    22351 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/clip/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20998 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/clip/modeling_bert.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/clip_interrogator/
+-rw-r--r--   0 runner    (1001) docker     (122)       37 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/clip_interrogator/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24293 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/clip_interrogator/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/diffusion/
+-rw-r--r--   0 runner    (1001) docker     (122)      140 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/diffusion/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    26652 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/diffusion/diffusion.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14447 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/diffusion/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    39910 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/diffusion/structbert.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11507 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/diffusion/tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11834 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/diffusion/unet_generator.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8566 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/diffusion/unet_upsampler_1024.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12361 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/diffusion/unet_upsampler_256.py
+-rw-r--r--   0 runner    (1001) docker     (122)    44602 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/dpm_solver_pytorch.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/efficient_diffusion_tuning/
+-rw-r--r--   0 runner    (1001) docker     (122)      540 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/efficient_diffusion_tuning/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12939 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/efficient_diffusion_tuning/efficient_stable_diffusion.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/gemm/
+-rw-r--r--   0 runner    (1001) docker     (122)      139 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/gemm/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21694 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/gemm/gemm_base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3718 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/gemm/gemm_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6982 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/gemm/tokenizer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/guided_diffusion/
+-rw-r--r--   0 runner    (1001) docker     (122)      548 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/guided_diffusion/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    36265 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/guided_diffusion/gaussian_diffusion.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2969 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/guided_diffusion/respace.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1365 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/guided_diffusion/script.py
+-rw-r--r--   0 runner    (1001) docker     (122)    36898 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/guided_diffusion/unet.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/mgeo/
+-rw-r--r--   0 runner    (1001) docker     (122)     1444 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/mgeo/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)   101942 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/mgeo/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8651 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/mgeo/text_classification.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3268 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/mgeo/text_ranking.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10331 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/mgeo/token_classification.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/mmr/
+-rw-r--r--   0 runner    (1001) docker     (122)      141 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/mmr/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/mmr/dataloaders/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/mmr/dataloaders/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3789 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/mmr/dataloaders/rawvideo_util.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/mmr/models/
+-rw-r--r--   0 runner    (1001) docker     (122)      162 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/mmr/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9790 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/mmr/models/clip_for_mm_video_embedding.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1442 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/mmr/models/dynamic_inverted_softmax.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21257 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/mmr/models/modeling.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18790 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/mmr/models/module_clip.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3577 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/mmr/models/module_cross.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5581 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/mmr/models/tokenization_clip.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4116 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/mmr/models/until_module.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/mplug/
+-rw-r--r--   0 runner    (1001) docker     (122)      739 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/mplug/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/mplug/clip/
+-rw-r--r--   0 runner    (1001) docker     (122)       86 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/mplug/clip/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16605 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/mplug/clip/clip.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6030 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/mplug/configuration_mplug.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)   120753 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/mplug/modeling_mplug.py
+-rw-r--r--   0 runner    (1001) docker     (122)    34049 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/mplug/mvit.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    21332 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/mplug/predictor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5536 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/mplug_for_all_tasks.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/mplug_owl/
+-rw-r--r--   0 runner    (1001) docker     (122)      835 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/mplug_owl/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10465 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/mplug_owl/configuration_mplug_owl.py
+-rw-r--r--   0 runner    (1001) docker     (122)    70240 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/mplug_owl/modeling_mplug_owl.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/multi_stage_diffusion/
+-rw-r--r--   0 runner    (1001) docker     (122)      150 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/multi_stage_diffusion/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10208 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/multi_stage_diffusion/clip.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11503 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/multi_stage_diffusion/decoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)    28436 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/multi_stage_diffusion/gaussian_diffusion.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13487 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/multi_stage_diffusion/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5276 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/multi_stage_diffusion/prior.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6898 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/multi_stage_diffusion/tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16675 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/multi_stage_diffusion/upsampler.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6565 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/multi_stage_diffusion/xglm.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa/
+-rw-r--r--   0 runner    (1001) docker     (122)      306 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa/adaptor/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa/adaptor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18720 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa/configuration_mmspeech.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16552 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa/configuration_ofa.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa/generate/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa/generate/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1785 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa/generate/incremental_decoding_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21049 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa/generate/multihead_attention.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5658 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa/generate/ngram_repeat_block.py
+-rw-r--r--   0 runner    (1001) docker     (122)    43439 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa/generate/search.py
+-rw-r--r--   0 runner    (1001) docker     (122)    41976 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa/generate/sequence_generator.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16690 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa/generate/token_generation_constraints.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4883 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa/generate/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    44296 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa/modeling_mmspeech.py
+-rw-r--r--   0 runner    (1001) docker     (122)   100933 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa/modeling_ofa.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13226 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa/resnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15028 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa/tokenization_ofa.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8004 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa/tokenization_ofa_fast.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      676 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa/utils/constant.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1877 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa/utils/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8435 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa/vit.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24779 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa_for_all_tasks.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12983 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa_for_text_to_image_synthesis_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/rleg/
+-rw-r--r--   0 runner    (1001) docker     (122)      538 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/rleg/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4865 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/rleg/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3410 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/rleg/rleg.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/soonet/
+-rw-r--r--   0 runner    (1001) docker     (122)      678 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/soonet/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9838 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/soonet/blocks.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11896 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/soonet/clip.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6165 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/soonet/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    22584 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/soonet/swin_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4885 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/soonet/tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1597 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/soonet/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/stable_diffusion/
+-rw-r--r--   0 runner    (1001) docker     (122)       96 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/stable_diffusion/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6903 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/stable_diffusion/stable_diffusion.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/team/
+-rw-r--r--   0 runner    (1001) docker     (122)      140 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/team/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5181 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/team/team_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11502 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/team/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/video_synthesis/
+-rw-r--r--   0 runner    (1001) docker     (122)      538 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/video_synthesis/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18477 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/video_synthesis/autoencoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9264 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/video_synthesis/diffusion.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9564 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/video_synthesis/text_to_video_synthesis_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    38498 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/video_synthesis/unet_sd.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/vldoc/
+-rw-r--r--   0 runner    (1001) docker     (122)       93 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/vldoc/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11339 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/vldoc/conv_fpn_trans.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6422 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/vldoc/convnext.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16687 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/vldoc/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    47348 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/vldoc/modeling_layout_roberta.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20956 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/vldoc/processing.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4680 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/vldoc/tokenization.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7333 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/multi_modal/vldoc/transformer_local.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/T5/
+-rw-r--r--   0 runner    (1001) docker     (122)      599 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/T5/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    67374 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/T5/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7541 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/T5/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21517 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/T5/text2text_generation.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7172 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/bart/
+-rw-r--r--   0 runner    (1001) docker     (122)      112 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/bart/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3058 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/bart/text_error_correction.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/bert/
+-rw-r--r--   0 runner    (1001) docker     (122)     1519 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/bert/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    40458 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/bert/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7308 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/bert/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4113 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/bert/document_segmentation.py
+-rw-r--r--   0 runner    (1001) docker     (122)      512 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/bert/fill_mask.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6074 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/bert/sentence_embedding.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7153 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/bert/siamese_uie.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1478 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/bert/text_classification.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1138 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/bert/text_ranking.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2140 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/bert/token_classification.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6904 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/bert/word_alignment.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/bloom/
+-rw-r--r--   0 runner    (1001) docker     (122)      472 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/bloom/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      505 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/bloom/backbone.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/canmt/
+-rw-r--r--   0 runner    (1001) docker     (122)      102 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/canmt/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    52235 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/canmt/canmt_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2940 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/canmt/canmt_translation.py
+-rw-r--r--   0 runner    (1001) docker     (122)    35688 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/canmt/sequence_generator.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/chatglm/
+-rw-r--r--   0 runner    (1001) docker     (122)     1578 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/chatglm/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4403 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/chatglm/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15789 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/chatglm/quantization.py
+-rw-r--r--   0 runner    (1001) docker     (122)    60869 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/chatglm/text_generation.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17422 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/chatglm/tokenization.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/chatglm2/
+-rw-r--r--   0 runner    (1001) docker     (122)     1584 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/chatglm2/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2576 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/chatglm2/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15206 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/chatglm2/quantization.py
+-rw-r--r--   0 runner    (1001) docker     (122)    55471 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/chatglm2/text_generation.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10173 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/chatglm2/tokenization.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/codegeex/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      730 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/codegeex/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    35473 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/codegeex/codegeex.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     3995 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/codegeex/codegeex_for_code_generation.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     4008 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/codegeex/codegeex_for_code_translation.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     9997 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/codegeex/inference.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     6111 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/codegeex/tokenizer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/csanmt/
+-rw-r--r--   0 runner    (1001) docker     (122)       96 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/csanmt/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    61868 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/csanmt/translation.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/deberta_v2/
+-rw-r--r--   0 runner    (1001) docker     (122)     1771 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/deberta_v2/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    47998 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/deberta_v2/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6771 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/deberta_v2/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10364 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/deberta_v2/fill_mask.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21421 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/deberta_v2/tokenization.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10698 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/deberta_v2/tokenization_fast.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/dgds/
+-rw-r--r--   0 runner    (1001) docker     (122)      939 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/dgds/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7092 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/dgds/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1656 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/dgds/document_grounded_dialog_generate.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1059 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/dgds/document_grounded_dialog_rerank.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2180 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/dgds/document_grounded_dialog_retrieval.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/fid_T5/
+-rw-r--r--   0 runner    (1001) docker     (122)     1082 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/fid_T5/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8108 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/fid_T5/text_generation.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/fid_plug/
+-rw-r--r--   0 runner    (1001) docker     (122)     1181 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/fid_plug/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    45112 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/fid_plug/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5045 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/fid_plug/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6709 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/fid_plug/text_generation.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/glm_130b/
+-rw-r--r--   0 runner    (1001) docker     (122)      540 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/glm_130b/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/glm_130b/generation/
+-rw-r--r--   0 runner    (1001) docker     (122)       87 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/glm_130b/generation/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    10112 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/glm_130b/generation/strategies.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     5152 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/glm_130b/initialize.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/glm_130b/kernels/
+-rw-r--r--   0 runner    (1001) docker     (122)     3263 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/glm_130b/kernels/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/glm_130b/quantization/
+-rw-r--r--   0 runner    (1001) docker     (122)     2635 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/glm_130b/quantization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1189 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/glm_130b/quantization/functional.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4510 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/glm_130b/quantization/layers.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    13504 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/glm_130b/text_generation.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/gpt2/
+-rw-r--r--   0 runner    (1001) docker     (122)      470 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/gpt2/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      498 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/gpt2/backbone.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/gpt3/
+-rw-r--r--   0 runner    (1001) docker     (122)      852 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/gpt3/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16094 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/gpt3/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8971 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/gpt3/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)    52139 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/gpt3/distributed_gpt3.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3207 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/gpt3/text_generation.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2586 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/gpt3/tokenizer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/gpt_moe/
+-rw-r--r--   0 runner    (1001) docker     (122)      874 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/gpt_moe/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13129 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/gpt_moe/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5198 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/gpt_moe/checkpointing.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4930 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/gpt_moe/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)    49316 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/gpt_moe/distributed_gpt_moe.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/gpt_moe/moe/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/gpt_moe/moe/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1194 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/gpt_moe/moe/experts.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3504 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/gpt_moe/moe/layer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2553 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/gpt_moe/moe/mappings.py
+-rw-r--r--   0 runner    (1001) docker     (122)    23756 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/gpt_moe/moe/sharded_moe.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4110 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/gpt_moe/moe/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2717 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/gpt_moe/text_generation.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2277 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/gpt_moe/tokenizer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/gpt_neo/
+-rw-r--r--   0 runner    (1001) docker     (122)      474 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/gpt_neo/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      518 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/gpt_neo/backbone.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/heads/
+-rw-r--r--   0 runner    (1001) docker     (122)      661 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/heads/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    25248 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/heads/crf_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6947 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/heads/fill_mask_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5145 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/heads/infromation_extraction_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2056 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/heads/text_classification_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)      964 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/heads/text_generation_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2029 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/heads/text_ranking_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2596 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/heads/token_classification_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)      948 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/heads/torch_pretrain_head.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/hf_transformers/
+-rw-r--r--   0 runner    (1001) docker     (122)      488 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/hf_transformers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4897 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/hf_transformers/backbone.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/llama/
+-rw-r--r--   0 runner    (1001) docker     (122)      866 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/llama/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    29189 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/llama/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4695 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/llama/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11349 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/llama/convert_llama_weights_to_hf.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7557 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/llama/text_generation.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10325 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/llama/tokenization.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4712 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/llama/tokenization_fast.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/llama2/
+-rw-r--r--   0 runner    (1001) docker     (122)      876 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/llama2/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    32097 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/llama2/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8226 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/llama2/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10447 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/llama2/text_generation.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16964 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/llama2/tokenization.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10110 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/llama2/tokenization_fast.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/lstm/
+-rw-r--r--   0 runner    (1001) docker     (122)      610 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/lstm/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1312 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/lstm/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2187 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/lstm/token_classification.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/megatron_bert/
+-rw-r--r--   0 runner    (1001) docker     (122)      688 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/megatron_bert/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    39864 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/megatron_bert/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6599 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/megatron_bert/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12452 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/megatron_bert/fill_mask.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/mglm/
+-rw-r--r--   0 runner    (1001) docker     (122)      572 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/mglm/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    28115 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/mglm/arguments.py
+-rw-r--r--   0 runner    (1001) docker     (122)    28162 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/mglm/blocklm_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17995 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/mglm/configure_data.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/mglm/data_utils/
+-rw-r--r--   0 runner    (1001) docker     (122)    13656 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/mglm/data_utils/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    20350 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/mglm/data_utils/corpora.py
+-rw-r--r--   0 runner    (1001) docker     (122)    45765 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/mglm/data_utils/datasets.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3342 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/mglm/data_utils/extraction.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     8459 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/mglm/data_utils/file_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9797 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/mglm/data_utils/lazy_loader.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7221 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/mglm/data_utils/samplers.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4661 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/mglm/data_utils/sp_tokenizer.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    53304 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/mglm/data_utils/tokenization.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14551 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/mglm/data_utils/tokenization_gpt2.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    15773 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/mglm/data_utils/wordpiece.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21669 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/mglm/generation_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16247 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/mglm/mglm_for_text_summarization.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/mglm/model/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      984 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/mglm/model/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     5443 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/mglm/model/distributed.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9967 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/mglm/model/downstream.py
+-rw-r--r--   0 runner    (1001) docker     (122)    70249 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/mglm/model/modeling_bert.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8743 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/mglm/model/modeling_glm.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2531 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/mglm/model/prompt.py
+-rw-r--r--   0 runner    (1001) docker     (122)    48886 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/mglm/model/transformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1955 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/mglm/process_grid.py
+-rw-r--r--   0 runner    (1001) docker     (122)      203 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/mglm/run_test.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/mglm/test/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/mglm/test/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      950 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/mglm/test/test_block.py
+-rw-r--r--   0 runner    (1001) docker     (122)      704 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/mglm/test/test_rel_shift.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17815 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/mglm/train_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19029 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/mglm/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/palm_v2/
+-rw-r--r--   0 runner    (1001) docker     (122)     1268 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/palm_v2/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5532 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/palm_v2/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)    29453 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/palm_v2/dureader_eval.py
+-rw-r--r--   0 runner    (1001) docker     (122)    55450 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/palm_v2/text_generation.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/peer/
+-rw-r--r--   0 runner    (1001) docker     (122)     1194 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/peer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    55772 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/peer/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11708 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/peer/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6166 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/peer/sas_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4933 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/peer/text_classification.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/plug/
+-rwxr-xr-x   0 runner    (1001) docker     (122)     3321 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/plug/AnnealingLR.py
+-rw-r--r--   0 runner    (1001) docker     (122)      660 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/plug/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    41209 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/plug/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11797 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/plug/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10983 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/plug/distributed_plug.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8427 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/plug/generator.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/plug_mental/
+-rw-r--r--   0 runner    (1001) docker     (122)     1359 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/plug_mental/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7671 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/plug_mental/adv_utils.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    47486 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/plug_mental/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7956 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/plug_mental/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10881 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/plug_mental/text_classification.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/polylm/
+-rw-r--r--   0 runner    (1001) docker     (122)      499 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/polylm/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1947 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/polylm/text_generation.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/ponet/
+-rw-r--r--   0 runner    (1001) docker     (122)     1490 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/ponet/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    37918 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/ponet/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6366 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/ponet/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4176 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/ponet/document_segmentation.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11011 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/ponet/fill_mask.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7147 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/ponet/tokenization.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/space/
+-rw-r--r--   0 runner    (1001) docker     (122)      987 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/space/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1208 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/space/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3832 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/space/dialog_intent_prediction.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4354 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/space/dialog_modeling.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16648 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/space/dialog_state_tracking.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/space/model/
+-rw-r--r--   0 runner    (1001) docker     (122)      421 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/space/model/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10281 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/space/model/gen_unified_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10757 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/space/model/generator.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7505 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/space/model/intent_unified_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3032 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/space/model/model_base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1189 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/space/model/tokenization_space.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11941 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/space/model/unified_transformer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/space/modules/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/space/modules/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2398 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/space/modules/embedder.py
+-rw-r--r--   0 runner    (1001) docker     (122)      956 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/space/modules/feedforward.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1721 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/space/modules/functions.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3397 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/space/modules/multihead_attention.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1922 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/space/modules/transformer_block.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/space_T_cn/
+-rw-r--r--   0 runner    (1001) docker     (122)      529 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/space_T_cn/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    44438 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/space_T_cn/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5194 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/space_T_cn/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)    30117 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/space_T_cn/table_question_answering.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/space_T_en/
+-rw-r--r--   0 runner    (1001) docker     (122)      492 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/space_T_en/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3715 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/space_T_en/text_to_sql.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/structbert/
+-rw-r--r--   0 runner    (1001) docker     (122)     1674 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/structbert/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7673 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/structbert/adv_utils.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    40848 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/structbert/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7686 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/structbert/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)    27203 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/structbert/faq_question_answering.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12581 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/structbert/fill_mask.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11862 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/structbert/text_classification.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11074 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/structbert/token_classification.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/task_models/
+-rw-r--r--   0 runner    (1001) docker     (122)     1453 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/task_models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5323 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/task_models/feature_extraction.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2698 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/task_models/fill_mask.py
+-rw-r--r--   0 runner    (1001) docker     (122)      766 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/task_models/information_extraction.py
+-rw-r--r--   0 runner    (1001) docker     (122)    28991 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/task_models/task_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1912 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/task_models/text_classification.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8678 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/task_models/text_generation.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2092 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/task_models/text_ranking.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5116 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/task_models/token_classification.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/unite/
+-rw-r--r--   0 runner    (1001) docker     (122)      626 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/unite/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      409 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/unite/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18320 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/unite/translation_evaluation.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/use/
+-rw-r--r--   0 runner    (1001) docker     (122)      545 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/use/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5133 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/use/transformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5832 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/use/user_satisfaction_estimation.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/veco/
+-rw-r--r--   0 runner    (1001) docker     (122)     1479 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/veco/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4031 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/veco/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1192 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/veco/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4278 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/veco/fill_mask.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6643 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/veco/text_classification.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4129 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/veco/token_classification.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/nlp/xlm_roberta/
+-rw-r--r--   0 runner    (1001) docker     (122)     1156 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/xlm_roberta/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    42932 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/xlm_roberta/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7697 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/nlp/xlm_roberta/configuration.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/science/
+-rw-r--r--   0 runner    (1001) docker     (122)      491 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/science/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/
+-rw-r--r--   0 runner    (1001) docker     (122)       46 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24057 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/config.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/data/
+-rw-r--r--   0 runner    (1001) docker     (122)      634 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/data/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    49139 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/data/data_ops.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19691 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/data/msa_pairing.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8941 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/data/process.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14792 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/data/process_multimer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11422 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/data/protein.py
+-rw-r--r--   0 runner    (1001) docker     (122)    42023 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/data/residue_constants.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4628 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/data/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19250 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2307 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/modules/
+-rw-r--r--   0 runner    (1001) docker     (122)      187 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/modules/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17066 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/modules/alphafold.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12323 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/modules/attentions.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5403 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/modules/auxillary_heads.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10676 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/modules/common.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5814 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/modules/confidence.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8771 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/modules/embedders.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11078 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/modules/evoformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6820 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/modules/featurization.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18025 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/modules/frame.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18476 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/modules/structure_module.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9771 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/modules/template.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5623 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/modules/triangle_multiplication.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/msa/
+-rw-r--r--   0 runner    (1001) docker     (122)       46 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/msa/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17940 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/msa/mmcif.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3128 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/msa/msa_identifiers.py
+-rw-r--r--   0 runner    (1001) docker     (122)    23503 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/msa/parsers.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11644 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/msa/pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    45468 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/msa/templates.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/msa/tools/
+-rw-r--r--   0 runner    (1001) docker     (122)      639 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/msa/tools/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6218 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/msa/tools/hhblits.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3991 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/msa/tools/hhsearch.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5098 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/msa/tools/hmmbuild.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5013 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/msa/tools/hmmsearch.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8662 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/msa/tools/jackhmmer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3752 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/msa/tools/kalign.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1255 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/msa/tools/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2892 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/models/science/unifold/msa/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/msdatasets/
+-rw-r--r--   0 runner    (1001) docker     (122)       84 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/msdatasets/audio/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/audio/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      346 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/audio/asr_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/msdatasets/auth/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/auth/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1326 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/auth/auth_config.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/msdatasets/context/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/context/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3444 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/context/dataset_context_config.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/msdatasets/data_files/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/data_files/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5270 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/data_files/data_files_manager.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/msdatasets/data_loader/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/data_loader/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12636 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/data_loader/data_loader.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5764 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/data_loader/data_loader_manager.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/
+-rw-r--r--   0 runner    (1001) docker     (122)      111 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/
+-rw-r--r--   0 runner    (1001) docker     (122)     4111 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/
+-rw-r--r--   0 runner    (1001) docker     (122)      730 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1919 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/asr_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8512 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/kws_farfield_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7723 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/kws_nearfield_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11689 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/kws_nearfield_processor.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/bad_image_detecting/
+-rw-r--r--   0 runner    (1001) docker     (122)      541 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/bad_image_detecting/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1264 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/bad_image_detecting/bad_image_detecting_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)      791 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/builder.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/
+-rw-r--r--   0 runner    (1001) docker     (122)      134 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4708 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/build.py
+-rw-r--r--   0 runner    (1001) docker     (122)      945 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/collate_batch.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/
+-rw-r--r--   0 runner    (1001) docker     (122)      177 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3815 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/coco.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16180 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/mosaic_wrapper.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/
+-rw-r--r--   0 runner    (1001) docker     (122)      934 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/coco/
+-rw-r--r--   0 runner    (1001) docker     (122)      544 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/coco/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11572 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/coco/coco_eval.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/
+-rw-r--r--   0 runner    (1001) docker     (122)      312 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2622 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/distributed.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4858 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/grouped_batch_sampler.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1469 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/iteration_based_batch_sampler.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/
+-rw-r--r--   0 runner    (1001) docker     (122)      196 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1086 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/build.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2509 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/transforms.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1511 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/easycv_base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2141 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/gopro_image_deblurring_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/image_colorization/
+-rw-r--r--   0 runner    (1001) docker     (122)      539 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/image_colorization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2196 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/image_colorization/image_colorization_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/
+-rw-r--r--   0 runner    (1001) docker     (122)      529 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2968 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/aug.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12152 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/image_inpainting_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12574 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/image_instance_segmentation_coco_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/
+-rw-r--r--   0 runner    (1001) docker     (122)      577 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1091 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/data_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1629 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/image_portrait_enhancement_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assessment_degradation/
+-rw-r--r--   0 runner    (1001) docker     (122)      615 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assessment_degradation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1487 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assessment_degradation/image_quality_assessment_degradation_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assmessment_mos/
+-rw-r--r--   0 runner    (1001) docker     (122)      583 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assmessment_mos/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1194 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assmessment_mos/image_quality_assessment_mos_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4849 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/language_guided_video_summarization_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6225 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/mgeo_ranking_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)      559 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6171 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/movie_scene_segmentation_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4013 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/sampler.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/
+-rw-r--r--   0 runner    (1001) docker     (122)      161 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1589 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/augmenter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4945 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/data_loader.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5437 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/image_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/
+-rw-r--r--   0 runner    (1001) docker     (122)       40 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7547 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/iou_evaluator.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3416 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/quad_measurer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/
+-rw-r--r--   0 runner    (1001) docker     (122)      309 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3172 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/augment_data.py
+-rw-r--r--   0 runner    (1001) docker     (122)      971 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/data_process.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5818 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_border_map.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2000 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_icdar_data.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3512 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_seg_detection_data.py
+-rw-r--r--   0 runner    (1001) docker     (122)      707 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/normalize_image.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4912 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/random_crop_data.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2402 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_recognition_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2017 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/reds_image_deblurring_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)      599 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16767 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/referring_video_object_segmentation_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8725 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/transformers.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/
+-rw-r--r--   0 runner    (1001) docker     (122)      545 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1475 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/data_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1984 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/sidd_image_denoising_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2684 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/transforms.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5114 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/text_ranking_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1635 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/torch_custom_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2664 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/veco_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/
+-rw-r--r--   0 runner    (1001) docker     (122)      573 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1390 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/data_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1780 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/video_frame_interpolation_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/video_stabilization/
+-rw-r--r--   0 runner    (1001) docker     (122)      543 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/video_stabilization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      809 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/video_stabilization/video_stabilization_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2624 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/video_summarization_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/video_super_resolution/
+-rw-r--r--   0 runner    (1001) docker     (122)      553 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/video_super_resolution/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2370 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/video_super_resolution/video_super_resolution_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11485 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/msdatasets/download/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/download/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21287 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/download/dataset_builder.py
+-rw-r--r--   0 runner    (1001) docker     (122)      640 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/download/download_config.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2487 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/download/download_manager.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/msdatasets/meta/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/meta/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1772 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/meta/data_meta_config.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8679 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/meta/data_meta_manager.py
+-rw-r--r--   0 runner    (1001) docker     (122)    36737 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/ms_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/msdatasets/task_datasets/
+-rw-r--r--   0 runner    (1001) docker     (122)     1072 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/task_datasets/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      408 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/task_datasets/gopro_image_deblurring_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)      401 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/task_datasets/reds_image_deblurring_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)      404 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/task_datasets/sidd_image_denoising.py
+-rw-r--r--   0 runner    (1001) docker     (122)      410 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/task_datasets/torch_base_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)      404 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/task_datasets/video_summarization_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/msdatasets/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8124 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/utils/dataset_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1026 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/utils/delete_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5540 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/utils/maxcompute_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6200 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/utils/oss_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2496 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/msdatasets/utils/upload_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/ops/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/ops/4knerf/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/ops/4knerf/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/ops/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/ops/ailut/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/ops/ailut/Ailut/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/ops/ailut/Ailut/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/ops/ailut/Ailut/csrc/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/ops/ailut/Ailut/csrc/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      105 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/ops/ailut/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4314 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/ops/ailut/pyinterfaces.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/ops/quadtree_attention/
+-rw-r--r--   0 runner    (1001) docker     (122)      106 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/ops/quadtree_attention/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/ops/quadtree_attention/functions/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/ops/quadtree_attention/functions/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2925 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/ops/quadtree_attention/functions/quadtree_attention.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/ops/quadtree_attention/modules/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/ops/quadtree_attention/modules/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13956 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/ops/quadtree_attention/modules/quadtree_attention.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/ops/quadtree_attention/src/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/ops/quadtree_attention/src/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/outputs/
+-rw-r--r--   0 runner    (1001) docker     (122)      132 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/outputs/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      908 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/outputs/cv_outputs.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19855 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/outputs/nlp_outputs.py
+-rw-r--r--   0 runner    (1001) docker     (122)    52143 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/outputs/outputs.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11586 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipeline_inputs.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/pipelines/
+-rw-r--r--   0 runner    (1001) docker     (122)      150 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/pipelines/audio/
+-rw-r--r--   0 runner    (1001) docker     (122)     1567 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/audio/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7519 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/audio/ans_dfsmn_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5016 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/audio/ans_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24110 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/audio/asr_inference_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3189 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/audio/asr_wenet_inference_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4357 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/audio/inverse_text_processing_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3830 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/audio/kws_farfield_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7276 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/audio/kws_kwsbp_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5698 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/audio/language_recognition_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5642 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/audio/linear_aec_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8068 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/audio/lm_infer_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6481 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/audio/punctuation_processing_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12545 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/audio/segmentation_clustering_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2560 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/audio/separation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4829 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/audio/speaker_change_locating_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6038 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/audio/speaker_diarization_dialogue_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11421 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/audio/speaker_diarization_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4246 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/audio/speaker_diarization_semantic_speaker_turn_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4078 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/audio/speaker_verification_eres2net_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6089 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/audio/speaker_verification_light_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10817 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/audio/speaker_verification_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4072 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/audio/speaker_verification_rdino_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1789 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/audio/text_to_speech_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11930 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/audio/timestamp_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9696 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/audio/voice_activity_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    23185 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7702 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/builder.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/
+-rw-r--r--   0 runner    (1001) docker     (122)    16754 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2545 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/action_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4852 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/action_recognition_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4024 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/animal_recognition_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2576 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/arc_face_recognition_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2614 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/bad_image_detecting_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9581 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/body_2d_keypoints_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14219 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/body_3d_keypoints_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4753 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/card_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5307 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/cmdssl_video_embedding_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2528 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/content_check_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5055 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/controllable_image_generation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5942 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/crowd_counting_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5136 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/ddcolor_image_colorization_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1965 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/ddpm_semantic_segmentation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2498 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/face_attribute_recognition_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3993 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/face_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1531 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/face_emotion_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1648 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/face_human_hand_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2894 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/face_image_generation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3312 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/face_liveness_ir_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3573 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/face_liveness_xc_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7748 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/face_processing_base_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3872 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/face_quality_assessment_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3528 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/face_recognition_onnx_fm_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3493 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/face_recognition_onnx_ir_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2878 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/face_recognition_ood_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2498 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/face_recognition_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19634 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/face_reconstruction_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2400 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/facial_expression_recognition_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2681 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/facial_landmark_confidence_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4442 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/fast_instance_segmentation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4030 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/general_recognition_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1360 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/hand_static_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2982 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/hicossl_video_embedding_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4372 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/human_reconstruction_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1399 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/image_body_reshaping_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2919 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/image_bts_depth_estimation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5251 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/image_cartoon_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5857 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/image_classification_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3296 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/image_color_enhance_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4441 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/image_colorization_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2648 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/image_debanding_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4624 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/image_deblur_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3517 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/image_defrcn_fewshot_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4160 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/image_denoise_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1943 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/image_depth_estimation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1854 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/image_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4134 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/image_driving_perception_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2317 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/image_face_fusion_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4866 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/image_human_parsing_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5854 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/image_inpainting_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4673 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/image_inpainting_sdv2_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3903 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/image_instance_segmentation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5929 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/image_matching_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2607 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/image_matting_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2796 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/image_mvs_depth_estimation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2770 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/image_open_vocabulary_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6113 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/image_paintbyexample_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3622 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/image_panoptic_segmentation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8567 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/image_portrait_enhancement_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3549 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/image_quality_assessment_degradation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2835 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/image_quality_assessment_man_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2801 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/image_quality_assessment_mos_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2026 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/image_reid_person_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1702 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/image_restoration_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1644 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/image_salient_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3207 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/image_semantic_segmentation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2237 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/image_skychange_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2839 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/image_structured_model_probing_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4615 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/image_style_transfer_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3138 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/image_super_resolution_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9718 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/image_to_image_generate_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12737 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/image_to_image_translation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2319 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/image_try_on_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1883 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/indoor_layout_estimation_pipeline.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     9872 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/language_guided_video_summarization_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4570 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/license_plate_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4182 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/lineless_table_recognition_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5276 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/live_category_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2757 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/mask_face_recognition_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2787 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/maskdino_instance_segmentation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3670 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/mobile_image_super_resolution_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1857 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/mog_face_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4844 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/motion_generation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2628 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/movie_scene_segmentation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1953 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/mtcnn_face_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3245 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/nerf_recon_4k_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3328 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/nerf_recon_acc_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3592 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/nerf_recon_vq_compression_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6018 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/object_detection_3d_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11033 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/ocr_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2509 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/ocr_recognition_pipeline.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/ocr_utils/
+-rw-r--r--   0 runner    (1001) docker     (122)      966 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/ocr_utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      720 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/ocr_utils/model_convnext_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19821 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/ocr_utils/model_dla34.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8768 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/ocr_utils/model_resnet18_half.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5286 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/ocr_utils/model_resnet_mutex_v4_linewithchar.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15421 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/ocr_utils/model_vlpt.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/ocr_utils/ocr_modules/
+-rw-r--r--   0 runner    (1001) docker     (122)      550 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/ocr_utils/ocr_modules/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6209 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/ocr_utils/ocr_modules/convnext.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11644 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/ocr_utils/ocr_modules/timm_tinyc.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1560 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/ocr_utils/ocr_modules/vitstr.py
+-rw-r--r--   0 runner    (1001) docker     (122)    38278 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/ocr_utils/ops.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19004 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/ocr_utils/resnet18_v1.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11371 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/ocr_utils/resnet_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11168 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/ocr_utils/table_process.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7971 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/ocr_utils/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3029 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/panorama_depth_estimation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2791 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/panorama_depth_estimation_s2net_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9781 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/pedestrian_attribute_recognition_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4199 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/pointcloud_sceneflow_estimation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1457 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/product_retrieval_embedding_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1451 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/product_segmentation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2020 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/realtime_video_object_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9239 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/referring_video_object_segmentation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1956 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/retina_face_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1760 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/shop_segmentation_pipleline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12129 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/skin_retouching_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4446 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/table_recognition_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4899 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/tbs_detection_pipeline.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/tbs_detection_utils/
+-rw-r--r--   0 runner    (1001) docker     (122)       10 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/tbs_detection_utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15542 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/tbs_detection_utils/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1841 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/text_driven_segmentation_pipleline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8865 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/text_to_360panorama_image_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3246 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/tinynas_classification_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3244 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/tinynas_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1888 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/ulfd_face_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13776 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/video_category_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6097 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/video_colorization_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7757 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/video_deinterlace_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1565 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/video_depth_estimation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24271 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/video_frame_interpolation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3635 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/video_human_matting_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1719 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/video_inpainting_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9652 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/video_instance_segmentation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3294 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/video_multi_object_tracking_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4724 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/video_object_segmentation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4683 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/video_panoptic_segmentation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3436 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/video_single_object_tracking_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4647 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/video_stabilization_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4289 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/video_summarization_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7078 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/video_super_resolution_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7564 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/vidt_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5240 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/virtual_try_on_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3986 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/vision_efficient_tuning_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2180 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/vision_middleware_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4762 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/vop_retrieval_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5787 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/cv/vop_retrieval_se_pipeline.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/
+-rw-r--r--   0 runner    (1001) docker     (122)     3057 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2366 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/asr_pipeline.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/diffusers_wrapped/
+-rw-r--r--   0 runner    (1001) docker     (122)      622 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/diffusers_wrapped/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2026 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/diffusers_wrapped/diffusers_pipeline.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/
+-rw-r--r--   0 runner    (1001) docker     (122)      704 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12129 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/chinese_stable_diffusion_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8750 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/stable_diffusion_pipeline.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/
+-rw-r--r--   0 runner    (1001) docker     (122)      585 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15631 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/disco_guided_diffusion.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18987 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2311 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/document_vl_embedding_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3086 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/efficient_diffusion_tuning_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1093 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/generative_multi_modal_embedding_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9352 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/gridvlp_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3219 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/image_captioning_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1612 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/image_text_retrieval_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7869 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/mgeo_ranking_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1648 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/multi_modal_embedding_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3254 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/multimodal_dialogue_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1780 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/ocr_recognition_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8561 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/soonet_video_temporal_grounding_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1848 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/sudoku_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1059 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/team_multi_modal_similarity_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1789 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/text2sql_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2064 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/text_to_image_synthesis_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3796 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/text_to_video_synthesis_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2150 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/video_captioning_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1329 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/video_multi_modal_embedding_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1956 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/video_question_answering_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1780 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/visual_entailment_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1781 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/visual_grounding_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2396 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/visual_question_answering_pipeline.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/
+-rw-r--r--   0 runner    (1001) docker     (122)     6953 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6939 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/automatic_post_editing_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3696 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/canmt_translation_pipeline.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     2202 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/codegeex_code_generation_pipeline.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     2885 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/codegeex_code_translation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2251 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/conversational_text_to_sql_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2577 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/dialog_intent_prediction_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2308 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/dialog_modeling_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7663 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/dialog_state_tracking_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3882 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/distributed_gpt3_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1851 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/distributed_gpt_moe_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4425 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/distributed_plug_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2754 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/document_grounded_dialog_generate_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    25997 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/document_grounded_dialog_rerank_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5480 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/document_grounded_dialog_retrieval_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9797 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/document_segmentation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6259 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/extractive_summarization_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3348 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/faq_question_answering_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2437 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/fasttext_text_classification_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3258 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/feature_extraction_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10171 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/fid_dialogue_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4655 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/fill_mask_pipeline.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)      999 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/glm130b_text_generation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2383 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/information_extraction_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6363 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/interactive_translation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9928 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/language_identification_pipline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4079 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/llama2_text_generation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1776 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/mglm_text_summarization_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2955 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/named_entity_recognition_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2671 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/polylm_text_generation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3184 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/sentence_embedding_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14334 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/siamese_uie_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2566 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/summarization_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16315 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/table_question_answering_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6787 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/text_classification_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3490 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/text_error_correction_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11117 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/text_generation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2877 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/text_ranking_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6399 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/token_classification_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4372 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/translation_evaluation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5586 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/translation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2339 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/translation_quality_estimation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4613 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/user_satisfaction_estimation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2713 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/word_alignment_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4523 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/word_segmentation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5875 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/nlp/zero_shot_classification_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2816 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/pipeline_template.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/pipelines/science/
+-rw-r--r--   0 runner    (1001) docker     (122)      538 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/science/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8255 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/science/protein_structure_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3492 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/pipelines/util.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/preprocessors/
+-rw-r--r--   0 runner    (1001) docker     (122)     5792 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10063 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/asr.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8680 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/audio.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15531 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)      812 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/builder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6982 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/common.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/preprocessors/cv/
+-rw-r--r--   0 runner    (1001) docker     (122)     1896 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/cv/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7446 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/cv/action_detection_mapper.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1126 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/cv/bad_image_detecting_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7789 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/cv/controllable_image_generation.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20909 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/cv/cv2_transforms.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12826 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/cv/image_classification_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1250 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/cv/image_quality_assessment_man.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2113 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/cv/image_quality_assessment_mos.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2790 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/cv/image_restoration_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2625 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/cv/mmcls_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3050 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/cv/timer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3606 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/cv/util.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1501 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/cv/video_stabilization.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8579 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/cv/video_super_resolution.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13406 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/image.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4884 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/kws.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/preprocessors/movie_scene_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)      483 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/movie_scene_segmentation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10809 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/movie_scene_segmentation/transforms.py
+-rw-r--r--   0 runner    (1001) docker     (122)    30257 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/multi_modal.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/
+-rw-r--r--   0 runner    (1001) docker     (122)     5966 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      762 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/bert_seq_cls_tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3929 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/canmt_translation.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2572 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/dialog_classification_use_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4102 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/document_grounded_dialog_generate_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4391 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/document_grounded_dialog_rerank_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4086 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/document_grounded_dialog_retrieval_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11004 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/document_segmentation_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6844 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/faq_question_answering_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3304 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/feature_extraction_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12086 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/fill_mask_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8199 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/mgeo_ranking_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1168 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/mglm_summarization_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1799 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/relation_extraction_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4023 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/sentence_embedding_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1424 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/siamese_uie_preprocessor.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space/
+-rw-r--r--   0 runner    (1001) docker     (122)     1219 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1908 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space/args.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1553 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space/batch.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3629 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space/data_loader.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2701 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space/dialog_intent_prediction_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2789 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space/dialog_modeling_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5457 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space/dialog_state_tracking_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)    56779 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space/dst_processors.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space/fields/
+-rw-r--r--   0 runner    (1001) docker     (122)      592 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space/fields/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    33884 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space/fields/gen_field.py
+-rw-r--r--   0 runner    (1001) docker     (122)    42467 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space/fields/intent_field.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1147 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space/lazy_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1935 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space/preprocess.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1610 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space/sampler.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2084 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space/tensorlistdataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24803 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space/tokenizer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space_T_cn/
+-rw-r--r--   0 runner    (1001) docker     (122)      654 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space_T_cn/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space_T_cn/fields/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space_T_cn/fields/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4936 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space_T_cn/fields/database.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15867 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space_T_cn/fields/schema_link.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4888 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space_T_cn/fields/struct.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4175 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space_T_cn/table_question_answering_preprocessor.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space_T_en/
+-rw-r--r--   0 runner    (1001) docker     (122)      846 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space_T_en/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4902 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space_T_en/conversational_text_to_sql_preprocessor.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space_T_en/fields/
+-rw-r--r--   0 runner    (1001) docker     (122)      818 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space_T_en/fields/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21521 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space_T_en/fields/common_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12500 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space_T_en/fields/parse.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1380 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space_T_en/fields/preprocess_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2111 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space_T_en/fields/process_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6422 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/text_classification_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1641 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/text_clean.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2298 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/text_error_correction.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14124 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/text_generation_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3834 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/text_ranking_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19629 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/token_classification_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1640 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/token_classification_thai_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1238 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/token_classification_viet_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4482 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/transformers_tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7304 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/translation_evaluation_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3622 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4962 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/word_alignment_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2584 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/nlp/zero_shot_classification_preprocessor.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/preprocessors/ofa/
+-rw-r--r--   0 runner    (1001) docker     (122)      762 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/ofa/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4839 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/ofa/asr.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11909 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/ofa/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4172 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/ofa/image_captioning.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6461 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/ofa/image_classification.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5740 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/ofa/ocr_recognition.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4603 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/ofa/sudoku.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5210 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/ofa/summarization.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16449 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/ofa/text2sql.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5368 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/ofa/text_classification.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2164 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/ofa/text_to_image_synthesis.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/preprocessors/ofa/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/ofa/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3221 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/ofa/utils/audio_helper.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8936 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/ofa/utils/bridge_content_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6809 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/ofa/utils/collate.py
+-rw-r--r--   0 runner    (1001) docker     (122)      660 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/ofa/utils/constant.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3421 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/ofa/utils/get_tables.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1302 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/ofa/utils/random_help.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5459 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/ofa/utils/text2phone.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19116 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/ofa/utils/transforms.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9581 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/ofa/utils/vision_helper.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7664 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/ofa/visual_entailment.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8484 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/ofa/visual_grounding.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6796 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/ofa/visual_question_answering.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/preprocessors/science/
+-rw-r--r--   0 runner    (1001) docker     (122)      478 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/science/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21698 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/science/uni_fold.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13735 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/speaker.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2101 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/tts.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13168 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/preprocessors/video.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/swift/
+-rw-r--r--   0 runner    (1001) docker     (122)     1394 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/swift/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7462 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/swift/adapter.py
+-rw-r--r--   0 runner    (1001) docker     (122)      841 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/swift/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)    38973 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/swift/control_sd_lora.py
+-rw-r--r--   0 runner    (1001) docker     (122)    27160 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/swift/lora.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/swift/optimizers/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/swift/optimizers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6931 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/swift/optimizers/child_tuning_adamw_optimizer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9560 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/swift/prompt.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8811 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/swift/sd_lora.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/tools/
+-rw-r--r--   0 runner    (1001) docker     (122)       49 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/tools/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      772 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/tools/eval.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5359 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/tools/speech_tts_autolabel.py
+-rw-r--r--   0 runner    (1001) docker     (122)      607 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/tools/train.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/trainers/
+-rw-r--r--   0 runner    (1001) docker     (122)     1799 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/trainers/audio/
+-rw-r--r--   0 runner    (1001) docker     (122)      826 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/audio/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1873 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/audio/ans_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6969 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/audio/asr_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12816 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/audio/kws_farfield_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    22065 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/audio/kws_nearfield_trainer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/trainers/audio/kws_utils/
+-rw-r--r--   0 runner    (1001) docker     (122)     1768 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/audio/kws_utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    22114 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/audio/kws_utils/batch_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11499 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/audio/kws_utils/det_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7022 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/audio/kws_utils/file_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4509 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/audio/kws_utils/model_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2854 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/audio/kws_utils/runtime_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21962 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/audio/separation_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10858 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/audio/tts_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4389 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1808 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/builder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5553 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/cli_argument_parser.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/trainers/cv/
+-rw-r--r--   0 runner    (1001) docker     (122)     1974 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/cv/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7494 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/cv/action_detection_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)      668 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/cv/card_detection_scrfd_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10044 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/cv/cartoon_translation_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6737 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/cv/face_detection_scrfd_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20323 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/cv/image_classifition_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11655 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/cv/image_defrcn_fewshot_detection_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    22965 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/cv/image_detection_damoyolo_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4405 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/cv/image_inpainting_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)      816 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/cv/image_instance_segmentation_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5520 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/cv/image_portrait_enhancement_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)      680 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/cv/movie_scene_segmentation_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20149 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/cv/nerf_recon_acc_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16911 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/cv/ocr_detection_db_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3153 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/cv/ocr_recognition_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2248 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/cv/referring_video_object_segmentation_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4602 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/cv/vision_efficient_tuning_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2607 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/default_config.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/trainers/hooks/
+-rw-r--r--   0 runner    (1001) docker     (122)     2064 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/hooks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      296 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/hooks/builder.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/trainers/hooks/checkpoint/
+-rw-r--r--   0 runner    (1001) docker     (122)      116 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/hooks/checkpoint/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19610 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/hooks/checkpoint/checkpoint_hook.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11015 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/hooks/checkpoint/checkpoint_processor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5541 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/hooks/checkpoint/load_checkpoint_hook.py
+-rw-r--r--   0 runner    (1001) docker     (122)      764 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/hooks/clip_clamp_logit_scale_hook.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/trainers/hooks/compression/
+-rw-r--r--   0 runner    (1001) docker     (122)      610 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/hooks/compression/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4812 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/hooks/compression/sparsity_hook.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6915 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/hooks/compression/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/trainers/hooks/distributed/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/hooks/distributed/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1458 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/hooks/distributed/ddp_hook.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17119 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/hooks/distributed/deepspeed_hook.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7030 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/hooks/distributed/megatron_hook.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4398 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/hooks/early_stop_hook.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3825 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/hooks/evaluation_hook.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6507 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/hooks/hook.py
+-rw-r--r--   0 runner    (1001) docker     (122)      731 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/hooks/iter_timer_hook.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/trainers/hooks/logger/
+-rw-r--r--   0 runner    (1001) docker     (122)      718 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/hooks/logger/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4498 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/hooks/logger/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4507 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/hooks/logger/tensorboard_hook.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7443 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/hooks/logger/text_logger_hook.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6567 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/hooks/lr_scheduler_hook.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/trainers/hooks/optimizer/
+-rw-r--r--   0 runner    (1001) docker     (122)      743 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/hooks/optimizer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3139 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/hooks/optimizer/apex_optimizer_hook.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3786 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/hooks/optimizer/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3729 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/hooks/optimizer/torch_optimizer_hook.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1707 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/hooks/priority.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/trainers/lrscheduler/
+-rw-r--r--   0 runner    (1001) docker     (122)      699 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/lrscheduler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2040 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/lrscheduler/builder.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/trainers/lrscheduler/warmup/
+-rw-r--r--   0 runner    (1001) docker     (122)      614 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/lrscheduler/warmup/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2547 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/lrscheduler/warmup/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2934 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/lrscheduler/warmup/warmup.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/trainers/multi_modal/
+-rw-r--r--   0 runner    (1001) docker     (122)      682 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/multi_modal/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/trainers/multi_modal/clip/
+-rw-r--r--   0 runner    (1001) docker     (122)       89 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/multi_modal/clip/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9702 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/multi_modal/clip/clip_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4388 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/multi_modal/clip/clip_trainer_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/trainers/multi_modal/custom_diffusion/
+-rw-r--r--   0 runner    (1001) docker     (122)      110 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/multi_modal/custom_diffusion/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    32625 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/multi_modal/custom_diffusion/custom_diffusion_trainer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/trainers/multi_modal/dreambooth_diffusion/
+-rw-r--r--   0 runner    (1001) docker     (122)      118 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/multi_modal/dreambooth_diffusion/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15264 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/multi_modal/dreambooth_diffusion/dreambooth_diffusion_trainer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/trainers/multi_modal/efficient_diffusion_tuning/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/multi_modal/efficient_diffusion_tuning/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3047 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/multi_modal/efficient_diffusion_tuning/efficient_diffusion_tuning_trainer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/trainers/multi_modal/lora_diffusion/
+-rw-r--r--   0 runner    (1001) docker     (122)      106 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/multi_modal/lora_diffusion/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3503 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/multi_modal/lora_diffusion/lora_diffusion_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8086 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/multi_modal/mgeo_ranking_trainer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/trainers/multi_modal/mplug/
+-rw-r--r--   0 runner    (1001) docker     (122)       91 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/multi_modal/mplug/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1642 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/multi_modal/mplug/mplug_trainer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/trainers/multi_modal/ofa/
+-rw-r--r--   0 runner    (1001) docker     (122)       87 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/multi_modal/ofa/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10186 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/multi_modal/ofa/ofa_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17647 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/multi_modal/ofa/ofa_trainer_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/trainers/multi_modal/stable_diffusion/
+-rw-r--r--   0 runner    (1001) docker     (122)      112 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/multi_modal/stable_diffusion/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1144 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/multi_modal/stable_diffusion/stable_diffusion_trainer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/trainers/multi_modal/team/
+-rw-r--r--   0 runner    (1001) docker     (122)       95 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/multi_modal/team/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5429 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/multi_modal/team/team_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2443 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/multi_modal/team/team_trainer_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/trainers/nlp/
+-rw-r--r--   0 runner    (1001) docker     (122)     1332 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/nlp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14266 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/nlp/csanmt_translation_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9999 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/nlp/document_grounded_dialog_generate_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24284 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/nlp/document_grounded_dialog_rerank_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8159 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/nlp/document_grounded_dialog_retrieval_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12769 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/nlp/faq_question_answering_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3064 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/nlp/gpt3_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2097 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/nlp/gpt_moe_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8171 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/nlp/plug_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3869 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/nlp/sentence_embedding_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8378 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/nlp/sequence_classification_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17110 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/nlp/siamese_uie_trainer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/trainers/nlp/space/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/nlp/space/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5324 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/nlp/space/dialog_intent_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4229 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/nlp/space/dialog_modeling_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    36354 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/nlp/space/eval.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/trainers/nlp/space/metrics/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/nlp/space/metrics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2447 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/nlp/space/metrics/metrics_tracker.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/trainers/nlp/space/trainer/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/nlp/space/trainer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    30567 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/nlp/space/trainer/gen_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    29570 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/nlp/space/trainer/intent_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20453 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/nlp/table_question_answering_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1367 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/nlp/text_generation_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7514 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/nlp/text_ranking_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15326 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/nlp/translation_evaluation_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7129 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/nlp_trainer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/trainers/optimizer/
+-rw-r--r--   0 runner    (1001) docker     (122)      210 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/optimizer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1753 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/optimizer/builder.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/trainers/parallel/
+-rw-r--r--   0 runner    (1001) docker     (122)       80 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/parallel/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      681 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/parallel/builder.py
+-rw-r--r--   0 runner    (1001) docker     (122)      754 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/parallel/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    59372 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17846 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/training_args.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/trainers/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11090 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/utils/inference.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1294 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/trainers/utils/log_buffer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)       56 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)   545816 2023-07-31 12:41:36.000000 modelscope-1.8.0rc0/modelscope/utils/ast_index_file.py
+-rw-r--r--   0 runner    (1001) docker     (122)    28920 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/ast_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/utils/audio/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/audio/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11795 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/audio/audio_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2428 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/audio/tts_exceptions.py
+-rw-r--r--   0 runner    (1001) docker     (122)    26390 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/checkpoint.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2573 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/chinese_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    27433 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/config.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1027 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/config_ds.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19245 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/constant.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/utils/cv/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/cv/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    23195 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/cv/image_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/utils/cv/motion_utils/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/cv/motion_utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2307 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/cv/motion_utils/motion_process.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3847 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/cv/motion_utils/plot_script.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4290 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/cv/motion_utils/rotation_conversions.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3145 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/data_collators.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1279 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/data_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3744 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/device.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6522 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/error.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1128 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/file_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4501 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/hf_util.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5939 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/hub.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15900 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/import_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    25766 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/input_output.py
+-rw-r--r--   0 runner    (1001) docker     (122)      296 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/input_output_typing.py
+-rw-r--r--   0 runner    (1001) docker     (122)      478 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/json_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2817 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/logger.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7612 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/megatron_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2335 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5099 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/model_tag.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/utils/nlp/
+-rw-r--r--   0 runner    (1001) docker     (122)      499 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/nlp/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     4424 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/nlp/distributed.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     4526 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/nlp/load_checkpoint.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/utils/nlp/space/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/nlp/space/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1909 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/nlp/space/args.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11818 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/nlp/space/clean_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1439 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/nlp/space/criterions.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11252 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/nlp/space/db_ops.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6174 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/nlp/space/ontology.py
+-rw-r--r--   0 runner    (1001) docker     (122)      197 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/nlp/space/scores.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6356 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/nlp/space/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1054 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/nlp/space/utils_dst.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/modelscope/utils/nlp/space_T_en/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/nlp/space_T_en/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      859 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/nlp/space_T_en/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2554 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/nlp/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    42024 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/plugins.py
+-rw-r--r--   0 runner    (1001) docker     (122)      697 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/pre_compile.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7833 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/registry.py
+-rw-r--r--   0 runner    (1001) docker     (122)    30195 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/regress_test_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6127 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/service_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    25333 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/streaming_output.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2500 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/task_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1593 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/tensor_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12532 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/test_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1209 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/timer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11129 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/torch_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)      597 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/trie.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1696 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/type_assert.py
+-rw-r--r--   0 runner    (1001) docker     (122)      783 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/utils/url_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)      275 2023-07-31 12:40:32.000000 modelscope-1.8.0rc0/modelscope/version.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (122)    19508 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (122)   138147 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (122)        1 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (122)       59 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope.egg-info/entry_points.txt
+-rw-r--r--   0 runner    (1001) docker     (122)        1 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope.egg-info/not-zip-safe
+-rw-r--r--   0 runner    (1001) docker     (122)     5022 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (122)       11 2023-07-31 12:41:37.000000 modelscope-1.8.0rc0/modelscope.egg-info/top_level.txt
+-rw-r--r--   0 runner    (1001) docker     (122)       38 2023-07-31 12:41:38.000000 modelscope-1.8.0rc0/setup.cfg
```

### Comparing `modelscope-1.8.0/PKG-INFO` & `modelscope-1.8.0rc0/README.md`

 * *Files 2% similar despite different names*

```diff
@@ -1,36 +1,7 @@
-Metadata-Version: 2.1
-Name: modelscope
-Version: 1.8.0
-Summary: ModelScope: bring the notion of Model-as-a-Service to life.
-Home-page: https://github.com/modelscope/modelscope
-Author: ModelScope team
-Author-email: contact@modelscope.cn
-License: Apache License 2.0
-Keywords: python,nlp,science,cv,speech,multi-modal
-Classifier: Development Status :: 4 - Beta
-Classifier: License :: OSI Approved :: Apache Software License
-Classifier: Operating System :: OS Independent
-Classifier: Programming Language :: Python :: 3
-Classifier: Programming Language :: Python :: 3.7
-Classifier: Programming Language :: Python :: 3.8
-Classifier: Programming Language :: Python :: 3.9
-Classifier: Programming Language :: Python :: 3.10
-Description-Content-Type: text/markdown
-Provides-Extra: audio
-Provides-Extra: cv
-Provides-Extra: multi-modal
-Provides-Extra: nlp
-Provides-Extra: science
-Provides-Extra: audio_asr
-Provides-Extra: audio_kws
-Provides-Extra: audio_signal
-Provides-Extra: audio_tts
-Provides-Extra: all
-
 
 <p align="center">
     <br>
     <img src="https://modelscope.oss-cn-beijing.aliyuncs.com/modelscope.gif" width="400"/>
     <br>
 <p>
```

#### html2text {}

```diff
@@ -1,21 +1,7 @@
-Metadata-Version: 2.1 Name: modelscope Version: 1.8.0 Summary: ModelScope:
-bring the notion of Model-as-a-Service to life. Home-page: https://github.com/
-modelscope/modelscope Author: ModelScope team Author-email:
-contact@modelscope.cn License: Apache License 2.0 Keywords:
-python,nlp,science,cv,speech,multi-modal Classifier: Development Status :: 4 -
-Beta Classifier: License :: OSI Approved :: Apache Software License Classifier:
-Operating System :: OS Independent Classifier: Programming Language :: Python
-:: 3 Classifier: Programming Language :: Python :: 3.7 Classifier: Programming
-Language :: Python :: 3.8 Classifier: Programming Language :: Python :: 3.9
-Classifier: Programming Language :: Python :: 3.10 Description-Content-Type:
-text/markdown Provides-Extra: audio Provides-Extra: cv Provides-Extra: multi-
-modal Provides-Extra: nlp Provides-Extra: science Provides-Extra: audio_asr
-Provides-Extra: audio_kws Provides-Extra: audio_signal Provides-Extra:
-audio_tts Provides-Extra: all
 
        [https://modelscope.oss-cn-beijing.aliyuncs.com/modelscope.gif]
  [![PyPI](https://img.shields.io/pypi/v/modelscope)](https://pypi.org/project/
   modelscope/)  [![license](https://img.shields.io/github/license/modelscope/
 modelscope.svg)](https://github.com/modelscope/modelscope/blob/master/LICENSE)
        [![open issues](https://isitmaintained.com/badge/open/modelscope/
   modelscope.svg)](https://github.com/modelscope/modelscope/issues) [![GitHub
```

### Comparing `modelscope-1.8.0/README.md` & `modelscope-1.8.0rc0/PKG-INFO`

 * *Files 18% similar despite different names*

```diff
@@ -1,300 +1,330 @@
-
-<p align="center">
-    <br>
-    <img src="https://modelscope.oss-cn-beijing.aliyuncs.com/modelscope.gif" width="400"/>
-    <br>
-<p>
-
-<div align="center">
-
-[![PyPI](https://img.shields.io/pypi/v/modelscope)](https://pypi.org/project/modelscope/)
-<!-- [![Documentation Status](https://readthedocs.org/projects/easy-cv/badge/?version=latest)](https://easy-cv.readthedocs.io/en/latest/) -->
-[![license](https://img.shields.io/github/license/modelscope/modelscope.svg)](https://github.com/modelscope/modelscope/blob/master/LICENSE)
-[![open issues](https://isitmaintained.com/badge/open/modelscope/modelscope.svg)](https://github.com/modelscope/modelscope/issues)
-[![GitHub pull-requests](https://img.shields.io/github/issues-pr/modelscope/modelscope.svg)](https://GitHub.com/modelscope/modelscope/pull/)
-[![GitHub latest commit](https://badgen.net/github/last-commit/modelscope/modelscope)](https://GitHub.com/modelscope/modelscope/commit/)
-[![Leaderboard](https://img.shields.io/badge/ModelScope-Check%20Your%20Contribution-orange)](https://opensource.alibaba.com/contribution_leaderboard/details?projectValue=modelscope)
-
-<!-- [![GitHub contributors](https://img.shields.io/github/contributors/modelscope/modelscope.svg)](https://GitHub.com/modelscope/modelscope/graphs/contributors/) -->
-<!-- [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com) -->
-
-<h4 align="center">
-    <p>
-        <b>English</b> |
-        <a href="https://github.com/modelscope/modelscope/blob/master/README_zh.md">中文</a> |
-        <a href="https://github.com/modelscope/modelscope/blob/master/README_ja.md">日本語</a>
-    <p>
-</h4>
-
-
-</div>
-
-# Introduction
-
-[ModelScope]( https://www.modelscope.cn) is built upon the notion of “Model-as-a-Service” (MaaS). It seeks to bring together most advanced machine learning models from the AI community, and streamlines the process of leveraging AI models in real-world applications. The core ModelScope library open-sourced in this repository provides the interfaces and implementations that allow developers to perform  model inference, training and evaluation.
-
-
-In particular, with rich layers of API-abstraction, the ModelScope library offers unified experience to explore state-of-the-art models spanning across domains such as CV, NLP, Speech, Multi-Modality, and Scientific-computation. Model contributors of different areas can integrate models into the ModelScope ecosystem through the layered-APIs, allowing easy and unified access to their models. Once integrated, model inference, fine-tuning, and evaluations can be done with only a few lines of codes. In the meantime, flexibilities are also provided so that different components in the model applications can be customized wherever necessary.
-
-Apart from harboring implementations of a wide range of different models, ModelScope library also enables the necessary interactions with ModelScope backend services, particularly with the Model-Hub and Dataset-Hub. Such interactions facilitate management of  various entities (models and datasets) to be performed seamlessly under-the-hood, including entity lookup, version control, cache management, and many others.
-
-# Models and Online Accessibility
-
-Hundreds of models are made publicly available on [ModelScope]( https://www.modelscope.cn)  (700+ and counting), covering the latest development in areas such as NLP, CV, Audio, Multi-modality, and AI for Science, etc. Many of these models represent the SOTA in their specific fields, and made their open-sourced debut on ModelScope. Users can visit ModelScope([modelscope.cn](http://www.modelscope.cn)) and experience first-hand how these models perform via online experience, with just a few clicks. Immediate developer-experience is also possible through the ModelScope Notebook, which is backed by ready-to-use CPU/GPU development environment in the cloud - only one click away on [ModelScope](https://www.modelscope.cn).
-
-
-<p align="center">
-    <br>
-    <img src="data/resource/inference.gif" width="1024"/>
-    <br>
-<p>
-
-Some representative examples include:
-
-NLP:
-
-* [nlp_gpt3_text-generation_2.7B](https://modelscope.cn/models/damo/nlp_gpt3_text-generation_2.7B)
-
-* [ChatYuan-large](https://modelscope.cn/models/ClueAI/ChatYuan-large)
-
-* [mengzi-t5-base](https://modelscope.cn/models/langboat/mengzi-t5-base)
-
-* [nlp_csanmt_translation_en2zh](https://modelscope.cn/models/damo/nlp_csanmt_translation_en2zh)
-
-* [nlp_raner_named-entity-recognition_chinese-base-news](https://modelscope.cn/models/damo/nlp_raner_named-entity-recognition_chinese-base-news)
-
-* [nlp_structbert_word-segmentation_chinese-base](https://modelscope.cn/models/damo/nlp_structbert_word-segmentation_chinese-base)
-
-* [Erlangshen-RoBERTa-330M-Sentiment](https://modelscope.cn/models/fengshenbang/Erlangshen-RoBERTa-330M-Sentiment)
-
-* [nlp_convai_text2sql_pretrain_cn](https://modelscope.cn/models/damo/nlp_convai_text2sql_pretrain_cn)
-
-Multi-Modal:
-
-* [multi-modal_clip-vit-base-patch16_zh](https://modelscope.cn/models/damo/multi-modal_clip-vit-base-patch16_zh)
-
-* [ofa_pretrain_base_zh](https://modelscope.cn/models/damo/ofa_pretrain_base_zh)
-
-* [Taiyi-Stable-Diffusion-1B-Chinese-v0.1](https://modelscope.cn/models/fengshenbang/Taiyi-Stable-Diffusion-1B-Chinese-v0.1)
-
-* [mplug_visual-question-answering_coco_large_en](https://modelscope.cn/models/damo/mplug_visual-question-answering_coco_large_en)
-
-CV:
-
-* [cv_controlnet_controllable-image-generation_nine-annotators](https://modelscope.cn/models/dienstag/cv_controlnet_controllable-image-generation_nine-annotators/summary)
-
-* [cv_tinynas_object-detection_damoyolo](https://modelscope.cn/models/damo/cv_tinynas_object-detection_damoyolo)
-
-* [cv_unet_person-image-cartoon_compound-models](https://modelscope.cn/models/damo/cv_unet_person-image-cartoon_compound-models)
-
-* [cv_convnextTiny_ocr-recognition-general_damo](https://modelscope.cn/models/damo/cv_convnextTiny_ocr-recognition-general_damo)
-
-* [cv_resnet18_human-detection](https://modelscope.cn/models/damo/cv_resnet18_human-detection)
-
-* [cv_resnet50_face-detection_retinaface](https://modelscope.cn/models/damo/cv_resnet50_face-detection_retinaface)
-
-* [cv_unet_image-matting](https://modelscope.cn/models/damo/cv_unet_image-matting)
-
-* [cv_F3Net_product-segmentation](https://modelscope.cn/models/damo/cv_F3Net_product-segmentation)
-
-* [cv_resnest101_general_recognition](https://modelscope.cn/models/damo/cv_resnest101_general_recognition)
-
-
-Audio:
-
-* [speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch](https://modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch)
-
-* [speech_sambert-hifigan_tts_zh-cn_16k](https://modelscope.cn/models/damo/speech_sambert-hifigan_tts_zh-cn_16k)
-
-* [speech_charctc_kws_phone-xiaoyun](https://modelscope.cn/models/damo/speech_charctc_kws_phone-xiaoyun)
-
-* [u2pp_conformer-asr-cn-16k-online](https://modelscope.cn/models/wenet/u2pp_conformer-asr-cn-16k-online)
-
-* [speech_fsmn_vad_zh-cn-16k-common-pytorch](https://modelscope.cn/models/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch/summary)
-
-* [punc_ct-transformer_zh-cn-common-vocab272727-pytorch](https://modelscope.cn/models/damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch/summary)
-
-* [speech_frcrn_ans_cirm_16k](https://modelscope.cn/models/damo/speech_frcrn_ans_cirm_16k)
-
-* [speech_dfsmn_aec_psm_16k](https://modelscope.cn/models/damo/speech_dfsmn_aec_psm_16k)
-
-
-
-AI for Science:
-
-* [uni-fold-monomer](https://modelscope.cn/models/DPTech/uni-fold-monomer/summary)
-
-* [uni-fold-multimer](https://modelscope.cn/models/DPTech/uni-fold-multimer/summary)
-
-**Note:** Most models on ModelScope are public and can be downloaded without account registration on modelscope website([www.modelscope.cn](www.modelscope.cn)), please refer to instructions for [model download](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8B%E8%BD%BD), for dowloading models with api provided by modelscope library or git.
-
-# QuickTour
-
-We provide unified interface for inference using `pipeline`, fine-tuning and evaluation using `Trainer` for different tasks.
-
-For any given task with any type of input (image, text, audio, video...), inference pipeline can be implemented with only a few lines of code, which will automatically load the underlying model to get inference result, as is exemplified below:
-
-```python
->>> from modelscope.pipelines import pipeline
->>> word_segmentation = pipeline('word-segmentation',model='damo/nlp_structbert_word-segmentation_chinese-base')
->>> word_segmentation('今天天气不错，适合出去游玩')
-{'output': '今天 天气 不错 ， 适合 出去 游玩'}
-```
-
-Given an image, portrait matting (aka. background-removal) can be accomplished with the following code snippet:
-
-![image](data/resource/portrait_input.png)
-
-```python
->>> import cv2
->>> from modelscope.pipelines import pipeline
-
->>> portrait_matting = pipeline('portrait-matting')
->>> result = portrait_matting('https://modelscope.oss-cn-beijing.aliyuncs.com/test/images/image_matting.png')
->>> cv2.imwrite('result.png', result['output_img'])
-```
-
-The output image with the background removed is:
-![image](data/resource/portrait_output.png)
-
-
-Fine-tuning and evaluation can also be done with a few more lines of code to set up training dataset and trainer, with the heavy-lifting work of training and evaluation a model encapsulated in the implementation of  `traner.train()` and
-`trainer.evaluate()`  interfaces.
-
-For example, the gpt3 base model (1.3B) can be fine-tuned with the chinese-poetry dataset, resulting in a model that can be used for chinese-poetry generation.
-
-```python
->>> from modelscope.metainfo import Trainers
->>> from modelscope.msdatasets import MsDataset
->>> from modelscope.trainers import build_trainer
-
->>> train_dataset = MsDataset.load('chinese-poetry-collection', split='train'). remap_columns({'text1': 'src_txt'})
->>> eval_dataset = MsDataset.load('chinese-poetry-collection', split='test').remap_columns({'text1': 'src_txt'})
->>> max_epochs = 10
->>> tmp_dir = './gpt3_poetry'
-
->>> kwargs = dict(
-     model='damo/nlp_gpt3_text-generation_1.3B',
-     train_dataset=train_dataset,
-     eval_dataset=eval_dataset,
-     max_epochs=max_epochs,
-     work_dir=tmp_dir)
-
->>> trainer = build_trainer(name=Trainers.gpt3_trainer, default_args=kwargs)
->>> trainer.train()
-```
-
-# Why should I use ModelScope library
-
-1. A unified and concise user interface is abstracted for different tasks and different models. Model inferences and training can be implemented by as few as 3 and 10 lines of code, respectively. It is convenient for users to explore models in different fields in the ModelScope community. All models integrated into ModelScope are ready to use, which makes it easy to get started with AI, in both educational and industrial settings.
-
-2. ModelScope offers a model-centric development and application experience. It streamlines the support for model training, inference, export and deployment, and facilitates users to build their own MLOps based on the ModelScope ecosystem.
-
-3. For the model inference and training process, a modular design is put in place, and a wealth of functional module implementations are provided, which is convenient for users to customize their own model inference, training and other processes.
-
-4. For distributed model training, especially for large models, it provides rich training strategy support, including data parallel, model parallel, hybrid parallel and so on.
-
-# Installation
-
-## Docker
-
-ModelScope Library currently supports popular deep learning framework for model training and inference, including PyTorch, TensorFlow and ONNX. All releases are tested and run on Python 3.7+, Pytorch 1.8+, Tensorflow1.15 or Tensorflow2.0+.
-
-To allow out-of-box usage for all the models on ModelScope, official docker images are provided for all releases. Based on the docker image, developers can skip all environment installation and configuration and use it directly. Currently, the latest version of the CPU image and GPU image can be obtained from:
-
-CPU docker image
-```shell
-# py37
-registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-py37-torch1.11.0-tf1.15.5-1.6.1
-
-# py38
-registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-py38-torch1.11.0-tf1.15.5-1.6.1
-```
-
-GPU docker image
-```shell
-# py37
-registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.3.0-py37-torch1.11.0-tf1.15.5-1.6.1
-
-# py38
-registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.3.0-py38-torch1.11.0-tf1.15.5-1.6.1
-```
-
-## Setup Local Python Environment
-
-One can also set up local ModelScope environment using pip and conda.  We suggest [anaconda](https://docs.anaconda.com/anaconda/install/) for creating local python environment:
-
-```shell
-conda create -n modelscope python=3.7
-conda activate modelscope
-```
-
-PyTorch or TensorFlow can be installed separately according to each model's requirements.
-* Install pytorch [doc](https://pytorch.org/get-started/locally/)
-* Install tensorflow [doc](https://www.tensorflow.org/install/pip)
-
-After installing the necessary machine-learning framework, you can install modelscope library as follows:
-
-If you only want to play around with the modelscope framework, of trying out model/dataset download, you can install the core modelscope components:
-```shell
-pip install modelscope
-```
-
-If you want to use multi-modal models:
-```shell
-pip install modelscope[multi-modal]
-```
-
-If you want to use nlp models:
-```shell
-pip install modelscope[nlp] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html
-```
-
-If you want to use cv models:
-```shell
-pip install modelscope[cv] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html
-```
-
-If you want to use audio models:
-```shell
-pip install modelscope[audio] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html
-```
-
-If you want to use science models:
-```shell
-pip install modelscope[science] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html
-```
-
-`Notes`:
-1. Currently, some audio-task models only support python3.7, tensorflow1.15.4 Linux environments. Most other models can be installed and used on Windows and Mac (x86).
-
-2. Some models in the audio field use the third-party library SoundFile for wav file processing. On the Linux system, users need to manually install libsndfile of SoundFile([doc link](https://github.com/bastibe/python-soundfile#installation)). On Windows and MacOS, it will be installed automatically without user operation. For example, on Ubuntu, you can use following commands:
-    ```shell
-    sudo apt-get update
-    sudo apt-get install libsndfile1
-    ```
-
-3. Some models in computer vision need mmcv-full, you can refer to mmcv [installation guide](https://github.com/open-mmlab/mmcv#installation), a minimal installation is as follows:
-
-    ```shell
-    pip uninstall mmcv # if you have installed mmcv, uninstall it
-    pip install -U openmim
-    mim install mmcv-full
-    ```
-
-
-
-# Learn More
-
-We  provide additional documentations including:
-* [More detailed Installation Guide](https://modelscope.cn/docs/%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85)
-* [Introduction to tasks](https://modelscope.cn/docs/%E4%BB%BB%E5%8A%A1%E7%9A%84%E4%BB%8B%E7%BB%8D)
-* [Use pipeline for model inference](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8E%A8%E7%90%86Pipeline)
-* [Finetuning example](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83Train)
-* [Preprocessing of data](https://modelscope.cn/docs/%E6%95%B0%E6%8D%AE%E7%9A%84%E9%A2%84%E5%A4%84%E7%90%86)
-* [Evaluation](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0)
-* [Contribute your own model to ModelScope](https://modelscope.cn/docs/ModelScope%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%85%A5%E6%B5%81%E7%A8%8B%E6%A6%82%E8%A7%88)
-
-# License
-
-This project is licensed under the [Apache License (Version 2.0)](https://github.com/modelscope/modelscope/blob/master/LICENSE).
+Metadata-Version: 2.1
+Name: modelscope
+Version: 1.8.0rc0
+Summary: ModelScope: bring the notion of Model-as-a-Service to life.
+Home-page: https://github.com/modelscope/modelscope
+Author: ModelScope team
+Author-email: contact@modelscope.cn
+License: Apache License 2.0
+Description: 
+        <p align="center">
+            <br>
+            <img src="https://modelscope.oss-cn-beijing.aliyuncs.com/modelscope.gif" width="400"/>
+            <br>
+        <p>
+        
+        <div align="center">
+        
+        [![PyPI](https://img.shields.io/pypi/v/modelscope)](https://pypi.org/project/modelscope/)
+        <!-- [![Documentation Status](https://readthedocs.org/projects/easy-cv/badge/?version=latest)](https://easy-cv.readthedocs.io/en/latest/) -->
+        [![license](https://img.shields.io/github/license/modelscope/modelscope.svg)](https://github.com/modelscope/modelscope/blob/master/LICENSE)
+        [![open issues](https://isitmaintained.com/badge/open/modelscope/modelscope.svg)](https://github.com/modelscope/modelscope/issues)
+        [![GitHub pull-requests](https://img.shields.io/github/issues-pr/modelscope/modelscope.svg)](https://GitHub.com/modelscope/modelscope/pull/)
+        [![GitHub latest commit](https://badgen.net/github/last-commit/modelscope/modelscope)](https://GitHub.com/modelscope/modelscope/commit/)
+        [![Leaderboard](https://img.shields.io/badge/ModelScope-Check%20Your%20Contribution-orange)](https://opensource.alibaba.com/contribution_leaderboard/details?projectValue=modelscope)
+        
+        <!-- [![GitHub contributors](https://img.shields.io/github/contributors/modelscope/modelscope.svg)](https://GitHub.com/modelscope/modelscope/graphs/contributors/) -->
+        <!-- [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com) -->
+        
+        <h4 align="center">
+            <p>
+                <b>English</b> |
+                <a href="https://github.com/modelscope/modelscope/blob/master/README_zh.md">中文</a> |
+                <a href="https://github.com/modelscope/modelscope/blob/master/README_ja.md">日本語</a>
+            <p>
+        </h4>
+        
+        
+        </div>
+        
+        # Introduction
+        
+        [ModelScope]( https://www.modelscope.cn) is built upon the notion of “Model-as-a-Service” (MaaS). It seeks to bring together most advanced machine learning models from the AI community, and streamlines the process of leveraging AI models in real-world applications. The core ModelScope library open-sourced in this repository provides the interfaces and implementations that allow developers to perform  model inference, training and evaluation.
+        
+        
+        In particular, with rich layers of API-abstraction, the ModelScope library offers unified experience to explore state-of-the-art models spanning across domains such as CV, NLP, Speech, Multi-Modality, and Scientific-computation. Model contributors of different areas can integrate models into the ModelScope ecosystem through the layered-APIs, allowing easy and unified access to their models. Once integrated, model inference, fine-tuning, and evaluations can be done with only a few lines of codes. In the meantime, flexibilities are also provided so that different components in the model applications can be customized wherever necessary.
+        
+        Apart from harboring implementations of a wide range of different models, ModelScope library also enables the necessary interactions with ModelScope backend services, particularly with the Model-Hub and Dataset-Hub. Such interactions facilitate management of  various entities (models and datasets) to be performed seamlessly under-the-hood, including entity lookup, version control, cache management, and many others.
+        
+        # Models and Online Accessibility
+        
+        Hundreds of models are made publicly available on [ModelScope]( https://www.modelscope.cn)  (700+ and counting), covering the latest development in areas such as NLP, CV, Audio, Multi-modality, and AI for Science, etc. Many of these models represent the SOTA in their specific fields, and made their open-sourced debut on ModelScope. Users can visit ModelScope([modelscope.cn](http://www.modelscope.cn)) and experience first-hand how these models perform via online experience, with just a few clicks. Immediate developer-experience is also possible through the ModelScope Notebook, which is backed by ready-to-use CPU/GPU development environment in the cloud - only one click away on [ModelScope](https://www.modelscope.cn).
+        
+        
+        <p align="center">
+            <br>
+            <img src="data/resource/inference.gif" width="1024"/>
+            <br>
+        <p>
+        
+        Some representative examples include:
+        
+        NLP:
+        
+        * [nlp_gpt3_text-generation_2.7B](https://modelscope.cn/models/damo/nlp_gpt3_text-generation_2.7B)
+        
+        * [ChatYuan-large](https://modelscope.cn/models/ClueAI/ChatYuan-large)
+        
+        * [mengzi-t5-base](https://modelscope.cn/models/langboat/mengzi-t5-base)
+        
+        * [nlp_csanmt_translation_en2zh](https://modelscope.cn/models/damo/nlp_csanmt_translation_en2zh)
+        
+        * [nlp_raner_named-entity-recognition_chinese-base-news](https://modelscope.cn/models/damo/nlp_raner_named-entity-recognition_chinese-base-news)
+        
+        * [nlp_structbert_word-segmentation_chinese-base](https://modelscope.cn/models/damo/nlp_structbert_word-segmentation_chinese-base)
+        
+        * [Erlangshen-RoBERTa-330M-Sentiment](https://modelscope.cn/models/fengshenbang/Erlangshen-RoBERTa-330M-Sentiment)
+        
+        * [nlp_convai_text2sql_pretrain_cn](https://modelscope.cn/models/damo/nlp_convai_text2sql_pretrain_cn)
+        
+        Multi-Modal:
+        
+        * [multi-modal_clip-vit-base-patch16_zh](https://modelscope.cn/models/damo/multi-modal_clip-vit-base-patch16_zh)
+        
+        * [ofa_pretrain_base_zh](https://modelscope.cn/models/damo/ofa_pretrain_base_zh)
+        
+        * [Taiyi-Stable-Diffusion-1B-Chinese-v0.1](https://modelscope.cn/models/fengshenbang/Taiyi-Stable-Diffusion-1B-Chinese-v0.1)
+        
+        * [mplug_visual-question-answering_coco_large_en](https://modelscope.cn/models/damo/mplug_visual-question-answering_coco_large_en)
+        
+        CV:
+        
+        * [cv_controlnet_controllable-image-generation_nine-annotators](https://modelscope.cn/models/dienstag/cv_controlnet_controllable-image-generation_nine-annotators/summary)
+        
+        * [cv_tinynas_object-detection_damoyolo](https://modelscope.cn/models/damo/cv_tinynas_object-detection_damoyolo)
+        
+        * [cv_unet_person-image-cartoon_compound-models](https://modelscope.cn/models/damo/cv_unet_person-image-cartoon_compound-models)
+        
+        * [cv_convnextTiny_ocr-recognition-general_damo](https://modelscope.cn/models/damo/cv_convnextTiny_ocr-recognition-general_damo)
+        
+        * [cv_resnet18_human-detection](https://modelscope.cn/models/damo/cv_resnet18_human-detection)
+        
+        * [cv_resnet50_face-detection_retinaface](https://modelscope.cn/models/damo/cv_resnet50_face-detection_retinaface)
+        
+        * [cv_unet_image-matting](https://modelscope.cn/models/damo/cv_unet_image-matting)
+        
+        * [cv_F3Net_product-segmentation](https://modelscope.cn/models/damo/cv_F3Net_product-segmentation)
+        
+        * [cv_resnest101_general_recognition](https://modelscope.cn/models/damo/cv_resnest101_general_recognition)
+        
+        
+        Audio:
+        
+        * [speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch](https://modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch)
+        
+        * [speech_sambert-hifigan_tts_zh-cn_16k](https://modelscope.cn/models/damo/speech_sambert-hifigan_tts_zh-cn_16k)
+        
+        * [speech_charctc_kws_phone-xiaoyun](https://modelscope.cn/models/damo/speech_charctc_kws_phone-xiaoyun)
+        
+        * [u2pp_conformer-asr-cn-16k-online](https://modelscope.cn/models/wenet/u2pp_conformer-asr-cn-16k-online)
+        
+        * [speech_fsmn_vad_zh-cn-16k-common-pytorch](https://modelscope.cn/models/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch/summary)
+        
+        * [punc_ct-transformer_zh-cn-common-vocab272727-pytorch](https://modelscope.cn/models/damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch/summary)
+        
+        * [speech_frcrn_ans_cirm_16k](https://modelscope.cn/models/damo/speech_frcrn_ans_cirm_16k)
+        
+        * [speech_dfsmn_aec_psm_16k](https://modelscope.cn/models/damo/speech_dfsmn_aec_psm_16k)
+        
+        
+        
+        AI for Science:
+        
+        * [uni-fold-monomer](https://modelscope.cn/models/DPTech/uni-fold-monomer/summary)
+        
+        * [uni-fold-multimer](https://modelscope.cn/models/DPTech/uni-fold-multimer/summary)
+        
+        **Note:** Most models on ModelScope are public and can be downloaded without account registration on modelscope website([www.modelscope.cn](www.modelscope.cn)), please refer to instructions for [model download](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8B%E8%BD%BD), for dowloading models with api provided by modelscope library or git.
+        
+        # QuickTour
+        
+        We provide unified interface for inference using `pipeline`, fine-tuning and evaluation using `Trainer` for different tasks.
+        
+        For any given task with any type of input (image, text, audio, video...), inference pipeline can be implemented with only a few lines of code, which will automatically load the underlying model to get inference result, as is exemplified below:
+        
+        ```python
+        >>> from modelscope.pipelines import pipeline
+        >>> word_segmentation = pipeline('word-segmentation',model='damo/nlp_structbert_word-segmentation_chinese-base')
+        >>> word_segmentation('今天天气不错，适合出去游玩')
+        {'output': '今天 天气 不错 ， 适合 出去 游玩'}
+        ```
+        
+        Given an image, portrait matting (aka. background-removal) can be accomplished with the following code snippet:
+        
+        ![image](data/resource/portrait_input.png)
+        
+        ```python
+        >>> import cv2
+        >>> from modelscope.pipelines import pipeline
+        
+        >>> portrait_matting = pipeline('portrait-matting')
+        >>> result = portrait_matting('https://modelscope.oss-cn-beijing.aliyuncs.com/test/images/image_matting.png')
+        >>> cv2.imwrite('result.png', result['output_img'])
+        ```
+        
+        The output image with the background removed is:
+        ![image](data/resource/portrait_output.png)
+        
+        
+        Fine-tuning and evaluation can also be done with a few more lines of code to set up training dataset and trainer, with the heavy-lifting work of training and evaluation a model encapsulated in the implementation of  `traner.train()` and
+        `trainer.evaluate()`  interfaces.
+        
+        For example, the gpt3 base model (1.3B) can be fine-tuned with the chinese-poetry dataset, resulting in a model that can be used for chinese-poetry generation.
+        
+        ```python
+        >>> from modelscope.metainfo import Trainers
+        >>> from modelscope.msdatasets import MsDataset
+        >>> from modelscope.trainers import build_trainer
+        
+        >>> train_dataset = MsDataset.load('chinese-poetry-collection', split='train'). remap_columns({'text1': 'src_txt'})
+        >>> eval_dataset = MsDataset.load('chinese-poetry-collection', split='test').remap_columns({'text1': 'src_txt'})
+        >>> max_epochs = 10
+        >>> tmp_dir = './gpt3_poetry'
+        
+        >>> kwargs = dict(
+             model='damo/nlp_gpt3_text-generation_1.3B',
+             train_dataset=train_dataset,
+             eval_dataset=eval_dataset,
+             max_epochs=max_epochs,
+             work_dir=tmp_dir)
+        
+        >>> trainer = build_trainer(name=Trainers.gpt3_trainer, default_args=kwargs)
+        >>> trainer.train()
+        ```
+        
+        # Why should I use ModelScope library
+        
+        1. A unified and concise user interface is abstracted for different tasks and different models. Model inferences and training can be implemented by as few as 3 and 10 lines of code, respectively. It is convenient for users to explore models in different fields in the ModelScope community. All models integrated into ModelScope are ready to use, which makes it easy to get started with AI, in both educational and industrial settings.
+        
+        2. ModelScope offers a model-centric development and application experience. It streamlines the support for model training, inference, export and deployment, and facilitates users to build their own MLOps based on the ModelScope ecosystem.
+        
+        3. For the model inference and training process, a modular design is put in place, and a wealth of functional module implementations are provided, which is convenient for users to customize their own model inference, training and other processes.
+        
+        4. For distributed model training, especially for large models, it provides rich training strategy support, including data parallel, model parallel, hybrid parallel and so on.
+        
+        # Installation
+        
+        ## Docker
+        
+        ModelScope Library currently supports popular deep learning framework for model training and inference, including PyTorch, TensorFlow and ONNX. All releases are tested and run on Python 3.7+, Pytorch 1.8+, Tensorflow1.15 or Tensorflow2.0+.
+        
+        To allow out-of-box usage for all the models on ModelScope, official docker images are provided for all releases. Based on the docker image, developers can skip all environment installation and configuration and use it directly. Currently, the latest version of the CPU image and GPU image can be obtained from:
+        
+        CPU docker image
+        ```shell
+        # py37
+        registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-py37-torch1.11.0-tf1.15.5-1.6.1
+        
+        # py38
+        registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-py38-torch1.11.0-tf1.15.5-1.6.1
+        ```
+        
+        GPU docker image
+        ```shell
+        # py37
+        registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.3.0-py37-torch1.11.0-tf1.15.5-1.6.1
+        
+        # py38
+        registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.3.0-py38-torch1.11.0-tf1.15.5-1.6.1
+        ```
+        
+        ## Setup Local Python Environment
+        
+        One can also set up local ModelScope environment using pip and conda.  We suggest [anaconda](https://docs.anaconda.com/anaconda/install/) for creating local python environment:
+        
+        ```shell
+        conda create -n modelscope python=3.7
+        conda activate modelscope
+        ```
+        
+        PyTorch or TensorFlow can be installed separately according to each model's requirements.
+        * Install pytorch [doc](https://pytorch.org/get-started/locally/)
+        * Install tensorflow [doc](https://www.tensorflow.org/install/pip)
+        
+        After installing the necessary machine-learning framework, you can install modelscope library as follows:
+        
+        If you only want to play around with the modelscope framework, of trying out model/dataset download, you can install the core modelscope components:
+        ```shell
+        pip install modelscope
+        ```
+        
+        If you want to use multi-modal models:
+        ```shell
+        pip install modelscope[multi-modal]
+        ```
+        
+        If you want to use nlp models:
+        ```shell
+        pip install modelscope[nlp] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html
+        ```
+        
+        If you want to use cv models:
+        ```shell
+        pip install modelscope[cv] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html
+        ```
+        
+        If you want to use audio models:
+        ```shell
+        pip install modelscope[audio] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html
+        ```
+        
+        If you want to use science models:
+        ```shell
+        pip install modelscope[science] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html
+        ```
+        
+        `Notes`:
+        1. Currently, some audio-task models only support python3.7, tensorflow1.15.4 Linux environments. Most other models can be installed and used on Windows and Mac (x86).
+        
+        2. Some models in the audio field use the third-party library SoundFile for wav file processing. On the Linux system, users need to manually install libsndfile of SoundFile([doc link](https://github.com/bastibe/python-soundfile#installation)). On Windows and MacOS, it will be installed automatically without user operation. For example, on Ubuntu, you can use following commands:
+            ```shell
+            sudo apt-get update
+            sudo apt-get install libsndfile1
+            ```
+        
+        3. Some models in computer vision need mmcv-full, you can refer to mmcv [installation guide](https://github.com/open-mmlab/mmcv#installation), a minimal installation is as follows:
+        
+            ```shell
+            pip uninstall mmcv # if you have installed mmcv, uninstall it
+            pip install -U openmim
+            mim install mmcv-full
+            ```
+        
+        
+        
+        # Learn More
+        
+        We  provide additional documentations including:
+        * [More detailed Installation Guide](https://modelscope.cn/docs/%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85)
+        * [Introduction to tasks](https://modelscope.cn/docs/%E4%BB%BB%E5%8A%A1%E7%9A%84%E4%BB%8B%E7%BB%8D)
+        * [Use pipeline for model inference](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8E%A8%E7%90%86Pipeline)
+        * [Finetuning example](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83Train)
+        * [Preprocessing of data](https://modelscope.cn/docs/%E6%95%B0%E6%8D%AE%E7%9A%84%E9%A2%84%E5%A4%84%E7%90%86)
+        * [Evaluation](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0)
+        * [Contribute your own model to ModelScope](https://modelscope.cn/docs/ModelScope%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%85%A5%E6%B5%81%E7%A8%8B%E6%A6%82%E8%A7%88)
+        
+        # License
+        
+        This project is licensed under the [Apache License (Version 2.0)](https://github.com/modelscope/modelscope/blob/master/LICENSE).
+        
+Keywords: python,nlp,science,cv,speech,multi-modal
+Platform: UNKNOWN
+Classifier: Development Status :: 4 - Beta
+Classifier: License :: OSI Approved :: Apache Software License
+Classifier: Operating System :: OS Independent
+Classifier: Programming Language :: Python :: 3
+Classifier: Programming Language :: Python :: 3.7
+Classifier: Programming Language :: Python :: 3.8
+Classifier: Programming Language :: Python :: 3.9
+Classifier: Programming Language :: Python :: 3.10
+Description-Content-Type: text/markdown
+Provides-Extra: audio
+Provides-Extra: cv
+Provides-Extra: multi-modal
+Provides-Extra: nlp
+Provides-Extra: science
+Provides-Extra: audio_asr
+Provides-Extra: audio_kws
+Provides-Extra: audio_signal
+Provides-Extra: audio_tts
+Provides-Extra: all
```

#### html2text {}

```diff
@@ -1,7 +1,11 @@
+Metadata-Version: 2.1 Name: modelscope Version: 1.8.0rc0 Summary: ModelScope:
+bring the notion of Model-as-a-Service to life. Home-page: https://github.com/
+modelscope/modelscope Author: ModelScope team Author-email:
+contact@modelscope.cn License: Apache License 2.0 Description:
 
        [https://modelscope.oss-cn-beijing.aliyuncs.com/modelscope.gif]
  [![PyPI](https://img.shields.io/pypi/v/modelscope)](https://pypi.org/project/
   modelscope/)  [![license](https://img.shields.io/github/license/modelscope/
 modelscope.svg)](https://github.com/modelscope/modelscope/blob/master/LICENSE)
        [![open issues](https://isitmaintained.com/badge/open/modelscope/
   modelscope.svg)](https://github.com/modelscope/modelscope/issues) [![GitHub
@@ -202,8 +206,18 @@
 (https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83Train)
 * [Preprocessing of data](https://modelscope.cn/docs/
 %E6%95%B0%E6%8D%AE%E7%9A%84%E9%A2%84%E5%A4%84%E7%90%86) * [Evaluation](https://
 modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0) * [Contribute
 your own model to ModelScope](https://modelscope.cn/docs/
 ModelScope%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%85%A5%E6%B5%81%E7%A8%8B%E6%A6%82%E8%A7%88)
 # License This project is licensed under the [Apache License (Version 2.0)]
-(https://github.com/modelscope/modelscope/blob/master/LICENSE).
+(https://github.com/modelscope/modelscope/blob/master/LICENSE). Keywords:
+python,nlp,science,cv,speech,multi-modal Platform: UNKNOWN Classifier:
+Development Status :: 4 - Beta Classifier: License :: OSI Approved :: Apache
+Software License Classifier: Operating System :: OS Independent Classifier:
+Programming Language :: Python :: 3 Classifier: Programming Language :: Python
+:: 3.7 Classifier: Programming Language :: Python :: 3.8 Classifier:
+Programming Language :: Python :: 3.9 Classifier: Programming Language ::
+Python :: 3.10 Description-Content-Type: text/markdown Provides-Extra: audio
+Provides-Extra: cv Provides-Extra: multi-modal Provides-Extra: nlp Provides-
+Extra: science Provides-Extra: audio_asr Provides-Extra: audio_kws Provides-
+Extra: audio_signal Provides-Extra: audio_tts Provides-Extra: all
```

### Comparing `modelscope-1.8.0/modelscope/__init__.py` & `modelscope-1.8.0rc0/modelscope/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/cli/cli.py` & `modelscope-1.8.0rc0/modelscope/cli/cli.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/cli/download.py` & `modelscope-1.8.0rc0/modelscope/cli/download.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/cli/modelcard.py` & `modelscope-1.8.0rc0/modelscope/cli/modelcard.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/cli/pipeline.py` & `modelscope-1.8.0rc0/modelscope/cli/pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/cli/plugins.py` & `modelscope-1.8.0rc0/modelscope/cli/plugins.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/cli/template/readme.tpl` & `modelscope-1.8.0rc0/modelscope/cli/template/readme.tpl`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/cli/template/template.tpl` & `modelscope-1.8.0rc0/modelscope/cli/template/template.tpl`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/exporters/__init__.py` & `modelscope-1.8.0rc0/modelscope/exporters/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/exporters/audio/ans_dfsmn_exporter.py` & `modelscope-1.8.0rc0/modelscope/exporters/audio/ans_dfsmn_exporter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/exporters/base.py` & `modelscope-1.8.0rc0/modelscope/exporters/base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/exporters/builder.py` & `modelscope-1.8.0rc0/modelscope/exporters/builder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/exporters/cv/__init__.py` & `modelscope-1.8.0rc0/modelscope/exporters/cv/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/exporters/cv/cartoon_translation_exporter.py` & `modelscope-1.8.0rc0/modelscope/exporters/cv/cartoon_translation_exporter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/exporters/cv/face_detection_scrfd_exporter.py` & `modelscope-1.8.0rc0/modelscope/exporters/cv/face_detection_scrfd_exporter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/exporters/cv/object_detection_damoyolo_exporter.py` & `modelscope-1.8.0rc0/modelscope/exporters/cv/object_detection_damoyolo_exporter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/exporters/multi_modal/__init__.py` & `modelscope-1.8.0rc0/modelscope/exporters/multi_modal/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/exporters/multi_modal/stable_diffusion_exporter.py` & `modelscope-1.8.0rc0/modelscope/exporters/multi_modal/stable_diffusion_exporter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/exporters/nlp/__init__.py` & `modelscope-1.8.0rc0/modelscope/exporters/nlp/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/exporters/nlp/csanmt_for_translation_exporter.py` & `modelscope-1.8.0rc0/modelscope/exporters/nlp/csanmt_for_translation_exporter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/exporters/nlp/model_for_token_classification_exporter.py` & `modelscope-1.8.0rc0/modelscope/exporters/nlp/model_for_token_classification_exporter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/exporters/nlp/sbert_for_sequence_classification_exporter.py` & `modelscope-1.8.0rc0/modelscope/exporters/nlp/sbert_for_sequence_classification_exporter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/exporters/nlp/sbert_for_zero_shot_classification_exporter.py` & `modelscope-1.8.0rc0/modelscope/exporters/nlp/sbert_for_zero_shot_classification_exporter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/exporters/tf_model_exporter.py` & `modelscope-1.8.0rc0/modelscope/exporters/tf_model_exporter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/exporters/torch_model_exporter.py` & `modelscope-1.8.0rc0/modelscope/exporters/torch_model_exporter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/fileio/file.py` & `modelscope-1.8.0rc0/modelscope/fileio/file.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/fileio/format/json.py` & `modelscope-1.8.0rc0/modelscope/fileio/format/json.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/fileio/format/jsonplus.py` & `modelscope-1.8.0rc0/modelscope/fileio/format/jsonplus.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/fileio/format/yaml.py` & `modelscope-1.8.0rc0/modelscope/fileio/format/yaml.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/fileio/io.py` & `modelscope-1.8.0rc0/modelscope/fileio/io.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/hub/api.py` & `modelscope-1.8.0rc0/modelscope/hub/api.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/hub/check_model.py` & `modelscope-1.8.0rc0/modelscope/hub/check_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/hub/constants.py` & `modelscope-1.8.0rc0/modelscope/hub/constants.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/hub/deploy.py` & `modelscope-1.8.0rc0/modelscope/hub/deploy.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/hub/errors.py` & `modelscope-1.8.0rc0/modelscope/hub/errors.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/hub/file_download.py` & `modelscope-1.8.0rc0/modelscope/hub/file_download.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/hub/git.py` & `modelscope-1.8.0rc0/modelscope/hub/git.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/hub/push_to_hub.py` & `modelscope-1.8.0rc0/modelscope/hub/push_to_hub.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/hub/repository.py` & `modelscope-1.8.0rc0/modelscope/hub/repository.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/hub/snapshot_download.py` & `modelscope-1.8.0rc0/modelscope/hub/snapshot_download.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/hub/utils/caching.py` & `modelscope-1.8.0rc0/modelscope/hub/utils/caching.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/hub/utils/utils.py` & `modelscope-1.8.0rc0/modelscope/hub/utils/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/metainfo.py` & `modelscope-1.8.0rc0/modelscope/metainfo.py`

 * *Files 0% similar despite different names*

```diff
@@ -169,15 +169,14 @@
     plug_mental = 'plug-mental'
     doc2bot = 'doc2bot'
     peer = 'peer'
     llama = 'llama'
     llama2 = 'llama2'
     chatglm_6b = 'chatglm6b'
     chatglm2_6b = 'chatglm2-6b'
-    qwen_7b = 'qwen-7b'
 
     # audio models
     sambert_hifigan = 'sambert-hifigan'
     speech_frcrn_ans_cirm_16k = 'speech_frcrn_ans_cirm_16k'
     speech_dfsmn_ans = 'speech_dfsmn_ans'
     speech_dfsmn_kws_char_farfield = 'speech_dfsmn_kws_char_farfield'
     speech_dfsmn_kws_char_farfield_iot = 'speech_dfsmn_kws_char_farfield_iot'
```

### Comparing `modelscope-1.8.0/modelscope/metrics/__init__.py` & `modelscope-1.8.0rc0/modelscope/metrics/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/metrics/accuracy_metric.py` & `modelscope-1.8.0rc0/modelscope/metrics/accuracy_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/metrics/action_detection_evaluator.py` & `modelscope-1.8.0rc0/modelscope/metrics/action_detection_evaluator.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/metrics/audio_noise_metric.py` & `modelscope-1.8.0rc0/modelscope/metrics/audio_noise_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/metrics/base.py` & `modelscope-1.8.0rc0/modelscope/metrics/base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/metrics/bleu_metric.py` & `modelscope-1.8.0rc0/modelscope/metrics/bleu_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/metrics/builder.py` & `modelscope-1.8.0rc0/modelscope/metrics/builder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/metrics/ciderD/ciderD.py` & `modelscope-1.8.0rc0/modelscope/metrics/ciderD/ciderD.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/metrics/ciderD/ciderD_scorer.py` & `modelscope-1.8.0rc0/modelscope/metrics/ciderD/ciderD_scorer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/metrics/image_color_enhance_metric.py` & `modelscope-1.8.0rc0/modelscope/metrics/image_color_enhance_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/metrics/image_colorization_metric.py` & `modelscope-1.8.0rc0/modelscope/metrics/image_colorization_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/metrics/image_denoise_metric.py` & `modelscope-1.8.0rc0/modelscope/metrics/image_denoise_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/metrics/image_inpainting_metric.py` & `modelscope-1.8.0rc0/modelscope/metrics/image_inpainting_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/metrics/image_instance_segmentation_metric.py` & `modelscope-1.8.0rc0/modelscope/metrics/image_instance_segmentation_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/metrics/image_portrait_enhancement_metric.py` & `modelscope-1.8.0rc0/modelscope/metrics/image_portrait_enhancement_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/metrics/image_quality_assessment_degradation_metric.py` & `modelscope-1.8.0rc0/modelscope/metrics/image_quality_assessment_degradation_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/metrics/image_quality_assessment_mos_metric.py` & `modelscope-1.8.0rc0/modelscope/metrics/image_quality_assessment_mos_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/metrics/inbatch_recall_metric.py` & `modelscope-1.8.0rc0/modelscope/metrics/inbatch_recall_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/metrics/loss_metric.py` & `modelscope-1.8.0rc0/modelscope/metrics/loss_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/metrics/map_metric.py` & `modelscope-1.8.0rc0/modelscope/metrics/map_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/metrics/movie_scene_segmentation_metric.py` & `modelscope-1.8.0rc0/modelscope/metrics/movie_scene_segmentation_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/metrics/ned_metric.py` & `modelscope-1.8.0rc0/modelscope/metrics/ned_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/metrics/ocr_recognition_metric.py` & `modelscope-1.8.0rc0/modelscope/metrics/ocr_recognition_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/metrics/ppl_metric.py` & `modelscope-1.8.0rc0/modelscope/metrics/ppl_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/metrics/prediction_saving_wrapper.py` & `modelscope-1.8.0rc0/modelscope/metrics/prediction_saving_wrapper.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/metrics/referring_video_object_segmentation_metric.py` & `modelscope-1.8.0rc0/modelscope/metrics/referring_video_object_segmentation_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/metrics/sequence_classification_metric.py` & `modelscope-1.8.0rc0/modelscope/metrics/sequence_classification_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/metrics/text_generation_metric.py` & `modelscope-1.8.0rc0/modelscope/metrics/text_generation_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/metrics/text_ranking_metric.py` & `modelscope-1.8.0rc0/modelscope/metrics/text_ranking_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/metrics/token_classification_metric.py` & `modelscope-1.8.0rc0/modelscope/metrics/token_classification_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/metrics/translation_evaluation_metric.py` & `modelscope-1.8.0rc0/modelscope/metrics/translation_evaluation_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/metrics/video_frame_interpolation_metric.py` & `modelscope-1.8.0rc0/modelscope/metrics/video_frame_interpolation_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/metrics/video_stabilization_metric.py` & `modelscope-1.8.0rc0/modelscope/metrics/video_stabilization_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/metrics/video_summarization_metric.py` & `modelscope-1.8.0rc0/modelscope/metrics/video_summarization_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/metrics/video_super_resolution_metric/matlab_functions.py` & `modelscope-1.8.0rc0/modelscope/metrics/video_super_resolution_metric/matlab_functions.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/metrics/video_super_resolution_metric/metric_util.py` & `modelscope-1.8.0rc0/modelscope/metrics/video_super_resolution_metric/metric_util.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/metrics/video_super_resolution_metric/niqe.py` & `modelscope-1.8.0rc0/modelscope/metrics/video_super_resolution_metric/niqe.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/metrics/video_super_resolution_metric/video_super_resolution_metric.py` & `modelscope-1.8.0rc0/modelscope/metrics/video_super_resolution_metric/video_super_resolution_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/aec/layers/activations.py` & `modelscope-1.8.0rc0/modelscope/models/audio/aec/layers/activations.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/aec/layers/affine_transform.py` & `modelscope-1.8.0rc0/modelscope/models/audio/aec/layers/affine_transform.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/aec/layers/deep_fsmn.py` & `modelscope-1.8.0rc0/modelscope/models/audio/aec/layers/deep_fsmn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/aec/layers/layer_base.py` & `modelscope-1.8.0rc0/modelscope/models/audio/aec/layers/layer_base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/aec/layers/uni_deep_fsmn.py` & `modelscope-1.8.0rc0/modelscope/models/audio/aec/layers/uni_deep_fsmn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/aec/network/loss.py` & `modelscope-1.8.0rc0/modelscope/models/audio/aec/network/loss.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/aec/network/modulation_loss.py` & `modelscope-1.8.0rc0/modelscope/models/audio/aec/network/modulation_loss.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/aec/network/se_net.py` & `modelscope-1.8.0rc0/modelscope/models/audio/aec/network/se_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/ans/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/audio/ans/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/ans/complex_nn.py` & `modelscope-1.8.0rc0/modelscope/models/audio/ans/complex_nn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/ans/conv_stft.py` & `modelscope-1.8.0rc0/modelscope/models/audio/ans/conv_stft.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/ans/denoise_net.py` & `modelscope-1.8.0rc0/modelscope/models/audio/ans/denoise_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/ans/frcrn.py` & `modelscope-1.8.0rc0/modelscope/models/audio/ans/frcrn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/ans/layers/activations.py` & `modelscope-1.8.0rc0/modelscope/models/audio/ans/layers/activations.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/ans/layers/affine_transform.py` & `modelscope-1.8.0rc0/modelscope/models/audio/ans/layers/affine_transform.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/ans/layers/layer_base.py` & `modelscope-1.8.0rc0/modelscope/models/audio/ans/layers/layer_base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/ans/layers/uni_deep_fsmn.py` & `modelscope-1.8.0rc0/modelscope/models/audio/ans/layers/uni_deep_fsmn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/ans/se_module_complex.py` & `modelscope-1.8.0rc0/modelscope/models/audio/ans/se_module_complex.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/ans/unet.py` & `modelscope-1.8.0rc0/modelscope/models/audio/ans/unet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/asr/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/audio/asr/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/asr/generic_automatic_speech_recognition.py` & `modelscope-1.8.0rc0/modelscope/models/audio/asr/generic_automatic_speech_recognition.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/asr/wenet_automatic_speech_recognition.py` & `modelscope-1.8.0rc0/modelscope/models/audio/asr/wenet_automatic_speech_recognition.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/itn/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/audio/itn/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/itn/generic_inverse_text_processing.py` & `modelscope-1.8.0rc0/modelscope/models/audio/itn/generic_inverse_text_processing.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/kws/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/audio/kws/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/kws/farfield/fsmn.py` & `modelscope-1.8.0rc0/modelscope/models/audio/kws/farfield/fsmn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/kws/farfield/fsmn_sele_v2.py` & `modelscope-1.8.0rc0/modelscope/models/audio/kws/farfield/fsmn_sele_v2.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/kws/farfield/fsmn_sele_v3.py` & `modelscope-1.8.0rc0/modelscope/models/audio/kws/farfield/fsmn_sele_v3.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/kws/farfield/model.py` & `modelscope-1.8.0rc0/modelscope/models/audio/kws/farfield/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/kws/farfield/model_def.py` & `modelscope-1.8.0rc0/modelscope/models/audio/kws/farfield/model_def.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/kws/generic_key_word_spotting.py` & `modelscope-1.8.0rc0/modelscope/models/audio/kws/generic_key_word_spotting.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/kws/nearfield/cmvn.py` & `modelscope-1.8.0rc0/modelscope/models/audio/kws/nearfield/cmvn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/kws/nearfield/fsmn.py` & `modelscope-1.8.0rc0/modelscope/models/audio/kws/nearfield/fsmn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/kws/nearfield/model.py` & `modelscope-1.8.0rc0/modelscope/models/audio/kws/nearfield/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/punc/generic_punctuation.py` & `modelscope-1.8.0rc0/modelscope/models/audio/punc/generic_punctuation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/separation/layer_norm.py` & `modelscope-1.8.0rc0/modelscope/models/audio/separation/layer_norm.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/separation/mossformer.py` & `modelscope-1.8.0rc0/modelscope/models/audio/separation/mossformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/separation/mossformer_block.py` & `modelscope-1.8.0rc0/modelscope/models/audio/separation/mossformer_block.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/separation/mossformer_conv_module.py` & `modelscope-1.8.0rc0/modelscope/models/audio/separation/mossformer_conv_module.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/sv/DTDNN.py` & `modelscope-1.8.0rc0/modelscope/models/audio/sv/DTDNN.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/sv/DTDNN_layers.py` & `modelscope-1.8.0rc0/modelscope/models/audio/sv/DTDNN_layers.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/sv/ERes2Net.py` & `modelscope-1.8.0rc0/modelscope/models/audio/sv/ERes2Net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/sv/ERes2Net_aug.py` & `modelscope-1.8.0rc0/modelscope/models/audio/sv/ERes2Net_aug.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/sv/cluster_backend.py` & `modelscope-1.8.0rc0/modelscope/models/audio/sv/cluster_backend.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/sv/ecapa_tdnn.py` & `modelscope-1.8.0rc0/modelscope/models/audio/sv/ecapa_tdnn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/sv/fusion.py` & `modelscope-1.8.0rc0/modelscope/models/audio/sv/fusion.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/sv/generic_speaker_verification.py` & `modelscope-1.8.0rc0/modelscope/models/audio/sv/generic_speaker_verification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/sv/lanuage_recognition_model.py` & `modelscope-1.8.0rc0/modelscope/models/audio/sv/lanuage_recognition_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/sv/pooling_layers.py` & `modelscope-1.8.0rc0/modelscope/models/audio/sv/pooling_layers.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/sv/rdino.py` & `modelscope-1.8.0rc0/modelscope/models/audio/sv/rdino.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/sv/speaker_change_locator.py` & `modelscope-1.8.0rc0/modelscope/models/audio/sv/speaker_change_locator.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/sv/speaker_diarization_dialogue_detection.py` & `modelscope-1.8.0rc0/modelscope/models/audio/sv/speaker_diarization_dialogue_detection.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/sv/speaker_diarization_semantic_speaker_turn_detection.py` & `modelscope-1.8.0rc0/modelscope/models/audio/sv/speaker_diarization_semantic_speaker_turn_detection.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/tts/sambert_hifi.py` & `modelscope-1.8.0rc0/modelscope/models/audio/tts/sambert_hifi.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/audio/tts/voice.py` & `modelscope-1.8.0rc0/modelscope/models/audio/tts/voice.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/base/base_head.py` & `modelscope-1.8.0rc0/modelscope/models/base/base_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/base/base_model.py` & `modelscope-1.8.0rc0/modelscope/models/base/base_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/base/base_torch_head.py` & `modelscope-1.8.0rc0/modelscope/models/base/base_torch_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/base/base_torch_model.py` & `modelscope-1.8.0rc0/modelscope/models/base/base_torch_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/builder.py` & `modelscope-1.8.0rc0/modelscope/models/builder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/abnormal_object_detection/mmdet_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/abnormal_object_detection/mmdet_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/mask_scoring_roi_head.py` & `modelscope-1.8.0rc0/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/mask_scoring_roi_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/roi_extractors/single_level_roi_extractor.py` & `modelscope-1.8.0rc0/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/roi_extractors/single_level_roi_extractor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/action_detection/action_detection_onnx.py` & `modelscope-1.8.0rc0/modelscope/models/cv/action_detection/action_detection_onnx.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/action_detection/modules/action_detection_pytorch.py` & `modelscope-1.8.0rc0/modelscope/models/cv/action_detection/modules/action_detection_pytorch.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/action_detection/modules/resnet.py` & `modelscope-1.8.0rc0/modelscope/models/cv/action_detection/modules/resnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/action_recognition/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/action_recognition/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/action_recognition/models.py` & `modelscope-1.8.0rc0/modelscope/models/cv/action_recognition/models.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/action_recognition/s3dg.py` & `modelscope-1.8.0rc0/modelscope/models/cv/action_recognition/s3dg.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/action_recognition/tada_convnext.py` & `modelscope-1.8.0rc0/modelscope/models/cv/action_recognition/tada_convnext.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/action_recognition/temporal_patch_shift_transformer.py` & `modelscope-1.8.0rc0/modelscope/models/cv/action_recognition/temporal_patch_shift_transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/animal_recognition/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/animal_recognition/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/animal_recognition/resnet.py` & `modelscope-1.8.0rc0/modelscope/models/cv/animal_recognition/resnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/animal_recognition/splat.py` & `modelscope-1.8.0rc0/modelscope/models/cv/animal_recognition/splat.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/bad_image_detecting/bad_image_detecting.py` & `modelscope-1.8.0rc0/modelscope/models/cv/bad_image_detecting/bad_image_detecting.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/body_2d_keypoints/hrnet_basic_modules.py` & `modelscope-1.8.0rc0/modelscope/models/cv/body_2d_keypoints/hrnet_basic_modules.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/body_2d_keypoints/hrnet_v2.py` & `modelscope-1.8.0rc0/modelscope/models/cv/body_2d_keypoints/hrnet_v2.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/body_2d_keypoints/w48.py` & `modelscope-1.8.0rc0/modelscope/models/cv/body_2d_keypoints/w48.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/body_3d_keypoints/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/body_3d_keypoints/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/body_3d_keypoints/cannonical_pose/body_3d_pose.py` & `modelscope-1.8.0rc0/modelscope/models/cv/body_3d_keypoints/cannonical_pose/body_3d_pose.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/body_3d_keypoints/cannonical_pose/canonical_pose_modules.py` & `modelscope-1.8.0rc0/modelscope/models/cv/body_3d_keypoints/cannonical_pose/canonical_pose_modules.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/body_3d_keypoints/hdformer/backbone.py` & `modelscope-1.8.0rc0/modelscope/models/cv/body_3d_keypoints/hdformer/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/body_3d_keypoints/hdformer/block.py` & `modelscope-1.8.0rc0/modelscope/models/cv/body_3d_keypoints/hdformer/block.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/body_3d_keypoints/hdformer/directed_graph.py` & `modelscope-1.8.0rc0/modelscope/models/cv/body_3d_keypoints/hdformer/directed_graph.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/body_3d_keypoints/hdformer/hdformer.py` & `modelscope-1.8.0rc0/modelscope/models/cv/body_3d_keypoints/hdformer/hdformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/body_3d_keypoints/hdformer/hdformer_detector.py` & `modelscope-1.8.0rc0/modelscope/models/cv/body_3d_keypoints/hdformer/hdformer_detector.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/body_3d_keypoints/hdformer/skeleton.py` & `modelscope-1.8.0rc0/modelscope/models/cv/body_3d_keypoints/hdformer/skeleton.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/cartoon/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/cartoon/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/cartoon/facelib/LK/lk.py` & `modelscope-1.8.0rc0/modelscope/models/cv/cartoon/facelib/LK/lk.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/cartoon/facelib/config.py` & `modelscope-1.8.0rc0/modelscope/models/cv/cartoon/facelib/config.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/cartoon/facelib/face_detector.py` & `modelscope-1.8.0rc0/modelscope/models/cv/cartoon/facelib/face_detector.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/cartoon/facelib/face_landmark.py` & `modelscope-1.8.0rc0/modelscope/models/cv/cartoon/facelib/face_landmark.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/cartoon/facelib/facer.py` & `modelscope-1.8.0rc0/modelscope/models/cv/cartoon/facelib/facer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/cartoon/loss.py` & `modelscope-1.8.0rc0/modelscope/models/cv/cartoon/loss.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/cartoon/model_tf.py` & `modelscope-1.8.0rc0/modelscope/models/cv/cartoon/model_tf.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/cartoon/mtcnn_pytorch/src/align_trans.py` & `modelscope-1.8.0rc0/modelscope/models/cv/cartoon/mtcnn_pytorch/src/align_trans.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/cartoon/mtcnn_pytorch/src/matlab_cp2tform.py` & `modelscope-1.8.0rc0/modelscope/models/cv/cartoon/mtcnn_pytorch/src/matlab_cp2tform.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/cartoon/network.py` & `modelscope-1.8.0rc0/modelscope/models/cv/cartoon/network.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/cartoon/utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/cartoon/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/cmdssl_video_embedding/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/cmdssl_video_embedding/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/cmdssl_video_embedding/c3d.py` & `modelscope-1.8.0rc0/modelscope/models/cv/cmdssl_video_embedding/c3d.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/cmdssl_video_embedding/resnet2p1d.py` & `modelscope-1.8.0rc0/modelscope/models/cv/cmdssl_video_embedding/resnet2p1d.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/cmdssl_video_embedding/resnet3d.py` & `modelscope-1.8.0rc0/modelscope/models/cv/cmdssl_video_embedding/resnet3d.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/annotator/annotator.py` & `modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/annotator/annotator.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/annotator/midas/api.py` & `modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/annotator/midas/api.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/blocks.py` & `modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/blocks.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/dpt_depth.py` & `modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/dpt_depth.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/midas_net.py` & `modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/midas_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/midas_net_custom.py` & `modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/midas_net_custom.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/transforms.py` & `modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/transforms.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/vit.py` & `modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/vit.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/annotator/midas/utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/annotator/midas/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/annotator/mlsd/mbv2_mlsd_large.py` & `modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/annotator/mlsd/mbv2_mlsd_large.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/annotator/mlsd/utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/annotator/mlsd/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/annotator/openpose/body.py` & `modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/annotator/openpose/body.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/annotator/openpose/hand.py` & `modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/annotator/openpose/hand.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/annotator/openpose/model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/annotator/openpose/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/annotator/openpose/util.py` & `modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/annotator/openpose/util.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/controllable_image_generation/controlnet.py` & `modelscope-1.8.0rc0/modelscope/models/cv/controllable_image_generation/controlnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/crowd_counting/cc_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/crowd_counting/cc_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/crowd_counting/hrnet_aspp_relu.py` & `modelscope-1.8.0rc0/modelscope/models/cv/crowd_counting/hrnet_aspp_relu.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_attribute_recognition/fair_face/face_attribute_recognition.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_attribute_recognition/fair_face/face_attribute_recognition.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/mogface/models/detectors.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/mogface/models/detectors.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/mogface/models/mogface.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/mogface/models/mogface.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/mogface/models/mogprednet.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/mogface/models/mogprednet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/mogface/models/resnet.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/mogface/models/resnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/mogface/models/utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/mogface/models/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/mtcnn/models/box_utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/mtcnn/models/box_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/mtcnn/models/detector.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/mtcnn/models/detector.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/mtcnn/models/first_stage.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/mtcnn/models/first_stage.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/mtcnn/models/get_nets.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/mtcnn/models/get_nets.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/peppa_pig_face/LK/lk.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/peppa_pig_face/LK/lk.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/peppa_pig_face/face_detector.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/peppa_pig_face/face_detector.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/peppa_pig_face/face_landmark.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/peppa_pig_face/face_landmark.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/peppa_pig_face/facer.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/peppa_pig_face/facer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/retinaface/detection.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/retinaface/detection.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/retinaface/models/net.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/retinaface/models/net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/retinaface/models/retinaface.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/retinaface/models/retinaface.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/retinaface/utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/retinaface/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/damofd_detect.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/damofd_detect.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/bbox/transforms.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/bbox/transforms.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/post_processing/bbox_nms.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/post_processing/bbox_nms.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/auto_augment.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/auto_augment.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/formating.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/formating.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/loading.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/loading.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/transforms.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/transforms.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/retinaface.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/retinaface.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/master_net.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/master_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/mobilenet.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/mobilenet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/resnet.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/resnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/dense_heads/scrfd_head.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/dense_heads/scrfd_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/base.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/scrfd.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/scrfd.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/single_stage.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/single_stage.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/tinymog.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/tinymog.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/preprocessor.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/scrfd_detect.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/scrfd_detect.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/scrfd/tinymog_detect.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/scrfd/tinymog_detect.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/ulfd_slim/detection.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/ulfd_slim/detection.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/ulfd_slim/vision/box_utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/ulfd_slim/vision/box_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/ulfd_slim/vision/mb_tiny.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/ulfd_slim/vision/mb_tiny.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/data_preprocessing.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/data_preprocessing.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/fd_config.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/fd_config.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/mb_tiny_fd.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/mb_tiny_fd.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/predictor.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/predictor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/ssd.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/ssd.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_detection/ulfd_slim/vision/transforms.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_detection/ulfd_slim/vision/transforms.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_emotion/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_emotion/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_emotion/efficient/model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_emotion/efficient/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_emotion/efficient/utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_emotion/efficient/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_emotion/emotion_infer.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_emotion/emotion_infer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_emotion/emotion_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_emotion/emotion_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_emotion/face_alignment/face.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_emotion/face_alignment/face.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_emotion/face_alignment/face_align.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_emotion/face_alignment/face_align.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_generation/op/conv2d_gradfix.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_generation/op/conv2d_gradfix.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_generation/op/fused_act.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_generation/op/fused_act.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_generation/op/upfirdn2d.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_generation/op/upfirdn2d.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_generation/stylegan2.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_generation/stylegan2.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_human_hand_detection/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_human_hand_detection/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_human_hand_detection/det_infer.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_human_hand_detection/det_infer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_human_hand_detection/ghost_pan.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_human_hand_detection/ghost_pan.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_human_hand_detection/nanodet_plus_head.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_human_hand_detection/nanodet_plus_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_human_hand_detection/one_stage_detector.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_human_hand_detection/one_stage_detector.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_human_hand_detection/shufflenetv2.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_human_hand_detection/shufflenetv2.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_human_hand_detection/utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_human_hand_detection/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_recognition/align_face.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_recognition/align_face.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_recognition/torchkit/backbone/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_recognition/torchkit/backbone/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_recognition/torchkit/backbone/arcface_backbone.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_recognition/torchkit/backbone/arcface_backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_recognition/torchkit/backbone/common.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_recognition/torchkit/backbone/common.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_recognition/torchkit/backbone/facemask_backbone.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_recognition/torchkit/backbone/facemask_backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_recognition/torchkit/backbone/model_irse.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_recognition/torchkit/backbone/model_irse.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_recognition/torchkit/backbone/model_resnet.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_recognition/torchkit/backbone/model_resnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_recognition/torchkit/rts_backbone.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_recognition/torchkit/rts_backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_reconstruction/models/bfm.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_reconstruction/models/bfm.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_reconstruction/models/de_retouching_module.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_reconstruction/models/de_retouching_module.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_reconstruction/models/facelandmark/large_base_lmks_infer.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_reconstruction/models/facelandmark/large_base_lmks_infer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_reconstruction/models/facelandmark/nets/large_base_lmks_net.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_reconstruction/models/facelandmark/nets/large_base_lmks_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_reconstruction/models/facelandmark/nets/large_eyeball_net.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_reconstruction/models/facelandmark/nets/large_eyeball_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_reconstruction/models/facerecon_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_reconstruction/models/facerecon_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_reconstruction/models/losses.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_reconstruction/models/losses.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_reconstruction/models/networks.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_reconstruction/models/networks.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_reconstruction/models/nv_diffrast.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_reconstruction/models/nv_diffrast.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_reconstruction/models/pix2pix/networks.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_reconstruction/models/pix2pix/networks.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_reconstruction/models/pix2pix/pix2pix_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_reconstruction/models/pix2pix/pix2pix_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_reconstruction/models/pix2pix/pix2pix_options.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_reconstruction/models/pix2pix/pix2pix_options.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_reconstruction/models/renderer.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_reconstruction/models/renderer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_reconstruction/models/unet.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_reconstruction/models/unet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/face_reconstruction/utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/face_reconstruction/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/facial_expression_recognition/fer/facial_expression_recognition.py` & `modelscope-1.8.0rc0/modelscope/models/cv/facial_expression_recognition/fer/facial_expression_recognition.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/facial_expression_recognition/fer/transforms.py` & `modelscope-1.8.0rc0/modelscope/models/cv/facial_expression_recognition/fer/transforms.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/facial_expression_recognition/fer/vgg.py` & `modelscope-1.8.0rc0/modelscope/models/cv/facial_expression_recognition/fer/vgg.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/facial_landmark_confidence/flc/facial_landmark_confidence.py` & `modelscope-1.8.0rc0/modelscope/models/cv/facial_landmark_confidence/flc/facial_landmark_confidence.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/facial_landmark_confidence/flc/manual_landmark_net.py` & `modelscope-1.8.0rc0/modelscope/models/cv/facial_landmark_confidence/flc/manual_landmark_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/hand_static/hand_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/hand_static/hand_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/hand_static/networks.py` & `modelscope-1.8.0rc0/modelscope/models/cv/hand_static/networks.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/human_reconstruction/Reconstruction.py` & `modelscope-1.8.0rc0/modelscope/models/cv/human_reconstruction/Reconstruction.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/human_reconstruction/models/Embedding.py` & `modelscope-1.8.0rc0/modelscope/models/cv/human_reconstruction/models/Embedding.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/human_reconstruction/models/PixToMesh.py` & `modelscope-1.8.0rc0/modelscope/models/cv/human_reconstruction/models/PixToMesh.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/human_reconstruction/models/Res_backbone.py` & `modelscope-1.8.0rc0/modelscope/models/cv/human_reconstruction/models/Res_backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/human_reconstruction/models/Surface_head.py` & `modelscope-1.8.0rc0/modelscope/models/cv/human_reconstruction/models/Surface_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/human_reconstruction/models/detectors.py` & `modelscope-1.8.0rc0/modelscope/models/cv/human_reconstruction/models/detectors.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/human_reconstruction/models/geometry.py` & `modelscope-1.8.0rc0/modelscope/models/cv/human_reconstruction/models/geometry.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/human_reconstruction/models/human_segmenter.py` & `modelscope-1.8.0rc0/modelscope/models/cv/human_reconstruction/models/human_segmenter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/human_reconstruction/models/networks.py` & `modelscope-1.8.0rc0/modelscope/models/cv/human_reconstruction/models/networks.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/human_reconstruction/utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/human_reconstruction/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_binary_quant_classification/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_binary_quant_classification/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_binary_quant_classification/binary_quant_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_binary_quant_classification/binary_quant_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_binary_quant_classification/bnext.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_binary_quant_classification/bnext.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_body_reshaping/image_body_reshaping.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_body_reshaping/image_body_reshaping.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_body_reshaping/model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_body_reshaping/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_body_reshaping/person_info.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_body_reshaping/person_info.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_body_reshaping/pose_estimator/body.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_body_reshaping/pose_estimator/body.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_body_reshaping/pose_estimator/model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_body_reshaping/pose_estimator/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_body_reshaping/pose_estimator/util.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_body_reshaping/pose_estimator/util.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_body_reshaping/slim_utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_body_reshaping/slim_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_classification/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_classification/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_classification/backbones/beit_v2.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_classification/backbones/beit_v2.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_classification/backbones/nextvit.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_classification/backbones/nextvit.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_classification/mmcls_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_classification/mmcls_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_classification/resnet50_cc.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_classification/resnet50_cc.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_classification/utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_classification/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_color_enhance/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_color_enhance/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_color_enhance/adaint/adaint.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_color_enhance/adaint/adaint.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_color_enhance/csrnet.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_color_enhance/csrnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_color_enhance/deeplpf/deeplpf_image_color_enhance.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_color_enhance/deeplpf/deeplpf_image_color_enhance.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_color_enhance/deeplpf/deeplpfnet.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_color_enhance/deeplpf/deeplpfnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_color_enhance/image_color_enhance.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_color_enhance/image_color_enhance.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_colorization/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_colorization/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_colorization/ddcolor/ddcolor.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_colorization/ddcolor/ddcolor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_colorization/ddcolor/ddcolor_for_image_colorization.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_colorization/ddcolor/ddcolor_for_image_colorization.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_colorization/ddcolor/loss.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_colorization/ddcolor/loss.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_colorization/ddcolor/utils/convnext.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_colorization/ddcolor/utils/convnext.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_colorization/ddcolor/utils/position_encoding.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_colorization/ddcolor/utils/position_encoding.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_colorization/ddcolor/utils/transformer_utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_colorization/ddcolor/utils/transformer_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_colorization/ddcolor/utils/unet.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_colorization/ddcolor/utils/unet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_colorization/ddcolor/utils/vgg.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_colorization/ddcolor/utils/vgg.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_colorization/unet/unet.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_colorization/unet/unet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_colorization/unet/utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_colorization/unet/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_debanding/rrdb/rrdb_image_debanding.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_debanding/rrdb/rrdb_image_debanding.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_deblur/nafnet_for_image_deblur.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_deblur/nafnet_for_image_deblur.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_defrcn_fewshot/defrcn_for_fewshot.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_defrcn_fewshot/defrcn_for_fewshot.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_defrcn_fewshot/evaluation/coco_evaluation.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_defrcn_fewshot/evaluation/coco_evaluation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_defrcn_fewshot/evaluation/evaluator.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_defrcn_fewshot/evaluation/evaluator.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_defrcn_fewshot/evaluation/pascal_voc_evaluation.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_defrcn_fewshot/evaluation/pascal_voc_evaluation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_defrcn_fewshot/models/calibration_layer.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_defrcn_fewshot/models/calibration_layer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_defrcn_fewshot/models/defrcn.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_defrcn_fewshot/models/defrcn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_defrcn_fewshot/models/fast_rcnn.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_defrcn_fewshot/models/fast_rcnn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_defrcn_fewshot/models/gdl.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_defrcn_fewshot/models/gdl.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_defrcn_fewshot/models/resnet.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_defrcn_fewshot/models/resnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_defrcn_fewshot/models/roi_heads.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_defrcn_fewshot/models/roi_heads.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_defrcn_fewshot/utils/coco_register.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_defrcn_fewshot/utils/coco_register.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_defrcn_fewshot/utils/configuration_mapper.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_defrcn_fewshot/utils/configuration_mapper.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_defrcn_fewshot/utils/model_surgery_op.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_defrcn_fewshot/utils/model_surgery_op.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_defrcn_fewshot/utils/register_data.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_defrcn_fewshot/utils/register_data.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_defrcn_fewshot/utils/requirements_check.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_defrcn_fewshot/utils/requirements_check.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_defrcn_fewshot/utils/voc_register.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_defrcn_fewshot/utils/voc_register.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_denoise/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_denoise/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_denoise/nafnet/NAFNet_arch.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_denoise/nafnet/NAFNet_arch.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_denoise/nafnet/arch_util.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_denoise/nafnet/arch_util.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_denoise/nafnet_for_image_denoise.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_denoise/nafnet_for_image_denoise.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_depth_estimation/networks/newcrf_depth.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_depth_estimation/networks/newcrf_depth.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_depth_estimation/networks/newcrf_layers.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_depth_estimation/networks/newcrf_layers.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_depth_estimation/networks/newcrf_utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_depth_estimation/networks/newcrf_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_depth_estimation/networks/swin_transformer.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_depth_estimation/networks/swin_transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_depth_estimation/networks/uper_crf_head.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_depth_estimation/networks/uper_crf_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_depth_estimation/newcrfs_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_depth_estimation/newcrfs_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_depth_estimation_bts/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_depth_estimation_bts/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_depth_estimation_bts/depth_estimation_bts_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_depth_estimation_bts/depth_estimation_bts_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_depth_estimation_bts/networks/bts_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_depth_estimation_bts/networks/bts_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_depth_estimation_bts/networks/decoder.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_depth_estimation_bts/networks/decoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_depth_estimation_bts/networks/encoder.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_depth_estimation_bts/networks/encoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_depth_estimation_bts/networks/utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_depth_estimation_bts/networks/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_driving_perception/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_driving_perception/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_driving_perception/image_driving_percetion_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_driving_perception/image_driving_percetion_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_driving_perception/preprocessor.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_driving_perception/preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_driving_perception/utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_driving_perception/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_face_fusion/facegan/gan_wrap.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_face_fusion/facegan/gan_wrap.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_face_fusion/facegan/model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_face_fusion/facegan/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_face_fusion/facegan/op/conv2d_gradfix.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_face_fusion/facegan/op/conv2d_gradfix.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_face_fusion/facegan/op/fused_act.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_face_fusion/facegan/op/fused_act.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_face_fusion/facegan/op/upfirdn2d.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_face_fusion/facegan/op/upfirdn2d.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_face_fusion/facelib/align_trans.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_face_fusion/facelib/align_trans.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_face_fusion/facelib/matlab_cp2tform.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_face_fusion/facelib/matlab_cp2tform.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_face_fusion/image_face_fusion.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_face_fusion/image_face_fusion.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_face_fusion/network/aad_layer.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_face_fusion/network/aad_layer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_face_fusion/network/aei_flow_net.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_face_fusion/network/aei_flow_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_face_fusion/network/bfm.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_face_fusion/network/bfm.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_face_fusion/network/dense_motion.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_face_fusion/network/dense_motion.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_face_fusion/network/facerecon_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_face_fusion/network/facerecon_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_face_fusion/network/model_irse.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_face_fusion/network/model_irse.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_face_fusion/network/ops.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_face_fusion/network/ops.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_human_parsing/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_human_parsing/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_human_parsing/backbone/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_human_parsing/backbone/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_human_parsing/backbone/deeplab_resnet.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_human_parsing/backbone/deeplab_resnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_human_parsing/m2fp/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_human_parsing/m2fp/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_human_parsing/m2fp/m2fp_decoder.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_human_parsing/m2fp/m2fp_decoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_human_parsing/m2fp/m2fp_encoder.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_human_parsing/m2fp/m2fp_encoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_human_parsing/m2fp_net.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_human_parsing/m2fp_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_human_parsing/parsing_utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_human_parsing/parsing_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_inpainting/base.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_inpainting/base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_inpainting/default.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_inpainting/default.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_inpainting/model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_inpainting/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_inpainting/modules/ade20k/base.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_inpainting/modules/ade20k/base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_inpainting/modules/ade20k/resnet.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_inpainting/modules/ade20k/resnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_inpainting/modules/adversarial.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_inpainting/modules/adversarial.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_inpainting/modules/feature_matching.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_inpainting/modules/feature_matching.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_inpainting/modules/ffc.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_inpainting/modules/ffc.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_inpainting/modules/inception.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_inpainting/modules/inception.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_inpainting/modules/perceptual.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_inpainting/modules/perceptual.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_inpainting/modules/pix2pixhd.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_inpainting/modules/pix2pixhd.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_inpainting/refinement.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_inpainting/refinement.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/backbones/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/backbones/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/backbones/resnet.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/backbones/resnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/backbones/swin_transformer.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/backbones/swin_transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/cascade_mask_rcnn_swin.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/cascade_mask_rcnn_swin.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/datasets/transforms.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/datasets/transforms.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/fastinst/fastinst_decoder.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/fastinst/fastinst_decoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/fastinst/fastinst_encoder.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/fastinst/fastinst_encoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/fastinst_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/fastinst_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/maskdino/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/maskdino/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/maskdino/dino_decoder.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/maskdino/dino_decoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/maskdino/maskdino_decoder.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/maskdino/maskdino_decoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/maskdino/maskdino_encoder.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/maskdino/maskdino_encoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/maskdino/ms_deform_attn.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/maskdino/ms_deform_attn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/maskdino/position_encoding.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/maskdino/position_encoding.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/maskdino/utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/maskdino/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/maskdino_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/maskdino_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/maskdino_swin.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/maskdino_swin.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_instance_segmentation/postprocess_utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_instance_segmentation/postprocess_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_matching/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_matching/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_matching/config/default.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_matching/config/default.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_matching/loftr_quadtree/backbone/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_matching/loftr_quadtree/backbone/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_matching/loftr_quadtree/backbone/resnet_fpn.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_matching/loftr_quadtree/backbone/resnet_fpn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_matching/loftr_quadtree/loftr.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_matching/loftr_quadtree/loftr.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/fine_preprocess.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/fine_preprocess.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/linear_attention.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/linear_attention.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/quadtree_attention.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/quadtree_attention.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/transformer.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_matching/loftr_quadtree/utils/coarse_matching.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_matching/loftr_quadtree/utils/coarse_matching.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_matching/loftr_quadtree/utils/fine_matching.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_matching/loftr_quadtree/utils/fine_matching.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_matching/loftr_quadtree/utils/position_encoding.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_matching/loftr_quadtree/utils/position_encoding.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_matching/quadtree_attention_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_matching/quadtree_attention_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_mvs_depth_estimation/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_mvs_depth_estimation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_mvs_depth_estimation/cas_mvsnet.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_mvs_depth_estimation/cas_mvsnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_mvs_depth_estimation/casmvs_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_mvs_depth_estimation/casmvs_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_mvs_depth_estimation/colmap2mvsnet.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_mvs_depth_estimation/colmap2mvsnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_mvs_depth_estimation/depth_filter.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_mvs_depth_estimation/depth_filter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_mvs_depth_estimation/general_eval_dataset.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_mvs_depth_estimation/general_eval_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_mvs_depth_estimation/module.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_mvs_depth_estimation/module.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_mvs_depth_estimation/utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_mvs_depth_estimation/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_paintbyexample/model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_paintbyexample/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_panoptic_segmentation/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_panoptic_segmentation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_panoptic_segmentation/panseg_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_panoptic_segmentation/panseg_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_portrait_enhancement/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_portrait_enhancement/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_portrait_enhancement/align_faces.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_portrait_enhancement/align_faces.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_portrait_enhancement/eqface/fqa.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_portrait_enhancement/eqface/fqa.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_portrait_enhancement/eqface/model_resnet.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_portrait_enhancement/eqface/model_resnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_portrait_enhancement/gpen.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_portrait_enhancement/gpen.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_portrait_enhancement/image_portrait_enhancement.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_portrait_enhancement/image_portrait_enhancement.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_portrait_enhancement/losses/helpers.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_portrait_enhancement/losses/helpers.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_portrait_enhancement/losses/losses.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_portrait_enhancement/losses/losses.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_portrait_enhancement/losses/model_irse.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_portrait_enhancement/losses/model_irse.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_portrait_enhancement/retinaface/detection.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_portrait_enhancement/retinaface/detection.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_portrait_enhancement/retinaface/models/net.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_portrait_enhancement/retinaface/models/net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_portrait_enhancement/retinaface/models/retinaface.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_portrait_enhancement/retinaface/models/retinaface.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_portrait_enhancement/retinaface/utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_portrait_enhancement/retinaface/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_probing_model/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_probing_model/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_probing_model/backbone.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_probing_model/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_probing_model/model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_probing_model/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_probing_model/utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_probing_model/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_quality_assessment_degradation/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_quality_assessment_degradation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_quality_assessment_degradation/degradation_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_quality_assessment_degradation/degradation_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_quality_assessment_degradation/image_quality_assessment_degradation.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_quality_assessment_degradation/image_quality_assessment_degradation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_quality_assessment_man/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_quality_assessment_man/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_quality_assessment_man/image_quality_assessment_man.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_quality_assessment_man/image_quality_assessment_man.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_quality_assessment_man/maniqa.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_quality_assessment_man/maniqa.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_quality_assessment_man/swin.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_quality_assessment_man/swin.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_quality_assessment_mos/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_quality_assessment_mos/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_quality_assessment_mos/backbones/resnet.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_quality_assessment_mos/backbones/resnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_quality_assessment_mos/censeo_ivqa_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_quality_assessment_mos/censeo_ivqa_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_quality_assessment_mos/heads/simple_head.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_quality_assessment_mos/heads/simple_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_quality_assessment_mos/image_quality_assessment_mos.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_quality_assessment_mos/image_quality_assessment_mos.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_reid_person/pass_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_reid_person/pass_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_reid_person/transreid_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_reid_person/transreid_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_restoration/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_restoration/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_restoration/demoire_models/nets.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_restoration/demoire_models/nets.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_restoration/image_restoration_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_restoration/image_restoration_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/data_util.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/data_util.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/feature_extractors.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/feature_extractors.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/pixel_classifier.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/pixel_classifier.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/ddpm_segmentation_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/ddpm_segmentation_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/pan_merge/base_panoptic_fusion_head.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/pan_merge/base_panoptic_fusion_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/pan_merge/maskformer_semantic_head.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/pan_merge/maskformer_semantic_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/semantic_seg_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/semantic_seg_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/adapter_modules.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/adapter_modules.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/base/beit.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/base/beit.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/beit_adapter.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/beit_adapter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/base_decode_head.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/base_decode_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/mask2former_head_from_mmseg.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/mask2former_head_from_mmseg.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/base_segmentor.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/base_segmentor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/encoder_decoder_mask2former.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/encoder_decoder_mask2former.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/utils/data_process_func.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/utils/data_process_func.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/utils/seg_func.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/utils/seg_func.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_skychange/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_skychange/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_skychange/preprocessor.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_skychange/preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_skychange/ptsemseg/BlockModules.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_skychange/ptsemseg/BlockModules.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_skychange/ptsemseg/hrnet_backnone.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_skychange/ptsemseg/hrnet_backnone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_skychange/ptsemseg/hrnet_super_and_ocr.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_skychange/ptsemseg/hrnet_super_and_ocr.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_skychange/ptsemseg/unet.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_skychange/ptsemseg/unet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_skychange/skychange.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_skychange/skychange.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_skychange/skychange_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_skychange/skychange_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_to_image_generation/data/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_generation/data/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_to_image_generation/data/transforms.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_generation/data/transforms.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_to_image_generation/model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_generation/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_to_image_generation/models/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_generation/models/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_to_image_generation/models/autoencoder.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_generation/models/autoencoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_to_image_generation/models/clip.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_generation/models/clip.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_to_image_generation/ops/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_generation/ops/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_to_image_generation/ops/diffusion.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_generation/ops/diffusion.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_to_image_generation/ops/losses.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_generation/ops/losses.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_to_image_translation/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_translation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_to_image_translation/data/transforms.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_translation/data/transforms.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_to_image_translation/model_translation.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_translation/model_translation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_to_image_translation/models/autoencoder.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_translation/models/autoencoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_to_image_translation/models/clip.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_translation/models/clip.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_to_image_translation/ops/apps.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_translation/ops/apps.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_to_image_translation/ops/degradation.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_translation/ops/degradation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_to_image_translation/ops/diffusion.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_translation/ops/diffusion.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_to_image_translation/ops/losses.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_translation/ops/losses.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_to_image_translation/ops/metrics.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_translation/ops/metrics.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_to_image_translation/ops/random_color.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_translation/ops/random_color.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_to_image_translation/ops/random_mask.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_translation/ops/random_mask.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_to_image_translation/ops/svd.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_translation/ops/svd.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_to_image_translation/ops/utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_to_image_translation/ops/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_try_on/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_try_on/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_try_on/generator.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_try_on/generator.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_try_on/landmark.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_try_on/landmark.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_try_on/try_on_infer.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_try_on/try_on_infer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/image_try_on/warping.py` & `modelscope-1.8.0rc0/modelscope/models/cv/image_try_on/warping.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/indoor_layout_estimation/networks/backbone/resnet_DA.py` & `modelscope-1.8.0rc0/modelscope/models/cv/indoor_layout_estimation/networks/backbone/resnet_DA.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/indoor_layout_estimation/networks/backbone/vit_horizon_pry_image.py` & `modelscope-1.8.0rc0/modelscope/models/cv/indoor_layout_estimation/networks/backbone/vit_horizon_pry_image.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/indoor_layout_estimation/networks/misc/fourier.py` & `modelscope-1.8.0rc0/modelscope/models/cv/indoor_layout_estimation/networks/misc/fourier.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/indoor_layout_estimation/networks/misc/panostretch.py` & `modelscope-1.8.0rc0/modelscope/models/cv/indoor_layout_estimation/networks/misc/panostretch.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/indoor_layout_estimation/networks/misc/post_proc.py` & `modelscope-1.8.0rc0/modelscope/models/cv/indoor_layout_estimation/networks/misc/post_proc.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/indoor_layout_estimation/networks/modality/layout.py` & `modelscope-1.8.0rc0/modelscope/models/cv/indoor_layout_estimation/networks/modality/layout.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/indoor_layout_estimation/networks/panovit.py` & `modelscope-1.8.0rc0/modelscope/models/cv/indoor_layout_estimation/networks/panovit.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/indoor_layout_estimation/networks/utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/indoor_layout_estimation/networks/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/indoor_layout_estimation/panovit.py` & `modelscope-1.8.0rc0/modelscope/models/cv/indoor_layout_estimation/panovit.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/language_guided_video_summarization/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/language_guided_video_summarization/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/language_guided_video_summarization/summarizer.py` & `modelscope-1.8.0rc0/modelscope/models/cv/language_guided_video_summarization/summarizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/language_guided_video_summarization/transformer/layers.py` & `modelscope-1.8.0rc0/modelscope/models/cv/language_guided_video_summarization/transformer/layers.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/language_guided_video_summarization/transformer/models.py` & `modelscope-1.8.0rc0/modelscope/models/cv/language_guided_video_summarization/transformer/models.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/language_guided_video_summarization/transformer/modules.py` & `modelscope-1.8.0rc0/modelscope/models/cv/language_guided_video_summarization/transformer/modules.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/language_guided_video_summarization/transformer/sub_layers.py` & `modelscope-1.8.0rc0/modelscope/models/cv/language_guided_video_summarization/transformer/sub_layers.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/motion_generation/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/motion_generation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/motion_generation/model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/motion_generation/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/motion_generation/modules/cfg_sampler.py` & `modelscope-1.8.0rc0/modelscope/models/cv/motion_generation/modules/cfg_sampler.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/motion_generation/modules/gaussian_diffusion.py` & `modelscope-1.8.0rc0/modelscope/models/cv/motion_generation/modules/gaussian_diffusion.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/motion_generation/modules/mdm.py` & `modelscope-1.8.0rc0/modelscope/models/cv/motion_generation/modules/mdm.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/motion_generation/modules/respace.py` & `modelscope-1.8.0rc0/modelscope/models/cv/motion_generation/modules/respace.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/motion_generation/modules/rotation2xyz.py` & `modelscope-1.8.0rc0/modelscope/models/cv/motion_generation/modules/rotation2xyz.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/motion_generation/modules/smpl.py` & `modelscope-1.8.0rc0/modelscope/models/cv/motion_generation/modules/smpl.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/movie_scene_segmentation/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/movie_scene_segmentation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/movie_scene_segmentation/get_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/movie_scene_segmentation/get_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/movie_scene_segmentation/model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/movie_scene_segmentation/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/movie_scene_segmentation/utils/head.py` & `modelscope-1.8.0rc0/modelscope/models/cv/movie_scene_segmentation/utils/head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/movie_scene_segmentation/utils/save_op.py` & `modelscope-1.8.0rc0/modelscope/models/cv/movie_scene_segmentation/utils/save_op.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/movie_scene_segmentation/utils/shot_encoder.py` & `modelscope-1.8.0rc0/modelscope/models/cv/movie_scene_segmentation/utils/shot_encoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/movie_scene_segmentation/utils/trn.py` & `modelscope-1.8.0rc0/modelscope/models/cv/movie_scene_segmentation/utils/trn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/nerf_recon_4k/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_4k/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/nerf_recon_4k/dataloader/load_blender.py` & `modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_4k/dataloader/load_blender.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/nerf_recon_4k/dataloader/load_data.py` & `modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_4k/dataloader/load_data.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/nerf_recon_4k/dataloader/load_llff.py` & `modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_4k/dataloader/load_llff.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/nerf_recon_4k/dataloader/load_tankstemple.py` & `modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_4k/dataloader/load_tankstemple.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/nerf_recon_4k/dataloader/read_write_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_4k/dataloader/read_write_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/nerf_recon_4k/nerf_preprocess.py` & `modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_4k/nerf_preprocess.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/nerf_recon_4k/nerf_recon_4k.py` & `modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_4k/nerf_recon_4k.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/nerf_recon_4k/network/dvgo.py` & `modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_4k/network/dvgo.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/nerf_recon_4k/network/utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_4k/network/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/nerf_recon_acc/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_acc/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/nerf_recon_acc/dataloader/nerf_dataset.py` & `modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_acc/dataloader/nerf_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/nerf_recon_acc/dataloader/read_write_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_acc/dataloader/read_write_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/nerf_recon_acc/nerf_preprocess.py` & `modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_acc/nerf_preprocess.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/nerf_recon_acc/nerf_recon_acc.py` & `modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_acc/nerf_recon_acc.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/nerf_recon_acc/network/nerf.py` & `modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_acc/network/nerf.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/nerf_recon_acc/network/segmenter.py` & `modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_acc/network/segmenter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/nerf_recon_acc/network/utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_acc/network/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/nerf_recon_vq_compression/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_vq_compression/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/nerf_recon_vq_compression/dataloader/blender.py` & `modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_vq_compression/dataloader/blender.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/nerf_recon_vq_compression/dataloader/llff.py` & `modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_vq_compression/dataloader/llff.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/nerf_recon_vq_compression/dataloader/nsvf.py` & `modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_vq_compression/dataloader/nsvf.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/nerf_recon_vq_compression/dataloader/ray_utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_vq_compression/dataloader/ray_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/nerf_recon_vq_compression/dataloader/tankstemple.py` & `modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_vq_compression/dataloader/tankstemple.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/nerf_recon_vq_compression/nerf_recon_vq_compression.py` & `modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_vq_compression/nerf_recon_vq_compression.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/nerf_recon_vq_compression/network/tensoRF.py` & `modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_vq_compression/network/tensoRF.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/nerf_recon_vq_compression/network/tensoRF_VQ.py` & `modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_vq_compression/network/tensoRF_VQ.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/nerf_recon_vq_compression/network/tensorBase.py` & `modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_vq_compression/network/tensorBase.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/nerf_recon_vq_compression/network/weighted_vq.py` & `modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_vq_compression/network/weighted_vq.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/nerf_recon_vq_compression/renderer.py` & `modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_vq_compression/renderer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/nerf_recon_vq_compression/utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/nerf_recon_vq_compression/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/object_detection/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/object_detection/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/object_detection/mmdet_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/object_detection/mmdet_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/object_detection/mmdet_ms/backbones/vit.py` & `modelscope-1.8.0rc0/modelscope/models/cv/object_detection/mmdet_ms/backbones/vit.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/object_detection/mmdet_ms/dense_heads/anchor_head.py` & `modelscope-1.8.0rc0/modelscope/models/cv/object_detection/mmdet_ms/dense_heads/anchor_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/object_detection/mmdet_ms/dense_heads/rpn_head.py` & `modelscope-1.8.0rc0/modelscope/models/cv/object_detection/mmdet_ms/dense_heads/rpn_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/object_detection/mmdet_ms/necks/fpn.py` & `modelscope-1.8.0rc0/modelscope/models/cv/object_detection/mmdet_ms/necks/fpn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/bbox_heads/convfc_bbox_head.py` & `modelscope-1.8.0rc0/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/bbox_heads/convfc_bbox_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/mask_heads/fcn_mask_head.py` & `modelscope-1.8.0rc0/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/mask_heads/fcn_mask_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/object_detection/mmdet_ms/utils/checkpoint.py` & `modelscope-1.8.0rc0/modelscope/models/cv/object_detection/mmdet_ms/utils/checkpoint.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/object_detection/mmdet_ms/utils/convModule_norm.py` & `modelscope-1.8.0rc0/modelscope/models/cv/object_detection/mmdet_ms/utils/convModule_norm.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/depe_detect.py` & `modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/depe_detect.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/assigners/hungarian_assigner_3d.py` & `modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/assigners/hungarian_assigner_3d.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/coders/nms_free_coder.py` & `modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/coders/nms_free_coder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/match_costs/match_cost.py` & `modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/match_costs/match_cost.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/util.py` & `modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/util.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/nuscenes_dataset.py` & `modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/nuscenes_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/loading.py` & `modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/loading.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/transform_3d.py` & `modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/transform_3d.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/backbones/vovnet.py` & `modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/backbones/vovnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/depth_net.py` & `modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/depth_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/petrv2_dednhead.py` & `modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/petrv2_dednhead.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/detectors/petr3d.py` & `modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/detectors/petr3d.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/necks/cp_fpn.py` & `modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/necks/cp_fpn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/petr_transformer.py` & `modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/petr_transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/positional_encoding.py` & `modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/positional_encoding.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/object_detection_3d/depe/result_vis.py` & `modelscope-1.8.0rc0/modelscope/models/cv/object_detection_3d/depe/result_vis.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/ocr_detection/model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/ocr_detection/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/ocr_detection/modules/dbnet.py` & `modelscope-1.8.0rc0/modelscope/models/cv/ocr_detection/modules/dbnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/ocr_detection/modules/layers.py` & `modelscope-1.8.0rc0/modelscope/models/cv/ocr_detection/modules/layers.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/ocr_detection/modules/mix_ops.py` & `modelscope-1.8.0rc0/modelscope/models/cv/ocr_detection/modules/mix_ops.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/ocr_detection/modules/proxyless.py` & `modelscope-1.8.0rc0/modelscope/models/cv/ocr_detection/modules/proxyless.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/ocr_detection/modules/seg_detector_loss.py` & `modelscope-1.8.0rc0/modelscope/models/cv/ocr_detection/modules/seg_detector_loss.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/ocr_detection/preprocessor.py` & `modelscope-1.8.0rc0/modelscope/models/cv/ocr_detection/preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/ocr_detection/utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/ocr_detection/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/ocr_recognition/model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/ocr_recognition/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/ocr_recognition/modules/CRNN/main_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/ocr_recognition/modules/CRNN/main_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/ocr_recognition/modules/ConvNextViT/convnext.py` & `modelscope-1.8.0rc0/modelscope/models/cv/ocr_recognition/modules/ConvNextViT/convnext.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/ocr_recognition/modules/ConvNextViT/main_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/ocr_recognition/modules/ConvNextViT/main_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/ocr_recognition/modules/ConvNextViT/timm_tinyc.py` & `modelscope-1.8.0rc0/modelscope/models/cv/ocr_recognition/modules/ConvNextViT/timm_tinyc.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/ocr_recognition/modules/ConvNextViT/vitstr.py` & `modelscope-1.8.0rc0/modelscope/models/cv/ocr_recognition/modules/ConvNextViT/vitstr.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/ocr_recognition/modules/LightweightEdge/main_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/ocr_recognition/modules/LightweightEdge/main_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/ocr_recognition/modules/LightweightEdge/nas_block/layers.py` & `modelscope-1.8.0rc0/modelscope/models/cv/ocr_recognition/modules/LightweightEdge/nas_block/layers.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/ocr_recognition/modules/LightweightEdge/nas_block/mix_ops.py` & `modelscope-1.8.0rc0/modelscope/models/cv/ocr_recognition/modules/LightweightEdge/nas_block/mix_ops.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/ocr_recognition/modules/LightweightEdge/nas_block/proxyless.py` & `modelscope-1.8.0rc0/modelscope/models/cv/ocr_recognition/modules/LightweightEdge/nas_block/proxyless.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/ocr_recognition/preprocessor.py` & `modelscope-1.8.0rc0/modelscope/models/cv/ocr_recognition/preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/open_vocabulary_detection_vild/vild.py` & `modelscope-1.8.0rc0/modelscope/models/cv/open_vocabulary_detection_vild/vild.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/panorama_depth_estimation/networks/equi.py` & `modelscope-1.8.0rc0/modelscope/models/cv/panorama_depth_estimation/networks/equi.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/panorama_depth_estimation/networks/layers.py` & `modelscope-1.8.0rc0/modelscope/models/cv/panorama_depth_estimation/networks/layers.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/panorama_depth_estimation/networks/mobilenet.py` & `modelscope-1.8.0rc0/modelscope/models/cv/panorama_depth_estimation/networks/mobilenet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/panorama_depth_estimation/networks/resnet.py` & `modelscope-1.8.0rc0/modelscope/models/cv/panorama_depth_estimation/networks/resnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/panorama_depth_estimation/networks/unifuse.py` & `modelscope-1.8.0rc0/modelscope/models/cv/panorama_depth_estimation/networks/unifuse.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/panorama_depth_estimation/networks/util.py` & `modelscope-1.8.0rc0/modelscope/models/cv/panorama_depth_estimation/networks/util.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/panorama_depth_estimation/unifuse_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/panorama_depth_estimation/unifuse_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/pedestrian_attribute_recognition/model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/pedestrian_attribute_recognition/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/pointcloud_sceneflow_estimation/common.py` & `modelscope-1.8.0rc0/modelscope/models/cv/pointcloud_sceneflow_estimation/common.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/pointcloud_sceneflow_estimation/pointnet2_utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/pointcloud_sceneflow_estimation/pointnet2_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/pointcloud_sceneflow_estimation/rcp_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/pointcloud_sceneflow_estimation/rcp_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/pointcloud_sceneflow_estimation/sf_rcp.py` & `modelscope-1.8.0rc0/modelscope/models/cv/pointcloud_sceneflow_estimation/sf_rcp.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/product_retrieval_embedding/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/product_retrieval_embedding/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/product_retrieval_embedding/item_detection.py` & `modelscope-1.8.0rc0/modelscope/models/cv/product_retrieval_embedding/item_detection.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/product_retrieval_embedding/item_embedding.py` & `modelscope-1.8.0rc0/modelscope/models/cv/product_retrieval_embedding/item_embedding.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/product_retrieval_embedding/item_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/product_retrieval_embedding/item_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/product_segmentation/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/product_segmentation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/product_segmentation/net.py` & `modelscope-1.8.0rc0/modelscope/models/cv/product_segmentation/net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/product_segmentation/seg_infer.py` & `modelscope-1.8.0rc0/modelscope/models/cv/product_segmentation/seg_infer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/referring_video_object_segmentation/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/referring_video_object_segmentation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/referring_video_object_segmentation/model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/referring_video_object_segmentation/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/referring_video_object_segmentation/utils/backbone.py` & `modelscope-1.8.0rc0/modelscope/models/cv/referring_video_object_segmentation/utils/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/referring_video_object_segmentation/utils/criterion.py` & `modelscope-1.8.0rc0/modelscope/models/cv/referring_video_object_segmentation/utils/criterion.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/referring_video_object_segmentation/utils/matcher.py` & `modelscope-1.8.0rc0/modelscope/models/cv/referring_video_object_segmentation/utils/matcher.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/referring_video_object_segmentation/utils/misc.py` & `modelscope-1.8.0rc0/modelscope/models/cv/referring_video_object_segmentation/utils/misc.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/referring_video_object_segmentation/utils/mttr.py` & `modelscope-1.8.0rc0/modelscope/models/cv/referring_video_object_segmentation/utils/mttr.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/referring_video_object_segmentation/utils/multimodal_transformer.py` & `modelscope-1.8.0rc0/modelscope/models/cv/referring_video_object_segmentation/utils/multimodal_transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/referring_video_object_segmentation/utils/position_encoding_2d.py` & `modelscope-1.8.0rc0/modelscope/models/cv/referring_video_object_segmentation/utils/position_encoding_2d.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/referring_video_object_segmentation/utils/postprocessing.py` & `modelscope-1.8.0rc0/modelscope/models/cv/referring_video_object_segmentation/utils/postprocessing.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/referring_video_object_segmentation/utils/segmentation.py` & `modelscope-1.8.0rc0/modelscope/models/cv/referring_video_object_segmentation/utils/segmentation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/referring_video_object_segmentation/utils/swin_transformer.py` & `modelscope-1.8.0rc0/modelscope/models/cv/referring_video_object_segmentation/utils/swin_transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/robust_image_classification/easyrobust_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/robust_image_classification/easyrobust_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/s2net_panorama_depth_estimation/networks/config.py` & `modelscope-1.8.0rc0/modelscope/models/cv/s2net_panorama_depth_estimation/networks/config.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/s2net_panorama_depth_estimation/networks/decoder.py` & `modelscope-1.8.0rc0/modelscope/models/cv/s2net_panorama_depth_estimation/networks/decoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/s2net_panorama_depth_estimation/networks/model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/s2net_panorama_depth_estimation/networks/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/s2net_panorama_depth_estimation/networks/resnet.py` & `modelscope-1.8.0rc0/modelscope/models/cv/s2net_panorama_depth_estimation/networks/resnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/s2net_panorama_depth_estimation/networks/swin_transformer.py` & `modelscope-1.8.0rc0/modelscope/models/cv/s2net_panorama_depth_estimation/networks/swin_transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/s2net_panorama_depth_estimation/networks/util_helper.py` & `modelscope-1.8.0rc0/modelscope/models/cv/s2net_panorama_depth_estimation/networks/util_helper.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/s2net_panorama_depth_estimation/s2net_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/s2net_panorama_depth_estimation/s2net_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/salient_detection/models/backbone/Res2Net_v1b.py` & `modelscope-1.8.0rc0/modelscope/models/cv/salient_detection/models/backbone/Res2Net_v1b.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/salient_detection/models/modules.py` & `modelscope-1.8.0rc0/modelscope/models/cv/salient_detection/models/modules.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/salient_detection/models/senet.py` & `modelscope-1.8.0rc0/modelscope/models/cv/salient_detection/models/senet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/salient_detection/models/u2net.py` & `modelscope-1.8.0rc0/modelscope/models/cv/salient_detection/models/u2net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/salient_detection/models/utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/salient_detection/models/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/salient_detection/salient_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/salient_detection/salient_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/shop_segmentation/common.py` & `modelscope-1.8.0rc0/modelscope/models/cv/shop_segmentation/common.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/shop_segmentation/head_fpn.py` & `modelscope-1.8.0rc0/modelscope/models/cv/shop_segmentation/head_fpn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/shop_segmentation/models.py` & `modelscope-1.8.0rc0/modelscope/models/cv/shop_segmentation/models.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/shop_segmentation/neck_fpn.py` & `modelscope-1.8.0rc0/modelscope/models/cv/shop_segmentation/neck_fpn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/shop_segmentation/shop_seg_base.py` & `modelscope-1.8.0rc0/modelscope/models/cv/shop_segmentation/shop_seg_base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/shop_segmentation/shop_seg_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/shop_segmentation/shop_seg_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/shop_segmentation/utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/shop_segmentation/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/skin_retouching/detection_model/detection_module.py` & `modelscope-1.8.0rc0/modelscope/models/cv/skin_retouching/detection_model/detection_module.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/skin_retouching/detection_model/detection_unet_in.py` & `modelscope-1.8.0rc0/modelscope/models/cv/skin_retouching/detection_model/detection_unet_in.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/skin_retouching/inpainting_model/gconv.py` & `modelscope-1.8.0rc0/modelscope/models/cv/skin_retouching/inpainting_model/gconv.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/skin_retouching/inpainting_model/inpainting_unet.py` & `modelscope-1.8.0rc0/modelscope/models/cv/skin_retouching/inpainting_model/inpainting_unet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/skin_retouching/retinaface/box_utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/skin_retouching/retinaface/box_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/skin_retouching/retinaface/net.py` & `modelscope-1.8.0rc0/modelscope/models/cv/skin_retouching/retinaface/net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/skin_retouching/retinaface/network.py` & `modelscope-1.8.0rc0/modelscope/models/cv/skin_retouching/retinaface/network.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/skin_retouching/retinaface/predict_single.py` & `modelscope-1.8.0rc0/modelscope/models/cv/skin_retouching/retinaface/predict_single.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/skin_retouching/retinaface/prior_box.py` & `modelscope-1.8.0rc0/modelscope/models/cv/skin_retouching/retinaface/prior_box.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/skin_retouching/retinaface/utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/skin_retouching/retinaface/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/skin_retouching/unet_deploy.py` & `modelscope-1.8.0rc0/modelscope/models/cv/skin_retouching/unet_deploy.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/skin_retouching/utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/skin_retouching/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/skin_retouching/weights_init.py` & `modelscope-1.8.0rc0/modelscope/models/cv/skin_retouching/weights_init.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/stream_yolo/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/stream_yolo/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/stream_yolo/data/data_augment.py` & `modelscope-1.8.0rc0/modelscope/models/cv/stream_yolo/data/data_augment.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/stream_yolo/exp/default/streamyolo.py` & `modelscope-1.8.0rc0/modelscope/models/cv/stream_yolo/exp/default/streamyolo.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/stream_yolo/exp/yolox_base.py` & `modelscope-1.8.0rc0/modelscope/models/cv/stream_yolo/exp/yolox_base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/stream_yolo/models/darknet.py` & `modelscope-1.8.0rc0/modelscope/models/cv/stream_yolo/models/darknet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/stream_yolo/models/dfp_pafpn.py` & `modelscope-1.8.0rc0/modelscope/models/cv/stream_yolo/models/dfp_pafpn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/stream_yolo/models/network_blocks.py` & `modelscope-1.8.0rc0/modelscope/models/cv/stream_yolo/models/network_blocks.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/stream_yolo/models/streamyolo.py` & `modelscope-1.8.0rc0/modelscope/models/cv/stream_yolo/models/streamyolo.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/stream_yolo/models/tal_head.py` & `modelscope-1.8.0rc0/modelscope/models/cv/stream_yolo/models/tal_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/stream_yolo/realtime_video_detector.py` & `modelscope-1.8.0rc0/modelscope/models/cv/stream_yolo/realtime_video_detector.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/stream_yolo/utils/boxes.py` & `modelscope-1.8.0rc0/modelscope/models/cv/stream_yolo/utils/boxes.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/super_resolution/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/super_resolution/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/super_resolution/arch_util.py` & `modelscope-1.8.0rc0/modelscope/models/cv/super_resolution/arch_util.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/super_resolution/ecb.py` & `modelscope-1.8.0rc0/modelscope/models/cv/super_resolution/ecb.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/super_resolution/ecbsr_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/super_resolution/ecbsr_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/super_resolution/rrdbnet_arch.py` & `modelscope-1.8.0rc0/modelscope/models/cv/super_resolution/rrdbnet_arch.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/table_recognition/lineless_table_process.py` & `modelscope-1.8.0rc0/modelscope/models/cv/table_recognition/lineless_table_process.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/table_recognition/model_lore.py` & `modelscope-1.8.0rc0/modelscope/models/cv/table_recognition/model_lore.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/table_recognition/modules/lore_detector.py` & `modelscope-1.8.0rc0/modelscope/models/cv/table_recognition/modules/lore_detector.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/table_recognition/modules/lore_processor.py` & `modelscope-1.8.0rc0/modelscope/models/cv/table_recognition/modules/lore_processor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/text_driven_segmentation/clip.py` & `modelscope-1.8.0rc0/modelscope/models/cv/text_driven_segmentation/clip.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/text_driven_segmentation/lseg_base.py` & `modelscope-1.8.0rc0/modelscope/models/cv/text_driven_segmentation/lseg_base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/text_driven_segmentation/lseg_blocks.py` & `modelscope-1.8.0rc0/modelscope/models/cv/text_driven_segmentation/lseg_blocks.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/text_driven_segmentation/lseg_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/text_driven_segmentation/lseg_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/text_driven_segmentation/lseg_net.py` & `modelscope-1.8.0rc0/modelscope/models/cv/text_driven_segmentation/lseg_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/text_driven_segmentation/lseg_vit.py` & `modelscope-1.8.0rc0/modelscope/models/cv/text_driven_segmentation/lseg_vit.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/text_driven_segmentation/model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/text_driven_segmentation/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/text_driven_segmentation/simple_tokenizer.py` & `modelscope-1.8.0rc0/modelscope/models/cv/text_driven_segmentation/simple_tokenizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/text_to_360panorama_image/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/text_to_360panorama_image/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/text_to_360panorama_image/pipeline_base.py` & `modelscope-1.8.0rc0/modelscope/models/cv/text_to_360panorama_image/pipeline_base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/text_to_360panorama_image/pipeline_sr.py` & `modelscope-1.8.0rc0/modelscope/models/cv/text_to_360panorama_image/pipeline_sr.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_classfication/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_classfication/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_classfication/basic_blocks.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_classfication/basic_blocks.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_classfication/global_utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_classfication/global_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_classfication/master_net.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_classfication/master_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_classfication/model_zoo.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_classfication/model_zoo.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_classfication/plain_net_utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_classfication/plain_net_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_classfication/super_blocks.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_classfication/super_blocks.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_classfication/super_res_idwexkx.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_classfication/super_res_idwexkx.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_classfication/super_res_k1kxk1.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_classfication/super_res_k1kxk1.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_classfication/super_res_kxkx.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_classfication/super_res_kxkx.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_detection/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/apis/detector_evaluater.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/apis/detector_evaluater.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/apis/detector_inference.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/apis/detector_inference.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/box_level_augs.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/box_level_augs.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/color_augs.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/color_augs.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/gaussian_maps.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/gaussian_maps.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/geometric_augs.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/geometric_augs.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/augmentations/scale_aware_aug.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/augmentations/scale_aware_aug.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/darknet.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/darknet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/tinynas_csp.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/tinynas_csp.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/tinynas_res.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/tinynas_res.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/base_ops.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/core/base_ops.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/neck_ops.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/core/neck_ops.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/ops.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/core/ops.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/ota_assigner.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/core/ota_assigner.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/repvgg_block.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/core/repvgg_block.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/core/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/weight_init.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/core/weight_init.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/heads/gfocal_v2_tiny.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/heads/gfocal_v2_tiny.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/heads/zero_head.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/heads/zero_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/losses/distill_loss.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/losses/distill_loss.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/losses/gfocal_loss.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/losses/gfocal_loss.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/necks/giraffe_config.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/necks/giraffe_config.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/necks/giraffe_fpn.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/necks/giraffe_fpn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/base_models/necks/giraffe_fpn_btn.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/base_models/necks/giraffe_fpn_btn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/detectors/detector.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/detectors/detector.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/structures/bounding_box.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/structures/bounding_box.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/structures/boxlist_ops.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/structures/boxlist_ops.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/structures/image_list.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/structures/image_list.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/utils/boxes.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/utils/boxes.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/utils/model_utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/utils/model_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_detection/damo/utils/scheduler.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/damo/utils/scheduler.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_detection/detector.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/detector.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_detection/tinynas_damoyolo.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/tinynas_damoyolo.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_detection/tinynas_detector.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/tinynas_detector.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/tinynas_detection/utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/tinynas_detection/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_deinterlace/UNet_for_video_deinterlace.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_deinterlace/UNet_for_video_deinterlace.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_deinterlace/deinterlace_arch.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_deinterlace/deinterlace_arch.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_deinterlace/models/archs.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_deinterlace/models/archs.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_deinterlace/models/deep_fourier_upsampling.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_deinterlace/models/deep_fourier_upsampling.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_deinterlace/models/enh.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_deinterlace/models/enh.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_deinterlace/models/fre.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_deinterlace/models/fre.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_deinterlace/models/utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_deinterlace/models/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/configs/default_config.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/configs/default_config.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/dro_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/dro_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/geometry/camera.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/geometry/camera.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/geometry/camera_utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/geometry/camera_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/geometry/pose.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/geometry/pose.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/geometry/pose_utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/geometry/pose_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/models/model_checkpoint.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/models/model_checkpoint.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/models/model_utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/models/model_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/models/model_wrapper.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/models/model_wrapper.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/models/sfm_model_mf.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/models/sfm_model_mf.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/models/sup_model_mf.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/models/sup_model_mf.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/networks/depth_pose/depth_pose_net.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/networks/depth_pose/depth_pose_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/depth_decoder.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/depth_decoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/layers.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/layers.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/pose_decoder.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/pose_decoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/resnet_encoder.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/resnet_encoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/networks/optim/extractor.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/networks/optim/extractor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/networks/optim/update.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/networks/optim/update.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/utils/augmentations.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/utils/augmentations.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/utils/config.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/utils/config.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/utils/depth.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/utils/depth.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/utils/horovod.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/utils/horovod.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/utils/image.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/utils/image.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/utils/image_gt.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/utils/image_gt.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/utils/load.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/utils/load.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/utils/misc.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/utils/misc.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_depth_estimation/utils/types.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_depth_estimation/utils/types.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_frame_interpolation/VFINet_arch.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_frame_interpolation/VFINet_arch.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_frame_interpolation/VFINet_for_video_frame_interpolation.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_frame_interpolation/VFINet_for_video_frame_interpolation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_frame_interpolation/flow_model/corr.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_frame_interpolation/flow_model/corr.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_frame_interpolation/flow_model/extractor.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_frame_interpolation/flow_model/extractor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_frame_interpolation/flow_model/raft.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_frame_interpolation/flow_model/raft.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_frame_interpolation/flow_model/update.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_frame_interpolation/flow_model/update.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_frame_interpolation/interp_model/IFNet_swin.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_frame_interpolation/interp_model/IFNet_swin.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_frame_interpolation/interp_model/UNet.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_frame_interpolation/interp_model/UNet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_frame_interpolation/interp_model/flow_reversal.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_frame_interpolation/interp_model/flow_reversal.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_frame_interpolation/interp_model/refinenet_arch.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_frame_interpolation/interp_model/refinenet_arch.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_frame_interpolation/interp_model/transformer_layers.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_frame_interpolation/interp_model/transformer_layers.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_frame_interpolation/utils/scene_change_detection.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_frame_interpolation/utils/scene_change_detection.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_frame_interpolation/utils/utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_frame_interpolation/utils/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_human_matting/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_human_matting/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_human_matting/model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_human_matting/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_human_matting/models/decoder.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_human_matting/models/decoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_human_matting/models/deep_guided_filter.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_human_matting/models/deep_guided_filter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_human_matting/models/effv2.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_human_matting/models/effv2.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_human_matting/models/lraspp.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_human_matting/models/lraspp.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_human_matting/models/matting.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_human_matting/models/matting.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_inpainting/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_inpainting/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_inpainting/inpainting.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_inpainting/inpainting.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_inpainting/inpainting_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_inpainting/inpainting_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_instance_segmentation/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_instance_segmentation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_instance_segmentation/head/kernel_frame_iter_head.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_instance_segmentation/head/kernel_frame_iter_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_instance_segmentation/head/kernel_head.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_instance_segmentation/head/kernel_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_instance_segmentation/head/kernel_iter_head.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_instance_segmentation/head/kernel_iter_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_instance_segmentation/head/kernel_update_head.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_instance_segmentation/head/kernel_update_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_instance_segmentation/head/kernel_updator.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_instance_segmentation/head/kernel_updator.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_instance_segmentation/neck/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_instance_segmentation/neck/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_instance_segmentation/neck/msdeformattn_decoder.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_instance_segmentation/neck/msdeformattn_decoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_instance_segmentation/track/kernel_update_head.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_instance_segmentation/track/kernel_update_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_instance_segmentation/track/mask_hungarian_assigner.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_instance_segmentation/track/mask_hungarian_assigner.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_instance_segmentation/utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_instance_segmentation/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_instance_segmentation/video_knet.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_instance_segmentation/video_knet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_multi_object_tracking/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_multi_object_tracking/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_multi_object_tracking/models/common.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_multi_object_tracking/models/common.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_multi_object_tracking/models/decode.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_multi_object_tracking/models/decode.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_multi_object_tracking/models/model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_multi_object_tracking/models/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_multi_object_tracking/models/yolo.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_multi_object_tracking/models/yolo.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_multi_object_tracking/tracker/basetrack.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_multi_object_tracking/tracker/basetrack.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_multi_object_tracking/tracker/matching.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_multi_object_tracking/tracker/matching.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_multi_object_tracking/tracker/multitracker.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_multi_object_tracking/tracker/multitracker.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_multi_object_tracking/utils/image.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_multi_object_tracking/utils/image.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_multi_object_tracking/utils/kalman_filter.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_multi_object_tracking/utils/kalman_filter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_multi_object_tracking/utils/utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_multi_object_tracking/utils/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_multi_object_tracking/utils/visualization.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_multi_object_tracking/utils/visualization.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_object_segmentation/aggregate.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_object_segmentation/aggregate.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_object_segmentation/cbam.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_object_segmentation/cbam.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_object_segmentation/eval_network.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_object_segmentation/eval_network.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_object_segmentation/inference_core.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_object_segmentation/inference_core.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_object_segmentation/inference_memory_bank.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_object_segmentation/inference_memory_bank.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_object_segmentation/mod_resnet.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_object_segmentation/mod_resnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_object_segmentation/model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_object_segmentation/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_object_segmentation/modules.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_object_segmentation/modules.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_object_segmentation/network.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_object_segmentation/network.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_panoptic_segmentation/backbone/swin_checkpoint.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_panoptic_segmentation/backbone/swin_checkpoint.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_panoptic_segmentation/backbone/swin_transformer.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_panoptic_segmentation/backbone/swin_transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_panoptic_segmentation/head/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_panoptic_segmentation/head/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_panoptic_segmentation/head/kernel_head.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_panoptic_segmentation/head/kernel_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_panoptic_segmentation/head/kernel_iter_head.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_panoptic_segmentation/head/kernel_iter_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_panoptic_segmentation/head/kernel_update_head.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_panoptic_segmentation/head/kernel_update_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_panoptic_segmentation/head/kernel_updator.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_panoptic_segmentation/head/kernel_updator.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_panoptic_segmentation/head/mask.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_panoptic_segmentation/head/mask.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_panoptic_segmentation/head/semantic_fpn_wrapper.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_panoptic_segmentation/head/semantic_fpn_wrapper.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_panoptic_segmentation/head/track_heads.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_panoptic_segmentation/head/track_heads.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_panoptic_segmentation/neck/fpn.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_panoptic_segmentation/neck/fpn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_panoptic_segmentation/track/quasi_dense_embed_tracker.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_panoptic_segmentation/track/quasi_dense_embed_tracker.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_panoptic_segmentation/video_k_net.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_panoptic_segmentation/video_k_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_panoptic_segmentation/visualizer.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_panoptic_segmentation/visualizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/config/ostrack.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/config/ostrack.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/models/layers/attn.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/models/layers/attn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/models/layers/attn_blocks.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/models/layers/attn_blocks.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/models/layers/head.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/models/layers/head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/models/layers/patch_embed.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/models/layers/patch_embed.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/models/ostrack/base_backbone.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/models/ostrack/base_backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/models/ostrack/ostrack.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/models/ostrack/ostrack.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/models/ostrack/utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/models/ostrack/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/models/ostrack/vit_ce.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/models/ostrack/vit_ce.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/models/procontext/procontext.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/models/procontext/procontext.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/models/procontext/utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/models/procontext/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/models/procontext/vit_ce.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/models/procontext/vit_ce.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/tracker/ostrack.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/tracker/ostrack.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/tracker/procontext.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/tracker/procontext.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_single_object_tracking/utils/utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_single_object_tracking/utils/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_stabilization/DUT/DUT_raft.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/DUT/DUT_raft.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_stabilization/DUT/MotionPro.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/DUT/MotionPro.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_stabilization/DUT/RAFT/corr.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/DUT/RAFT/corr.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_stabilization/DUT/RAFT/extractor.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/DUT/RAFT/extractor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_stabilization/DUT/RAFT/raft.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/DUT/RAFT/raft.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_stabilization/DUT/RAFT/update.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/DUT/RAFT/update.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_stabilization/DUT/Smoother.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/DUT/Smoother.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_stabilization/DUT/config.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/DUT/config.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_stabilization/DUT/rf_det_module.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/DUT/rf_det_module.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_stabilization/DUT/rf_det_so.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/DUT/rf_det_so.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_stabilization/DUTRAFTStabilizer.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/DUTRAFTStabilizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_stabilization/utils/IterativeSmooth.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/utils/IterativeSmooth.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_stabilization/utils/MedianFilter.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/utils/MedianFilter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_stabilization/utils/ProjectionUtils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/utils/ProjectionUtils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_stabilization/utils/RAFTUtils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/utils/RAFTUtils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_stabilization/utils/WarpUtils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/utils/WarpUtils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_stabilization/utils/image_utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/utils/image_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_stabilization/utils/math_utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_stabilization/utils/math_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_streaming_perception/longshortnet/exp/longshortnet_base.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_streaming_perception/longshortnet/exp/longshortnet_base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_streaming_perception/longshortnet/longshortnet.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_streaming_perception/longshortnet/longshortnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_streaming_perception/longshortnet/models/dfp_pafpn_long.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_streaming_perception/longshortnet/models/dfp_pafpn_long.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_streaming_perception/longshortnet/models/dfp_pafpn_short.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_streaming_perception/longshortnet/models/dfp_pafpn_short.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_streaming_perception/longshortnet/models/longshort.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_streaming_perception/longshortnet/models/longshort.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_streaming_perception/longshortnet/models/longshort_backbone_neck.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_streaming_perception/longshortnet/models/longshort_backbone_neck.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_summarization/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_summarization/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_summarization/base_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_summarization/base_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_summarization/kts/cpd_auto.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_summarization/kts/cpd_auto.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_summarization/kts/cpd_nonlin.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_summarization/kts/cpd_nonlin.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_summarization/pgl_sum.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_summarization/pgl_sum.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_summarization/summarizer.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_summarization/summarizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_super_resolution/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_super_resolution/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_super_resolution/basicvsr_net.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_super_resolution/basicvsr_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_super_resolution/common.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_super_resolution/common.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_super_resolution/msrresnet_lite_model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_super_resolution/msrresnet_lite_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_super_resolution/real_basicvsr_for_video_super_resolution.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_super_resolution/real_basicvsr_for_video_super_resolution.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/video_super_resolution/real_basicvsr_net.py` & `modelscope-1.8.0rc0/modelscope/models/cv/video_super_resolution/real_basicvsr_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/vidt/backbone.py` & `modelscope-1.8.0rc0/modelscope/models/cv/vidt/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/vidt/deformable_transformer.py` & `modelscope-1.8.0rc0/modelscope/models/cv/vidt/deformable_transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/vidt/fpn_fusion.py` & `modelscope-1.8.0rc0/modelscope/models/cv/vidt/fpn_fusion.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/vidt/head.py` & `modelscope-1.8.0rc0/modelscope/models/cv/vidt/head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/vidt/model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/vidt/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/virual_tryon/sdafnet.py` & `modelscope-1.8.0rc0/modelscope/models/cv/virual_tryon/sdafnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/vision_efficient_tuning/backbone.py` & `modelscope-1.8.0rc0/modelscope/models/cv/vision_efficient_tuning/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/vision_efficient_tuning/head.py` & `modelscope-1.8.0rc0/modelscope/models/cv/vision_efficient_tuning/head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/vision_efficient_tuning/model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/vision_efficient_tuning/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/vision_efficient_tuning/petl.py` & `modelscope-1.8.0rc0/modelscope/models/cv/vision_efficient_tuning/petl.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/vision_efficient_tuning/timm_helpers.py` & `modelscope-1.8.0rc0/modelscope/models/cv/vision_efficient_tuning/timm_helpers.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/vision_efficient_tuning/timm_vision_transformer.py` & `modelscope-1.8.0rc0/modelscope/models/cv/vision_efficient_tuning/timm_vision_transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/vision_efficient_tuning/timm_weight_init.py` & `modelscope-1.8.0rc0/modelscope/models/cv/vision_efficient_tuning/timm_weight_init.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/vision_efficient_tuning/vision_efficient_tuning.py` & `modelscope-1.8.0rc0/modelscope/models/cv/vision_efficient_tuning/vision_efficient_tuning.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/vision_middleware/backbone.py` & `modelscope-1.8.0rc0/modelscope/models/cv/vision_middleware/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/vision_middleware/head.py` & `modelscope-1.8.0rc0/modelscope/models/cv/vision_middleware/head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/vision_middleware/model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/vision_middleware/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/vision_middleware/vim.py` & `modelscope-1.8.0rc0/modelscope/models/cv/vision_middleware/vim.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/vop_retrieval/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/cv/vop_retrieval/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/vop_retrieval/backbone.py` & `modelscope-1.8.0rc0/modelscope/models/cv/vop_retrieval/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/vop_retrieval/basic_utils.py` & `modelscope-1.8.0rc0/modelscope/models/cv/vop_retrieval/basic_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/vop_retrieval/model.py` & `modelscope-1.8.0rc0/modelscope/models/cv/vop_retrieval/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/vop_retrieval/model_se.py` & `modelscope-1.8.0rc0/modelscope/models/cv/vop_retrieval/model_se.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/cv/vop_retrieval/tokenization_clip.py` & `modelscope-1.8.0rc0/modelscope/models/cv/vop_retrieval/tokenization_clip.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/clip/bert_tokenizer.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/clip/bert_tokenizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/clip/configuration_bert.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/clip/configuration_bert.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/clip/model.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/clip/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/clip/modeling_bert.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/clip/modeling_bert.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/clip_interrogator/model.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/clip_interrogator/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/diffusion/diffusion.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/diffusion/diffusion.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/diffusion/model.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/diffusion/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/diffusion/structbert.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/diffusion/structbert.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/diffusion/tokenizer.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/diffusion/tokenizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/diffusion/unet_generator.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/diffusion/unet_generator.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/diffusion/unet_upsampler_1024.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/diffusion/unet_upsampler_1024.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/diffusion/unet_upsampler_256.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/diffusion/unet_upsampler_256.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/dpm_solver_pytorch.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/dpm_solver_pytorch.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/efficient_diffusion_tuning/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/efficient_diffusion_tuning/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/efficient_diffusion_tuning/efficient_stable_diffusion.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/efficient_diffusion_tuning/efficient_stable_diffusion.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/gemm/gemm_base.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/gemm/gemm_base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/gemm/gemm_model.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/gemm/gemm_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/gemm/tokenizer.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/gemm/tokenizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/guided_diffusion/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/guided_diffusion/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/guided_diffusion/gaussian_diffusion.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/guided_diffusion/gaussian_diffusion.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/guided_diffusion/respace.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/guided_diffusion/respace.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/guided_diffusion/script.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/guided_diffusion/script.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/guided_diffusion/unet.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/guided_diffusion/unet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/mgeo/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/mgeo/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/mgeo/backbone.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/mgeo/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/mgeo/text_classification.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/mgeo/text_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/mgeo/text_ranking.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/mgeo/text_ranking.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/mgeo/token_classification.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/mgeo/token_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/mmr/dataloaders/rawvideo_util.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/mmr/dataloaders/rawvideo_util.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/mmr/models/clip_for_mm_video_embedding.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/mmr/models/clip_for_mm_video_embedding.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/mmr/models/dynamic_inverted_softmax.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/mmr/models/dynamic_inverted_softmax.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/mmr/models/modeling.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/mmr/models/modeling.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/mmr/models/module_clip.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/mmr/models/module_clip.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/mmr/models/module_cross.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/mmr/models/module_cross.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/mmr/models/tokenization_clip.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/mmr/models/tokenization_clip.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/mmr/models/until_module.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/mmr/models/until_module.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/mplug/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/mplug/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/mplug/clip/clip.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/mplug/clip/clip.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/mplug/configuration_mplug.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/mplug/configuration_mplug.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/mplug/modeling_mplug.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/mplug/modeling_mplug.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/mplug/mvit.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/mplug/mvit.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/mplug/predictor.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/mplug/predictor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/mplug_for_all_tasks.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/mplug_for_all_tasks.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/mplug_owl/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/mplug_owl/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/mplug_owl/configuration_mplug_owl.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/mplug_owl/configuration_mplug_owl.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/mplug_owl/modeling_mplug_owl.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/mplug_owl/modeling_mplug_owl.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/multi_stage_diffusion/clip.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/multi_stage_diffusion/clip.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/multi_stage_diffusion/decoder.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/multi_stage_diffusion/decoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/multi_stage_diffusion/gaussian_diffusion.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/multi_stage_diffusion/gaussian_diffusion.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/multi_stage_diffusion/model.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/multi_stage_diffusion/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/multi_stage_diffusion/prior.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/multi_stage_diffusion/prior.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/multi_stage_diffusion/tokenizer.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/multi_stage_diffusion/tokenizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/multi_stage_diffusion/upsampler.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/multi_stage_diffusion/upsampler.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/multi_stage_diffusion/xglm.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/multi_stage_diffusion/xglm.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/ofa/configuration_mmspeech.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa/configuration_mmspeech.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/ofa/configuration_ofa.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa/configuration_ofa.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/ofa/generate/incremental_decoding_utils.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa/generate/incremental_decoding_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/ofa/generate/multihead_attention.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa/generate/multihead_attention.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/ofa/generate/ngram_repeat_block.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa/generate/ngram_repeat_block.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/ofa/generate/search.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa/generate/search.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/ofa/generate/sequence_generator.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa/generate/sequence_generator.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/ofa/generate/token_generation_constraints.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa/generate/token_generation_constraints.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/ofa/generate/utils.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa/generate/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/ofa/modeling_mmspeech.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa/modeling_mmspeech.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/ofa/modeling_ofa.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa/modeling_ofa.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/ofa/resnet.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa/resnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/ofa/tokenization_ofa.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa/tokenization_ofa.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/ofa/tokenization_ofa_fast.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa/tokenization_ofa_fast.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/ofa/utils/constant.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa/utils/constant.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/ofa/utils/utils.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa/utils/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/ofa/vit.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa/vit.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/ofa_for_all_tasks.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa_for_all_tasks.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/ofa_for_text_to_image_synthesis_model.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/ofa_for_text_to_image_synthesis_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/rleg/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/rleg/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/rleg/model.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/rleg/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/rleg/rleg.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/rleg/rleg.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/soonet/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/soonet/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/soonet/blocks.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/soonet/blocks.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/soonet/clip.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/soonet/clip.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/soonet/model.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/soonet/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/soonet/swin_transformer.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/soonet/swin_transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/soonet/tokenizer.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/soonet/tokenizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/soonet/utils.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/soonet/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/stable_diffusion/stable_diffusion.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/stable_diffusion/stable_diffusion.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/team/team_model.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/team/team_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/team/utils.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/team/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/video_synthesis/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/video_synthesis/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/video_synthesis/autoencoder.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/video_synthesis/autoencoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/video_synthesis/diffusion.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/video_synthesis/diffusion.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/video_synthesis/text_to_video_synthesis_model.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/video_synthesis/text_to_video_synthesis_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/video_synthesis/unet_sd.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/video_synthesis/unet_sd.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/vldoc/conv_fpn_trans.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/vldoc/conv_fpn_trans.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/vldoc/convnext.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/vldoc/convnext.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/vldoc/model.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/vldoc/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/vldoc/modeling_layout_roberta.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/vldoc/modeling_layout_roberta.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/vldoc/processing.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/vldoc/processing.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/vldoc/tokenization.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/vldoc/tokenization.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/multi_modal/vldoc/transformer_local.py` & `modelscope-1.8.0rc0/modelscope/models/multi_modal/vldoc/transformer_local.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/T5/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/T5/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/T5/backbone.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/T5/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/T5/configuration.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/T5/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/T5/text2text_generation.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/T5/text2text_generation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -73,15 +73,14 @@
                        VecoForTokenClassification, VecoModel)
     from .dgds import (DocumentGroundedDialogGenerateModel,
                        DocumentGroundedDialogRetrievalModel,
                        DocumentGroundedDialogRerankModel)
     from .xlm_roberta import XLMRobertaConfig, XLMRobertaModel
     from .llama import LlamaForTextGeneration, LlamaConfig, LlamaModel, LlamaTokenizer, LlamaTokenizerFast
     from .llama2 import Llama2ForTextGeneration, Llama2Config, Llama2Model, Llama2Tokenizer, Llama2TokenizerFast
-    from .qwen import QWenForTextGeneration, QWenConfig, QWenModel, QWenTokenizer
 
 else:
     _import_structure = {
         'bart': ['BartForTextErrorCorrection'],
         'bert': [
             'BertForMaskedLM',
             'BertForTextRanking',
@@ -174,16 +173,14 @@
             'LlamaForTextGeneration', 'LlamaConfig', 'LlamaModel',
             'LlamaTokenizer', 'LlamaTokenizerFast'
         ],
         'llama2': [
             'Llama2ForTextGeneration', 'Llama2Config', 'Llama2Model',
             'Llama2Tokenizer', 'Llama2TokenizerFast'
         ],
-        'qwen':
-        ['QWenForTextGeneration', 'QWenConfig', 'QWenModel', 'QWenTokenizer'],
     }
 
     import sys
 
     sys.modules[__name__] = LazyImportModule(
         __name__,
         globals()['__file__'],
```

### Comparing `modelscope-1.8.0/modelscope/models/nlp/bart/text_error_correction.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/bart/text_error_correction.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/bert/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/bert/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/bert/backbone.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/bert/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/bert/configuration.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/bert/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/bert/document_segmentation.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/bert/document_segmentation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/bert/fill_mask.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/bert/fill_mask.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/bert/sentence_embedding.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/bert/sentence_embedding.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/bert/siamese_uie.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/bert/siamese_uie.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/bert/text_classification.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/bert/text_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/bert/text_ranking.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/bert/text_ranking.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/bert/token_classification.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/bert/token_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/bert/word_alignment.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/bert/word_alignment.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/canmt/canmt_model.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/canmt/canmt_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/canmt/canmt_translation.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/canmt/canmt_translation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/canmt/sequence_generator.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/canmt/sequence_generator.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/chatglm/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/chatglm/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/chatglm/configuration.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/chatglm/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/chatglm/quantization.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/chatglm/quantization.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/chatglm/text_generation.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/chatglm/text_generation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/chatglm/tokenization.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/chatglm/tokenization.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/chatglm2/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/chatglm2/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/chatglm2/configuration.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/chatglm2/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/chatglm2/quantization.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/chatglm2/quantization.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/chatglm2/text_generation.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/chatglm2/text_generation.py`

 * *Files 1% similar despite different names*

```diff
@@ -118,27 +118,21 @@
         return tuple(chunk.contiguous() for chunk in tensor_list)
 
     return tensor_list
 
 
 class RotaryEmbedding(nn.Module):
 
-    def __init__(self,
-                 dim,
-                 rope_ratio=1,
-                 original_impl=False,
-                 device=None,
-                 dtype=None):
+    def __init__(self, dim, original_impl=False, device=None, dtype=None):
         super().__init__()
         inv_freq = 1.0 / (10000**(
             torch.arange(0, dim, 2, device=device).to(dtype=dtype) / dim))
         self.register_buffer('inv_freq', inv_freq)
         self.dim = dim
         self.original_impl = original_impl
-        self.rope_ratio = rope_ratio
 
     def forward_impl(self,
                      seq_len: int,
                      n_elem: int,
                      dtype: torch.dtype,
                      device: torch.device,
                      base: int = 10000):
@@ -150,16 +144,15 @@
         """
         # $\Theta = {\theta_i = 10000^{\frac{2(i-1)}{d}}, i \in [1, 2, ..., \frac{d}{2}]}$
         theta = 1.0 / (
             base**(torch.arange(0, n_elem, 2, dtype=dtype, device=device)
                    / n_elem))
 
         # Create position indexes `[0, 1, ..., seq_len - 1]`
-        seq_idx = torch.arange(
-            seq_len, dtype=dtype, device=device) / self.rope_ratio
+        seq_idx = torch.arange(seq_len, dtype=dtype, device=device)
 
         # Calculate the product of position index and $\theta_i$
         idx_theta = torch.outer(seq_idx, theta).float()
 
         cache = torch.stack(
             [torch.cos(idx_theta), torch.sin(idx_theta)], dim=-1)
 
@@ -867,15 +860,14 @@
         self.seq_length = config.seq_length
         rotary_dim = (
             config.hidden_size // config.num_attention_heads
             if config.kv_channels is None else config.kv_channels)
 
         self.rotary_pos_emb = RotaryEmbedding(
             rotary_dim // 2,
-            rope_ratio=config.rope_ratio,
             original_impl=config.original_rope,
             device=device,
             dtype=config.torch_dtype)
         self.encoder = init_method(GLMTransformer, config, **init_kwargs)
         self.output_layer = init_method(
             nn.Linear,
             config.hidden_size,
@@ -1173,28 +1165,26 @@
         return inputs
 
     @torch.no_grad()
     def _chat(self,
               tokenizer,
               query: str,
               history: List[Tuple[str, str]] = None,
-              max_length: int = None,
+              max_length: int = 8192,
               num_beams=1,
               do_sample=True,
               top_p=0.8,
               temperature=0.8,
               logits_processor=None,
               **kwargs):
         if history is None:
             history = []
         if logits_processor is None:
             logits_processor = LogitsProcessorList()
         logits_processor.append(InvalidScoreLogitsProcessor())
-        if max_length is None:
-            max_length = self.seq_length
         gen_kwargs = {
             'max_length': max_length,
             'num_beams': num_beams,
             'do_sample': do_sample,
             'top_p': top_p,
             'temperature': temperature,
             'logits_processor': logits_processor,
@@ -1210,28 +1200,26 @@
 
     @torch.no_grad()
     def stream_chat(self,
                     tokenizer,
                     query: str,
                     history: List[Tuple[str, str]] = None,
                     past_key_values=None,
-                    max_length: int = None,
+                    max_length: int = 8192,
                     do_sample=True,
                     top_p=0.8,
                     temperature=0.8,
                     logits_processor=None,
                     return_past_key_values=False,
                     **kwargs):
         if history is None:
             history = []
         if logits_processor is None:
             logits_processor = LogitsProcessorList()
         logits_processor.append(InvalidScoreLogitsProcessor())
-        if max_length is None:
-            max_length = self.seq_length
         gen_kwargs = {
             'max_length': max_length,
             'do_sample': do_sample,
             'top_p': top_p,
             'temperature': temperature,
             'logits_processor': logits_processor,
             **kwargs
```

### Comparing `modelscope-1.8.0/modelscope/models/nlp/chatglm2/tokenization.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/chatglm2/tokenization.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/codegeex/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/codegeex/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/codegeex/codegeex.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/codegeex/codegeex.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/codegeex/codegeex_for_code_generation.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/codegeex/codegeex_for_code_generation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/codegeex/codegeex_for_code_translation.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/codegeex/codegeex_for_code_translation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/codegeex/inference.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/codegeex/inference.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/codegeex/tokenizer.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/codegeex/tokenizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/csanmt/translation.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/csanmt/translation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/deberta_v2/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/deberta_v2/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/deberta_v2/backbone.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/deberta_v2/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/deberta_v2/configuration.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/deberta_v2/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/deberta_v2/fill_mask.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/deberta_v2/fill_mask.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/deberta_v2/tokenization.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/deberta_v2/tokenization.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/deberta_v2/tokenization_fast.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/deberta_v2/tokenization_fast.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/dgds/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/dgds/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/dgds/backbone.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/dgds/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/dgds/document_grounded_dialog_generate.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/dgds/document_grounded_dialog_generate.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/dgds/document_grounded_dialog_rerank.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/dgds/document_grounded_dialog_rerank.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/dgds/document_grounded_dialog_retrieval.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/dgds/document_grounded_dialog_retrieval.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/fid_T5/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/fid_T5/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/fid_T5/text_generation.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/fid_T5/text_generation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/fid_plug/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/fid_plug/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/fid_plug/backbone.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/fid_plug/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/fid_plug/configuration.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/fid_plug/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/fid_plug/text_generation.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/fid_plug/text_generation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/glm_130b/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/glm_130b/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/glm_130b/generation/strategies.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/glm_130b/generation/strategies.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/glm_130b/initialize.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/glm_130b/initialize.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/glm_130b/kernels/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/glm_130b/kernels/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/glm_130b/quantization/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/glm_130b/quantization/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/glm_130b/quantization/functional.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/glm_130b/quantization/functional.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/glm_130b/quantization/layers.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/glm_130b/quantization/layers.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/glm_130b/text_generation.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/glm_130b/text_generation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/gpt3/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/gpt3/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/gpt3/backbone.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/gpt3/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/gpt3/configuration.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/gpt3/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/gpt3/distributed_gpt3.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/gpt3/distributed_gpt3.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/gpt3/text_generation.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/gpt3/text_generation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/gpt3/tokenizer.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/gpt3/tokenizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/gpt_moe/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/gpt_moe/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/gpt_moe/backbone.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/gpt_moe/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/gpt_moe/checkpointing.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/gpt_moe/checkpointing.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/gpt_moe/configuration.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/gpt_moe/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/gpt_moe/distributed_gpt_moe.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/gpt_moe/distributed_gpt_moe.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/gpt_moe/moe/experts.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/gpt_moe/moe/experts.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/gpt_moe/moe/layer.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/gpt_moe/moe/layer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/gpt_moe/moe/mappings.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/gpt_moe/moe/mappings.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/gpt_moe/moe/sharded_moe.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/gpt_moe/moe/sharded_moe.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/gpt_moe/moe/utils.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/gpt_moe/moe/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/gpt_moe/text_generation.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/gpt_moe/text_generation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/gpt_moe/tokenizer.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/gpt_moe/tokenizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/gpt_neo/backbone.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/gpt_neo/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/heads/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/heads/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/heads/crf_head.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/heads/crf_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/heads/fill_mask_head.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/heads/fill_mask_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/heads/infromation_extraction_head.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/heads/infromation_extraction_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/heads/text_classification_head.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/heads/text_classification_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/heads/text_generation_head.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/heads/text_generation_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/heads/text_ranking_head.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/heads/text_ranking_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/heads/token_classification_head.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/heads/token_classification_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/heads/torch_pretrain_head.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/heads/torch_pretrain_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/hf_transformers/backbone.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/hf_transformers/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/llama/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/llama/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/llama/backbone.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/llama/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/llama/configuration.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/llama/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/llama/convert_llama_weights_to_hf.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/llama/convert_llama_weights_to_hf.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/llama/text_generation.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/llama/text_generation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/llama/tokenization.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/llama/tokenization.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/llama/tokenization_fast.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/llama/tokenization_fast.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/llama2/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/llama2/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/llama2/backbone.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/llama2/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/llama2/configuration.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/llama2/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/llama2/text_generation.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/llama2/text_generation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/llama2/tokenization.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/llama2/tokenization.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/llama2/tokenization_fast.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/llama2/tokenization_fast.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/lstm/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/lstm/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/lstm/backbone.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/lstm/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/lstm/token_classification.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/lstm/token_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/megatron_bert/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/megatron_bert/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/megatron_bert/backbone.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/megatron_bert/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/megatron_bert/configuration.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/megatron_bert/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/megatron_bert/fill_mask.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/megatron_bert/fill_mask.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/mglm/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/mglm/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/mglm/arguments.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/mglm/arguments.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/mglm/blocklm_utils.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/mglm/blocklm_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/mglm/configure_data.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/mglm/configure_data.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/mglm/data_utils/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/mglm/data_utils/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/mglm/data_utils/corpora.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/mglm/data_utils/corpora.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/mglm/data_utils/datasets.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/mglm/data_utils/datasets.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/mglm/data_utils/extraction.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/mglm/data_utils/extraction.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/mglm/data_utils/file_utils.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/mglm/data_utils/file_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/mglm/data_utils/lazy_loader.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/mglm/data_utils/lazy_loader.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/mglm/data_utils/samplers.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/mglm/data_utils/samplers.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/mglm/data_utils/sp_tokenizer.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/mglm/data_utils/sp_tokenizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/mglm/data_utils/tokenization.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/mglm/data_utils/tokenization.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/mglm/data_utils/tokenization_gpt2.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/mglm/data_utils/tokenization_gpt2.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/mglm/data_utils/wordpiece.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/mglm/data_utils/wordpiece.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/mglm/generation_utils.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/mglm/generation_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/mglm/mglm_for_text_summarization.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/mglm/mglm_for_text_summarization.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/mglm/model/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/mglm/model/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/mglm/model/distributed.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/mglm/model/distributed.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/mglm/model/downstream.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/mglm/model/downstream.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/mglm/model/modeling_bert.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/mglm/model/modeling_bert.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/mglm/model/modeling_glm.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/mglm/model/modeling_glm.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/mglm/model/prompt.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/mglm/model/prompt.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/mglm/model/transformer.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/mglm/model/transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/mglm/process_grid.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/mglm/process_grid.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/mglm/test/test_block.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/mglm/test/test_block.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/mglm/test/test_rel_shift.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/mglm/test/test_rel_shift.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/mglm/train_utils.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/mglm/train_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/mglm/utils.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/mglm/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/palm_v2/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/palm_v2/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/palm_v2/configuration.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/palm_v2/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/palm_v2/dureader_eval.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/palm_v2/dureader_eval.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/palm_v2/text_generation.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/palm_v2/text_generation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/peer/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/peer/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/peer/backbone.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/peer/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/peer/configuration.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/peer/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/peer/sas_utils.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/peer/sas_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/peer/text_classification.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/peer/text_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/plug/AnnealingLR.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/plug/AnnealingLR.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/plug/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/plug/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/plug/backbone.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/plug/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/plug/configuration.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/plug/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/plug/distributed_plug.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/plug/distributed_plug.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/plug/generator.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/plug/generator.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/plug_mental/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/plug_mental/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/plug_mental/adv_utils.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/plug_mental/adv_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/plug_mental/backbone.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/plug_mental/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/plug_mental/configuration.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/plug_mental/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/plug_mental/text_classification.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/plug_mental/text_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/polylm/text_generation.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/polylm/text_generation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/ponet/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/ponet/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/ponet/backbone.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/ponet/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/ponet/configuration.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/ponet/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/ponet/document_segmentation.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/ponet/document_segmentation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/ponet/fill_mask.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/ponet/fill_mask.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/ponet/tokenization.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/ponet/tokenization.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/qwen/backbone.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/multi_modal.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,778 +1,804 @@
-# Copyright (c) Alibaba Cloud.
-#
-# This source code is licensed under the license found in the
-# LICENSE file in the root directory of this source tree.
-
-import importlib
-import math
-from typing import Optional, Tuple, Union
-
+# Copyright (c) Alibaba, Inc. and its affiliates.
+import os.path as osp
+import re
+from io import BytesIO
+from typing import Any, Dict, List, Tuple, Union
+
+import decord
+import json
+import numpy as np
 import torch
-import torch.nn.functional as F
-import torch.utils.checkpoint
-from torch import nn
-from torch.cuda.amp import autocast
-from torch.nn import CrossEntropyLoss
-from transformers import PreTrainedTokenizer
-from transformers.modeling_outputs import (BaseModelOutputWithPast,
-                                           CausalLMOutputWithPast)
-from transformers.modeling_utils import PreTrainedModel
-from transformers.trainer_utils import set_seed
-from transformers.utils import (ModelOutput, add_code_sample_docstrings,
-                                add_start_docstrings,
-                                add_start_docstrings_to_model_forward, logging)
-from transformers.utils.model_parallel_utils import (assert_device_map,
-                                                     get_device_map)
-
-from modelscope import Model, TorchModel
-from modelscope.metainfo import Models
-from modelscope.utils.constant import Tasks
-from modelscope.utils.logger import get_logger
-from ... import MODELS
-from .configuration import QWenConfig
-
-try:
-    from einops import rearrange
-except ImportError:
-    rearrange = None
-try:
-    from flash_attn.layers.rotary import apply_rotary_emb_func
-    from einops import rearrange
-    use_flash_rotary = True
-    print('use flash_attn rotary')
-except ImportError:
-    use_flash_rotary = False
-    print('import flash_attn rotary fail')
-
-try:
-    from flash_attn.ops.rms_norm import rms_norm
-    print('use flash_attn rms_norm')
-except ImportError:
-    rms_norm = None
-    print('import flash_attn rms_norm fail')
-
-logger = get_logger()
-
-_CHECKPOINT_FOR_DOC = 'qwen-7b'
-_CONFIG_FOR_DOC = 'QWenConfig'
-
-QWen_PRETRAINED_MODEL_ARCHIVE_LIST = ['qwen-7b']
-
-try:
-    from flash_attn.flash_attn_interface import flash_attn_unpadded_func
-except ImportError:
-    flash_attn_unpadded_func = None
-
-
-class FlashSelfAttention(torch.nn.Module):
-
-    def __init__(self,
-                 causal=False,
-                 softmax_scale=None,
-                 attention_dropout=0.0,
-                 device=None,
-                 dtype=None):
-        super().__init__()
-        assert flash_attn_unpadded_func is not None, (
-            'Please install FlashAttention first, '
-            'e.g., with pip install flash-attn')
-        assert rearrange is not None, 'Please install einops first, e.g., with pip install einops'
-        self.causal = causal
-        self.softmax_scale = softmax_scale
-        self.dropout_p = attention_dropout
-
-    def forward(self, q, k, v):
-        assert all(
-            (i.dtype in [torch.float16, torch.bfloat16] for i in (q, k, v)))
-        assert all((i.is_cuda for i in (q, k, v)))
-        batch_size, seqlen_q = q.shape[0], q.shape[1]
-        seqlen_k = k.shape[1]
-        q, k, v = [rearrange(x, 'b s ... -> (b s) ...') for x in [q, k, v]]
-        cu_seqlens_q = torch.arange(
-            0, (batch_size + 1) * seqlen_q,
-            step=seqlen_q,
-            dtype=torch.int32,
-            device=q.device)
-
-        if self.training:
-            assert seqlen_k == seqlen_q
-
-            is_causal = self.causal
-            cu_seqlens_k = cu_seqlens_q
-        else:
-            is_causal = seqlen_q == seqlen_k
-            cu_seqlens_k = torch.arange(
-                0, (batch_size + 1) * seqlen_k,
-                step=seqlen_k,
-                dtype=torch.int32,
-                device=q.device)
-            self.dropout_p = 0
-        output = flash_attn_unpadded_func(
-            q,
-            k,
-            v,
-            cu_seqlens_q,
-            cu_seqlens_k,
-            seqlen_q,
-            seqlen_k,
-            self.dropout_p,
-            softmax_scale=self.softmax_scale,
-            causal=is_causal)
+from PIL import Image
+from timm.data import create_transform
+from torchvision import transforms
+from torchvision.datasets import ImageFolder
+from torchvision.transforms import Compose, Normalize, Resize, ToTensor
+
+from modelscope.hub.snapshot_download import snapshot_download
+from modelscope.metainfo import Preprocessors
+from modelscope.pipelines.base import Input
+from modelscope.pipelines.cv.cmdssl_video_embedding_pipeline import (
+    VCenterCrop, VCompose, VNormalize, VRescale, VToTensor)
+from modelscope.preprocessors import load_image
+from modelscope.utils.config import Config
+from modelscope.utils.constant import (Fields, Invoke, ModeKeys, ModelFile,
+                                       Tasks)
+from .base import Preprocessor
+from .builder import PREPROCESSORS
+from .ofa import *  # noqa
+from .ofa.utils.collate import collate_fn
+from .ofa.utils.constant import OFA_TASK_KEY_MAPPING
+
+__all__ = [
+    'DiffusionImageGenerationPreprocessor', 'OfaPreprocessor',
+    'MPlugPreprocessor', 'HiTeAPreprocessor', 'MplugOwlPreprocessor'
+]
+
+
+@PREPROCESSORS.register_module(
+    Fields.multi_modal,
+    module_name=Preprocessors.diffusion_image_generation_preprocessor)
+class DiffusionImageGenerationPreprocessor(Preprocessor):
+    """ Preprocessor the data with the combination of image and text.
+        Args:
+            data: process the value as an image for keys ending with 'FILE'
+                or existing in preprocessor_image_keys and pass-through the values of other keys.
+
+    """
+
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        self.preprocessor_resolution = kwargs.pop('resolution', 512)
+        self.preprocessor_mean = kwargs.pop('mean', [0.5])
+        self.preprocessor_std = kwargs.pop('std', [0.5])
+        self.preprocessor_image_keys = set(kwargs.pop('image_keys', []))
+        self.transform_input = transforms.Compose([
+            transforms.Resize(
+                self.preprocessor_resolution,
+                interpolation=transforms.InterpolationMode.BILINEAR),
+            transforms.ToTensor(),
+            transforms.Normalize(self.preprocessor_mean,
+                                 self.preprocessor_std),
+        ])
 
-        output = rearrange(output, '(b s) ... -> b s ...', b=batch_size)
-        return output
+    def __call__(self, data) -> Dict[str, Any]:
+        results = {}
+        for key, value in data.items():
+            if key.endswith(':FILE') or key in self.preprocessor_image_keys:
+                image = load_image(value)
+                img = self.transform_input(image)
+                results[key.replace(':FILE', '').lower()] = img
+            else:
+                results[key.lower()] = value if value else ''
+        return results
 
 
-class QWenAttention(nn.Module):
+@PREPROCESSORS.register_module(
+    Fields.multi_modal, module_name=Preprocessors.ofa_tasks_preprocessor)
+class OfaPreprocessor(Preprocessor):
 
-    def __init__(self, config, layer_number=None):
-        super().__init__()
+    def __init__(self,
+                 model_dir: str,
+                 mode=ModeKeys.INFERENCE,
+                 *args,
+                 **kwargs):
+        """preprocess the data
+
+        Args:
+            model_dir (str): model path
+            mode: preprocessor mode (model mode)
+        """
+        super().__init__(*args, **kwargs)
+        preprocess_mapping = {
+            Tasks.ocr_recognition: OfaOcrRecognitionPreprocessor,
+            Tasks.image_captioning: OfaImageCaptioningPreprocessor,
+            Tasks.visual_grounding: OfaVisualGroundingPreprocessor,
+            Tasks.visual_question_answering:
+            OfaVisualQuestionAnsweringPreprocessor,
+            Tasks.visual_entailment: OfaVisualEntailmentPreprocessor,
+            Tasks.image_classification: OfaImageClassificationPreprocessor,
+            Tasks.text_classification: OfaTextClassificationPreprocessor,
+            Tasks.text_summarization: OfaSummarizationPreprocessor,
+            Tasks.text_to_image_synthesis: OfaTextToImageSynthesisPreprocessor,
+            Tasks.auto_speech_recognition: OfaASRPreprocessor,
+            Tasks.sudoku: OfaSudokuPreprocessor,
+            Tasks.text2sql: OfaTextToSqlPreprocessor
+        }
+        model_dir = model_dir if osp.exists(model_dir) else snapshot_download(
+            model_dir, user_agent={Invoke.KEY: Invoke.PREPROCESSOR})
+        self.cfg = Config.from_file(
+            osp.join(model_dir, ModelFile.CONFIGURATION))
+        self.preprocess = preprocess_mapping[self.cfg.task](
+            cfg=self.cfg, model_dir=model_dir, mode=mode)
+        self.keys = OFA_TASK_KEY_MAPPING[self.cfg.task]
+        self.tokenizer = self.preprocess.tokenizer
+        if kwargs.get('no_collate', None):
+            self.no_collate = True
+        else:
+            self.no_collate = False
+
+    # just for modelscope demo
+    def _build_dict(self, input: Union[Input, List[Input]]) -> Dict[str, Any]:
+        data = dict()
+        if not isinstance(input, tuple) and not isinstance(input, list):
+            input = (input, )
+        for key, item in zip(self.keys, input):
+            data[key] = item
+        return data
+
+    def _ofa_input_compatibility_conversion(self, data):  # fake
+        if 'image' in data and self.cfg.model.get('type', None) == 'ofa':
+            if isinstance(data['image'], str):
+                image = load_image(data['image'])
+            else:
+                image = data['image']
+            if image.mode != 'RGB':
+                image = image.convert('RGB')
+            img_buffer = BytesIO()
+            image.save(img_buffer, format='JPEG')
+            data['image'] = Image.open(img_buffer)
+        return data
+
+    def __call__(self, input: Union[str, tuple, Dict[str, Any]], *args,
+                 **kwargs) -> Dict[str, Any]:
+        if isinstance(input, dict):
+            data = input
+        else:
+            data = self._build_dict(input)
+        sample = self.preprocess(data)
+        str_data = dict()
+        for k, v in data.items():
+            str_data[k] = str(v)
+        sample['sample'] = str_data
+        if self.no_collate:
+            return sample
+        else:
+            return collate_fn([sample],
+                              pad_idx=self.tokenizer.pad_token_id,
+                              eos_idx=self.tokenizer.eos_token_id)
+
+
+def _convert_to_rgb(image):
+    return image.convert('RGB')
+
+
+@PREPROCESSORS.register_module(
+    Fields.multi_modal, module_name=Preprocessors.clip_preprocessor)
+class CLIPPreprocessor(Preprocessor):
 
-        max_positions = config.max_position_embeddings
-        self.register_buffer(
-            'bias',
-            torch.tril(
-                torch.ones((max_positions, max_positions),
-                           dtype=torch.bool)).view(1, 1, max_positions,
-                                                   max_positions),
-            persistent=False,
-        )
-        self.register_buffer(
-            'masked_bias', torch.tensor(-1e4), persistent=False)
-        self.layer_number = max(1, layer_number)
-        self.params_dtype = config.params_dtype
-        self.seq_length = config.seq_length
-
-        self.hidden_size = config.hidden_size
-        self.split_size = config.hidden_size
-        self.num_heads = config.num_attention_heads
-        self.head_dim = self.hidden_size // self.num_heads
-
-        self.use_flash_attn = config.use_flash_attn
-        self.scale_attn_weights = True
-
-        self.layer_idx = None
-
-        self.projection_size = config.kv_channels * config.num_attention_heads
-
-        assert self.projection_size % config.num_attention_heads == 0
-        self.hidden_size_per_attention_head = \
-            self.projection_size // config.num_attention_heads
-
-        self.c_attn = nn.Linear(config.hidden_size, 3 * self.projection_size)
-
-        self.c_proj = nn.Linear(
-            config.hidden_size, self.projection_size, bias=not config.no_bias)
-
-        if self.use_flash_attn:
-            self.core_attention_flash = FlashSelfAttention(
-                causal=True, attention_dropout=config.attn_pdrop)
-
-        self.bf16 = config.bf16
-
-        if config.rotary_pct == 1.0:
-            self.rotary_ndims = None
-        else:
-            assert config.rotary_pct < 1
-            self.rotary_ndims = int(self.hidden_size_per_attention_head
-                                    * config.rotary_pct)
-        dim = (
-            self.rotary_ndims if self.rotary_ndims is not None else
-            self.hidden_size_per_attention_head)
-        self.rotary_emb = RotaryEmbedding(
-            dim, base=config.rotary_emb_base, ntk_alpha=config.ntk_alpha)
-
-        self.attn_dropout = nn.Dropout(config.attn_pdrop)
-
-    def _attn(self, query, key, value, attention_mask=None, head_mask=None):
-        attn_weights = torch.matmul(query, key.transpose(-1, -2))
-
-        if self.scale_attn_weights:
-            attn_weights = attn_weights / torch.full(
-                [],
-                value.size(-1)**0.5,
-                dtype=attn_weights.dtype,
-                device=attn_weights.device)
-
-        query_length, key_length = query.size(-2), key.size(-2)
-        causal_mask = self.bias[:, :, key_length
-                                - query_length:key_length, :key_length]
-        mask_value = torch.finfo(attn_weights.dtype).min
-        mask_value = torch.full([], mask_value, dtype=attn_weights.dtype).to(
-            attn_weights.device)
-        attn_weights = torch.where(causal_mask,
-                                   attn_weights.to(attn_weights.dtype),
-                                   mask_value)
-
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
-
-        attn_weights = attn_weights.type(value.dtype)
-        attn_weights = self.attn_dropout(attn_weights)
-
-        if head_mask is not None:
-            attn_weights = attn_weights * head_mask
-
-        attn_output = torch.matmul(attn_weights, value)
-        attn_output = attn_output.transpose(1, 2)
-
-        return attn_output, attn_weights
-
-    def _upcast_and_reordered_attn(self,
-                                   query,
-                                   key,
-                                   value,
-                                   attention_mask=None,
-                                   head_mask=None):
-        bsz, num_heads, q_seq_len, dk = query.size()
-        _, _, k_seq_len, _ = key.size()
-
-        attn_weights = torch.empty(
-            bsz * num_heads,
-            q_seq_len,
-            k_seq_len,
-            dtype=torch.float32,
-            device=query.device)
-
-        scale_factor = 1.0
-        if self.scale_attn_weights:
-            scale_factor /= float(value.size(-1))**0.5
-
-        with autocast(enabled=False):
-            q, k = query.reshape(-1, q_seq_len,
-                                 dk), key.transpose(-1, -2).reshape(
-                                     -1, dk, k_seq_len)
-            attn_weights = torch.baddbmm(
-                attn_weights, q.float(), k.float(), beta=0, alpha=scale_factor)
-            attn_weights = attn_weights.reshape(bsz, num_heads, q_seq_len,
-                                                k_seq_len)
-
-        query_length, key_length = query.size(-2), key.size(-2)
-        causal_mask = self.bias[:, :, key_length
-                                - query_length:key_length, :key_length]
-        mask_value = torch.finfo(attn_weights.dtype).min
-        mask_value = torch.tensor(
-            mask_value, dtype=attn_weights.dtype).to(attn_weights.device)
-        attn_weights = torch.where(causal_mask, attn_weights, mask_value)
-
-        if attention_mask is not None:
-            attn_weights = attn_weights + attention_mask
-
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
-
-        if attn_weights.dtype != torch.float32:
-            raise RuntimeError(
-                'Error with upcasting, attn_weights does not have dtype torch.float32'
+    def __init__(self,
+                 model_dir: str,
+                 mode=ModeKeys.INFERENCE,
+                 *args,
+                 **kwargs):
+        """preprocess the data
+
+        Args:
+            model_dir (str): model path
+            mode: preprocessor mode (model mode)
+        """
+        super().__init__(*args, **kwargs)
+        model_dir = model_dir if osp.exists(model_dir) else snapshot_download(
+            model_dir, user_agent={Invoke.KEY: Invoke.PREPROCESSOR})
+        self.mode = mode
+        # text tokenizer
+        from modelscope.models.multi_modal.clip.bert_tokenizer import FullTokenizer
+        if 'tokenizer' in kwargs and isinstance(kwargs['tokenizer'],
+                                                FullTokenizer):
+            self.tokenizer = kwargs['tokenizer']
+        else:
+            vocab_file = f'{model_dir}/{ModelFile.VOCAB_FILE}'
+            self.tokenizer = FullTokenizer(vocab_file=vocab_file)
+        # image preprocessor
+        if 'resolution' in kwargs and isinstance(kwargs['resolution'], int):
+            self.image_resolution = kwargs['resolution']
+        else:
+            self.image_resolution = json.load(
+                open(
+                    '{}/vision_model_config.json'.format(model_dir),
+                    encoding='utf-8'))['image_resolution']
+        self.img_preprocess = self._build_image_transform()
+        # key mapping
+        # specify the input keys, compatible with training and inference whose key names may be different
+        self.input_keys = {'img': 'img', 'text': 'text'}
+
+    def _build_image_transform(self):
+
+        if self.mode == ModeKeys.TRAIN:
+            transform = create_transform(
+                input_size=self.image_resolution,
+                scale=(0.9, 1.0),
+                is_training=True,
+                color_jitter=None,
+                auto_augment='original',
+                interpolation='bicubic',
+                mean=(0.48145466, 0.4578275, 0.40821073),
+                std=(0.26862954, 0.26130258, 0.27577711),
             )
-        attn_weights = attn_weights.type(value.dtype)
-        attn_weights = self.attn_dropout(attn_weights)
-
-        if head_mask is not None:
-            attn_weights = attn_weights * head_mask
-
-        attn_output = torch.matmul(attn_weights, value)
-
-        return attn_output, attn_weights
-
-    def _split_heads(self, tensor, num_heads, attn_head_size):
-        new_shape = tensor.size()[:-1] + (num_heads, attn_head_size)
-        tensor = tensor.view(new_shape)
-        return tensor
-
-    def _merge_heads(self, tensor, num_heads, attn_head_size):
-        tensor = tensor.contiguous()
-        new_shape = tensor.size()[:-2] + (num_heads * attn_head_size, )
-        return tensor.view(new_shape)
-
-    def forward(self,
-                hidden_states: Optional[Tuple[torch.FloatTensor]],
-                layer_past: Optional[Tuple[torch.Tensor]] = None,
-                attention_mask: Optional[torch.FloatTensor] = None,
-                head_mask: Optional[torch.FloatTensor] = None,
-                encoder_hidden_states: Optional[torch.Tensor] = None,
-                encoder_attention_mask: Optional[torch.FloatTensor] = None,
-                output_attentions: Optional[bool] = False,
-                use_cache: Optional[bool] = False):
-
-        mixed_x_layer = self.c_attn(hidden_states)
-        query, key, value = mixed_x_layer.split(self.split_size, dim=2)
-
-        query = self._split_heads(query, self.num_heads, self.head_dim)
-        key = self._split_heads(key, self.num_heads, self.head_dim)
-        value = self._split_heads(value, self.num_heads, self.head_dim)
-
-        kv_seq_len = hidden_states.size()[1]
-        if layer_past:
-            kv_seq_len += layer_past[0].shape[1]
-        rotary_pos_emb = self.rotary_emb(kv_seq_len).to(hidden_states.device)
-
-        if rotary_pos_emb is not None:
-            if isinstance(rotary_pos_emb, tuple):
-                rotary_pos_emb = rotary_pos_emb
+            transform = Compose(transform.transforms[:-3] + [_convert_to_rgb]
+                                + transform.transforms[-3:])
+        else:
+            transform = Compose([
+                Resize((self.image_resolution, self.image_resolution),
+                       interpolation=Image.BICUBIC),
+                _convert_to_rgb,
+                ToTensor(),
+                Normalize((0.48145466, 0.4578275, 0.40821073),
+                          (0.26862954, 0.26130258, 0.27577711)),
+            ])
+        return transform
+
+    def tokenize(self,
+                 texts: Union[str, List[str]],
+                 context_length: int = 52) -> torch.LongTensor:
+        """
+        Returns the tokenized representation of given input string(s)
+        Parameters
+        ----------
+        texts : Union[str, List[str]]
+            An input string or a list of input strings to tokenize
+        context_length : int
+            The context length to use; all baseline models use 24 as the context length
+        Returns
+        -------
+        A two-dimensional tensor containing the resulting tokens, shape = [number of input strings, context_length]
+        """
+        if isinstance(texts, str):
+            texts = [texts]
+
+        all_tokens = []
+        for text in texts:
+            all_tokens.append(
+                [self.tokenizer.vocab['[CLS]']]
+                + self.tokenizer.convert_tokens_to_ids(
+                    self.tokenizer.tokenize(text))[:context_length - 2]
+                + [self.tokenizer.vocab['[SEP]']])
+
+        result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)
+
+        for i, tokens in enumerate(all_tokens):
+            assert len(tokens) <= context_length
+            result[i, :len(tokens)] = torch.tensor(tokens)
+
+        return result
+
+    def set_input_img_key(self, new_key: str):
+        self.input_keys['img'] = new_key
+
+    def set_input_text_key(self, new_key: str):
+        self.input_keys['text'] = new_key
+
+    def __call__(self, input: Union[str, tuple, Dict[str, Any]], *args,
+                 **kwargs) -> Dict[str, Any]:
+        output = {}
+        # preprocess the image input
+        input_img_key = self.input_keys['img']
+        if input_img_key in input and input[input_img_key] is not None:
+            image_input = input[input_img_key]
+
+            # single image input
+            if isinstance(image_input, Image.Image):
+                image_tensor = self.img_preprocess(image_input).unsqueeze(0)
+            # multi images input
+            elif isinstance(image_input, list):
+                if all([isinstance(elem, Image.Image)
+                        for elem in image_input]):
+                    image_tensor = torch.stack(
+                        [self.img_preprocess(elem)
+                         for elem in image_input],  # noqa
+                        dim=0)  # noqa
+                else:
+                    unsupported_elem_type = [
+                        type(elem) for elem in image_input
+                        if not isinstance(elem, Image.Image)
+                    ][0]
+                    raise TypeError(
+                        f'img should be PIL.Image or List[PIL.Image], \
+                            but got a List containing one {unsupported_elem_type}'
+                    )
+            # others
             else:
-                rotary_pos_emb = ((rotary_pos_emb, ) * 2)
+                raise TypeError(
+                    f'img should be PIL.Image or List[PIL.Image], but got {type(image_input)}'
+                )
+            output['img'] = image_tensor
 
-        if rotary_pos_emb is not None:
-            q_pos_emb, k_pos_emb = rotary_pos_emb
-            cur_len = query.shape[1]
-            q_pos_emb = q_pos_emb[:, -cur_len:, :, :]
-            k_pos_emb = k_pos_emb[:, -cur_len:, :, :]
-            query = apply_rotary_pos_emb(query, q_pos_emb)
-            key = apply_rotary_pos_emb(key, k_pos_emb)
-
-        if layer_past is not None:
-            past_key, past_value = layer_past[0], layer_past[1]
-            key = torch.cat((past_key, key), dim=1)
-            value = torch.cat((past_value, value), dim=1)
-
-        if use_cache:
-            present = (key, value)
-        else:
-            present = None
-
-        if self.use_flash_attn:
-            q, k, v = query, key, value
-            context_layer = self.core_attention_flash(q, k, v)
-
-            context_layer = rearrange(context_layer,
-                                      'b s h d -> b s (h d)').contiguous()
-        else:
-            query = query.permute(0, 2, 1, 3)
-            key = key.permute(0, 2, 1, 3)
-            value = value.permute(0, 2, 1, 3)
-            attn_output, attn_weight = self._attn(query, key, value,
-                                                  attention_mask, head_mask)
-            context_layer = self._merge_heads(attn_output, self.num_heads,
-                                              self.head_dim)
-
-        attn_output = self.c_proj(context_layer)
-        outputs = (attn_output, present)
-        if output_attentions:
-            if self.use_flash_attn:
-                raise ValueError(
-                    'Cannot output attentions while using flash-attn')
+        # preprocess the text input
+        input_text_key = self.input_keys['text']
+        if input_text_key in input and input[input_text_key] is not None:
+            text_input = input[input_text_key]
+
+            # single text input
+            if isinstance(text_input, str):
+                text_tensor = self.tokenize(text_input)
+            # multi texts input
+            elif isinstance(text_input, list):
+                if all([isinstance(elem, str) for elem in text_input]):
+                    text_tensor = self.tokenize(text_input)
+                else:
+                    unsupported_elem_type = [
+                        type(elem) for elem in text_input
+                        if not isinstance(elem, str)
+                    ][0]
+                    raise TypeError(
+                        f'text should be str or List[str], but got a List containing one {unsupported_elem_type}'
+                    )
+            # others
             else:
-                outputs += (attn_weight, )
-
-        return outputs
-
-
-class QWenMLP(nn.Module):
-
-    def __init__(self, intermediate_size, config):
-        super().__init__()
-
-        self.w1 = nn.Linear(
-            config.hidden_size,
-            config.ffn_hidden_size // 2,
-            bias=not config.no_bias)
-        self.w2 = nn.Linear(
-            config.hidden_size,
-            config.ffn_hidden_size // 2,
-            bias=not config.no_bias)
-
-        ff_dim_in = config.ffn_hidden_size // 2
-        self.c_proj = nn.Linear(
-            ff_dim_in, config.hidden_size, bias=not config.no_bias)
-
-    def forward(self, hidden_states):
-
-        a1 = self.w1(hidden_states)
-        a2 = self.w2(hidden_states)
-        intermediate_parallel = a1 * F.silu(a2)
+                raise TypeError(
+                    f'text should be str or List[str], but got {type(text_input)}'
+                )
+            output['text'] = text_tensor
 
-        output = self.c_proj(intermediate_parallel)
         return output
 
 
-class QWenBlock(nn.Module):
-
-    def __init__(self, config, layer_idx=None, num_expert=1):
-        super().__init__()
-        self.num_expert = num_expert
-        self.layer_number = layer_idx
-        self.apply_residual_connection_post_layernorm = config.apply_residual_connection_post_layernorm
-        hidden_size = config.hidden_size
-        self.apply_residual_connection_post_layernorm = config.apply_residual_connection_post_layernorm
-        self.bf16 = config.bf16
-
-        if config.n_inner is not None:
-            inner_dim = config.n_inner
-        else:
-            ff_mult = 4 * 2 / 3
-            inner_dim = ff_mult * hidden_size
-
-        self.ln_1 = RMSNorm(
-            hidden_size,
-            eps=config.layer_norm_epsilon,
-        )
-        self.attn = QWenAttention(config, layer_number=layer_idx)
-        self.ln_2 = RMSNorm(
-            hidden_size,
-            eps=config.layer_norm_epsilon,
-        )
+@PREPROCESSORS.register_module(
+    Fields.multi_modal, module_name=Preprocessors.mplug_tasks_preprocessor)
+class MPlugPreprocessor(Preprocessor):
 
-        self.mlp = QWenMLP(inner_dim, config)
+    def __init__(self,
+                 model_dir: str,
+                 mode: str = ModeKeys.INFERENCE,
+                 tokenizer_max_length: int = 25,
+                 *args,
+                 **kwargs):
+        super().__init__(*args, **kwargs)
+        self.model_dir = model_dir
+        self.mode = mode
+        self.tokenizer_max_length = tokenizer_max_length
+
+        self._tokenizer = None
+        self._patch_resize_transform = None
+        self._image_map = {}
+
+    @property
+    def tokenizer(self):
+        from transformers import BertTokenizer
+
+        if self._tokenizer is None:
+            self._tokenizer = BertTokenizer.from_pretrained(self.model_dir)
+        return self._tokenizer
+
+    @property
+    def patch_resize_transform(self):
+        if self._patch_resize_transform is None:
+            from torchvision import transforms
+            from modelscope.models.multi_modal.mplug import CONFIG_NAME, MPlugConfig
+
+            config = MPlugConfig.from_yaml_file(
+                osp.join(self.model_dir, CONFIG_NAME))
+
+            mean = (0.48145466, 0.4578275, 0.40821073)
+            std = (0.26862954, 0.26130258, 0.27577711)
+
+            self._patch_resize_transform = transforms.Compose([
+                transforms.Resize((config.image_res, config.image_res),
+                                  interpolation=Image.BICUBIC),
+                transforms.ToTensor(),
+                transforms.Normalize(mean=mean, std=std),
+            ])
+        return self._patch_resize_transform
+
+    def image_open(self, path: str) -> Tuple[Image.Image, int]:
+        if path not in self._image_map:
+            index = len(self._image_map)
+            self._image_map[path] = (load_image(path), index)
+        return self._image_map[path]
+
+    def __call__(
+            self, data: Union[Image.Image, tuple,
+                              Dict[str, Any]]) -> Dict[str, Any]:
+        self.cfg = Config.from_file(
+            osp.join(self.model_dir, ModelFile.CONFIGURATION))
+
+        if isinstance(data, (Image.Image, str)):
+            image = data
+        elif isinstance(data, tuple):
+            image = data[0]
+        else:
+            image = data['image']
+        index = 0
+        if isinstance(image, str):
+            image, index = self.image_open(image)
+        image = image.convert('RGB')
+        image = self.patch_resize_transform(image)
+        question = '' if self.cfg.task == Tasks.image_captioning \
+            else data[1 if isinstance(data, tuple)
+                      else ('text' if 'text' in data else 'question')]
+        question = self.tokenizer(
+            question.lower(),
+            padding='max_length',
+            truncation=True,
+            max_length=self.tokenizer_max_length,
+            return_tensors='pt')
+
+        if self.mode == ModeKeys.INFERENCE:
+            image = torch.stack([image], dim=0)
+            return {'image': image, 'question': question}
+        else:
+            answer = data['answer']
+            answer = self.tokenizer(
+                answer,
+                padding='max_length',
+                truncation=True,
+                max_length=self.tokenizer_max_length,
+                return_tensors='pt')
+            output = {
+                'image': image,
+                'question_input_ids': question.input_ids.squeeze(),
+                'question_attention_mask': question.attention_mask.squeeze(),
+                'answer_input_ids': answer.input_ids.squeeze(),
+                'answer_attention_mask': answer.attention_mask.squeeze(),
+            }
+            if self.cfg.task == Tasks.image_text_retrieval:
+                output['index'] = index
+            return output
+
+
+@PREPROCESSORS.register_module(
+    Fields.multi_modal, module_name=Preprocessors.vldoc_preprocessor)
+class VLDocPreprocessor(Preprocessor):
 
-    def forward(
-        self,
-        hidden_states: Optional[Tuple[torch.FloatTensor]],
-        layer_past: Optional[Tuple[torch.Tensor]] = None,
-        attention_mask: Optional[torch.FloatTensor] = None,
-        head_mask: Optional[torch.FloatTensor] = None,
-        encoder_hidden_states: Optional[torch.Tensor] = None,
-        encoder_attention_mask: Optional[torch.FloatTensor] = None,
-        use_cache: Optional[bool] = False,
-        output_attentions: Optional[bool] = False,
-    ):
-        layernorm_output = self.ln_1(hidden_states)
-
-        attn_outputs = self.attn(
-            layernorm_output,
-            layer_past=layer_past,
-            attention_mask=attention_mask,
-            head_mask=head_mask,
-            use_cache=use_cache,
-            output_attentions=output_attentions)
-        attn_output = attn_outputs[0]
-
-        outputs = attn_outputs[1:]
-
-        if self.apply_residual_connection_post_layernorm:
-            residual = layernorm_output
-        else:
-            residual = hidden_states
-        layernorm_input = attn_output + residual
-
-        layernorm_output = self.ln_2(layernorm_input)
-
-        if self.apply_residual_connection_post_layernorm:
-            residual = layernorm_output
-        else:
-            residual = layernorm_input
-
-        mlp_output = self.mlp(layernorm_output)
-        hidden_states = residual + mlp_output
-
-        if use_cache:
-            outputs = (hidden_states, ) + outputs
-        else:
-            outputs = (hidden_states, ) + outputs[1:]
-
-        return outputs
-
-
-class QWenPreTrainedModel(TorchModel, PreTrainedModel):
-    config_class = QWenConfig
-    base_model_prefix = 'transformer'
-    is_parallelizable = False
-    supports_gradient_checkpointing = True
-    _no_split_modules = ['QWenBlock']
-
-    def __init__(self, config, **kwargs):
-        super().__init__(config.name_or_path, **kwargs)
-        super(Model, self).__init__(config)
-
-    def _init_weights(self, module):
-        """Initialize the weights."""
-        if isinstance(module, nn.Linear):
-            module.weight.data.normal_(
-                mean=0.0, std=self.config.initializer_range)
-            if module.bias is not None:
-                module.bias.data.zero_()
-        elif isinstance(module, nn.Embedding):
-            module.weight.data.normal_(
-                mean=0.0, std=self.config.initializer_range)
-            if module.padding_idx is not None:
-                module.weight.data[module.padding_idx].zero_()
-        elif isinstance(module, RMSNorm):
-            module.weight.data.fill_(1.0)
-
-        for name, p in module.named_parameters():
-            if name == 'c_proj.weight':
-                p.data.normal_(
-                    mean=0.0,
-                    std=(self.config.initializer_range
-                         / math.sqrt(2 * self.config.n_layer)))
-
-    def _set_gradient_checkpointing(self, module, value=False):
-        if isinstance(module, QWenModel):
-            module.gradient_checkpointing = value
-
-    @classmethod
-    def _instantiate(cls, **kwargs):
-        model_dir = kwargs.pop('model_dir', None)
-        if model_dir is None:
-            config = QWenConfig(**kwargs)
-            model = cls(config)
-        else:
-            model = super(Model, cls).from_pretrained(
-                pretrained_model_name_or_path=model_dir, **kwargs)
-        model.model_dir = model_dir
-        return model
-
-
-@MODELS.register_module(Tasks.backbone, module_name=Models.qwen_7b)
-class QWenModel(QWenPreTrainedModel):
-    _keys_to_ignore_on_load_missing = ['attn.masked_bias']
-
-    def __init__(self, config):
-        super().__init__(config)
-        self.vocab_size = config.padded_vocab_size
-        self.num_hidden_layers = config.num_hidden_layers
-        self.embed_dim = config.hidden_size
-
-        max_sequence_length = config.max_position_embeddings
-        self.position_embedding_type = config.pos_emb
-        self.gradient_checkpointing = False
-
-        if self.position_embedding_type == 'learned':
-            self.wpe = nn.Embedding(max_sequence_length, self.embed_dim)
-            self.init_method(self.position_embeddings.weight)
-            self._position_embeddings_key = 'position_embeddings'
-            self.init_method(self.position_embeddings.weight)
-        else:
-            self.wpe = None
-            self._position_embeddings_key = ''
-
-        self.wte = nn.Embedding(self.vocab_size, self.embed_dim)
-
-        self.drop = nn.Dropout(config.embd_pdrop)
-        self.h = nn.ModuleList([
-            QWenBlock(
-                config,
-                layer_idx=i,
-            ) for i in range(config.num_hidden_layers)
-        ])
-        self.ln_f = RMSNorm(
-            self.embed_dim,
-            eps=config.layer_norm_epsilon,
+    def __init__(self,
+                 model_dir: str,
+                 mode: str = ModeKeys.INFERENCE,
+                 *args,
+                 **kwargs):
+        """Preprocess data for the model `VLDocForDocVLEmbedding`.
+
+        Args:
+            model_dir (str): model path in model hub.
+            mode (str): model mode, in ('train', 'eval', 'inference').
+        """
+        super().__init__(*args, **kwargs)
+
+        self.model_dir = model_dir
+        self.mode = mode
+
+        model_cfg_path = osp.join(model_dir, 'config.json')
+        with open(model_cfg_path, 'r', encoding='utf-8') as f:
+            model_cfg = json.load(f)
+
+        from modelscope.models.multi_modal.vldoc.tokenization import VLDocXLMTokenizer
+        tokenizer_path = osp.join(model_dir, ModelFile.TOKENIZER_FOLDER)
+        self.tokenizer = VLDocXLMTokenizer.from_pretrained(tokenizer_path)
+
+        from modelscope.models.multi_modal.vldoc.processing import Processor, ImageProcessor
+        self.img_proc = ImageProcessor(
+            do_preprocess=True,
+            do_resize=True,
+            image_size={
+                'height': model_cfg['image_size'][0],
+                'width': model_cfg['image_size'][1],
+            },
+            do_normalize=True,
+            apply_ocr=False)
+        self.proc = Processor(
+            max_seq_length=model_cfg['max_seq_length'],
+            max_block_num=model_cfg['max_block_num'],
+            img_processor=self.img_proc,
+            tokenizer=self.tokenizer,
+            width=model_cfg['image_size'][1],
+            height=model_cfg['image_size'][0],
         )
 
-        self.post_init()
-
-    def get_input_embeddings(self):
-        return self.wte
-
-    def set_input_embeddings(self, new_embeddings):
-        self.wte = new_embeddings
-
-    def forward(
-        self,
-        input_ids: Optional[torch.LongTensor] = None,
-        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,
-        attention_mask: Optional[torch.FloatTensor] = None,
-        token_type_ids: Optional[torch.LongTensor] = None,
-        position_ids: Optional[torch.LongTensor] = None,
-        head_mask: Optional[torch.FloatTensor] = None,
-        inputs_embeds: Optional[torch.FloatTensor] = None,
-        encoder_hidden_states: Optional[torch.Tensor] = None,
-        encoder_attention_mask: Optional[torch.FloatTensor] = None,
-        use_cache: Optional[bool] = None,
-        output_attentions: Optional[bool] = None,
-        output_hidden_states: Optional[bool] = None,
-        return_dict: Optional[bool] = None,
-    ):
-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
-        output_hidden_states = (
-            output_hidden_states if output_hidden_states is not None else
-            self.config.output_hidden_states)
-        use_cache = use_cache if use_cache is not None else self.config.use_cache
-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
-
-        if input_ids is not None and inputs_embeds is not None:
-            raise ValueError(
-                'You cannot specify both input_ids and inputs_embeds at the same time'
-            )
-        elif input_ids is not None:
-            input_shape = input_ids.size()
-            input_ids = input_ids.view(-1, input_shape[-1])
-            batch_size = input_ids.shape[0]
-        elif inputs_embeds is not None:
-            input_shape = inputs_embeds.size()[:-1]
-            batch_size = inputs_embeds.shape[0]
-        else:
-            raise ValueError(
-                'You have to specify either input_ids or inputs_embeds')
-
-        device = input_ids.device if input_ids is not None else inputs_embeds.device
-
-        if token_type_ids is not None:
-            token_type_ids = token_type_ids.view(-1, input_shape[-1])
-        if position_ids is not None:
-            position_ids = position_ids.view(-1, input_shape[-1])
-
-        if past_key_values is None:
-            past_length = 0
-            past_key_values = tuple([None] * len(self.h))
-        else:
-            past_length = past_key_values[0][0].size(-2)
-
-        if position_ids is None:
-            position_ids = torch.arange(
-                past_length,
-                input_shape[-1] + past_length,
-                dtype=torch.long,
-                device=device)
-            position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])
-
-        if attention_mask is not None:
-            if batch_size <= 0:
-                raise ValueError('batch_size has to be defined and > 0')
-            attention_mask = attention_mask.view(batch_size, -1)
-            attention_mask = attention_mask[:, None, None, :]
-
-            attention_mask = attention_mask.to(dtype=self.dtype)
-            attention_mask = (1.0 - attention_mask) * torch.finfo(
-                self.dtype).min
-
-        encoder_attention_mask = None
-
-        head_mask = self.get_head_mask(head_mask, self.config.n_layer)
-
-        if inputs_embeds is None:
-            inputs_embeds = self.wte(input_ids)
-        hidden_states = inputs_embeds
-        if self.wpe is not None:
-            position_embeds = self.wpe(position_ids)
-            hidden_states = hidden_states + position_embeds
-
-        hidden_states = self.drop(hidden_states)
-        output_shape = input_shape + (hidden_states.size(-1), )
-
-        if self.gradient_checkpointing and self.training:
-            if use_cache:
-                logger.warning_once(
-                    '`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...'
-                )
-                use_cache = False
-
-        presents = () if use_cache else None
-        all_self_attentions = () if output_attentions else None
-        all_hidden_states = () if output_hidden_states else None
-        for i, (block, layer_past) in enumerate(zip(self.h, past_key_values)):
+    def __call__(self, input: Dict[str, Any], *args,
+                 **kwargs) -> Dict[str, Any]:
+        """
+        Args:
+            input: {
+                'images': ['img_path1', 'img_path2', ...],
+                'ocr_info_paths': ['json_path1', 'json_path2', ...]
+            }
+        Return:
+            encodings: Dict[str, Tensor]
+        """
+
+        ocr_infos = []
+        for one_ocr_info_path in input['ocr_info_paths']:
+            with open(one_ocr_info_path, 'r') as f:
+                ocr_info = json.load(f)
+                ocr_info = ocr_info['form']
+                ocr_infos.append(ocr_info)
+
+        proc_input = {'images': input['images'], 'ocr_infos': ocr_infos}
+        encodings = self.proc(**proc_input)
+
+        return encodings
+
+
+@PREPROCESSORS.register_module(
+    Fields.multi_modal, module_name=Preprocessors.hitea_tasks_preprocessor)
+class HiTeAPreprocessor(Preprocessor):
 
-            if output_hidden_states:
-                all_hidden_states = all_hidden_states + (hidden_states, )
-
-            if self.gradient_checkpointing and self.training:
-
-                def create_custom_forward(module):
-
-                    def custom_forward(*inputs):
-                        return module(*inputs, use_cache, output_attentions)
-
-                    return custom_forward
+    def __init__(self,
+                 model_dir: str,
+                 mode: str = ModeKeys.INFERENCE,
+                 tokenizer_max_length: int = 25,
+                 *args,
+                 **kwargs):
+        super().__init__(*args, **kwargs)
+        self.model_dir = model_dir
+        self.mode = mode
+        self.tokenizer_max_length = tokenizer_max_length
+
+        self._tokenizer = None
+        self._patch_resize_transform = None
+        self._num_frames = None
+        self._video_map = {}
+
+    @property
+    def tokenizer(self):
+        from transformers import BertTokenizer
+
+        if self._tokenizer is None:
+            self._tokenizer = BertTokenizer.from_pretrained(self.model_dir)
+        return self._tokenizer
+
+    @property
+    def patch_resize_transform(self):
+        if self._patch_resize_transform is None:
+            from torchvision import transforms
+            from modelscope.models.multi_modal.mplug import CONFIG_NAME, HiTeAConfig
+
+            config = HiTeAConfig.from_yaml_file(
+                osp.join(self.model_dir, CONFIG_NAME))
+
+            mean = (0.48145466, 0.4578275, 0.40821073)
+            std = (0.26862954, 0.26130258, 0.27577711)
+
+            self._patch_resize_transform = transforms.Compose([
+                transforms.Resize((config.image_res, config.image_res),
+                                  interpolation=Image.BICUBIC),
+                transforms.ToTensor(),
+                transforms.Normalize(mean=mean, std=std),
+            ])
+        return self._patch_resize_transform
+
+    @property
+    def num_frames(self):
+        if self._num_frames is None:
+            from torchvision import transforms
+            from modelscope.models.multi_modal.mplug import CONFIG_NAME, HiTeAConfig
+
+            config = HiTeAConfig.from_yaml_file(
+                osp.join(self.model_dir, CONFIG_NAME))
+
+            self._num_frames = config.num_frames
+        return self._num_frames
+
+    def video_open(self, path: str) -> Tuple[decord.VideoReader, int]:
+        if path not in self._video_map:
+            index = len(self._video_map)
+            vr = decord.VideoReader(path, ctx=decord.cpu(0))
+            self._video_map[path] = (vr, index)
+        return self._video_map[path]
+
+    def sample_frames(self, num_frames: int, vlen: int) -> List[int]:
+        acc_samples = min(num_frames, vlen)
+        # split the video into `acc_samples` intervals, and sample from each interval.
+        intervals = np.linspace(
+            start=0, stop=vlen, num=acc_samples + 1).astype(int)
+        ranges = []
+        for idx, interv in enumerate(intervals[:-1]):
+            ranges.append((interv, intervals[idx + 1] - 1))
+
+        frame_indices = [(x[0] + x[1]) // 2 for x in ranges]
+
+        if len(frame_indices) < num_frames:  # padded with last frame
+            padded_frame_indices = [frame_indices[-1]] * num_frames
+            padded_frame_indices[:len(frame_indices)] = frame_indices
+            frame_indices = padded_frame_indices
+        return frame_indices
+
+    def __call__(
+        self, data: Union[decord.VideoReader, tuple,
+                          Dict[str, Any]]) -> Dict[str, Any]:
+        self.cfg = Config.from_file(
+            osp.join(self.model_dir, ModelFile.CONFIGURATION))
+
+        if isinstance(data, (decord.VideoReader, str)):
+            video = data
+        elif isinstance(data, tuple):
+            video = data[0]
+        else:
+            video = data['video']
+        index = 0
+        if isinstance(video, str):
+            video, index = self.video_open(video)
+        frame_indices = self.sample_frames(self.num_frames, len(video))
+        video.seek(0)
+        video = torch.from_numpy(video.get_batch(frame_indices).asnumpy())
+        video = [
+            self.patch_resize_transform(Image.fromarray(f))
+            for f in video.numpy()
+        ]
+        video = torch.stack(video, dim=0)
+        question = '' if self.cfg.task == Tasks.video_captioning \
+            else data[1 if isinstance(data, tuple)
+                      else ('text' if 'text' in data else 'question')]
+        question = self.tokenizer(
+            question.lower(),
+            padding='max_length',
+            truncation=True,
+            max_length=self.tokenizer_max_length,
+            return_tensors='pt')
+
+        if self.mode == ModeKeys.INFERENCE:
+            video = torch.stack([video], dim=0)
+            return {'video': video, 'question': question}
+        else:
+            answer = data['answer']
+            answer = self.tokenizer(
+                answer,
+                padding='max_length',
+                truncation=True,
+                max_length=self.tokenizer_max_length,
+                return_tensors='pt')
+            output = {
+                'video': video,
+                'question_input_ids': question.input_ids.squeeze(),
+                'question_attention_mask': question.attention_mask.squeeze(),
+                'answer_input_ids': answer.input_ids.squeeze(),
+                'answer_attention_mask': answer.attention_mask.squeeze(),
+            }
+            return output
+
+
+@PREPROCESSORS.register_module(
+    Fields.multi_modal, module_name=Preprocessors.mplug_owl_preprocessor)
+class MplugOwlPreprocessor(Preprocessor):
 
-                outputs = torch.utils.checkpoint.checkpoint(
-                    create_custom_forward(block),
-                    hidden_states,
-                    None,
-                    attention_mask,
-                    head_mask[i],
-                    encoder_hidden_states,
-                    encoder_attention_mask,
-                )
+    def __init__(self,
+                 model_dir: str,
+                 mode: str = ModeKeys.INFERENCE,
+                 *args,
+                 **kwargs):
+        super().__init__(*args, **kwargs)
+        self.model_dir = model_dir
+        self.mode = mode
+
+        self._tokenizer = None
+        self._patch_resize_transform = None
+        self.media_token = {'<|image|>': 65}
+        self._image_map = {}
+
+    @property
+    def tokenizer(self):
+        from modelscope.models.nlp.llama import LlamaTokenizer
+
+        if self._tokenizer is None:
+            self._tokenizer = LlamaTokenizer.from_pretrained(self.model_dir)
+        return self._tokenizer
+
+    @property
+    def patch_resize_transform(self):
+        if self._patch_resize_transform is None:
+            from torchvision import transforms
+
+            mean = (0.48145466, 0.4578275, 0.40821073)
+            std = (0.26862954, 0.26130258, 0.27577711)
+
+            self._patch_resize_transform = transforms.Compose([
+                transforms.Resize((224, 224), interpolation=Image.BICUBIC),
+                transforms.ToTensor(),
+                transforms.Normalize(mean=mean, std=std),
+            ])
+        return self._patch_resize_transform
+
+    def image_open(self, path: str) -> Tuple[Image.Image, int]:
+        if path not in self._image_map:
+            index = len(self._image_map)
+            self._image_map[path] = (load_image(path), index)
+        return self._image_map[path]
+
+    def tokenize_text(self, text: str) -> List[int]:
+        media_tokens = {
+            k: -int(i + 1)
+            for i, k in enumerate(self.media_token.keys())
+        }
+        media_lengths = self.media_token.copy()
+
+        prompt_chunk = [self.tokenizer.bos_token_id]
+
+        # Pure Text
+        condition = [
+            media_token not in text for media_token in media_tokens.keys()
+        ]
+        if all(condition):
+            enc_chunk = prompt_chunk + \
+                self.tokenizer(text, add_special_tokens=False)['input_ids']
+
+        # Multi-Modal Text
+        else:
+            enc_chunk = prompt_chunk
+            pattern = '|'.join(map(re.escape, list(media_tokens.keys())))
+            chunk_strs = re.split(f'({pattern})', text)
+            chunk_strs = [x for x in chunk_strs if len(x) > 0]
+            for idx, chunk_str in enumerate(chunk_strs):
+                if chunk_str in media_tokens:
+                    enc_chunk += [media_tokens[chunk_str]] * \
+                        media_lengths[chunk_str]
+                else:
+                    tmp_chunk = self.tokenizer(
+                        chunk_str, add_special_tokens=False)['input_ids']
+                    enc_chunk += tmp_chunk
+        return enc_chunk
+
+    def convert(self, messages: Dict[str, List[Dict]]) -> str:
+        texts = []
+        image = []
+        messages = messages['messages']
+        for turn in messages:
+            if turn['role'] == 'system':
+                role = ''
+            elif turn['role'] == 'user':
+                role = 'Human: '
             else:
-                outputs = block(
-                    hidden_states,
-                    layer_past=layer_past,
-                    attention_mask=attention_mask,
-                    head_mask=head_mask[i],
-                    encoder_hidden_states=encoder_hidden_states,
-                    encoder_attention_mask=encoder_attention_mask,
-                    use_cache=use_cache,
-                    output_attentions=output_attentions,
-                )
+                role = 'AI: '
+            if isinstance(turn['content'], str):
+                text = f"{role}{turn['content']}"
+                texts.append(text)
+            else:
+                for t in turn['content']:
+                    if isinstance(t, str):
+                        text = f'{role}{t}'
+                    else:
+                        text = f'{role}<|image|>'
+                        image.append(t['image'])
+                    texts.append(text)
+        texts = '\n'.join(texts)
+        texts += '\nAI: '
+        return image, texts
+
+    def __call__(self, messages: Dict[str, Any],
+                 **forward_params) -> Dict[str, Any]:
+        """
+        Args:
+            messages: {[
+                {'role': 'system', 'content': 'message1'},
+                {'role': 'user', 'content': 'message2'},
+                {'role': 'user', 'content': ['message2', {"image": 'image_path'}, 'message3', ...]},
+            ]}
+            The 'role' should be choose from ['system', 'user', 'assistant'].
+            The 'content' can be either str or List[Union[str, Dict]]
+        Return:
+            output: Dict[str, Tensor]
+        """
+        output = {}
+        images, text = self.convert(messages)
+
+        if len(images) > 0:
+            pixel_values = []
+            for image in images:
+                pixel_values.append(
+                    self.patch_resize_transform(self.image_open(image)[0]))
+                pixel_values = torch.stack(pixel_values, dim=0)
+        else:
+            pixel_values = None
+
+        input_ids = self.tokenize_text(text)
+        input_ids = torch.LongTensor([input_ids])
+
+        output = {
+            'pixel_values': pixel_values,
+            'input_ids': input_ids,
+            **forward_params
+        }
 
-            hidden_states = outputs[0]
-            if use_cache is True:
-                presents = presents + (
-                    outputs[2 if output_attentions else 1], )
-
-            if output_attentions:
-                all_self_attentions = all_self_attentions + (outputs[1], )
-
-        hidden_states = self.ln_f(hidden_states)
-
-        hidden_states = hidden_states.view(output_shape)
-
-        if not return_dict:
-            return tuple(v
-                         for v in [hidden_states, presents, all_hidden_states]
-                         if v is not None)
-
-        return BaseModelOutputWithPast(
-            last_hidden_state=hidden_states,
-            past_key_values=presents,
-            hidden_states=all_hidden_states,
-            attentions=all_self_attentions)
-
-
-class RotaryEmbedding(torch.nn.Module):
-
-    def __init__(self, dim, base=10000, ntk_alpha=1.0):
-        super().__init__()
-        base = base * ntk_alpha**(dim / (dim - 2))
-        inv_freq = 1.0 / (base**(torch.arange(0, dim, 2).float() / dim))
-        self.register_buffer('inv_freq', inv_freq)
-        if importlib.util.find_spec('einops') is None:
-            raise RuntimeError('einops is required for Rotary Embedding')
-
-        self._rotary_pos_emb_cache = None
-        self._seq_len_cached = 0
-
-    def update_rotary_pos_emb_cache(self, max_seq_len, offset=0):
-        seqlen = max_seq_len + offset
-        if seqlen > self._seq_len_cached:
-            self._seq_len_cached = seqlen
-            seq = torch.arange(seqlen, device=self.inv_freq.device)
-            freqs = torch.outer(seq.type_as(self.inv_freq), self.inv_freq)
-            emb = torch.cat((freqs, freqs), dim=-1)
-            from einops import rearrange
-            self._rotary_pos_emb_cache = rearrange(emb, 'n d -> 1 n 1 d')
-
-    def forward(self, max_seq_len, offset=0):
-        self.update_rotary_pos_emb_cache(max_seq_len, offset)
-        return self._rotary_pos_emb_cache[:, offset:offset + max_seq_len]
-
-
-def _rotate_half(x):
-    from einops import rearrange
-    x = rearrange(x, '... (j d) -> ... j d', j=2)
-    x1, x2 = x.unbind(dim=-2)
-    return torch.cat((-x2, x1), dim=-1)
-
-
-def apply_rotary_pos_emb(t, freqs, use_flash_rotary=False):
-    if use_flash_rotary:
-        t_ = t.float()
-        freqs = freqs.squeeze(0).squeeze(1)
-        cos = freqs[:, :freqs.shape[-1] // 2].cos()
-        sin = freqs[:, :freqs.shape[-1] // 2].sin()
-        output = apply_rotary_emb_func(t_, cos, sin).type_as(t)
         return output
-    else:
-        rot_dim = freqs.shape[-1]
-        t_, t_pass_ = t[..., :rot_dim], t[..., rot_dim:]
-        t_ = t_.float()
-        t_pass_ = t_pass_.float()
-        t_ = (t_ * freqs.cos()) + (_rotate_half(t_) * freqs.sin())
-        return torch.cat((t_, t_pass_), dim=-1).type_as(t)
-
-
-class RMSNorm(torch.nn.Module):
-
-    def __init__(self, dim: int, eps: float = 1e-6):
-        super().__init__()
-        self.eps = eps
-        self.weight = nn.Parameter(torch.ones(dim))
-
-    def _norm(self, x):
-        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
-
-    def forward(self, x):
-        if rms_norm is not None:
-            return rms_norm(x, self.weight, self.eps)
-        else:
-            output = self._norm(x.float()).type_as(x)
-            return output * self.weight
+
+
+@PREPROCESSORS.register_module(
+    Fields.multi_modal,
+    module_name=Preprocessors.image_captioning_clip_interrogator_preprocessor)
+class ImageCaptioningClipInterrogatorPreprocessor(Preprocessor):
+
+    def __init__(self, **kwargs):
+        super().__init__(**kwargs)
+
+    def __call__(self, data) -> Dict[str, Any]:
+        image = load_image(data)
+        data = np.array(image).transpose(2, 0, 1)
+        return data
```

### Comparing `modelscope-1.8.0/modelscope/models/nlp/qwen/qwen_generation_utils.py` & `modelscope-1.8.0rc0/modelscope/utils/test_utils.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,403 +1,387 @@
-# Copyright (c) Alibaba Cloud.
-#
-# This source code is licensed under the license found in the
-# LICENSE file in the root directory of this source tree.
-"""Generation support."""
-from typing import Iterable, List, Tuple, Union
+#!/usr/bin/env python
+# Copyright (c) Alibaba, Inc. and its affiliates.
+
+import copy
+import os
+import pickle
+import shutil
+import socket
+import subprocess
+import sys
+import tarfile
+import tempfile
+import unittest
+from collections import OrderedDict
+from collections.abc import Mapping
+from os.path import expanduser
 
 import numpy as np
-import torch
-import torch.nn.functional as F
-from transformers import PreTrainedTokenizer
-from transformers.generation import LogitsProcessor
-
-from modelscope.utils.logger import get_logger
-
-logger = get_logger()
-
-# Types.
-HistoryType = List[Tuple[str, str]]
-TokensType = List[int]
-BatchTokensType = List[List[int]]
-
-
-def pad_batch(batch: BatchTokensType, pad_id: int,
-              seq_length: int) -> BatchTokensType:
-    for tokens in batch:
-        context_length = len(tokens)
-        if context_length < seq_length:
-            tokens.extend([pad_id] * (seq_length - context_length))
-    return batch
-
-
-def get_ltor_masks_and_position_ids(
-    data,
-    eod_token,
-    reset_position_ids,
-    reset_attention_mask,
-    eod_mask_loss,
-):
-    """Build masks and position id for left to right model."""
-
-    # Extract batch size and sequence length.
-    micro_batch_size, seq_length = data.size()
-
-    # Attention mask (lower triangular).
-    if reset_attention_mask:
-        att_mask_batch = micro_batch_size
-    else:
-        att_mask_batch = 1
-    attention_mask = torch.tril(
-        torch.ones((att_mask_batch, seq_length, seq_length),
-                   device=data.device)).view(att_mask_batch, 1, seq_length,
-                                             seq_length)
-
-    # Loss mask.
-    loss_mask = torch.ones(data.size(), dtype=torch.float, device=data.device)
-    if eod_mask_loss:
-        loss_mask[data == eod_token] = 0.0
-
-    # Position ids.
-    position_ids = torch.arange(
-        seq_length, dtype=torch.long, device=data.device)
-    position_ids = position_ids.unsqueeze(0).expand_as(data)
-    # We need to clone as the ids will be modifed based on batch index.
-    if reset_position_ids:
-        position_ids = position_ids.clone()
-
-    if reset_position_ids or reset_attention_mask:
-        # Loop through the batches:
-        for b in range(micro_batch_size):
-
-            # Find indecies where EOD token is.
-            eod_index = position_ids[b, data[b] == eod_token]
-            # Detach indecies from positions if going to modify positions.
-            if reset_position_ids:
-                eod_index = eod_index.clone()
-
-            # Loop through EOD indecies:
-            prev_index = 0
-            for j in range(eod_index.size()[0]):
-                i = eod_index[j]
-                # Mask attention loss.
-                if reset_attention_mask:
-                    attention_mask[b, 0, (i + 1):, :(i + 1)] = 0
-                # Reset positions.
-                if reset_position_ids:
-                    position_ids[b, (i + 1):] -= (i + 1 - prev_index)
-                    prev_index = i + 1
-
-    # Convert attention mask to binary:
-    attention_mask = (attention_mask < 0.5)
-
-    return attention_mask, loss_mask, position_ids
-
-
-def get_batch(context_tokens: torch.LongTensor, eod_id: int):
-    """Generate batch from context tokens."""
-    # Move to GPU.
-    tokens = context_tokens.contiguous().to(context_tokens.device)
-    # Get the attention mask and postition ids.
-    attention_mask, _, position_ids = get_ltor_masks_and_position_ids(
-        tokens,
-        eod_id,
-        reset_position_ids=False,
-        reset_attention_mask=False,
-        eod_mask_loss=False)
-    return tokens, attention_mask, position_ids
-
-
-def get_stop_words_ids(chat_format, tokenizer):
-    if chat_format == 'raw':
-        stop_words_ids = [tokenizer.encode('Human:'), [tokenizer.eod_id]]
-    elif chat_format == 'chatml':
-        stop_words_ids = [[tokenizer.im_end_id], [tokenizer.im_start_id]]
-    else:
-        raise NotImplementedError(f'Unknown chat format {chat_format!r}')
-    return stop_words_ids
+import requests
 
+from modelscope.hub.constants import DEFAULT_CREDENTIALS_PATH
+from modelscope.utils.import_utils import is_tf_available, is_torch_available
 
-def make_context(
-    tokenizer: PreTrainedTokenizer,
-    query: str,
-    history: List[Tuple[str, str]] = [],
-    system: str = '',
-    max_window_size: int = 6144,
-    chat_format: str = 'chatml',
-):
-
-    if chat_format == 'chatml':
-        im_start, im_end = '<|im_start|>', '<|im_end|>'
-        im_start_tokens = [tokenizer.im_start_id]
-        im_end_tokens = [tokenizer.im_end_id]
-        nl_tokens = tokenizer.encode('\n')
-
-        def _tokenize_str(role, content):
-            return f'{role}\n{content}', tokenizer.encode(
-                role) + nl_tokens + tokenizer.encode(content)
-
-        system_text, system_tokens_part = _tokenize_str('system', system)
-        system_tokens = im_start_tokens + system_tokens_part + im_end_tokens
-
-        raw_text = ''
-        context_tokens = []
-
-        for turn_query, turn_response in reversed(history):
-            query_text, query_tokens_part = _tokenize_str('user', turn_query)
-            query_tokens = im_start_tokens + query_tokens_part + im_end_tokens
-            response_text, response_tokens_part = _tokenize_str(
-                'assistant', turn_response)
-            response_tokens = im_start_tokens + response_tokens_part + im_end_tokens
-
-            next_context_tokens = nl_tokens + query_tokens + nl_tokens + response_tokens
-            prev_chat = f'\n{im_start}{query_text}{im_end}\n{im_start}{response_text}{im_end}'
-
-            current_context_size = len(system_tokens) + len(
-                next_context_tokens) + len(context_tokens)
-            if current_context_size < max_window_size:
-                context_tokens = next_context_tokens + context_tokens
-                raw_text = prev_chat + raw_text
-            else:
-                break
-
-        context_tokens = system_tokens + context_tokens
-        raw_text = f'{im_start}{system_text}{im_end}' + raw_text
-        context_tokens += (
-            nl_tokens + im_start_tokens + _tokenize_str('user', query)[1]
-            + im_end_tokens + nl_tokens + im_start_tokens
-            + tokenizer.encode('assistant') + nl_tokens)
-        raw_text += f'\n{im_start}user\n{query}{im_end}\n{im_start}assistant\n'
-
-    elif chat_format == 'raw':
-        raw_text = query
-        context_tokens = tokenizer.encode(raw_text)
-    else:
-        raise NotImplementedError(f'Unknown chat format {chat_format!r}')
+TEST_LEVEL = 2
+TEST_LEVEL_STR = 'TEST_LEVEL'
 
-    return raw_text, context_tokens
+# for user citest and sdkdev
+TEST_ACCESS_TOKEN1 = os.environ.get('TEST_ACCESS_TOKEN_CITEST', None)
+TEST_ACCESS_TOKEN2 = os.environ.get('TEST_ACCESS_TOKEN_SDKDEV', None)
 
+TEST_MODEL_CHINESE_NAME = '内部测试模型'
+TEST_MODEL_ORG = 'citest'
 
-def _decode_default(
-    tokens: List[int],
-    *,
-    stop_words: List[str],
-    eod_words: List[str],
-    tokenizer: PreTrainedTokenizer,
-    raw_text_len: int,
-    verbose: bool = False,
-    return_end_reason: bool = False,
-):
-    trim_decode_tokens = tokenizer.decode(tokens)[raw_text_len:]
-    if verbose:
-        print('\nRaw Generate: ', trim_decode_tokens)
-
-    end_reason = f'Gen length {len(tokens)}'
-    for stop_word in stop_words:
-        trim_decode_tokens = trim_decode_tokens.replace(stop_word, '').strip()
-    for eod_word in eod_words:
-        if eod_word in trim_decode_tokens:
-            end_reason = f'Gen {eod_word!r}'
-        trim_decode_tokens = trim_decode_tokens.split(eod_word)[0]
-    trim_decode_tokens = trim_decode_tokens.strip()
-    if verbose:
-        print('\nEnd Reason:', end_reason)
-        print('\nGenerate: ', trim_decode_tokens)
 
-    if return_end_reason:
-        return trim_decode_tokens, end_reason
-    else:
-        return trim_decode_tokens
+def delete_credential():
+    path_credential = expanduser(DEFAULT_CREDENTIALS_PATH)
+    shutil.rmtree(path_credential, ignore_errors=True)
 
 
-def _decode_chatml(
-    tokens: List[int],
-    *,
-    stop_words: List[str],
-    eod_token_ids: List[int],
-    tokenizer: PreTrainedTokenizer,
-    raw_text_len: int,
-    context_length: int,
-    verbose: bool = False,
-    return_end_reason: bool = False,
-):
-    end_reason = f'Gen length {len(tokens)}'
-    eod_token_idx = context_length
-    for eod_token_idx in range(context_length, len(tokens)):
-        if tokens[eod_token_idx] in eod_token_ids:
-            end_reason = f'Gen {tokenizer.decode([tokens[eod_token_idx]])!r}'
-            break
-
-    trim_decode_tokens = tokenizer.decode(
-        tokens[:eod_token_idx])[raw_text_len:]
-    if verbose:
-        print('\nRaw Generate w/o EOD:',
-              tokenizer.decode(tokens)[raw_text_len:])
-        print('\nRaw Generate:', trim_decode_tokens)
-        print('\nEnd Reason:', end_reason)
-    for stop_word in stop_words:
-        trim_decode_tokens = trim_decode_tokens.replace(stop_word, '').strip()
-    trim_decode_tokens = trim_decode_tokens.strip()
-    if verbose:
-        print('\nGenerate:', trim_decode_tokens)
+def test_level():
+    global TEST_LEVEL
+    if TEST_LEVEL_STR in os.environ:
+        TEST_LEVEL = int(os.environ[TEST_LEVEL_STR])
 
-    if return_end_reason:
-        return trim_decode_tokens, end_reason
-    else:
-        return trim_decode_tokens
+    return TEST_LEVEL
 
 
-def decode_tokens(
-    tokens: Union[torch.LongTensor, TokensType],
-    tokenizer: PreTrainedTokenizer,
-    raw_text_len: int,
-    context_length: int,
-    chat_format: str,
-    verbose: bool = False,
-    return_end_reason: bool = False,
-) -> str:
-    if torch.is_tensor(tokens):
-        tokens = tokens.cpu().numpy().tolist()
-
-    if chat_format == 'chatml':
-        return _decode_chatml(
-            tokens,
-            stop_words=[],
-            eod_token_ids=[tokenizer.im_start_id, tokenizer.im_end_id],
-            tokenizer=tokenizer,
-            raw_text_len=raw_text_len,
-            context_length=context_length,
-            verbose=verbose,
-            return_end_reason=return_end_reason,
-        )
-    elif chat_format == 'raw':
-        return _decode_default(
-            tokens,
-            stop_words=['<|endoftext|>'],
-            eod_words=['<|endoftext|>'],
-            tokenizer=tokenizer,
-            raw_text_len=raw_text_len,
-            verbose=verbose,
-            return_end_reason=return_end_reason,
-        )
-    else:
-        raise NotImplementedError(f'Unknown chat format {chat_format!r}')
+def require_tf(test_case):
+    if not is_tf_available():
+        test_case = unittest.skip('test requires TensorFlow')(test_case)
+    return test_case
 
 
-class StopWordsLogitsProcessor(LogitsProcessor):
-    """
-    :class:`transformers.LogitsProcessor` that enforces that when specified sequences appear, stop geration.
+def require_torch(test_case):
+    if not is_torch_available():
+        test_case = unittest.skip('test requires PyTorch')(test_case)
+    return test_case
 
-    Args:
-        stop_words_ids (:obj:`List[List[int]]`):
-            List of list of token ids of stop ids. In order to get the tokens of the words
-            that should not appear in the generated text, use :obj:`tokenizer(bad_word,
-            add_prefix_space=True).input_ids`.
-        eos_token_id (:obj:`int`):
-            The id of the `end-of-sequence` token.
-    """
 
-    def __init__(self, stop_words_ids: Iterable[Iterable[int]],
-                 eos_token_id: int):
+def set_test_level(level: int):
+    global TEST_LEVEL
+    TEST_LEVEL = level
 
-        if not isinstance(stop_words_ids, List) or len(stop_words_ids) == 0:
-            raise ValueError(
-                f'`stop_words_ids` has to be a non-emtpy list, but is {stop_words_ids}.'
-            )
-        if any(not isinstance(bad_word_ids, list)
-               for bad_word_ids in stop_words_ids):
-            raise ValueError(
-                f'`stop_words_ids` has to be a list of lists, but is {stop_words_ids}.'
-            )
-        if any(
-                any((not isinstance(token_id, (int,
-                                               np.integer)) or token_id < 0)
-                    for token_id in stop_word_ids)
-                for stop_word_ids in stop_words_ids):
-            raise ValueError(
-                f'Each list in `stop_words_ids` has to be a list of positive integers, but is {stop_words_ids}.'
+
+class DummyTorchDataset:
+
+    def __init__(self, feat, label, num) -> None:
+        self.feat = feat
+        self.label = label
+        self.num = num
+
+    def __getitem__(self, index):
+        import torch
+        return {
+            'feat': torch.Tensor(self.feat),
+            'labels': torch.Tensor(self.label)
+        }
+
+    def __len__(self):
+        return self.num
+
+
+def create_dummy_test_dataset(feat, label, num):
+    return DummyTorchDataset(feat, label, num)
+
+
+def download_and_untar(fpath, furl, dst) -> str:
+    if not os.path.exists(fpath):
+        r = requests.get(furl)
+        with open(fpath, 'wb') as f:
+            f.write(r.content)
+
+    file_name = os.path.basename(fpath)
+    root_dir = os.path.dirname(fpath)
+    target_dir_name = os.path.splitext(os.path.splitext(file_name)[0])[0]
+    target_dir_path = os.path.join(root_dir, target_dir_name)
+
+    # untar the file
+    t = tarfile.open(fpath)
+    t.extractall(path=dst)
+
+    return target_dir_path
+
+
+def get_case_model_info():
+    status_code, result = subprocess.getstatusoutput(
+        'grep -rn "damo/" tests/  | grep -v ".pyc" | grep -v "Binary file" | grep -v run.py '
+    )
+    lines = result.split('\n')
+    test_cases = OrderedDict()
+    model_cases = OrderedDict()
+    for line in lines:
+        # "tests/msdatasets/test_ms_dataset.py:92:        model_id = 'damo/bert-base-sst2'"
+        line = line.strip()
+        elements = line.split(':')
+        test_file = elements[0]
+        model_pos = line.find('damo')
+        left_quote = line[model_pos - 1]
+        rquote_idx = line.rfind(left_quote)
+        model_name = line[model_pos:rquote_idx]
+        if test_file not in test_cases:
+            test_cases[test_file] = set()
+        model_info = test_cases[test_file]
+        model_info.add(model_name)
+
+        if model_name not in model_cases:
+            model_cases[model_name] = set()
+        case_info = model_cases[model_name]
+        case_info.add(
+            test_file.replace('tests/', '').replace('.py',
+                                                    '').replace('/', '.'))
+
+    return model_cases
+
+
+def compare_arguments_nested(print_content,
+                             arg1,
+                             arg2,
+                             rtol=1.e-3,
+                             atol=1.e-8,
+                             ignore_unknown_type=True):
+    type1 = type(arg1)
+    type2 = type(arg2)
+    if type1.__name__ != type2.__name__:
+        if print_content is not None:
+            print(
+                f'{print_content}, type not equal:{type1.__name__} and {type2.__name__}'
             )
+        return False
 
-        self.stop_words_ids = list(
-            filter(lambda bad_token_seq: bad_token_seq != [eos_token_id],
-                   stop_words_ids))
-        self.eos_token_id = eos_token_id
-        for stop_token_seq in self.stop_words_ids:
-            assert len(
-                stop_token_seq
-            ) > 0, 'Stop words token sequences {} cannot have an empty list'.format(
-                stop_words_ids)
-
-    def __call__(self, input_ids: torch.LongTensor,
-                 scores: torch.FloatTensor) -> torch.FloatTensor:
-        stopped_samples = self._calc_stopped_samples(input_ids)
-        for i, should_stop in enumerate(stopped_samples):
-            if should_stop:
-                scores[i, self.eos_token_id] = float(2**30)
-        return scores
-
-    def _tokens_match(self, prev_tokens: torch.LongTensor,
-                      tokens: List[int]) -> bool:
-        if len(tokens) == 0:
-            # if bad word tokens is just one token always ban it
-            return True
-        elif len(tokens) > len(prev_tokens):
-            # if bad word tokens are longer then prev input_ids they can't be equal
+    if arg1 is None:
+        return True
+    elif isinstance(arg1, (int, str, bool, np.bool_, np.integer, np.str_)):
+        if arg1 != arg2:
+            if print_content is not None:
+                print(f'{print_content}, arg1:{arg1}, arg2:{arg2}')
+            return False
+        return True
+    elif isinstance(arg1, (float, np.floating)):
+        if not np.isclose(arg1, arg2, rtol=rtol, atol=atol, equal_nan=True):
+            if print_content is not None:
+                print(f'{print_content}, arg1:{arg1}, arg2:{arg2}')
+            return False
+        return True
+    elif isinstance(arg1, (tuple, list)):
+        if len(arg1) != len(arg2):
+            if print_content is not None:
+                print(
+                    f'{print_content}, length is not equal:{len(arg1)}, {len(arg2)}'
+                )
+            return False
+        if not all([
+                compare_arguments_nested(
+                    None, sub_arg1, sub_arg2, rtol=rtol, atol=atol)
+                for sub_arg1, sub_arg2 in zip(arg1, arg2)
+        ]):
+            if print_content is not None:
+                print(f'{print_content}')
+            return False
+        return True
+    elif isinstance(arg1, Mapping):
+        keys1 = arg1.keys()
+        keys2 = arg2.keys()
+        if len(keys1) != len(keys2):
+            if print_content is not None:
+                print(
+                    f'{print_content}, key length is not equal:{len(keys1)}, {len(keys2)}'
+                )
+            return False
+        if len(set(keys1) - set(keys2)) > 0:
+            if print_content is not None:
+                print(f'{print_content}, key diff:{set(keys1) - set(keys2)}')
+            return False
+        if not all([
+                compare_arguments_nested(
+                    None, arg1[key], arg2[key], rtol=rtol, atol=atol)
+                for key in keys1
+        ]):
+            if print_content is not None:
+                print(f'{print_content}')
             return False
-        elif prev_tokens[-len(tokens):].tolist() == tokens:
-            # if tokens match
+        return True
+    elif isinstance(arg1, np.ndarray):
+        arg1 = np.where(np.equal(arg1, None), np.NaN, arg1).astype(dtype=float)
+        arg2 = np.where(np.equal(arg2, None), np.NaN, arg2).astype(dtype=float)
+        if not all(
+                np.isclose(arg1, arg2, rtol=rtol, atol=atol,
+                           equal_nan=True).flatten()):
+            if print_content is not None:
+                print(f'{print_content}')
+            return False
+        return True
+    else:
+        if ignore_unknown_type:
             return True
         else:
-            return False
+            raise ValueError(f'type not supported: {type1}')
+
+
+_DIST_SCRIPT_TEMPLATE = """
+import ast
+import argparse
+import pickle
+import torch
+from torch import distributed as dist
+from modelscope.utils.torch_utils import get_dist_info
+import {}
+
+parser = argparse.ArgumentParser()
+parser.add_argument('--save_all_ranks', type=ast.literal_eval, help='save all ranks results')
+parser.add_argument('--save_file', type=str, help='save file')
+parser.add_argument('--local_rank', type=int, default=0)
+args = parser.parse_args()
+
+
+def main():
+    results = {}.{}({})  # module.func(params)
+    if args.save_all_ranks:
+        save_file = args.save_file + str(dist.get_rank())
+        with open(save_file, 'wb') as f:
+            pickle.dump(results, f)
+    else:
+        rank, _ = get_dist_info()
+        if rank == 0:
+            with open(args.save_file, 'wb') as f:
+                pickle.dump(results, f)
+
+
+if __name__ == '__main__':
+    main()
+"""
+
+
+class DistributedTestCase(unittest.TestCase):
+    """Distributed TestCase for test function with distributed mode.
+    Examples:
+        >>> import torch
+        >>> from torch import distributed as dist
+        >>> from modelscope.utils.torch_utils import init_dist
+
+        >>> def _test_func(*args, **kwargs):
+        >>>     init_dist(launcher='pytorch')
+        >>>     rank = dist.get_rank()
+        >>>     if rank == 0:
+        >>>         value = torch.tensor(1.0).cuda()
+        >>>     else:
+        >>>         value = torch.tensor(2.0).cuda()
+        >>>     dist.all_reduce(value)
+        >>>     return value.cpu().numpy()
+
+        >>> class DistTest(DistributedTestCase):
+        >>>     def test_function_dist(self):
+        >>>         args = ()  # args should be python builtin type
+        >>>         kwargs = {}  # kwargs should be python builtin type
+        >>>         self.start(
+        >>>             _test_func,
+        >>>             num_gpus=2,
+        >>>             assert_callback=lambda x: self.assertEqual(x, 3.0),
+        >>>             *args,
+        >>>             **kwargs,
+        >>>         )
+    """
+
+    def _start(self,
+               dist_start_cmd,
+               func,
+               num_gpus,
+               assert_callback=None,
+               save_all_ranks=False,
+               *args,
+               **kwargs):
+        script_path = func.__code__.co_filename
+        script_dir, script_name = os.path.split(script_path)
+        script_name = os.path.splitext(script_name)[0]
+        func_name = func.__qualname__
+
+        func_params = []
+        for arg in args:
+            if isinstance(arg, str):
+                arg = ('\'{}\''.format(arg))
+            func_params.append(str(arg))
+
+        for k, v in kwargs.items():
+            if isinstance(v, str):
+                v = ('\'{}\''.format(v))
+            func_params.append('{}={}'.format(k, v))
+
+        func_params = ','.join(func_params).strip(',')
+
+        tmp_run_file = tempfile.NamedTemporaryFile(suffix='.py').name
+        tmp_res_file = tempfile.NamedTemporaryFile(suffix='.pkl').name
+
+        with open(tmp_run_file, 'w') as f:
+            print('save temporary run file to : {}'.format(tmp_run_file))
+            print('save results to : {}'.format(tmp_res_file))
+            run_file_content = _DIST_SCRIPT_TEMPLATE.format(
+                script_name, script_name, func_name, func_params)
+            f.write(run_file_content)
+
+        tmp_res_files = []
+        if save_all_ranks:
+            for i in range(num_gpus):
+                tmp_res_files.append(tmp_res_file + str(i))
+        else:
+            tmp_res_files = [tmp_res_file]
+        self.addCleanup(self.clean_tmp, [tmp_run_file] + tmp_res_files)
+
+        tmp_env = copy.deepcopy(os.environ)
+        tmp_env['PYTHONPATH'] = ':'.join(
+            (tmp_env.get('PYTHONPATH', ''), script_dir)).lstrip(':')
+        # avoid distributed test hang
+        tmp_env['NCCL_P2P_DISABLE'] = '1'
+        script_params = '--save_all_ranks=%s --save_file=%s' % (save_all_ranks,
+                                                                tmp_res_file)
+        script_cmd = '%s %s %s' % (dist_start_cmd, tmp_run_file, script_params)
+        print('script command: %s' % script_cmd)
+        res = subprocess.call(script_cmd, shell=True, env=tmp_env)
+
+        script_res = []
+        for res_file in tmp_res_files:
+            with open(res_file, 'rb') as f:
+                script_res.append(pickle.load(f))
+        if not save_all_ranks:
+            script_res = script_res[0]
+
+        if assert_callback:
+            assert_callback(script_res)
+
+        self.assertEqual(
+            res,
+            0,
+            msg='The test function ``{}`` in ``{}`` run failed!'.format(
+                func_name, script_name))
+
+        return script_res
+
+    def start(self,
+              func,
+              num_gpus,
+              assert_callback=None,
+              save_all_ranks=False,
+              *args,
+              **kwargs):
+        from .torch_utils import _find_free_port
+        ip = socket.gethostbyname(socket.gethostname())
+        if 'dist_start_cmd' in kwargs:
+            dist_start_cmd = kwargs.pop('dist_start_cmd')
+        else:
+            dist_start_cmd = '%s -m torch.distributed.launch --nproc_per_node=%d ' \
+                             '--master_addr=\'%s\' --master_port=%s' % (sys.executable, num_gpus, ip, _find_free_port())
 
-    def _calc_stopped_samples(self,
-                              prev_input_ids: Iterable[int]) -> Iterable[int]:
-        stopped_samples = []
-        for prev_input_ids_slice in prev_input_ids:
-            match = False
-            for stop_token_seq in self.stop_words_ids:
-                if self._tokens_match(prev_input_ids_slice, stop_token_seq):
-                    # if tokens do not match continue
-                    match = True
-                    break
-            stopped_samples.append(match)
-
-        return stopped_samples
-
-
-def top_k_logits(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):
-    """ This function has been mostly taken from huggingface conversational
-     ai code at
-         https://medium.com/huggingface/how-to-build-a-state-of-the-art-
-              conversational-ai-with-transfer-learning-2d818ac26313 """
-
-    if top_k > 0:
-        # Remove all tokens with a probability less than the
-        # last token of the top-k
-        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1,
-                                                                  None]
-        logits[indices_to_remove] = filter_value
-
-    if top_p > 0.0:
-        # Cconvert to 1D
-        sorted_logits, sorted_indices = torch.sort(
-            logits, descending=True, dim=-1)
-        cumulative_probs = torch.cumsum(
-            F.softmax(sorted_logits, dim=-1), dim=-1)
-
-        # Remove tokens with cumulative probability above the threshold
-        sorted_indices_to_remove = cumulative_probs > top_p
-        # Shift the indices to the right to keep also the first token
-        # above the threshold
-        sorted_indices_to_remove[..., 1:] \
-            = sorted_indices_to_remove[..., :-1].clone()
-        sorted_indices_to_remove[..., 0] = 0
-        for i in range(sorted_indices.size(0)):
-            indices_to_remove = sorted_indices[i][sorted_indices_to_remove[i]]
-            logits[i][indices_to_remove] = filter_value
-
-    return logits
-
-
-def switch(val1, val2, boolean):
-    boolean = boolean.type_as(val1)
-    return (1 - boolean) * val1 + boolean * val2
+        return self._start(
+            dist_start_cmd=dist_start_cmd,
+            func=func,
+            num_gpus=num_gpus,
+            assert_callback=assert_callback,
+            save_all_ranks=save_all_ranks,
+            *args,
+            **kwargs)
+
+    def clean_tmp(self, tmp_file_list):
+        for file in tmp_file_list:
+            if os.path.exists(file):
+                if os.path.isdir(file):
+                    shutil.rmtree(file)
+                else:
+                    os.remove(file)
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `modelscope-1.8.0/modelscope/models/nlp/space/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/space/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/space/configuration.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/space/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/space/dialog_intent_prediction.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/space/dialog_intent_prediction.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/space/dialog_modeling.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/space/dialog_modeling.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/space/dialog_state_tracking.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/space/dialog_state_tracking.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/space/model/gen_unified_transformer.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/space/model/gen_unified_transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/space/model/generator.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/space/model/generator.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/space/model/intent_unified_transformer.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/space/model/intent_unified_transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/space/model/model_base.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/space/model/model_base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/space/model/tokenization_space.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/space/model/tokenization_space.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/space/model/unified_transformer.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/space/model/unified_transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/space/modules/embedder.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/space/modules/embedder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/space/modules/feedforward.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/space/modules/feedforward.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/space/modules/functions.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/space/modules/functions.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/space/modules/multihead_attention.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/space/modules/multihead_attention.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/space/modules/transformer_block.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/space/modules/transformer_block.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/space_T_cn/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/space_T_cn/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/space_T_cn/backbone.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/space_T_cn/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/space_T_cn/configuration.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/space_T_cn/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/space_T_cn/table_question_answering.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/space_T_cn/table_question_answering.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/space_T_en/text_to_sql.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/space_T_en/text_to_sql.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/structbert/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/structbert/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/structbert/adv_utils.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/structbert/adv_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/structbert/backbone.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/structbert/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/structbert/configuration.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/structbert/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/structbert/faq_question_answering.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/structbert/faq_question_answering.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/structbert/fill_mask.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/structbert/fill_mask.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/structbert/text_classification.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/structbert/text_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/structbert/token_classification.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/structbert/token_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/task_models/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/task_models/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/task_models/feature_extraction.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/task_models/feature_extraction.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/task_models/fill_mask.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/task_models/fill_mask.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/task_models/information_extraction.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/task_models/information_extraction.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/task_models/task_model.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/task_models/task_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/task_models/text_classification.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/task_models/text_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/task_models/text_generation.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/task_models/text_generation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/task_models/text_ranking.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/task_models/text_ranking.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/task_models/token_classification.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/task_models/token_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/unite/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/unite/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/unite/translation_evaluation.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/unite/translation_evaluation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/use/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/use/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/use/transformer.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/use/transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/use/user_satisfaction_estimation.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/use/user_satisfaction_estimation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/veco/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/veco/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/veco/backbone.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/veco/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/veco/configuration.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/veco/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/veco/fill_mask.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/veco/fill_mask.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/veco/text_classification.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/veco/text_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/veco/token_classification.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/veco/token_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/xlm_roberta/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/xlm_roberta/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/xlm_roberta/backbone.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/xlm_roberta/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/nlp/xlm_roberta/configuration.py` & `modelscope-1.8.0rc0/modelscope/models/nlp/xlm_roberta/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/science/unifold/config.py` & `modelscope-1.8.0rc0/modelscope/models/science/unifold/config.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/science/unifold/data/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/science/unifold/data/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/science/unifold/data/data_ops.py` & `modelscope-1.8.0rc0/modelscope/models/science/unifold/data/data_ops.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/science/unifold/data/msa_pairing.py` & `modelscope-1.8.0rc0/modelscope/models/science/unifold/data/msa_pairing.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/science/unifold/data/process.py` & `modelscope-1.8.0rc0/modelscope/models/science/unifold/data/process.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/science/unifold/data/process_multimer.py` & `modelscope-1.8.0rc0/modelscope/models/science/unifold/data/process_multimer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/science/unifold/data/protein.py` & `modelscope-1.8.0rc0/modelscope/models/science/unifold/data/protein.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/science/unifold/data/residue_constants.py` & `modelscope-1.8.0rc0/modelscope/models/science/unifold/data/residue_constants.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/science/unifold/data/utils.py` & `modelscope-1.8.0rc0/modelscope/models/science/unifold/data/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/science/unifold/dataset.py` & `modelscope-1.8.0rc0/modelscope/models/science/unifold/dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/science/unifold/model.py` & `modelscope-1.8.0rc0/modelscope/models/science/unifold/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/science/unifold/modules/alphafold.py` & `modelscope-1.8.0rc0/modelscope/models/science/unifold/modules/alphafold.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/science/unifold/modules/attentions.py` & `modelscope-1.8.0rc0/modelscope/models/science/unifold/modules/attentions.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/science/unifold/modules/auxillary_heads.py` & `modelscope-1.8.0rc0/modelscope/models/science/unifold/modules/auxillary_heads.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/science/unifold/modules/common.py` & `modelscope-1.8.0rc0/modelscope/models/science/unifold/modules/common.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/science/unifold/modules/confidence.py` & `modelscope-1.8.0rc0/modelscope/models/science/unifold/modules/confidence.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/science/unifold/modules/embedders.py` & `modelscope-1.8.0rc0/modelscope/models/science/unifold/modules/embedders.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/science/unifold/modules/evoformer.py` & `modelscope-1.8.0rc0/modelscope/models/science/unifold/modules/evoformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/science/unifold/modules/featurization.py` & `modelscope-1.8.0rc0/modelscope/models/science/unifold/modules/featurization.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/science/unifold/modules/frame.py` & `modelscope-1.8.0rc0/modelscope/models/science/unifold/modules/frame.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/science/unifold/modules/structure_module.py` & `modelscope-1.8.0rc0/modelscope/models/science/unifold/modules/structure_module.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/science/unifold/modules/template.py` & `modelscope-1.8.0rc0/modelscope/models/science/unifold/modules/template.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/science/unifold/modules/triangle_multiplication.py` & `modelscope-1.8.0rc0/modelscope/models/science/unifold/modules/triangle_multiplication.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/science/unifold/msa/mmcif.py` & `modelscope-1.8.0rc0/modelscope/models/science/unifold/msa/mmcif.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/science/unifold/msa/msa_identifiers.py` & `modelscope-1.8.0rc0/modelscope/models/science/unifold/msa/msa_identifiers.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/science/unifold/msa/parsers.py` & `modelscope-1.8.0rc0/modelscope/models/science/unifold/msa/parsers.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/science/unifold/msa/pipeline.py` & `modelscope-1.8.0rc0/modelscope/models/science/unifold/msa/pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/science/unifold/msa/templates.py` & `modelscope-1.8.0rc0/modelscope/models/science/unifold/msa/templates.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/science/unifold/msa/tools/__init__.py` & `modelscope-1.8.0rc0/modelscope/models/science/unifold/msa/tools/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/science/unifold/msa/tools/hhblits.py` & `modelscope-1.8.0rc0/modelscope/models/science/unifold/msa/tools/hhblits.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/science/unifold/msa/tools/hhsearch.py` & `modelscope-1.8.0rc0/modelscope/models/science/unifold/msa/tools/hhsearch.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/science/unifold/msa/tools/hmmbuild.py` & `modelscope-1.8.0rc0/modelscope/models/science/unifold/msa/tools/hmmbuild.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/science/unifold/msa/tools/hmmsearch.py` & `modelscope-1.8.0rc0/modelscope/models/science/unifold/msa/tools/hmmsearch.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/science/unifold/msa/tools/jackhmmer.py` & `modelscope-1.8.0rc0/modelscope/models/science/unifold/msa/tools/jackhmmer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/science/unifold/msa/tools/kalign.py` & `modelscope-1.8.0rc0/modelscope/models/science/unifold/msa/tools/kalign.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/science/unifold/msa/tools/utils.py` & `modelscope-1.8.0rc0/modelscope/models/science/unifold/msa/tools/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/models/science/unifold/msa/utils.py` & `modelscope-1.8.0rc0/modelscope/models/science/unifold/msa/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/auth/auth_config.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/auth/auth_config.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/context/dataset_context_config.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/context/dataset_context_config.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/data_files/data_files_manager.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/data_files/data_files_manager.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/data_loader/data_loader.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/data_loader/data_loader.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/data_loader/data_loader_manager.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/data_loader/data_loader_manager.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/__init__.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/__init__.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/asr_dataset.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/asr_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/kws_farfield_dataset.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/kws_farfield_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/kws_nearfield_dataset.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/kws_nearfield_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/kws_nearfield_processor.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/kws_nearfield_processor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/bad_image_detecting/__init__.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/bad_image_detecting/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/bad_image_detecting/bad_image_detecting_dataset.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/bad_image_detecting/bad_image_detecting_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/builder.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/builder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/build.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/build.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/collate_batch.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/collate_batch.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/coco.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/coco.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/mosaic_wrapper.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/mosaic_wrapper.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/__init__.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/coco/__init__.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/coco/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/coco/coco_eval.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/coco/coco_eval.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/distributed.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/distributed.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/grouped_batch_sampler.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/grouped_batch_sampler.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/iteration_based_batch_sampler.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/iteration_based_batch_sampler.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/build.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/build.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/transforms.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/transforms.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/easycv_base.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/easycv_base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/gopro_image_deblurring_dataset.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/gopro_image_deblurring_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_colorization/__init__.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/image_colorization/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_colorization/image_colorization_dataset.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/image_colorization/image_colorization_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/__init__.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/aug.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/aug.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/image_inpainting_dataset.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/image_inpainting_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_instance_segmentation_coco_dataset.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/image_instance_segmentation_coco_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/__init__.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/data_utils.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/data_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/image_portrait_enhancement_dataset.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/image_portrait_enhancement_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assessment_degradation/__init__.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assessment_degradation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assessment_degradation/image_quality_assessment_degradation_dataset.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assessment_degradation/image_quality_assessment_degradation_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assmessment_mos/__init__.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assmessment_mos/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assmessment_mos/image_quality_assessment_mos_dataset.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assmessment_mos/image_quality_assessment_mos_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/language_guided_video_summarization_dataset.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/language_guided_video_summarization_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/mgeo_ranking_dataset.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/mgeo_ranking_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/__init__.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/movie_scene_segmentation_dataset.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/movie_scene_segmentation_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/sampler.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/sampler.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/augmenter.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/augmenter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/data_loader.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/data_loader.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/image_dataset.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/image_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/iou_evaluator.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/iou_evaluator.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/quad_measurer.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/quad_measurer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/augment_data.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/augment_data.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/data_process.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/data_process.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_border_map.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_border_map.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_icdar_data.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_icdar_data.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_seg_detection_data.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_seg_detection_data.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/normalize_image.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/normalize_image.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/random_crop_data.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/random_crop_data.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_recognition_dataset.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_recognition_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/reds_image_deblurring_dataset.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/reds_image_deblurring_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/__init__.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/referring_video_object_segmentation_dataset.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/referring_video_object_segmentation_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/transformers.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/transformers.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/__init__.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/data_utils.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/data_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/sidd_image_denoising_dataset.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/sidd_image_denoising_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/transforms.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/transforms.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/text_ranking_dataset.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/text_ranking_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/torch_custom_dataset.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/torch_custom_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/veco_dataset.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/veco_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/__init__.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/data_utils.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/data_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/video_frame_interpolation_dataset.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/video_frame_interpolation_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_stabilization/__init__.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/video_stabilization/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_stabilization/video_stabilization_dataset.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/video_stabilization/video_stabilization_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_summarization_dataset.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/video_summarization_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_super_resolution/__init__.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/video_super_resolution/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_super_resolution/video_super_resolution_dataset.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/custom_datasets/video_super_resolution/video_super_resolution_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/dataset_cls/dataset.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/dataset_cls/dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/download/dataset_builder.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/download/dataset_builder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/download/download_config.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/download/download_config.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/download/download_manager.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/download/download_manager.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/meta/data_meta_config.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/meta/data_meta_config.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/meta/data_meta_manager.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/meta/data_meta_manager.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/ms_dataset.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/ms_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/task_datasets/__init__.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/task_datasets/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/utils/dataset_utils.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/utils/dataset_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/utils/delete_utils.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/utils/delete_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/utils/maxcompute_utils.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/utils/maxcompute_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/utils/oss_utils.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/utils/oss_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/msdatasets/utils/upload_utils.py` & `modelscope-1.8.0rc0/modelscope/msdatasets/utils/upload_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/ops/ailut/pyinterfaces.py` & `modelscope-1.8.0rc0/modelscope/ops/ailut/pyinterfaces.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/ops/quadtree_attention/functions/quadtree_attention.py` & `modelscope-1.8.0rc0/modelscope/ops/quadtree_attention/functions/quadtree_attention.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/ops/quadtree_attention/modules/quadtree_attention.py` & `modelscope-1.8.0rc0/modelscope/ops/quadtree_attention/modules/quadtree_attention.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/outputs/cv_outputs.py` & `modelscope-1.8.0rc0/modelscope/outputs/cv_outputs.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/outputs/nlp_outputs.py` & `modelscope-1.8.0rc0/modelscope/outputs/nlp_outputs.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/outputs/outputs.py` & `modelscope-1.8.0rc0/modelscope/outputs/outputs.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipeline_inputs.py` & `modelscope-1.8.0rc0/modelscope/pipeline_inputs.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/audio/__init__.py` & `modelscope-1.8.0rc0/modelscope/pipelines/audio/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/audio/ans_dfsmn_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/audio/ans_dfsmn_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/audio/ans_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/audio/ans_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/audio/asr_inference_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/audio/asr_inference_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/audio/asr_wenet_inference_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/audio/asr_wenet_inference_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/audio/inverse_text_processing_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/audio/inverse_text_processing_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/audio/kws_farfield_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/audio/kws_farfield_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/audio/kws_kwsbp_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/audio/kws_kwsbp_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/audio/language_recognition_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/audio/language_recognition_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/audio/linear_aec_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/audio/linear_aec_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/audio/lm_infer_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/audio/lm_infer_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/audio/punctuation_processing_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/audio/punctuation_processing_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/audio/segmentation_clustering_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/audio/segmentation_clustering_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/audio/separation_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/audio/separation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/audio/speaker_change_locating_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/audio/speaker_change_locating_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/audio/speaker_diarization_dialogue_detection_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/audio/speaker_diarization_dialogue_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/audio/speaker_diarization_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/audio/speaker_diarization_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/audio/speaker_diarization_semantic_speaker_turn_detection_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/audio/speaker_diarization_semantic_speaker_turn_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/audio/speaker_verification_eres2net_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/audio/speaker_verification_eres2net_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/audio/speaker_verification_light_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/audio/speaker_verification_light_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/audio/speaker_verification_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/audio/speaker_verification_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/audio/speaker_verification_rdino_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/audio/speaker_verification_rdino_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/audio/text_to_speech_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/audio/text_to_speech_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/audio/timestamp_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/audio/timestamp_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/audio/voice_activity_detection_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/audio/voice_activity_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/base.py` & `modelscope-1.8.0rc0/modelscope/pipelines/base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/builder.py` & `modelscope-1.8.0rc0/modelscope/pipelines/builder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/__init__.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/action_detection_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/action_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/action_recognition_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/action_recognition_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/animal_recognition_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/animal_recognition_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/arc_face_recognition_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/arc_face_recognition_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/bad_image_detecting_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/bad_image_detecting_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/body_2d_keypoints_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/body_2d_keypoints_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/body_3d_keypoints_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/body_3d_keypoints_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/card_detection_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/card_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/cmdssl_video_embedding_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/cmdssl_video_embedding_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/content_check_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/content_check_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/controllable_image_generation_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/controllable_image_generation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/crowd_counting_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/crowd_counting_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/ddcolor_image_colorization_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/ddcolor_image_colorization_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/ddpm_semantic_segmentation_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/ddpm_semantic_segmentation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/face_attribute_recognition_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/face_attribute_recognition_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/face_detection_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/face_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/face_emotion_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/face_emotion_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/face_human_hand_detection_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/face_human_hand_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/face_image_generation_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/face_image_generation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/face_liveness_ir_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/face_liveness_ir_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/face_liveness_xc_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/face_liveness_xc_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/face_processing_base_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/face_processing_base_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/face_quality_assessment_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/face_quality_assessment_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/face_recognition_onnx_fm_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/face_recognition_onnx_fm_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/face_recognition_onnx_ir_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/face_recognition_onnx_ir_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/face_recognition_ood_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/face_recognition_ood_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/face_recognition_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/face_recognition_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/face_reconstruction_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/face_reconstruction_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/facial_expression_recognition_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/facial_expression_recognition_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/facial_landmark_confidence_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/facial_landmark_confidence_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/fast_instance_segmentation_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/fast_instance_segmentation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/general_recognition_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/general_recognition_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/hand_static_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/hand_static_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/hicossl_video_embedding_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/hicossl_video_embedding_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/human_reconstruction_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/human_reconstruction_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/image_body_reshaping_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/image_body_reshaping_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/image_bts_depth_estimation_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/image_bts_depth_estimation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/image_cartoon_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/image_cartoon_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/image_classification_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/image_classification_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/image_color_enhance_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/image_color_enhance_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/image_colorization_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/image_colorization_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/image_debanding_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/image_debanding_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/image_deblur_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/image_deblur_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/image_defrcn_fewshot_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/image_defrcn_fewshot_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/image_denoise_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/image_denoise_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/image_depth_estimation_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/image_depth_estimation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/image_detection_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/image_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/image_driving_perception_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/image_driving_perception_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/image_face_fusion_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/image_face_fusion_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/image_human_parsing_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/image_human_parsing_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/image_inpainting_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/image_inpainting_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/image_inpainting_sdv2_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/image_inpainting_sdv2_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/image_instance_segmentation_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/image_instance_segmentation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/image_matching_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/image_matching_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/image_matting_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/image_matting_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/image_mvs_depth_estimation_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/image_mvs_depth_estimation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/image_open_vocabulary_detection_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/image_open_vocabulary_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/image_paintbyexample_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/image_paintbyexample_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/image_panoptic_segmentation_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/image_panoptic_segmentation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/image_portrait_enhancement_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/image_portrait_enhancement_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/image_quality_assessment_degradation_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/image_quality_assessment_degradation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/image_quality_assessment_man_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/image_quality_assessment_man_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/image_quality_assessment_mos_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/image_quality_assessment_mos_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/image_reid_person_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/image_reid_person_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/image_restoration_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/image_restoration_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/image_salient_detection_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/image_salient_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/image_semantic_segmentation_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/image_semantic_segmentation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/image_skychange_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/image_skychange_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/image_structured_model_probing_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/image_structured_model_probing_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/image_style_transfer_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/image_style_transfer_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/image_super_resolution_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/image_super_resolution_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/image_to_image_generate_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/image_to_image_generate_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/image_to_image_translation_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/image_to_image_translation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/image_try_on_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/image_try_on_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/indoor_layout_estimation_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/indoor_layout_estimation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/language_guided_video_summarization_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/language_guided_video_summarization_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/license_plate_detection_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/license_plate_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/lineless_table_recognition_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/lineless_table_recognition_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/live_category_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/live_category_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/mask_face_recognition_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/mask_face_recognition_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/maskdino_instance_segmentation_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/maskdino_instance_segmentation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/mobile_image_super_resolution_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/mobile_image_super_resolution_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/mog_face_detection_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/mog_face_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/motion_generation_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/motion_generation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/movie_scene_segmentation_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/movie_scene_segmentation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/mtcnn_face_detection_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/mtcnn_face_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/nerf_recon_4k_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/nerf_recon_4k_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/nerf_recon_acc_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/nerf_recon_acc_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/nerf_recon_vq_compression_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/nerf_recon_vq_compression_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/object_detection_3d_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/object_detection_3d_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/ocr_detection_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/ocr_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/ocr_recognition_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/ocr_recognition_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/ocr_utils/__init__.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/ocr_utils/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/ocr_utils/model_convnext_transformer.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/ocr_utils/model_convnext_transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/ocr_utils/model_dla34.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/ocr_utils/model_dla34.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/ocr_utils/model_resnet18_half.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/ocr_utils/model_resnet18_half.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/ocr_utils/model_resnet_mutex_v4_linewithchar.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/ocr_utils/model_resnet_mutex_v4_linewithchar.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/ocr_utils/model_vlpt.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/ocr_utils/model_vlpt.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/ocr_utils/ocr_modules/__init__.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/ocr_utils/ocr_modules/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/ocr_utils/ocr_modules/convnext.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/ocr_utils/ocr_modules/convnext.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/ocr_utils/ocr_modules/timm_tinyc.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/ocr_utils/ocr_modules/timm_tinyc.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/ocr_utils/ocr_modules/vitstr.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/ocr_utils/ocr_modules/vitstr.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/ocr_utils/ops.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/ocr_utils/ops.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/ocr_utils/resnet18_v1.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/ocr_utils/resnet18_v1.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/ocr_utils/resnet_utils.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/ocr_utils/resnet_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/ocr_utils/table_process.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/ocr_utils/table_process.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/ocr_utils/utils.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/ocr_utils/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/panorama_depth_estimation_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/panorama_depth_estimation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/panorama_depth_estimation_s2net_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/panorama_depth_estimation_s2net_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/pedestrian_attribute_recognition_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/pedestrian_attribute_recognition_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/pointcloud_sceneflow_estimation_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/pointcloud_sceneflow_estimation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/product_retrieval_embedding_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/product_retrieval_embedding_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/product_segmentation_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/product_segmentation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/realtime_video_object_detection_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/realtime_video_object_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/referring_video_object_segmentation_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/referring_video_object_segmentation_pipeline.py`

 * *Files 1% similar despite different names*

```diff
@@ -175,15 +175,15 @@
                     border=(0, len(self.text_queries)
                             * text_border_height_per_query, 0, 0))
                 W, H = vid_frame.size
                 draw = ImageDraw.Draw(vid_frame)
 
                 for i, (text_query, color) in enumerate(
                         zip(self.text_queries, colors), start=1):
-                    _, _, w, h = draw.textbbox([0, 0], text_query, font=font)
+                    w, h = draw.textsize(text_query, font=font)
                     draw.text(((W - w) / 2,
                                (text_border_height_per_query * i) - h - 3),
                               text_query,
                               fill=tuple(color) + (255, ),
                               font=font)
                 masked_video.append(np.array(vid_frame))
             # generate and save the output clip:
```

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/retina_face_detection_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/retina_face_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/shop_segmentation_pipleline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/shop_segmentation_pipleline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/skin_retouching_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/skin_retouching_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/table_recognition_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/table_recognition_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/tbs_detection_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/tbs_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/tbs_detection_utils/utils.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/tbs_detection_utils/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/text_driven_segmentation_pipleline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/text_driven_segmentation_pipleline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/text_to_360panorama_image_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/text_to_360panorama_image_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/tinynas_classification_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/tinynas_classification_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/tinynas_detection_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/tinynas_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/ulfd_face_detection_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/ulfd_face_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/video_category_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/video_category_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/video_colorization_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/video_colorization_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/video_deinterlace_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/video_deinterlace_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/video_depth_estimation_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/video_depth_estimation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/video_frame_interpolation_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/video_frame_interpolation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/video_human_matting_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/video_human_matting_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/video_inpainting_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/video_inpainting_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/video_instance_segmentation_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/video_instance_segmentation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/video_multi_object_tracking_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/video_multi_object_tracking_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/video_object_segmentation_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/video_object_segmentation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/video_panoptic_segmentation_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/video_panoptic_segmentation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/video_single_object_tracking_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/video_single_object_tracking_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/video_stabilization_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/video_stabilization_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/video_summarization_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/video_summarization_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/video_super_resolution_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/video_super_resolution_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/vidt_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/vidt_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/virtual_try_on_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/virtual_try_on_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/vision_efficient_tuning_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/vision_efficient_tuning_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/vision_middleware_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/vision_middleware_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/vop_retrieval_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/vop_retrieval_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/cv/vop_retrieval_se_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/cv/vop_retrieval_se_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/multi_modal/__init__.py` & `modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/multi_modal/asr_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/asr_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/multi_modal/diffusers_wrapped/__init__.py` & `modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/diffusers_wrapped/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/multi_modal/diffusers_wrapped/diffusers_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/diffusers_wrapped/diffusers_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/__init__.py` & `modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/chinese_stable_diffusion_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/chinese_stable_diffusion_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/stable_diffusion_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/stable_diffusion_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/__init__.py` & `modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/disco_guided_diffusion.py` & `modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/disco_guided_diffusion.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/utils.py` & `modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/multi_modal/document_vl_embedding_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/document_vl_embedding_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/multi_modal/efficient_diffusion_tuning_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/efficient_diffusion_tuning_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/multi_modal/generative_multi_modal_embedding_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/generative_multi_modal_embedding_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/multi_modal/gridvlp_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/gridvlp_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/multi_modal/image_captioning_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/image_captioning_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/multi_modal/image_text_retrieval_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/image_text_retrieval_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/multi_modal/mgeo_ranking_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/mgeo_ranking_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/multi_modal/multi_modal_embedding_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/multi_modal_embedding_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/multi_modal/multimodal_dialogue_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/multimodal_dialogue_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/multi_modal/ocr_recognition_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/ocr_recognition_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/multi_modal/soonet_video_temporal_grounding_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/soonet_video_temporal_grounding_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/multi_modal/sudoku_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/sudoku_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/multi_modal/team_multi_modal_similarity_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/team_multi_modal_similarity_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/multi_modal/text2sql_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/text2sql_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/multi_modal/text_to_image_synthesis_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/text_to_image_synthesis_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/multi_modal/text_to_video_synthesis_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/text_to_video_synthesis_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/multi_modal/video_captioning_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/video_captioning_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/multi_modal/video_multi_modal_embedding_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/video_multi_modal_embedding_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/multi_modal/video_question_answering_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/video_question_answering_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/multi_modal/visual_entailment_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/visual_entailment_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/multi_modal/visual_grounding_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/visual_grounding_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/multi_modal/visual_question_answering_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/multi_modal/visual_question_answering_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/__init__.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/automatic_post_editing_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/automatic_post_editing_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/canmt_translation_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/canmt_translation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/codegeex_code_generation_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/codegeex_code_generation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/codegeex_code_translation_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/codegeex_code_translation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/conversational_text_to_sql_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/conversational_text_to_sql_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/dialog_intent_prediction_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/dialog_intent_prediction_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/dialog_modeling_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/dialog_modeling_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/dialog_state_tracking_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/dialog_state_tracking_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/distributed_gpt3_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/distributed_gpt3_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/distributed_gpt_moe_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/distributed_gpt_moe_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/distributed_plug_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/distributed_plug_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/document_grounded_dialog_generate_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/document_grounded_dialog_generate_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/document_grounded_dialog_rerank_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/document_grounded_dialog_rerank_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/document_grounded_dialog_retrieval_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/document_grounded_dialog_retrieval_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/document_segmentation_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/document_segmentation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/extractive_summarization_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/extractive_summarization_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/faq_question_answering_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/faq_question_answering_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/fasttext_text_classification_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/fasttext_text_classification_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/feature_extraction_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/feature_extraction_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/fid_dialogue_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/fid_dialogue_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/fill_mask_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/fill_mask_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/glm130b_text_generation_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/glm130b_text_generation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/information_extraction_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/information_extraction_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/interactive_translation_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/interactive_translation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/language_identification_pipline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/language_identification_pipline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/llama2_text_generation_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/llama2_text_generation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/mglm_text_summarization_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/mglm_text_summarization_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/named_entity_recognition_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/named_entity_recognition_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/polylm_text_generation_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/polylm_text_generation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/sentence_embedding_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/sentence_embedding_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/siamese_uie_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/siamese_uie_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/summarization_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/summarization_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/table_question_answering_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/table_question_answering_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/text_classification_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/text_classification_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/text_error_correction_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/text_error_correction_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/text_generation_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/text_generation_pipeline.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,15 +1,14 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 
 # Copyright (c) 2022 Zhipu.AI
 import os
 from typing import Any, Dict, Optional, Union
 
 import torch
-from transformers import GenerationConfig
 
 from modelscope import snapshot_download
 from modelscope.metainfo import Pipelines
 from modelscope.models.base import Model
 from modelscope.outputs import (ModelOutputBase, OutputKeys,
                                 TokenGeneratorOutput)
 from modelscope.pipelines.base import Pipeline, Tensor
@@ -17,20 +16,16 @@
 from modelscope.preprocessors import Preprocessor
 from modelscope.utils.chinese_utils import remove_space_between_chinese_chars
 from modelscope.utils.constant import ModelFile, Tasks
 from modelscope.utils.hub import Config, read_config
 from modelscope.utils.streaming_output import PipelineStreamingOutputMixin
 
 __all__ = [
-    'TextGenerationPipeline',
-    'TextGenerationT5Pipeline',
-    'ChatGLM6bTextGenerationPipeline',
-    'ChatGLM6bV2TextGenerationPipeline',
-    'QWenChatPipeline',
-    'QWenTextGenerationPipeline',
+    'TextGenerationPipeline', 'TextGenerationT5Pipeline',
+    'ChatGLM6bTextGenerationPipeline', 'ChatGLM6bV2TextGenerationPipeline'
 ]
 
 
 @PIPELINES.register_module(
     Tasks.text_generation, module_name=Pipelines.text_generation)
 class TextGenerationPipeline(Pipeline, PipelineStreamingOutputMixin):
 
@@ -195,16 +190,15 @@
 class ChatGLM6bTextGenerationPipeline(Pipeline):
 
     def __init__(self,
                  model: Union[Model, str],
                  quantization_bit=None,
                  use_bf16=False,
                  **kwargs):
-        from modelscope.models.nlp.chatglm.text_generation import (
-            ChatGLMConfig, ChatGLMForConditionalGeneration)
+        from modelscope.models.nlp.chatglm.text_generation import ChatGLMForConditionalGeneration, ChatGLMConfig
         if isinstance(model, str):
             model_dir = snapshot_download(
                 model) if not os.path.exists(model) else model
             model = ChatGLMForConditionalGeneration.from_pretrained(
                 model_dir).half()
             if torch.cuda.is_available():
                 model = model.cuda()
@@ -238,17 +232,15 @@
 class ChatGLM6bV2TextGenerationPipeline(Pipeline):
 
     def __init__(self,
                  model: Union[Model, str],
                  quantization_bit=None,
                  use_bf16=False,
                  **kwargs):
-        from modelscope.models.nlp import (ChatGLM2Config,
-                                           ChatGLM2ForConditionalGeneration,
-                                           ChatGLM2Tokenizer)
+        from modelscope.models.nlp import ChatGLM2ForConditionalGeneration, ChatGLM2Tokenizer, ChatGLM2Config
         if isinstance(model, str):
             model_dir = snapshot_download(
                 model) if not os.path.exists(model) else model
             model = ChatGLM2ForConditionalGeneration.from_pretrained(model_dir)
             if torch.cuda.is_available():
                 model = model.cuda()
         if quantization_bit is not None:
@@ -272,132 +264,7 @@
     def forward(self, inputs: Dict, **forward_params) -> Dict[str, Any]:
         inputs.update(forward_params)
         return self.model.chat(inputs, self.tokenizer)
 
     # format the outputs from pipeline
     def postprocess(self, input, **kwargs) -> Dict[str, Any]:
         return input
-
-
-@PIPELINES.register_module(group_key=Tasks.chat, module_name='qwen-chat')
-class QWenChatPipeline(Pipeline):
-
-    def __init__(self, model: Union[Model, str], **kwargs):
-        from modelscope.models.nlp import (QWenConfig, QWenForTextGeneration,
-                                           QWenTokenizer)
-        torch_dtype = kwargs.get('torch_dtype', torch.bfloat16)
-        device_map = kwargs.get('device_map', 'auto')
-        use_max_memory = kwargs.get('use_max_memory', False)
-        quantization_config = kwargs.get('quantization_config', None)
-
-        if use_max_memory:
-            max_memory = f'{int(torch.cuda.mem_get_info()[0] / 1024 ** 3) - 2}GB'
-            n_gpus = torch.cuda.device_count()
-            max_memory = {i: max_memory for i in range(n_gpus)}
-        else:
-            max_memory = None
-
-        if isinstance(model, str):
-            model_dir = snapshot_download(
-                model) if not os.path.exists(model) else model
-
-            config = read_config(model_dir)
-            model_config = QWenConfig.from_pretrained(model_dir)
-            model_config.torch_dtype = torch_dtype
-
-            model = QWenForTextGeneration.from_pretrained(
-                model_dir,
-                cfg_dict=config,
-                config=model_config,
-                device_map=device_map,
-                torch_dtype=torch_dtype,
-                quantization_config=quantization_config,
-                max_memory=max_memory)
-            model.generation_config = GenerationConfig.from_pretrained(
-                model_dir)
-
-        self.model = model
-        self.model.eval()
-        self.tokenizer = QWenTokenizer.from_pretrained(self.model.model_dir)
-
-        super().__init__(model=model, **kwargs)
-
-    def _sanitize_parameters(self, **pipeline_parameters):
-        return {}, pipeline_parameters, {}
-
-    def preprocess(self, inputs, **preprocess_params) -> Dict[str, Any]:
-        return inputs
-
-    # define the forward pass
-    def forward(self, inputs: str, **forward_params) -> Dict[str, Any]:
-        history = forward_params.get('history', None)
-        system = forward_params.get('system', 'You are a helpful assistant.')
-        append_history = forward_params.get('append_history', True)
-        return self.model.chat(self.tokenizer, inputs, history, system,
-                               append_history)
-
-    # format the outputs from pipeline
-    def postprocess(self, input, **kwargs) -> Dict[str, Any]:
-        return input
-
-
-@PIPELINES.register_module(
-    group_key=Tasks.text_generation, module_name='qwen-text-generation')
-class QWenTextGenerationPipeline(Pipeline):
-
-    def __init__(self, model: Union[Model, str], **kwargs):
-        from modelscope.models.nlp import (QWenConfig, QWenForTextGeneration,
-                                           QWenTokenizer)
-        torch_dtype = kwargs.get('torch_dtype', torch.bfloat16)
-        device_map = kwargs.get('device_map', 'auto')
-        use_max_memory = kwargs.get('use_max_memory', False)
-        quantization_config = kwargs.get('quantization_config', None)
-
-        if use_max_memory:
-            max_memory = f'{int(torch.cuda.mem_get_info()[0] / 1024 ** 3) - 2}GB'
-            n_gpus = torch.cuda.device_count()
-            max_memory = {i: max_memory for i in range(n_gpus)}
-        else:
-            max_memory = None
-
-        if isinstance(model, str):
-            model_dir = snapshot_download(
-                model) if not os.path.exists(model) else model
-
-            config = read_config(model_dir)
-            model_config = QWenConfig.from_pretrained(model_dir)
-            model_config.torch_dtype = torch_dtype
-
-            model = QWenForTextGeneration.from_pretrained(
-                model_dir,
-                cfg_dict=config,
-                config=model_config,
-                device_map=device_map,
-                torch_dtype=torch_dtype,
-                quantization_config=quantization_config,
-                max_memory=max_memory)
-            model.generation_config = GenerationConfig.from_pretrained(
-                model_dir)
-
-        self.model = model
-        self.model.eval()
-        self.tokenizer = QWenTokenizer.from_pretrained(self.model.model_dir)
-
-        super().__init__(model=model, **kwargs)
-
-    def _sanitize_parameters(self, **pipeline_parameters):
-        return {}, pipeline_parameters, {}
-
-    def preprocess(self, inputs, **preprocess_params) -> Dict[str, Any]:
-        return inputs
-
-    # define the forward pass
-    def forward(self, inputs: str, **forward_params) -> Dict[str, Any]:
-        return {
-            OutputKeys.TEXT:
-            self.model.chat(self.tokenizer, inputs,
-                            history=None)[OutputKeys.RESPONSE]
-        }
-
-    # format the outputs from pipeline
-    def postprocess(self, input, **kwargs) -> Dict[str, Any]:
-        return input
```

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/text_ranking_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/text_ranking_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/token_classification_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/token_classification_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/translation_evaluation_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/translation_evaluation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/translation_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/translation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/translation_quality_estimation_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/translation_quality_estimation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/user_satisfaction_estimation_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/user_satisfaction_estimation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/word_alignment_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/word_alignment_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/word_segmentation_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/word_segmentation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/nlp/zero_shot_classification_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/nlp/zero_shot_classification_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/pipeline_template.py` & `modelscope-1.8.0rc0/modelscope/pipelines/pipeline_template.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/science/__init__.py` & `modelscope-1.8.0rc0/modelscope/pipelines/science/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/science/protein_structure_pipeline.py` & `modelscope-1.8.0rc0/modelscope/pipelines/science/protein_structure_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/pipelines/util.py` & `modelscope-1.8.0rc0/modelscope/pipelines/util.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/__init__.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/asr.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/asr.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/audio.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/audio.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/base.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/builder.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/builder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/common.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/common.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/cv/__init__.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/cv/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/cv/action_detection_mapper.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/cv/action_detection_mapper.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/cv/bad_image_detecting_preprocessor.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/cv/bad_image_detecting_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/cv/controllable_image_generation.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/cv/controllable_image_generation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/cv/cv2_transforms.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/cv/cv2_transforms.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/cv/image_classification_preprocessor.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/cv/image_classification_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/cv/image_quality_assessment_man.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/cv/image_quality_assessment_man.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/cv/image_quality_assessment_mos.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/cv/image_quality_assessment_mos.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/cv/image_restoration_preprocessor.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/cv/image_restoration_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/cv/mmcls_preprocessor.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/cv/mmcls_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/cv/timer.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/cv/timer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/cv/util.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/cv/util.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/cv/video_stabilization.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/cv/video_stabilization.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/cv/video_super_resolution.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/cv/video_super_resolution.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/image.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/image.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/kws.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/kws.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/movie_scene_segmentation/transforms.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/movie_scene_segmentation/transforms.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/multi_modal.py` & `modelscope-1.8.0rc0/modelscope/trainers/multi_modal/custom_diffusion/custom_diffusion_trainer.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,804 +1,743 @@
-# Copyright (c) Alibaba, Inc. and its affiliates.
-import os.path as osp
-import re
-from io import BytesIO
-from typing import Any, Dict, List, Tuple, Union
+# Copyright 2022-2023 The Alibaba Fundamental Vision Team Authors. All rights reserved.
+import hashlib
+import itertools
+import os
+import random
+import warnings
+from pathlib import Path
+from typing import Union
 
-import decord
 import json
 import numpy as np
 import torch
+import torch.nn.functional as F
+from diffusers import DiffusionPipeline
+from diffusers.loaders import AttnProcsLayers
+from diffusers.models.attention_processor import CustomDiffusionAttnProcessor
 from PIL import Image
-from timm.data import create_transform
+from PIL.ImageOps import exif_transpose
+from torch.utils.data import Dataset
 from torchvision import transforms
-from torchvision.datasets import ImageFolder
-from torchvision.transforms import Compose, Normalize, Resize, ToTensor
+from tqdm.auto import tqdm
 
-from modelscope.hub.snapshot_download import snapshot_download
-from modelscope.metainfo import Preprocessors
-from modelscope.pipelines.base import Input
-from modelscope.pipelines.cv.cmdssl_video_embedding_pipeline import (
-    VCenterCrop, VCompose, VNormalize, VRescale, VToTensor)
-from modelscope.preprocessors import load_image
-from modelscope.utils.config import Config
-from modelscope.utils.constant import (Fields, Invoke, ModeKeys, ModelFile,
-                                       Tasks)
-from .base import Preprocessor
-from .builder import PREPROCESSORS
-from .ofa import *  # noqa
-from .ofa.utils.collate import collate_fn
-from .ofa.utils.constant import OFA_TASK_KEY_MAPPING
-
-__all__ = [
-    'DiffusionImageGenerationPreprocessor', 'OfaPreprocessor',
-    'MPlugPreprocessor', 'HiTeAPreprocessor', 'MplugOwlPreprocessor'
-]
-
-
-@PREPROCESSORS.register_module(
-    Fields.multi_modal,
-    module_name=Preprocessors.diffusion_image_generation_preprocessor)
-class DiffusionImageGenerationPreprocessor(Preprocessor):
-    """ Preprocessor the data with the combination of image and text.
-        Args:
-            data: process the value as an image for keys ending with 'FILE'
-                or existing in preprocessor_image_keys and pass-through the values of other keys.
+from modelscope.metainfo import Trainers
+from modelscope.msdatasets import MsDataset
+from modelscope.outputs import OutputKeys
+from modelscope.trainers.builder import TRAINERS
+from modelscope.trainers.hooks.checkpoint.checkpoint_hook import CheckpointHook
+from modelscope.trainers.hooks.checkpoint.checkpoint_processor import \
+    CheckpointProcessor
+from modelscope.trainers.optimizer.builder import build_optimizer
+from modelscope.trainers.trainer import EpochBasedTrainer
+from modelscope.utils.config import ConfigDict
+from modelscope.utils.constant import ModeKeys, TrainerStages
+from modelscope.utils.data_utils import to_device
+from modelscope.utils.torch_utils import is_dist
 
-    """
 
-    def __init__(self, *args, **kwargs):
-        super().__init__(*args, **kwargs)
-        self.preprocessor_resolution = kwargs.pop('resolution', 512)
-        self.preprocessor_mean = kwargs.pop('mean', [0.5])
-        self.preprocessor_std = kwargs.pop('std', [0.5])
-        self.preprocessor_image_keys = set(kwargs.pop('image_keys', []))
-        self.transform_input = transforms.Compose([
-            transforms.Resize(
-                self.preprocessor_resolution,
-                interpolation=transforms.InterpolationMode.BILINEAR),
-            transforms.ToTensor(),
-            transforms.Normalize(self.preprocessor_mean,
-                                 self.preprocessor_std),
-        ])
+class CustomCheckpointProcessor(CheckpointProcessor):
 
-    def __call__(self, data) -> Dict[str, Any]:
-        results = {}
-        for key, value in data.items():
-            if key.endswith(':FILE') or key in self.preprocessor_image_keys:
-                image = load_image(value)
-                img = self.transform_input(image)
-                results[key.replace(':FILE', '').lower()] = img
-            else:
-                results[key.lower()] = value if value else ''
-        return results
+    def __init__(self, modifier_token, modifier_token_id):
+        """Checkpoint processor for custom diffusion.
 
+        Args:
+            modifier_token: The token to use as a modifier for the concept.
+            modifier_token_id: The modifier token id for the concept.
+        """
+        self.modifier_token = modifier_token
+        self.modifier_token_id = modifier_token_id
+
+    def save_checkpoints(self,
+                         trainer,
+                         checkpoint_path_prefix,
+                         output_dir,
+                         meta=None):
+        """Save the state dict for custom diffusion model.
+        """
+        trainer.model.unet = trainer.model.unet.to(torch.float32)
+        trainer.model.unet.save_attn_procs(output_dir)
 
-@PREPROCESSORS.register_module(
-    Fields.multi_modal, module_name=Preprocessors.ofa_tasks_preprocessor)
-class OfaPreprocessor(Preprocessor):
-
-    def __init__(self,
-                 model_dir: str,
-                 mode=ModeKeys.INFERENCE,
-                 *args,
-                 **kwargs):
-        """preprocess the data
+        learned_embeds = trainer.model.text_encoder.get_input_embeddings(
+        ).weight
+        if not isinstance(self.modifier_token_id, list):
+            self.modifier_token_id = [self.modifier_token_id]
+        for x, y in zip(self.modifier_token_id, self.modifier_token):
+            learned_embeds_dict = {}
+            learned_embeds_dict[y] = learned_embeds[x]
+            torch.save(learned_embeds_dict, f'{output_dir}/{y}.bin')
+
+
+class CustomDiffusionDataset(Dataset):
+
+    def __init__(
+        self,
+        concepts_list,
+        tokenizer,
+        size=512,
+        mask_size=64,
+        center_crop=False,
+        with_prior_preservation=False,
+        num_class_images=200,
+        hflip=False,
+        aug=True,
+    ):
+        """A dataset to prepare the instance and class images with the prompts for fine-tuning the model.
+        It pre-processes the images and the tokenizes prompts.
 
         Args:
-            model_dir (str): model path
-            mode: preprocessor mode (model mode)
+            concepts_list: contain multiple concepts, instance_prompt, class_prompt, etc.
+            tokenizer: pretrained tokenizer.
+            size: the size of images.
+            mask_size: the mask size of images.
+            center_crop: execute center crop or not.
+            with_prior_preservation: flag to add prior preservation loss.
+            hflip: whether to flip horizontally.
+            aug: perform data augmentation.
+
         """
-        super().__init__(*args, **kwargs)
-        preprocess_mapping = {
-            Tasks.ocr_recognition: OfaOcrRecognitionPreprocessor,
-            Tasks.image_captioning: OfaImageCaptioningPreprocessor,
-            Tasks.visual_grounding: OfaVisualGroundingPreprocessor,
-            Tasks.visual_question_answering:
-            OfaVisualQuestionAnsweringPreprocessor,
-            Tasks.visual_entailment: OfaVisualEntailmentPreprocessor,
-            Tasks.image_classification: OfaImageClassificationPreprocessor,
-            Tasks.text_classification: OfaTextClassificationPreprocessor,
-            Tasks.text_summarization: OfaSummarizationPreprocessor,
-            Tasks.text_to_image_synthesis: OfaTextToImageSynthesisPreprocessor,
-            Tasks.auto_speech_recognition: OfaASRPreprocessor,
-            Tasks.sudoku: OfaSudokuPreprocessor,
-            Tasks.text2sql: OfaTextToSqlPreprocessor
-        }
-        model_dir = model_dir if osp.exists(model_dir) else snapshot_download(
-            model_dir, user_agent={Invoke.KEY: Invoke.PREPROCESSOR})
-        self.cfg = Config.from_file(
-            osp.join(model_dir, ModelFile.CONFIGURATION))
-        self.preprocess = preprocess_mapping[self.cfg.task](
-            cfg=self.cfg, model_dir=model_dir, mode=mode)
-        self.keys = OFA_TASK_KEY_MAPPING[self.cfg.task]
-        self.tokenizer = self.preprocess.tokenizer
-        if kwargs.get('no_collate', None):
-            self.no_collate = True
-        else:
-            self.no_collate = False
+        self.size = size
+        self.mask_size = mask_size
+        self.center_crop = center_crop
+        self.tokenizer = tokenizer
+        self.interpolation = Image.BILINEAR
+        self.aug = aug
+
+        self.instance_images_path = []
+        self.class_images_path = []
+        self.with_prior_preservation = with_prior_preservation
+        for concept in concepts_list:
+            inst_img_path = [
+                (x, concept['instance_prompt'])
+                for x in Path(concept['instance_data_dir']).iterdir()
+                if x.is_file()
+            ]
+            self.instance_images_path.extend(inst_img_path)
+
+            if with_prior_preservation:
+                class_data_root = Path(concept['class_data_dir'])
+                if os.path.isdir(class_data_root):
+                    class_images_path = list(class_data_root.iterdir())
+                    class_prompt = [
+                        concept['class_prompt']
+                        for _ in range(len(class_images_path))
+                    ]
+                else:
+                    with open(class_data_root, 'r') as f:
+                        class_images_path = f.read().splitlines()
+                    with open(concept['class_prompt'], 'r') as f:
+                        class_prompt = f.read().splitlines()
+
+                class_img_path = [
+                    (x, y) for (x, y) in zip(class_images_path, class_prompt)
+                ]
+                self.class_images_path.extend(
+                    class_img_path[:num_class_images])
+
+        random.shuffle(self.instance_images_path)
+        self.num_instance_images = len(self.instance_images_path)
+        self.num_class_images = len(self.class_images_path)
+        self._length = max(self.num_class_images, self.num_instance_images)
+        self.flip = transforms.RandomHorizontalFlip(0.5 * hflip)
 
-    # just for modelscope demo
-    def _build_dict(self, input: Union[Input, List[Input]]) -> Dict[str, Any]:
-        data = dict()
-        if not isinstance(input, tuple) and not isinstance(input, list):
-            input = (input, )
-        for key, item in zip(self.keys, input):
-            data[key] = item
-        return data
-
-    def _ofa_input_compatibility_conversion(self, data):  # fake
-        if 'image' in data and self.cfg.model.get('type', None) == 'ofa':
-            if isinstance(data['image'], str):
-                image = load_image(data['image'])
-            else:
-                image = data['image']
-            if image.mode != 'RGB':
-                image = image.convert('RGB')
-            img_buffer = BytesIO()
-            image.save(img_buffer, format='JPEG')
-            data['image'] = Image.open(img_buffer)
-        return data
-
-    def __call__(self, input: Union[str, tuple, Dict[str, Any]], *args,
-                 **kwargs) -> Dict[str, Any]:
-        if isinstance(input, dict):
-            data = input
-        else:
-            data = self._build_dict(input)
-        sample = self.preprocess(data)
-        str_data = dict()
-        for k, v in data.items():
-            str_data[k] = str(v)
-        sample['sample'] = str_data
-        if self.no_collate:
-            return sample
-        else:
-            return collate_fn([sample],
-                              pad_idx=self.tokenizer.pad_token_id,
-                              eos_idx=self.tokenizer.eos_token_id)
+        self.image_transforms = transforms.Compose([
+            self.flip,
+            transforms.Resize(
+                size, interpolation=transforms.InterpolationMode.BILINEAR),
+            transforms.CenterCrop(size)
+            if center_crop else transforms.RandomCrop(size),
+            transforms.ToTensor(),
+            transforms.Normalize([0.5], [0.5]),
+        ])
+
+    def __len__(self):
+        return self._length
 
+    def preprocess(self, image, scale, resample):
+        outer, inner = self.size, scale
+        factor = self.size // self.mask_size
+        if scale > self.size:
+            outer, inner = scale, self.size
+        top, left = np.random.randint(0, outer - inner + 1), np.random.randint(
+            0, outer - inner + 1)
+        image = image.resize((scale, scale), resample=resample)
+        image = np.array(image).astype(np.uint8)
+        image = (image / 127.5 - 1.0).astype(np.float32)
+        instance_image = np.zeros((self.size, self.size, 3), dtype=np.float32)
+        mask = np.zeros((self.size // factor, self.size // factor))
+        if scale > self.size:
+            instance_image = image[top:top + inner, left:left + inner, :]
+            mask = np.ones((self.size // factor, self.size // factor))
+        else:
+            instance_image[top:top + inner, left:left + inner, :] = image
+            mask[top // factor + 1:(top + scale) // factor - 1,
+                 left // factor + 1:(left + scale) // factor - 1] = 1.0
+        return instance_image, mask
+
+    def __getitem__(self, index):
+        example = {}
+        instance_image, instance_prompt = self.instance_images_path[
+            index % self.num_instance_images]
+        instance_image = Image.open(instance_image)
+        if not instance_image.mode == 'RGB':
+            instance_image = instance_image.convert('RGB')
+        instance_image = self.flip(instance_image)
+
+        # apply resize augmentation and create a valid image region mask
+        random_scale = self.size
+        if self.aug:
+            random_scale = (
+                np.random.randint(self.size // 3, self.size
+                                  + 1) if np.random.uniform() < 0.66 else
+                np.random.randint(int(1.2 * self.size), int(1.4 * self.size)))
+        instance_image, mask = self.preprocess(instance_image, random_scale,
+                                               self.interpolation)
+
+        if random_scale < 0.6 * self.size:
+            instance_prompt = np.random.choice(['a far away ', 'very small '
+                                                ]) + instance_prompt
+        elif random_scale > self.size:
+            instance_prompt = np.random.choice(['zoomed in ', 'close up '
+                                                ]) + instance_prompt
+
+        example['instance_images'] = torch.from_numpy(instance_image).permute(
+            2, 0, 1)
+        example['mask'] = torch.from_numpy(mask)
+        example['instance_prompt_ids'] = self.tokenizer(
+            instance_prompt,
+            truncation=True,
+            padding='max_length',
+            max_length=self.tokenizer.model_max_length,
+            return_tensors='pt',
+        ).input_ids
+
+        if self.with_prior_preservation:
+            class_image, class_prompt = self.class_images_path[
+                index % self.num_class_images]
+            class_image = Image.open(class_image)
+            if not class_image.mode == 'RGB':
+                class_image = class_image.convert('RGB')
+            example['class_images'] = self.image_transforms(class_image)
+            example['class_mask'] = torch.ones_like(example['mask'])
+            example['class_prompt_ids'] = self.tokenizer(
+                class_prompt,
+                truncation=True,
+                padding='max_length',
+                max_length=self.tokenizer.model_max_length,
+                return_tensors='pt',
+            ).input_ids
 
-def _convert_to_rgb(image):
-    return image.convert('RGB')
+        return example
 
 
-@PREPROCESSORS.register_module(
-    Fields.multi_modal, module_name=Preprocessors.clip_preprocessor)
-class CLIPPreprocessor(Preprocessor):
+class PromptDataset(Dataset):
 
-    def __init__(self,
-                 model_dir: str,
-                 mode=ModeKeys.INFERENCE,
-                 *args,
-                 **kwargs):
-        """preprocess the data
+    def __init__(self, prompt, num_samples):
+        """Dataset to prepare the prompts to generate class images.
 
         Args:
-            model_dir (str): model path
-            mode: preprocessor mode (model mode)
-        """
-        super().__init__(*args, **kwargs)
-        model_dir = model_dir if osp.exists(model_dir) else snapshot_download(
-            model_dir, user_agent={Invoke.KEY: Invoke.PREPROCESSOR})
-        self.mode = mode
-        # text tokenizer
-        from modelscope.models.multi_modal.clip.bert_tokenizer import FullTokenizer
-        if 'tokenizer' in kwargs and isinstance(kwargs['tokenizer'],
-                                                FullTokenizer):
-            self.tokenizer = kwargs['tokenizer']
-        else:
-            vocab_file = f'{model_dir}/{ModelFile.VOCAB_FILE}'
-            self.tokenizer = FullTokenizer(vocab_file=vocab_file)
-        # image preprocessor
-        if 'resolution' in kwargs and isinstance(kwargs['resolution'], int):
-            self.image_resolution = kwargs['resolution']
-        else:
-            self.image_resolution = json.load(
-                open(
-                    '{}/vision_model_config.json'.format(model_dir),
-                    encoding='utf-8'))['image_resolution']
-        self.img_preprocess = self._build_image_transform()
-        # key mapping
-        # specify the input keys, compatible with training and inference whose key names may be different
-        self.input_keys = {'img': 'img', 'text': 'text'}
-
-    def _build_image_transform(self):
-
-        if self.mode == ModeKeys.TRAIN:
-            transform = create_transform(
-                input_size=self.image_resolution,
-                scale=(0.9, 1.0),
-                is_training=True,
-                color_jitter=None,
-                auto_augment='original',
-                interpolation='bicubic',
-                mean=(0.48145466, 0.4578275, 0.40821073),
-                std=(0.26862954, 0.26130258, 0.27577711),
-            )
-            transform = Compose(transform.transforms[:-3] + [_convert_to_rgb]
-                                + transform.transforms[-3:])
-        else:
-            transform = Compose([
-                Resize((self.image_resolution, self.image_resolution),
-                       interpolation=Image.BICUBIC),
-                _convert_to_rgb,
-                ToTensor(),
-                Normalize((0.48145466, 0.4578275, 0.40821073),
-                          (0.26862954, 0.26130258, 0.27577711)),
-            ])
-        return transform
-
-    def tokenize(self,
-                 texts: Union[str, List[str]],
-                 context_length: int = 52) -> torch.LongTensor:
-        """
-        Returns the tokenized representation of given input string(s)
-        Parameters
-        ----------
-        texts : Union[str, List[str]]
-            An input string or a list of input strings to tokenize
-        context_length : int
-            The context length to use; all baseline models use 24 as the context length
-        Returns
-        -------
-        A two-dimensional tensor containing the resulting tokens, shape = [number of input strings, context_length]
-        """
-        if isinstance(texts, str):
-            texts = [texts]
+            prompt: Class prompt.
+            num_samples: The number sample for class images.
 
-        all_tokens = []
-        for text in texts:
-            all_tokens.append(
-                [self.tokenizer.vocab['[CLS]']]
-                + self.tokenizer.convert_tokens_to_ids(
-                    self.tokenizer.tokenize(text))[:context_length - 2]
-                + [self.tokenizer.vocab['[SEP]']])
-
-        result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)
-
-        for i, tokens in enumerate(all_tokens):
-            assert len(tokens) <= context_length
-            result[i, :len(tokens)] = torch.tensor(tokens)
-
-        return result
-
-    def set_input_img_key(self, new_key: str):
-        self.input_keys['img'] = new_key
-
-    def set_input_text_key(self, new_key: str):
-        self.input_keys['text'] = new_key
-
-    def __call__(self, input: Union[str, tuple, Dict[str, Any]], *args,
-                 **kwargs) -> Dict[str, Any]:
-        output = {}
-        # preprocess the image input
-        input_img_key = self.input_keys['img']
-        if input_img_key in input and input[input_img_key] is not None:
-            image_input = input[input_img_key]
-
-            # single image input
-            if isinstance(image_input, Image.Image):
-                image_tensor = self.img_preprocess(image_input).unsqueeze(0)
-            # multi images input
-            elif isinstance(image_input, list):
-                if all([isinstance(elem, Image.Image)
-                        for elem in image_input]):
-                    image_tensor = torch.stack(
-                        [self.img_preprocess(elem)
-                         for elem in image_input],  # noqa
-                        dim=0)  # noqa
-                else:
-                    unsupported_elem_type = [
-                        type(elem) for elem in image_input
-                        if not isinstance(elem, Image.Image)
-                    ][0]
-                    raise TypeError(
-                        f'img should be PIL.Image or List[PIL.Image], \
-                            but got a List containing one {unsupported_elem_type}'
-                    )
-            # others
-            else:
-                raise TypeError(
-                    f'img should be PIL.Image or List[PIL.Image], but got {type(image_input)}'
-                )
-            output['img'] = image_tensor
+        """
+        self.prompt = prompt
+        self.num_samples = num_samples
 
-        # preprocess the text input
-        input_text_key = self.input_keys['text']
-        if input_text_key in input and input[input_text_key] is not None:
-            text_input = input[input_text_key]
-
-            # single text input
-            if isinstance(text_input, str):
-                text_tensor = self.tokenize(text_input)
-            # multi texts input
-            elif isinstance(text_input, list):
-                if all([isinstance(elem, str) for elem in text_input]):
-                    text_tensor = self.tokenize(text_input)
-                else:
-                    unsupported_elem_type = [
-                        type(elem) for elem in text_input
-                        if not isinstance(elem, str)
-                    ][0]
-                    raise TypeError(
-                        f'text should be str or List[str], but got a List containing one {unsupported_elem_type}'
-                    )
-            # others
-            else:
-                raise TypeError(
-                    f'text should be str or List[str], but got {type(text_input)}'
-                )
-            output['text'] = text_tensor
+    def __len__(self):
+        return self.num_samples
 
-        return output
+    def __getitem__(self, index):
+        example = {}
+        example['prompt'] = self.prompt
+        example['index'] = index
+        return example
 
 
-@PREPROCESSORS.register_module(
-    Fields.multi_modal, module_name=Preprocessors.mplug_tasks_preprocessor)
-class MPlugPreprocessor(Preprocessor):
+@TRAINERS.register_module(module_name=Trainers.custom_diffusion)
+class CustomDiffusionTrainer(EpochBasedTrainer):
 
-    def __init__(self,
-                 model_dir: str,
-                 mode: str = ModeKeys.INFERENCE,
-                 tokenizer_max_length: int = 25,
-                 *args,
-                 **kwargs):
+    def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
-        self.model_dir = model_dir
-        self.mode = mode
-        self.tokenizer_max_length = tokenizer_max_length
-
-        self._tokenizer = None
-        self._patch_resize_transform = None
-        self._image_map = {}
-
-    @property
-    def tokenizer(self):
-        from transformers import BertTokenizer
-
-        if self._tokenizer is None:
-            self._tokenizer = BertTokenizer.from_pretrained(self.model_dir)
-        return self._tokenizer
-
-    @property
-    def patch_resize_transform(self):
-        if self._patch_resize_transform is None:
-            from torchvision import transforms
-            from modelscope.models.multi_modal.mplug import CONFIG_NAME, MPlugConfig
-
-            config = MPlugConfig.from_yaml_file(
-                osp.join(self.model_dir, CONFIG_NAME))
-
-            mean = (0.48145466, 0.4578275, 0.40821073)
-            std = (0.26862954, 0.26130258, 0.27577711)
-
-            self._patch_resize_transform = transforms.Compose([
-                transforms.Resize((config.image_res, config.image_res),
-                                  interpolation=Image.BICUBIC),
-                transforms.ToTensor(),
-                transforms.Normalize(mean=mean, std=std),
-            ])
-        return self._patch_resize_transform
-
-    def image_open(self, path: str) -> Tuple[Image.Image, int]:
-        if path not in self._image_map:
-            index = len(self._image_map)
-            self._image_map[path] = (load_image(path), index)
-        return self._image_map[path]
-
-    def __call__(
-            self, data: Union[Image.Image, tuple,
-                              Dict[str, Any]]) -> Dict[str, Any]:
-        self.cfg = Config.from_file(
-            osp.join(self.model_dir, ModelFile.CONFIGURATION))
-
-        if isinstance(data, (Image.Image, str)):
-            image = data
-        elif isinstance(data, tuple):
-            image = data[0]
-        else:
-            image = data['image']
-        index = 0
-        if isinstance(image, str):
-            image, index = self.image_open(image)
-        image = image.convert('RGB')
-        image = self.patch_resize_transform(image)
-        question = '' if self.cfg.task == Tasks.image_captioning \
-            else data[1 if isinstance(data, tuple)
-                      else ('text' if 'text' in data else 'question')]
-        question = self.tokenizer(
-            question.lower(),
-            padding='max_length',
-            truncation=True,
-            max_length=self.tokenizer_max_length,
-            return_tensors='pt')
-
-        if self.mode == ModeKeys.INFERENCE:
-            image = torch.stack([image], dim=0)
-            return {'image': image, 'question': question}
-        else:
-            answer = data['answer']
-            answer = self.tokenizer(
-                answer,
-                padding='max_length',
-                truncation=True,
-                max_length=self.tokenizer_max_length,
-                return_tensors='pt')
-            output = {
-                'image': image,
-                'question_input_ids': question.input_ids.squeeze(),
-                'question_attention_mask': question.attention_mask.squeeze(),
-                'answer_input_ids': answer.input_ids.squeeze(),
-                'answer_attention_mask': answer.attention_mask.squeeze(),
-            }
-            if self.cfg.task == Tasks.image_text_retrieval:
-                output['index'] = index
-            return output
-
-
-@PREPROCESSORS.register_module(
-    Fields.multi_modal, module_name=Preprocessors.vldoc_preprocessor)
-class VLDocPreprocessor(Preprocessor):
-
-    def __init__(self,
-                 model_dir: str,
-                 mode: str = ModeKeys.INFERENCE,
-                 *args,
-                 **kwargs):
-        """Preprocess data for the model `VLDocForDocVLEmbedding`.
+        """Custom diffusion trainers for fine-tuning stable diffusion.
 
         Args:
-            model_dir (str): model path in model hub.
-            mode (str): model mode, in ('train', 'eval', 'inference').
+            with_prior_preservation: a boolean indicating whether to enable prior loss.
+            instance_prompt: a string specifying the instance prompt.
+            class_prompt: a string specifying the class prompt.
+            class_data_dir: the path to the class data directory.
+            num_class_images: the number of class images to generate.
+            prior_loss_weight: the weight of the prior loss.
+            modifier_token: A token to use as a modifier for the concept.
+            initializer_token: A token to use as initializer word.
+            freeze_model: crossattn to enable fine-tuning of all params in the cross attention.
+            sample_batch_size: Batch size (per device) for sampling images.
+            train_batch_size: Batch size (per device) for the training dataloader.
+            center_crop: execute center crop or not.
+            concepts_list: Path to json containing multiple concepts, will overwrite parameters.
+            instance_data_name: The instance data local dir or online ID.
+
         """
-        super().__init__(*args, **kwargs)
+        self.with_prior_preservation = kwargs.pop('with_prior_preservation',
+                                                  True)
+        instance_prompt = kwargs.pop('instance_prompt', 'a photo of sks dog')
+        class_prompt = kwargs.pop('class_prompt', 'dog')
+        class_data_dir = kwargs.pop('class_data_dir', '/tmp/class_data')
+        self.real_prior = kwargs.pop('real_prior', False)
+        self.num_class_images = kwargs.pop('num_class_images', 200)
+        self.resolution = kwargs.pop('resolution', 512)
+        self.prior_loss_weight = kwargs.pop('prior_loss_weight', 1.0)
+        self.modifier_token = kwargs.pop('modifier_token', '<new1>')
+        self.initializer_token = kwargs.pop('initializer_token', 'ktn+pll+ucd')
+        self.freeze_model = kwargs.pop('freeze_model', 'crossattn_kv')
+        self.sample_batch_size = kwargs.pop('sample_batch_size', 4)
+        self.train_batch_size = kwargs.pop('train_batch_size', 2)
+        self.center_crop = kwargs.pop('center_crop', False)
+        self.concepts_list = kwargs.pop('concepts_list', None)
+        instance_data_name = kwargs.pop(
+            'instance_data_name', 'buptwq/lora-stable-diffusion-finetune-dog')
+
+        # Extract downloaded image folder
+        if self.concepts_list is None:
+            if os.path.isdir(instance_data_name):
+                instance_data_dir = instance_data_name
+            else:
+                ds = MsDataset.load(instance_data_name, split='train')
+                instance_data_dir = os.path.dirname(
+                    next(iter(ds))['Target:FILE'])
+
+        # construct concept list
+        if self.concepts_list is None:
+            self.concepts_list = [{
+                'instance_prompt': instance_prompt,
+                'class_prompt': class_prompt,
+                'instance_data_dir': instance_data_dir,
+                'class_data_dir': class_data_dir,
+            }]
+        else:
+            with open(self.concepts_list, 'r') as f:
+                self.concepts_list = json.load(f)
+
+        for concept in self.concepts_list:
+            if not os.path.exists(concept['class_data_dir']):
+                os.makedirs(concept['class_data_dir'])
+            if not os.path.exists(concept['instance_data_dir']):
+                raise Exception(
+                    f"instance dataset {concept['instance_data_dir']} does not exist."
+                )
 
-        self.model_dir = model_dir
-        self.mode = mode
+        # Adding a modifier token which is optimized
+        self.modifier_token_id = []
+        initializer_token_id = []
+        if self.modifier_token is not None:
+            self.modifier_token = self.modifier_token.split('+')
+            self.initializer_token = self.initializer_token.split('+')
+            if len(self.modifier_token) > len(self.initializer_token):
+                raise ValueError(
+                    'You must specify + separated initializer token for each modifier token.'
+                )
+            for modifier_token, initializer_token in zip(
+                    self.modifier_token,
+                    self.initializer_token[:len(self.modifier_token)]):
+                # Add the placeholder token in tokenizer
+                num_added_tokens = self.model.tokenizer.add_tokens(
+                    modifier_token)
+                if num_added_tokens == 0:
+                    raise ValueError(
+                        f'The tokenizer already contains the token {modifier_token}. Please pass a different'
+                        ' `modifier_token` that is not already in the tokenizer.'
+                    )
 
-        model_cfg_path = osp.join(model_dir, 'config.json')
-        with open(model_cfg_path, 'r', encoding='utf-8') as f:
-            model_cfg = json.load(f)
-
-        from modelscope.models.multi_modal.vldoc.tokenization import VLDocXLMTokenizer
-        tokenizer_path = osp.join(model_dir, ModelFile.TOKENIZER_FOLDER)
-        self.tokenizer = VLDocXLMTokenizer.from_pretrained(tokenizer_path)
-
-        from modelscope.models.multi_modal.vldoc.processing import Processor, ImageProcessor
-        self.img_proc = ImageProcessor(
-            do_preprocess=True,
-            do_resize=True,
-            image_size={
-                'height': model_cfg['image_size'][0],
-                'width': model_cfg['image_size'][1],
-            },
-            do_normalize=True,
-            apply_ocr=False)
-        self.proc = Processor(
-            max_seq_length=model_cfg['max_seq_length'],
-            max_block_num=model_cfg['max_block_num'],
-            img_processor=self.img_proc,
-            tokenizer=self.tokenizer,
-            width=model_cfg['image_size'][1],
-            height=model_cfg['image_size'][0],
+                # Convert the initializer_token, placeholder_token to ids
+                token_ids = self.model.tokenizer.encode(
+                    [initializer_token], add_special_tokens=False)
+                # Check if initializer_token is a single token or a sequence of tokens
+                if len(token_ids) > 1:
+                    raise ValueError(
+                        'The initializer token must be a single token.')
+
+                initializer_token_id.append(token_ids[0])
+                self.modifier_token_id.append(
+                    self.model.tokenizer.convert_tokens_to_ids(modifier_token))
+
+        # Resize the token embeddings as we are adding new special tokens to the tokenizer
+        self.model.text_encoder.resize_token_embeddings(
+            len(self.model.tokenizer))
+
+        # Resize the token embeddings as we are adding new special tokens to the tokenizer
+        self.model.text_encoder.resize_token_embeddings(
+            len(self.model.tokenizer))
+
+        # Initialise the newly added placeholder token with the embeddings of the initializer token
+        token_embeds = self.model.text_encoder.get_input_embeddings(
+        ).weight.data
+        for x, y in zip(self.modifier_token_id, initializer_token_id):
+            token_embeds[x] = token_embeds[y]
+
+        # Freeze all parameters except for the token embeddings in text encoder
+        params_to_freeze = itertools.chain(
+            self.model.text_encoder.text_model.encoder.parameters(),
+            self.model.text_encoder.text_model.final_layer_norm.parameters(),
+            self.model.text_encoder.text_model.embeddings.position_embedding.
+            parameters(),
         )
+        self.freeze_params(params_to_freeze)
 
-    def __call__(self, input: Dict[str, Any], *args,
-                 **kwargs) -> Dict[str, Any]:
-        """
-        Args:
-            input: {
-                'images': ['img_path1', 'img_path2', ...],
-                'ocr_info_paths': ['json_path1', 'json_path2', ...]
-            }
-        Return:
-            encodings: Dict[str, Tensor]
-        """
-
-        ocr_infos = []
-        for one_ocr_info_path in input['ocr_info_paths']:
-            with open(one_ocr_info_path, 'r') as f:
-                ocr_info = json.load(f)
-                ocr_info = ocr_info['form']
-                ocr_infos.append(ocr_info)
-
-        proc_input = {'images': input['images'], 'ocr_infos': ocr_infos}
-        encodings = self.proc(**proc_input)
-
-        return encodings
-
-
-@PREPROCESSORS.register_module(
-    Fields.multi_modal, module_name=Preprocessors.hitea_tasks_preprocessor)
-class HiTeAPreprocessor(Preprocessor):
-
-    def __init__(self,
-                 model_dir: str,
-                 mode: str = ModeKeys.INFERENCE,
-                 tokenizer_max_length: int = 25,
-                 *args,
-                 **kwargs):
-        super().__init__(*args, **kwargs)
-        self.model_dir = model_dir
-        self.mode = mode
-        self.tokenizer_max_length = tokenizer_max_length
-
-        self._tokenizer = None
-        self._patch_resize_transform = None
-        self._num_frames = None
-        self._video_map = {}
-
-    @property
-    def tokenizer(self):
-        from transformers import BertTokenizer
-
-        if self._tokenizer is None:
-            self._tokenizer = BertTokenizer.from_pretrained(self.model_dir)
-        return self._tokenizer
-
-    @property
-    def patch_resize_transform(self):
-        if self._patch_resize_transform is None:
-            from torchvision import transforms
-            from modelscope.models.multi_modal.mplug import CONFIG_NAME, HiTeAConfig
-
-            config = HiTeAConfig.from_yaml_file(
-                osp.join(self.model_dir, CONFIG_NAME))
-
-            mean = (0.48145466, 0.4578275, 0.40821073)
-            std = (0.26862954, 0.26130258, 0.27577711)
-
-            self._patch_resize_transform = transforms.Compose([
-                transforms.Resize((config.image_res, config.image_res),
-                                  interpolation=Image.BICUBIC),
-                transforms.ToTensor(),
-                transforms.Normalize(mean=mean, std=std),
-            ])
-        return self._patch_resize_transform
-
-    @property
-    def num_frames(self):
-        if self._num_frames is None:
-            from torchvision import transforms
-            from modelscope.models.multi_modal.mplug import CONFIG_NAME, HiTeAConfig
-
-            config = HiTeAConfig.from_yaml_file(
-                osp.join(self.model_dir, CONFIG_NAME))
-
-            self._num_frames = config.num_frames
-        return self._num_frames
-
-    def video_open(self, path: str) -> Tuple[decord.VideoReader, int]:
-        if path not in self._video_map:
-            index = len(self._video_map)
-            vr = decord.VideoReader(path, ctx=decord.cpu(0))
-            self._video_map[path] = (vr, index)
-        return self._video_map[path]
-
-    def sample_frames(self, num_frames: int, vlen: int) -> List[int]:
-        acc_samples = min(num_frames, vlen)
-        # split the video into `acc_samples` intervals, and sample from each interval.
-        intervals = np.linspace(
-            start=0, stop=vlen, num=acc_samples + 1).astype(int)
-        ranges = []
-        for idx, interv in enumerate(intervals[:-1]):
-            ranges.append((interv, intervals[idx + 1] - 1))
-
-        frame_indices = [(x[0] + x[1]) // 2 for x in ranges]
-
-        if len(frame_indices) < num_frames:  # padded with last frame
-            padded_frame_indices = [frame_indices[-1]] * num_frames
-            padded_frame_indices[:len(frame_indices)] = frame_indices
-            frame_indices = padded_frame_indices
-        return frame_indices
-
-    def __call__(
-        self, data: Union[decord.VideoReader, tuple,
-                          Dict[str, Any]]) -> Dict[str, Any]:
-        self.cfg = Config.from_file(
-            osp.join(self.model_dir, ModelFile.CONFIGURATION))
-
-        if isinstance(data, (decord.VideoReader, str)):
-            video = data
-        elif isinstance(data, tuple):
-            video = data[0]
-        else:
-            video = data['video']
-        index = 0
-        if isinstance(video, str):
-            video, index = self.video_open(video)
-        frame_indices = self.sample_frames(self.num_frames, len(video))
-        video.seek(0)
-        video = torch.from_numpy(video.get_batch(frame_indices).asnumpy())
-        video = [
-            self.patch_resize_transform(Image.fromarray(f))
-            for f in video.numpy()
-        ]
-        video = torch.stack(video, dim=0)
-        question = '' if self.cfg.task == Tasks.video_captioning \
-            else data[1 if isinstance(data, tuple)
-                      else ('text' if 'text' in data else 'question')]
-        question = self.tokenizer(
-            question.lower(),
-            padding='max_length',
-            truncation=True,
-            max_length=self.tokenizer_max_length,
-            return_tensors='pt')
-
-        if self.mode == ModeKeys.INFERENCE:
-            video = torch.stack([video], dim=0)
-            return {'video': video, 'question': question}
-        else:
-            answer = data['answer']
-            answer = self.tokenizer(
-                answer,
-                padding='max_length',
-                truncation=True,
-                max_length=self.tokenizer_max_length,
-                return_tensors='pt')
-            output = {
-                'video': video,
-                'question_input_ids': question.input_ids.squeeze(),
-                'question_attention_mask': question.attention_mask.squeeze(),
-                'answer_input_ids': answer.input_ids.squeeze(),
-                'answer_attention_mask': answer.attention_mask.squeeze(),
+        # Save checkpoint and configurate files
+        ckpt_hook = list(
+            filter(lambda hook: isinstance(hook, CheckpointHook),
+                   self.hooks))[0]
+        ckpt_hook.set_processor(
+            CustomCheckpointProcessor(self.modifier_token,
+                                      self.modifier_token_id))
+
+        # Add new Custom Diffusion weights to the attention layers
+        attention_class = CustomDiffusionAttnProcessor
+        # Only train key, value projection layers if freeze_model = 'crossattn_kv' else train all params.
+        train_q_out = False if self.freeze_model == 'crossattn_kv' else True
+        custom_diffusion_attn_procs = {}
+
+        st = self.model.unet.state_dict()
+        for name, _ in self.model.unet.attn_processors.items():
+            cross_attention_dim = None if name.endswith(
+                'attn1.processor'
+            ) else self.model.unet.config.cross_attention_dim
+            if name.startswith('mid_block'):
+                hidden_size = self.model.unet.config.block_out_channels[-1]
+            elif name.startswith('up_blocks'):
+                block_id = int(name[len('up_blocks.')])
+                hidden_size = list(
+                    reversed(
+                        self.model.unet.config.block_out_channels))[block_id]
+            elif name.startswith('down_blocks'):
+                block_id = int(name[len('down_blocks.')])
+                hidden_size = self.model.unet.config.block_out_channels[
+                    block_id]
+            layer_name = name.split('.processor')[0]
+            weights = {
+                'to_k_custom_diffusion.weight':
+                st[layer_name + '.to_k.weight'],
+                'to_v_custom_diffusion.weight':
+                st[layer_name + '.to_v.weight'],
             }
-            return output
+            if train_q_out:
+                weights['to_q_custom_diffusion.weight'] = st[layer_name
+                                                             + '.to_q.weight']
+                weights['to_out_custom_diffusion.0.weight'] = st[
+                    layer_name + '.to_out.0.weight']
+                weights['to_out_custom_diffusion.0.bias'] = st[
+                    layer_name + '.to_out.0.bias']
+            if cross_attention_dim is not None:
+                custom_diffusion_attn_procs[name] = attention_class(
+                    train_kv=True,
+                    train_q_out=train_q_out,
+                    hidden_size=hidden_size,
+                    cross_attention_dim=cross_attention_dim,
+                ).to(self.model.unet.device)
+                custom_diffusion_attn_procs[name].load_state_dict(weights)
+            else:
+                custom_diffusion_attn_procs[name] = attention_class(
+                    train_kv=False,
+                    train_q_out=False,
+                    hidden_size=hidden_size,
+                    cross_attention_dim=cross_attention_dim,
+                )
+        del st
+        self.model.unet.set_attn_processor(custom_diffusion_attn_procs)
+        self.custom_diffusion_layers = AttnProcsLayers(
+            self.model.unet.attn_processors)
+
+        # Check for conflicts and conflicts
+        if self.with_prior_preservation:
+            for concept in self.concepts_list:
+                if concept['class_data_dir'] is None:
+                    raise ValueError(
+                        'You must specify a data directory for class images.')
+                if concept['class_prompt'] is None:
+                    raise ValueError(
+                        'You must specify prompt for class images.')
+        else:
+            for concept in self.concepts_list:
+                if concept['class_data_dir'] is not None:
+                    warnings.warn(
+                        'You need not use --class_data_dir without --with_prior_preservation.'
+                    )
+                if concept['class_prompt'] is not None:
+                    warnings.warn(
+                        'You need not use --class_prompt without --with_prior_preservation.'
+                    )
 
+        # Generate class images if prior preservation is enabled.
+        if self.with_prior_preservation:
+            self.generate_image()
+
+        # Dataset and DataLoaders creation:
+        train_dataset = CustomDiffusionDataset(
+            concepts_list=self.concepts_list,
+            tokenizer=self.model.tokenizer,
+            with_prior_preservation=self.with_prior_preservation,
+            size=self.resolution,
+            mask_size=self.model.vae.encode(
+                torch.randn(1, 3, self.resolution,
+                            self.resolution).to(dtype=torch.float32).to(
+                                self.device)).latent_dist.sample().size()[-1],
+            center_crop=self.center_crop,
+            num_class_images=self.num_class_images,
+            hflip=False,
+            aug=True,
+        )
+        train_dataloader = torch.utils.data.DataLoader(
+            train_dataset,
+            batch_size=self.train_batch_size,
+            shuffle=True,
+            collate_fn=lambda examples: self.collate_fn(examples),
+            num_workers=2,
+        )
+        self.iter_train_dataloader = itertools.cycle(train_dataloader)
 
-@PREPROCESSORS.register_module(
-    Fields.multi_modal, module_name=Preprocessors.mplug_owl_preprocessor)
-class MplugOwlPreprocessor(Preprocessor):
-
-    def __init__(self,
-                 model_dir: str,
-                 mode: str = ModeKeys.INFERENCE,
-                 *args,
-                 **kwargs):
-        super().__init__(*args, **kwargs)
-        self.model_dir = model_dir
-        self.mode = mode
+    def freeze_params(self, params):
+        for param in params:
+            param.requires_grad = False
+
+    def collate_fn(self, examples):
+        input_ids = [example['instance_prompt_ids'] for example in examples]
+        pixel_values = [example['instance_images'] for example in examples]
+        mask = [example['mask'] for example in examples]
+        # Concat class and instance examples which avoid doing two forward passes.
+        if self.with_prior_preservation:
+            input_ids += [example['class_prompt_ids'] for example in examples]
+            pixel_values += [example['class_images'] for example in examples]
+            mask += [example['class_mask'] for example in examples]
+
+        input_ids = torch.cat(input_ids, dim=0)
+        pixel_values = torch.stack(pixel_values)
+        mask = torch.stack(mask)
+        pixel_values = pixel_values.to(
+            memory_format=torch.contiguous_format).float()
+        mask = mask.to(memory_format=torch.contiguous_format).float()
 
-        self._tokenizer = None
-        self._patch_resize_transform = None
-        self.media_token = {'<|image|>': 65}
-        self._image_map = {}
-
-    @property
-    def tokenizer(self):
-        from modelscope.models.nlp.llama import LlamaTokenizer
-
-        if self._tokenizer is None:
-            self._tokenizer = LlamaTokenizer.from_pretrained(self.model_dir)
-        return self._tokenizer
-
-    @property
-    def patch_resize_transform(self):
-        if self._patch_resize_transform is None:
-            from torchvision import transforms
-
-            mean = (0.48145466, 0.4578275, 0.40821073)
-            std = (0.26862954, 0.26130258, 0.27577711)
-
-            self._patch_resize_transform = transforms.Compose([
-                transforms.Resize((224, 224), interpolation=Image.BICUBIC),
-                transforms.ToTensor(),
-                transforms.Normalize(mean=mean, std=std),
-            ])
-        return self._patch_resize_transform
-
-    def image_open(self, path: str) -> Tuple[Image.Image, int]:
-        if path not in self._image_map:
-            index = len(self._image_map)
-            self._image_map[path] = (load_image(path), index)
-        return self._image_map[path]
-
-    def tokenize_text(self, text: str) -> List[int]:
-        media_tokens = {
-            k: -int(i + 1)
-            for i, k in enumerate(self.media_token.keys())
+        batch = {
+            'input_ids': input_ids,
+            'pixel_values': pixel_values,
+            'mask': mask.unsqueeze(1)
         }
-        media_lengths = self.media_token.copy()
+        return batch
 
-        prompt_chunk = [self.tokenizer.bos_token_id]
+    def generate_image(self):
+        """ Generate class images if prior preservation is enabled.
+        """
+        for i, concept in enumerate(self.concepts_list):
+            class_images_dir = Path(concept['class_data_dir'])
+            if not class_images_dir.exists():
+                class_images_dir.mkdir(parents=True, exist_ok=True)
+
+            cur_class_images = len(list(class_images_dir.iterdir()))
+
+            if cur_class_images < self.num_class_images:
+                pipeline = DiffusionPipeline.from_pretrained(
+                    self.model_dir,
+                    torch_dtype=torch.float32,
+                    safety_checker=None,
+                    revision=None,
+                )
+                pipeline.set_progress_bar_config(disable=True)
 
-        # Pure Text
-        condition = [
-            media_token not in text for media_token in media_tokens.keys()
-        ]
-        if all(condition):
-            enc_chunk = prompt_chunk + \
-                self.tokenizer(text, add_special_tokens=False)['input_ids']
+                num_new_images = self.num_class_images - cur_class_images
 
-        # Multi-Modal Text
-        else:
-            enc_chunk = prompt_chunk
-            pattern = '|'.join(map(re.escape, list(media_tokens.keys())))
-            chunk_strs = re.split(f'({pattern})', text)
-            chunk_strs = [x for x in chunk_strs if len(x) > 0]
-            for idx, chunk_str in enumerate(chunk_strs):
-                if chunk_str in media_tokens:
-                    enc_chunk += [media_tokens[chunk_str]] * \
-                        media_lengths[chunk_str]
-                else:
-                    tmp_chunk = self.tokenizer(
-                        chunk_str, add_special_tokens=False)['input_ids']
-                    enc_chunk += tmp_chunk
-        return enc_chunk
-
-    def convert(self, messages: Dict[str, List[Dict]]) -> str:
-        texts = []
-        image = []
-        messages = messages['messages']
-        for turn in messages:
-            if turn['role'] == 'system':
-                role = ''
-            elif turn['role'] == 'user':
-                role = 'Human: '
-            else:
-                role = 'AI: '
-            if isinstance(turn['content'], str):
-                text = f"{role}{turn['content']}"
-                texts.append(text)
-            else:
-                for t in turn['content']:
-                    if isinstance(t, str):
-                        text = f'{role}{t}'
-                    else:
-                        text = f'{role}<|image|>'
-                        image.append(t['image'])
-                    texts.append(text)
-        texts = '\n'.join(texts)
-        texts += '\nAI: '
-        return image, texts
+                sample_dataset = PromptDataset(concept['class_prompt'],
+                                               num_new_images)
+                sample_dataloader = torch.utils.data.DataLoader(
+                    sample_dataset, batch_size=self.sample_batch_size)
+
+                pipeline.to(self.device)
+
+                for example in tqdm(
+                        sample_dataloader,
+                        desc='Generating class images',
+                        # disable=not accelerator.is_local_main_process,
+                ):
+                    images = pipeline(example['prompt']).images
+
+                    for i, image in enumerate(images):
+                        hash_image = hashlib.sha1(image.tobytes()).hexdigest()
+                        save_index = example['index'][i] + cur_class_images
+                        image_filename = class_images_dir / f'{save_index}-{hash_image}.jpg'
+                        image.save(image_filename)
+
+                del pipeline
+                if torch.cuda.is_available():
+                    torch.cuda.empty_cache()
+
+    def build_optimizer(self, cfg: ConfigDict, default_args: dict = None):
+        try:
+            return build_optimizer(
+                itertools.chain(
+                    self.model.text_encoder.get_input_embeddings().parameters(
+                    ), self.custom_diffusion_layers.parameters()),
+                cfg=cfg,
+                default_args=default_args)
+        except KeyError as e:
+            self.logger.error(
+                f'Build optimizer error, the optimizer {cfg} is a torch native component, '
+                f'please check if your torch with version: {torch.__version__} matches the config.'
+            )
+            raise e
 
-    def __call__(self, messages: Dict[str, Any],
-                 **forward_params) -> Dict[str, Any]:
+    def train_loop(self, data_loader):
+        """ Training loop used by `EpochBasedTrainer.train()`
         """
-        Args:
-            messages: {[
-                {'role': 'system', 'content': 'message1'},
-                {'role': 'user', 'content': 'message2'},
-                {'role': 'user', 'content': ['message2', {"image": 'image_path'}, 'message3', ...]},
-            ]}
-            The 'role' should be choose from ['system', 'user', 'assistant'].
-            The 'content' can be either str or List[Union[str, Dict]]
-        Return:
-            output: Dict[str, Tensor]
-        """
-        output = {}
-        images, text = self.convert(messages)
+        self.invoke_hook(TrainerStages.before_run)
+        self.model.train()
+        for _ in range(self._epoch, self._max_epochs):
+            self.invoke_hook(TrainerStages.before_train_epoch)
+            for i, data_batch in enumerate(data_loader):
+                if i < self.inner_iter:
+                    # inner_iter may be read out from the checkpoint file, so skip the trained iters in the epoch.
+                    continue
+                data_batch = to_device(data_batch, self.device)
+                self.data_batch = data_batch
+                self._inner_iter = i
+                self.invoke_hook(TrainerStages.before_train_iter)
+                self.train_step(self.model, data_batch)
+                self.invoke_hook(TrainerStages.after_train_iter)
+                # Zero out the gradients for all token embeddings except the newly added
+                # embeddings for the concept to optimize the concept embeddings.
+                if self.modifier_token is not None:
+                    grads_text_encoder = self.model.text_encoder.get_input_embeddings(
+                    ).weight.grad
+                    # Get the index for tokens that we want to zero the grads.
+                    index_grads_to_zero = torch.arange(
+                        len(self.model.tokenizer)) != self.modifier_token_id[0]
+                    for i in range(len(self.modifier_token_id[1:])):
+                        modifier_flag = torch.arange(
+                            len(self.model.tokenizer)
+                        ) != self.modifier_token_id[i]
+                        index_grads_to_zero = index_grads_to_zero & modifier_flag
+                    grads_data = grads_text_encoder.data[
+                        index_grads_to_zero, :].fill_(0)
+                    grads_text_encoder.data[
+                        index_grads_to_zero, :] = grads_data
+                # Value changed after the hooks are invoked, do not move them above the invoke_hook code.
+                del self.data_batch
+                self._iter += 1
+                self._mode = ModeKeys.TRAIN
+
+                if i + 1 >= self.iters_per_epoch:
+                    break
+
+            self.invoke_hook(TrainerStages.after_train_epoch)
+            # Value changed after the hooks are invoked, do not move them above the invoke_hook code.
+            self._inner_iter = 0
+            self._epoch += 1
+            if self._stop_training:
+                break
 
-        if len(images) > 0:
-            pixel_values = []
-            for image in images:
-                pixel_values.append(
-                    self.patch_resize_transform(self.image_open(image)[0]))
-                pixel_values = torch.stack(pixel_values, dim=0)
-        else:
-            pixel_values = None
+        self.invoke_hook(TrainerStages.after_run)
 
-        input_ids = self.tokenize_text(text)
-        input_ids = torch.LongTensor([input_ids])
+    def train_step(self, model, inputs):
+        """ Perform a training step on a batch of inputs.
 
-        output = {
-            'pixel_values': pixel_values,
-            'input_ids': input_ids,
-            **forward_params
-        }
+        Subclass and override to inject custom behavior.
 
-        return output
+        Args:
+            model (`TorchModel`): The model to train.
+            inputs (`Dict[str, Union[torch.Tensor, Any]]`):
+                The inputs and targets of the model.
 
+                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the
+                argument `labels`. Check your model's documentation for all accepted arguments.
 
-@PREPROCESSORS.register_module(
-    Fields.multi_modal,
-    module_name=Preprocessors.image_captioning_clip_interrogator_preprocessor)
-class ImageCaptioningClipInterrogatorPreprocessor(Preprocessor):
+        Return:
+            `torch.Tensor`: The tensor with training loss on this batch.
+        """
+        self.model.unet.train()
+        if self.modifier_token is not None:
+            self.model.text_encoder.train()
+        self._mode = ModeKeys.TRAIN
+
+        batch = next(self.iter_train_dataloader)
+        # Convert images to latent space
+        latents = self.model.vae.encode(batch['pixel_values'].to(
+            dtype=torch.float32).to(self.device)).latent_dist.sample()
+        latents = latents * self.model.vae.config.scaling_factor
+
+        # Sample noise that we'll add to the latents
+        noise = torch.randn_like(latents)
+        bsz = latents.shape[0]
+        # Sample a random timestep for each image
+        timesteps = torch.randint(
+            0,
+            self.model.noise_scheduler.config.num_train_timesteps, (bsz, ),
+            device=latents.device)
+        timesteps = timesteps.long()
+
+        # Add noise to the latents according to the noise magnitude at each timestep
+        # (this is the forward diffusion process)
+        noisy_latents = self.model.noise_scheduler.add_noise(
+            latents, noise, timesteps)
+
+        # Get the text embedding for conditioning
+        encoder_hidden_states = self.model.text_encoder(batch['input_ids'].to(
+            self.device))[0]
+
+        # Predict the noise residual
+        model_pred = self.model.unet(noisy_latents, timesteps,
+                                     encoder_hidden_states).sample
+
+        # Get the target for loss depending on the prediction type
+        if self.model.noise_scheduler.config.prediction_type == 'epsilon':
+            target = noise
+        elif self.model.noise_scheduler.config.prediction_type == 'v_prediction':
+            target = self.model.noise_scheduler.get_velocity(
+                latents, noise, timesteps)
+        else:
+            raise ValueError(
+                f'Unknown prediction type {self.model.noise_scheduler.config.prediction_type}'
+            )
 
-    def __init__(self, **kwargs):
-        super().__init__(**kwargs)
+        if self.with_prior_preservation:
+            # Chunk the noise and model_pred into two parts and compute the loss on each part separately.
+            model_pred, model_pred_prior = torch.chunk(model_pred, 2, dim=0)
+            target, target_prior = torch.chunk(target, 2, dim=0)
+            mask = torch.chunk(batch['mask'].to(self.device), 2, dim=0)[0]
+            # Compute instance loss
+            loss = F.mse_loss(
+                model_pred.float(), target.float(), reduction='none')
+            loss = ((loss * mask).sum([1, 2, 3]) / mask.sum([1, 2, 3])).mean()
+
+            # Compute prior loss
+            prior_loss = F.mse_loss(
+                model_pred_prior.float(),
+                target_prior.float(),
+                reduction='mean')
+
+            # Add the prior loss to the instance loss.
+            loss = loss + self.prior_loss_weight * prior_loss
+        else:
+            mask = batch['mask'].to(self.device)
+            loss = F.mse_loss(
+                model_pred.float(), target.float(), reduction='none')
+            loss = ((loss * mask).sum([1, 2, 3]) / mask.sum([1, 2, 3])).mean()
+
+        train_outputs = {}
+        train_outputs[OutputKeys.LOSS] = loss
+
+        # add model output info to log
+        if 'log_vars' not in train_outputs:
+            default_keys_pattern = ['loss']
+            match_keys = set([])
+            for key_p in default_keys_pattern:
+                match_keys.update(
+                    [key for key in train_outputs.keys() if key_p in key])
+
+            log_vars = {}
+            for key in match_keys:
+                value = train_outputs.get(key, None)
+                if value is not None:
+                    if is_dist():
+                        value = value.data.clone().to('cuda')
+                        dist.all_reduce(value.div_(dist.get_world_size()))
+                    log_vars.update({key: value.item()})
+            self.log_buffer.update(log_vars)
+        else:
+            self.log_buffer.update(train_outputs['log_vars'])
 
-    def __call__(self, data) -> Dict[str, Any]:
-        image = load_image(data)
-        data = np.array(image).transpose(2, 0, 1)
-        return data
+        self.train_outputs = train_outputs
```

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/__init__.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/bert_seq_cls_tokenizer.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/bert_seq_cls_tokenizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/canmt_translation.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/canmt_translation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/dialog_classification_use_preprocessor.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/dialog_classification_use_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/document_grounded_dialog_generate_preprocessor.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/document_grounded_dialog_generate_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/document_grounded_dialog_rerank_preprocessor.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/document_grounded_dialog_rerank_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/document_grounded_dialog_retrieval_preprocessor.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/document_grounded_dialog_retrieval_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/document_segmentation_preprocessor.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/document_segmentation_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/faq_question_answering_preprocessor.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/faq_question_answering_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/feature_extraction_preprocessor.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/feature_extraction_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/fill_mask_preprocessor.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/fill_mask_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/mgeo_ranking_preprocessor.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/mgeo_ranking_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/mglm_summarization_preprocessor.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/mglm_summarization_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/relation_extraction_preprocessor.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/relation_extraction_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/sentence_embedding_preprocessor.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/sentence_embedding_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/siamese_uie_preprocessor.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/siamese_uie_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/space/__init__.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/space/args.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space/args.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/space/batch.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space/batch.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/space/data_loader.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space/data_loader.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/space/dialog_intent_prediction_preprocessor.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space/dialog_intent_prediction_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/space/dialog_modeling_preprocessor.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space/dialog_modeling_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/space/dialog_state_tracking_preprocessor.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space/dialog_state_tracking_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/space/dst_processors.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space/dst_processors.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/space/fields/__init__.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space/fields/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/space/fields/gen_field.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space/fields/gen_field.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/space/fields/intent_field.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space/fields/intent_field.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/space/lazy_dataset.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space/lazy_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/space/preprocess.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space/preprocess.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/space/sampler.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space/sampler.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/space/tensorlistdataset.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space/tensorlistdataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/space/tokenizer.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space/tokenizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/space_T_cn/__init__.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space_T_cn/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/space_T_cn/fields/database.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space_T_cn/fields/database.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/space_T_cn/fields/schema_link.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space_T_cn/fields/schema_link.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/space_T_cn/fields/struct.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space_T_cn/fields/struct.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/space_T_cn/table_question_answering_preprocessor.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space_T_cn/table_question_answering_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/space_T_en/__init__.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space_T_en/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/space_T_en/conversational_text_to_sql_preprocessor.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space_T_en/conversational_text_to_sql_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/space_T_en/fields/__init__.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space_T_en/fields/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/space_T_en/fields/common_utils.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space_T_en/fields/common_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/space_T_en/fields/parse.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space_T_en/fields/parse.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/space_T_en/fields/preprocess_dataset.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space_T_en/fields/preprocess_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/space_T_en/fields/process_dataset.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/space_T_en/fields/process_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/text_classification_preprocessor.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/text_classification_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/text_clean.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/text_clean.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/text_error_correction.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/text_error_correction.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/text_generation_preprocessor.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/text_generation_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/text_ranking_preprocessor.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/text_ranking_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/token_classification_preprocessor.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/token_classification_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/token_classification_thai_preprocessor.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/token_classification_thai_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/token_classification_viet_preprocessor.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/token_classification_viet_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/transformers_tokenizer.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/transformers_tokenizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/translation_evaluation_preprocessor.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/translation_evaluation_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/utils.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/word_alignment_preprocessor.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/word_alignment_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/nlp/zero_shot_classification_preprocessor.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/nlp/zero_shot_classification_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/ofa/__init__.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/ofa/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/ofa/asr.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/ofa/asr.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/ofa/base.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/ofa/base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/ofa/image_captioning.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/ofa/image_captioning.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/ofa/image_classification.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/ofa/image_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/ofa/ocr_recognition.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/ofa/ocr_recognition.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/ofa/sudoku.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/ofa/sudoku.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/ofa/summarization.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/ofa/summarization.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/ofa/text2sql.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/ofa/text2sql.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/ofa/text_classification.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/ofa/text_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/ofa/text_to_image_synthesis.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/ofa/text_to_image_synthesis.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/ofa/utils/audio_helper.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/ofa/utils/audio_helper.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/ofa/utils/bridge_content_encoder.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/ofa/utils/bridge_content_encoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/ofa/utils/collate.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/ofa/utils/collate.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/ofa/utils/constant.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/ofa/utils/constant.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/ofa/utils/get_tables.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/ofa/utils/get_tables.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/ofa/utils/random_help.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/ofa/utils/random_help.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/ofa/utils/text2phone.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/ofa/utils/text2phone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/ofa/utils/transforms.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/ofa/utils/transforms.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/ofa/utils/vision_helper.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/ofa/utils/vision_helper.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/ofa/visual_entailment.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/ofa/visual_entailment.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/ofa/visual_grounding.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/ofa/visual_grounding.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/ofa/visual_question_answering.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/ofa/visual_question_answering.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/science/uni_fold.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/science/uni_fold.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/speaker.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/speaker.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/tts.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/tts.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/preprocessors/video.py` & `modelscope-1.8.0rc0/modelscope/preprocessors/video.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/swift/__init__.py` & `modelscope-1.8.0rc0/modelscope/swift/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/swift/adapter.py` & `modelscope-1.8.0rc0/modelscope/swift/adapter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/swift/base.py` & `modelscope-1.8.0rc0/modelscope/swift/base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/swift/control_sd_lora.py` & `modelscope-1.8.0rc0/modelscope/swift/control_sd_lora.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/swift/lora.py` & `modelscope-1.8.0rc0/modelscope/swift/lora.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/swift/optimizers/child_tuning_adamw_optimizer.py` & `modelscope-1.8.0rc0/modelscope/swift/optimizers/child_tuning_adamw_optimizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/swift/prompt.py` & `modelscope-1.8.0rc0/modelscope/swift/prompt.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/swift/sd_lora.py` & `modelscope-1.8.0rc0/modelscope/swift/sd_lora.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/tools/eval.py` & `modelscope-1.8.0rc0/modelscope/tools/eval.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/tools/speech_tts_autolabel.py` & `modelscope-1.8.0rc0/modelscope/tools/speech_tts_autolabel.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/tools/train.py` & `modelscope-1.8.0rc0/modelscope/tools/train.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/__init__.py` & `modelscope-1.8.0rc0/modelscope/trainers/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/audio/__init__.py` & `modelscope-1.8.0rc0/modelscope/trainers/audio/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/audio/ans_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/audio/ans_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/audio/asr_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/audio/asr_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/audio/kws_farfield_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/audio/kws_farfield_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/audio/kws_nearfield_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/audio/kws_nearfield_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/audio/kws_utils/__init__.py` & `modelscope-1.8.0rc0/modelscope/trainers/audio/kws_utils/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/audio/kws_utils/batch_utils.py` & `modelscope-1.8.0rc0/modelscope/trainers/audio/kws_utils/batch_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/audio/kws_utils/det_utils.py` & `modelscope-1.8.0rc0/modelscope/trainers/audio/kws_utils/det_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/audio/kws_utils/file_utils.py` & `modelscope-1.8.0rc0/modelscope/trainers/audio/kws_utils/file_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/audio/kws_utils/model_utils.py` & `modelscope-1.8.0rc0/modelscope/trainers/audio/kws_utils/model_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/audio/kws_utils/runtime_utils.py` & `modelscope-1.8.0rc0/modelscope/trainers/audio/kws_utils/runtime_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/audio/separation_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/audio/separation_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/audio/tts_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/audio/tts_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/base.py` & `modelscope-1.8.0rc0/modelscope/trainers/base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/builder.py` & `modelscope-1.8.0rc0/modelscope/trainers/builder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/cli_argument_parser.py` & `modelscope-1.8.0rc0/modelscope/trainers/cli_argument_parser.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/cv/__init__.py` & `modelscope-1.8.0rc0/modelscope/trainers/cv/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/cv/action_detection_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/cv/action_detection_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/cv/card_detection_scrfd_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/cv/card_detection_scrfd_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/cv/cartoon_translation_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/cv/cartoon_translation_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/cv/face_detection_scrfd_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/cv/face_detection_scrfd_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/cv/image_classifition_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/cv/image_classifition_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/cv/image_defrcn_fewshot_detection_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/cv/image_defrcn_fewshot_detection_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/cv/image_detection_damoyolo_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/cv/image_detection_damoyolo_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/cv/image_inpainting_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/cv/image_inpainting_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/cv/image_instance_segmentation_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/cv/image_instance_segmentation_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/cv/image_portrait_enhancement_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/cv/image_portrait_enhancement_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/cv/movie_scene_segmentation_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/cv/movie_scene_segmentation_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/cv/nerf_recon_acc_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/cv/nerf_recon_acc_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/cv/ocr_detection_db_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/cv/ocr_detection_db_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/cv/ocr_recognition_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/cv/ocr_recognition_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/cv/referring_video_object_segmentation_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/cv/referring_video_object_segmentation_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/cv/vision_efficient_tuning_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/cv/vision_efficient_tuning_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/default_config.py` & `modelscope-1.8.0rc0/modelscope/trainers/default_config.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/hooks/__init__.py` & `modelscope-1.8.0rc0/modelscope/trainers/hooks/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/hooks/checkpoint/checkpoint_hook.py` & `modelscope-1.8.0rc0/modelscope/trainers/hooks/checkpoint/checkpoint_hook.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/hooks/checkpoint/checkpoint_processor.py` & `modelscope-1.8.0rc0/modelscope/trainers/hooks/checkpoint/checkpoint_processor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/hooks/checkpoint/load_checkpoint_hook.py` & `modelscope-1.8.0rc0/modelscope/trainers/hooks/checkpoint/load_checkpoint_hook.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/hooks/clip_clamp_logit_scale_hook.py` & `modelscope-1.8.0rc0/modelscope/trainers/hooks/clip_clamp_logit_scale_hook.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/hooks/compression/__init__.py` & `modelscope-1.8.0rc0/modelscope/trainers/hooks/compression/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/hooks/compression/sparsity_hook.py` & `modelscope-1.8.0rc0/modelscope/trainers/hooks/compression/sparsity_hook.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/hooks/compression/utils.py` & `modelscope-1.8.0rc0/modelscope/trainers/hooks/compression/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/hooks/distributed/ddp_hook.py` & `modelscope-1.8.0rc0/modelscope/trainers/hooks/distributed/ddp_hook.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/hooks/distributed/deepspeed_hook.py` & `modelscope-1.8.0rc0/modelscope/trainers/hooks/distributed/deepspeed_hook.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/hooks/distributed/megatron_hook.py` & `modelscope-1.8.0rc0/modelscope/trainers/hooks/distributed/megatron_hook.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/hooks/early_stop_hook.py` & `modelscope-1.8.0rc0/modelscope/trainers/hooks/early_stop_hook.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/hooks/evaluation_hook.py` & `modelscope-1.8.0rc0/modelscope/trainers/hooks/evaluation_hook.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/hooks/hook.py` & `modelscope-1.8.0rc0/modelscope/trainers/hooks/hook.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/hooks/iter_timer_hook.py` & `modelscope-1.8.0rc0/modelscope/trainers/hooks/iter_timer_hook.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/hooks/logger/__init__.py` & `modelscope-1.8.0rc0/modelscope/trainers/hooks/logger/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/hooks/logger/base.py` & `modelscope-1.8.0rc0/modelscope/trainers/hooks/logger/base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/hooks/logger/tensorboard_hook.py` & `modelscope-1.8.0rc0/modelscope/trainers/hooks/logger/tensorboard_hook.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/hooks/logger/text_logger_hook.py` & `modelscope-1.8.0rc0/modelscope/trainers/hooks/logger/text_logger_hook.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/hooks/lr_scheduler_hook.py` & `modelscope-1.8.0rc0/modelscope/trainers/hooks/lr_scheduler_hook.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/hooks/optimizer/__init__.py` & `modelscope-1.8.0rc0/modelscope/trainers/hooks/optimizer/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/hooks/optimizer/apex_optimizer_hook.py` & `modelscope-1.8.0rc0/modelscope/trainers/hooks/optimizer/apex_optimizer_hook.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/hooks/optimizer/base.py` & `modelscope-1.8.0rc0/modelscope/trainers/hooks/optimizer/base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/hooks/optimizer/torch_optimizer_hook.py` & `modelscope-1.8.0rc0/modelscope/trainers/hooks/optimizer/torch_optimizer_hook.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/hooks/priority.py` & `modelscope-1.8.0rc0/modelscope/trainers/hooks/priority.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/lrscheduler/__init__.py` & `modelscope-1.8.0rc0/modelscope/trainers/lrscheduler/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/lrscheduler/builder.py` & `modelscope-1.8.0rc0/modelscope/trainers/lrscheduler/builder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/lrscheduler/warmup/__init__.py` & `modelscope-1.8.0rc0/modelscope/trainers/lrscheduler/warmup/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/lrscheduler/warmup/base.py` & `modelscope-1.8.0rc0/modelscope/trainers/lrscheduler/warmup/base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/lrscheduler/warmup/warmup.py` & `modelscope-1.8.0rc0/modelscope/trainers/lrscheduler/warmup/warmup.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/multi_modal/__init__.py` & `modelscope-1.8.0rc0/modelscope/trainers/multi_modal/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/multi_modal/clip/clip_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/multi_modal/clip/clip_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/multi_modal/clip/clip_trainer_utils.py` & `modelscope-1.8.0rc0/modelscope/trainers/multi_modal/clip/clip_trainer_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/multi_modal/custom_diffusion/custom_diffusion_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/nlp/space/trainer/intent_trainer.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,743 +1,705 @@
-# Copyright 2022-2023 The Alibaba Fundamental Vision Team Authors. All rights reserved.
-import hashlib
-import itertools
+# Copyright (c) Alibaba, Inc. and its affiliates.
+
 import os
-import random
-import warnings
-from pathlib import Path
-from typing import Union
+import time
+from collections import OrderedDict
 
 import json
 import numpy as np
 import torch
-import torch.nn.functional as F
-from diffusers import DiffusionPipeline
-from diffusers.loaders import AttnProcsLayers
-from diffusers.models.attention_processor import CustomDiffusionAttnProcessor
-from PIL import Image
-from PIL.ImageOps import exif_transpose
-from torch.utils.data import Dataset
-from torchvision import transforms
-from tqdm.auto import tqdm
-
-from modelscope.metainfo import Trainers
-from modelscope.msdatasets import MsDataset
-from modelscope.outputs import OutputKeys
-from modelscope.trainers.builder import TRAINERS
-from modelscope.trainers.hooks.checkpoint.checkpoint_hook import CheckpointHook
-from modelscope.trainers.hooks.checkpoint.checkpoint_processor import \
-    CheckpointProcessor
-from modelscope.trainers.optimizer.builder import build_optimizer
-from modelscope.trainers.trainer import EpochBasedTrainer
-from modelscope.utils.config import ConfigDict
-from modelscope.utils.constant import ModeKeys, TrainerStages
-from modelscope.utils.data_utils import to_device
-from modelscope.utils.torch_utils import is_dist
-
-
-class CustomCheckpointProcessor(CheckpointProcessor):
-
-    def __init__(self, modifier_token, modifier_token_id):
-        """Checkpoint processor for custom diffusion.
-
-        Args:
-            modifier_token: The token to use as a modifier for the concept.
-            modifier_token_id: The modifier token id for the concept.
-        """
-        self.modifier_token = modifier_token
-        self.modifier_token_id = modifier_token_id
+from tqdm import tqdm
+from transformers.optimization import AdamW, get_linear_schedule_with_warmup
+
+from modelscope.trainers.nlp.space.metrics.metrics_tracker import \
+    MetricsTracker
+from modelscope.utils.constant import ModelFile
+from modelscope.utils.logger import get_logger
+
+
+class Trainer(object):
+
+    def __init__(self,
+                 model,
+                 to_tensor,
+                 config,
+                 reader=None,
+                 logger=None,
+                 lr_scheduler=None,
+                 optimizer=None):
+        self.model = model
+        self.to_tensor = to_tensor
+        self.do_train = config.do_train
+        self.do_infer = config.do_infer
+
+        self.is_decreased_valid_metric = config.Trainer.valid_metric_name[
+            0] == '-'
+        self.valid_metric_name = config.Trainer.valid_metric_name[1:]
+        self.num_epochs = config.Trainer.num_epochs
+        self.save_dir = config.Trainer.save_dir
+        self.log_steps = config.Trainer.log_steps
+        self.valid_steps = config.Trainer.valid_steps
+        self.save_checkpoint = config.Trainer.save_checkpoint
+        self.save_summary = config.Trainer.save_summary
+        self.learning_method = config.Dataset.learning_method
+        self.weight_decay = config.Model.weight_decay
+        self.warmup_steps = config.Model.warmup_steps
+        self.batch_size_label = config.Trainer.batch_size_label
+        self.batch_size_nolabel = config.Trainer.batch_size_nolabel
+        self.gpu = config.Trainer.gpu
+        self.lr = config.Model.lr
+
+        self.model = model
+        self.func_model = self.model.module if self.gpu > 1 else self.model
+        self.reader = reader
+        self.tokenizer = reader.tokenizer
+
+        self.lr_scheduler = lr_scheduler
+        self.optimizer = optimizer
+
+        self.logger = logger or get_logger()
+
+        self.batch_metrics_tracker_label = MetricsTracker()
+        self.token_metrics_tracker_label = MetricsTracker()
+        self.batch_metrics_tracker_nolabel = MetricsTracker()
+        self.token_metrics_tracker_nolabel = MetricsTracker()
+
+        self.best_valid_metric = float(
+            'inf' if self.is_decreased_valid_metric else '-inf')
+        self.epoch = 0
+        self.batch_num = 0
 
-    def save_checkpoints(self,
-                         trainer,
-                         checkpoint_path_prefix,
-                         output_dir,
-                         meta=None):
-        """Save the state dict for custom diffusion model.
+    def set_optimizers(self, num_training_steps_per_epoch):
         """
-        trainer.model.unet = trainer.model.unet.to(torch.float32)
-        trainer.model.unet.save_attn_procs(output_dir)
+        Setup the optimizer and the learning rate scheduler.
 
-        learned_embeds = trainer.model.text_encoder.get_input_embeddings(
-        ).weight
-        if not isinstance(self.modifier_token_id, list):
-            self.modifier_token_id = [self.modifier_token_id]
-        for x, y in zip(self.modifier_token_id, self.modifier_token):
-            learned_embeds_dict = {}
-            learned_embeds_dict[y] = learned_embeds[x]
-            torch.save(learned_embeds_dict, f'{output_dir}/{y}.bin')
-
-
-class CustomDiffusionDataset(Dataset):
-
-    def __init__(
-        self,
-        concepts_list,
-        tokenizer,
-        size=512,
-        mask_size=64,
-        center_crop=False,
-        with_prior_preservation=False,
-        num_class_images=200,
-        hflip=False,
-        aug=True,
-    ):
-        """A dataset to prepare the instance and class images with the prompts for fine-tuning the model.
-        It pre-processes the images and the tokenizes prompts.
-
-        Args:
-            concepts_list: contain multiple concepts, instance_prompt, class_prompt, etc.
-            tokenizer: pretrained tokenizer.
-            size: the size of images.
-            mask_size: the mask size of images.
-            center_crop: execute center crop or not.
-            with_prior_preservation: flag to add prior preservation loss.
-            hflip: whether to flip horizontally.
-            aug: perform data augmentation.
+        from transformers.Trainer
 
+        parameters from cfg: lr (1e-3); warmup_steps
         """
-        self.size = size
-        self.mask_size = mask_size
-        self.center_crop = center_crop
-        self.tokenizer = tokenizer
-        self.interpolation = Image.BILINEAR
-        self.aug = aug
-
-        self.instance_images_path = []
-        self.class_images_path = []
-        self.with_prior_preservation = with_prior_preservation
-        for concept in concepts_list:
-            inst_img_path = [
-                (x, concept['instance_prompt'])
-                for x in Path(concept['instance_data_dir']).iterdir()
-                if x.is_file()
-            ]
-            self.instance_images_path.extend(inst_img_path)
-
-            if with_prior_preservation:
-                class_data_root = Path(concept['class_data_dir'])
-                if os.path.isdir(class_data_root):
-                    class_images_path = list(class_data_root.iterdir())
-                    class_prompt = [
-                        concept['class_prompt']
-                        for _ in range(len(class_images_path))
-                    ]
-                else:
-                    with open(class_data_root, 'r') as f:
-                        class_images_path = f.read().splitlines()
-                    with open(concept['class_prompt'], 'r') as f:
-                        class_prompt = f.read().splitlines()
-
-                class_img_path = [
-                    (x, y) for (x, y) in zip(class_images_path, class_prompt)
-                ]
-                self.class_images_path.extend(
-                    class_img_path[:num_class_images])
-
-        random.shuffle(self.instance_images_path)
-        self.num_instance_images = len(self.instance_images_path)
-        self.num_class_images = len(self.class_images_path)
-        self._length = max(self.num_class_images, self.num_instance_images)
-        self.flip = transforms.RandomHorizontalFlip(0.5 * hflip)
-
-        self.image_transforms = transforms.Compose([
-            self.flip,
-            transforms.Resize(
-                size, interpolation=transforms.InterpolationMode.BILINEAR),
-            transforms.CenterCrop(size)
-            if center_crop else transforms.RandomCrop(size),
-            transforms.ToTensor(),
-            transforms.Normalize([0.5], [0.5]),
-        ])
-
-    def __len__(self):
-        return self._length
-
-    def preprocess(self, image, scale, resample):
-        outer, inner = self.size, scale
-        factor = self.size // self.mask_size
-        if scale > self.size:
-            outer, inner = scale, self.size
-        top, left = np.random.randint(0, outer - inner + 1), np.random.randint(
-            0, outer - inner + 1)
-        image = image.resize((scale, scale), resample=resample)
-        image = np.array(image).astype(np.uint8)
-        image = (image / 127.5 - 1.0).astype(np.float32)
-        instance_image = np.zeros((self.size, self.size, 3), dtype=np.float32)
-        mask = np.zeros((self.size // factor, self.size // factor))
-        if scale > self.size:
-            instance_image = image[top:top + inner, left:left + inner, :]
-            mask = np.ones((self.size // factor, self.size // factor))
-        else:
-            instance_image[top:top + inner, left:left + inner, :] = image
-            mask[top // factor + 1:(top + scale) // factor - 1,
-                 left // factor + 1:(left + scale) // factor - 1] = 1.0
-        return instance_image, mask
-
-    def __getitem__(self, index):
-        example = {}
-        instance_image, instance_prompt = self.instance_images_path[
-            index % self.num_instance_images]
-        instance_image = Image.open(instance_image)
-        if not instance_image.mode == 'RGB':
-            instance_image = instance_image.convert('RGB')
-        instance_image = self.flip(instance_image)
-
-        # apply resize augmentation and create a valid image region mask
-        random_scale = self.size
-        if self.aug:
-            random_scale = (
-                np.random.randint(self.size // 3, self.size
-                                  + 1) if np.random.uniform() < 0.66 else
-                np.random.randint(int(1.2 * self.size), int(1.4 * self.size)))
-        instance_image, mask = self.preprocess(instance_image, random_scale,
-                                               self.interpolation)
-
-        if random_scale < 0.6 * self.size:
-            instance_prompt = np.random.choice(['a far away ', 'very small '
-                                                ]) + instance_prompt
-        elif random_scale > self.size:
-            instance_prompt = np.random.choice(['zoomed in ', 'close up '
-                                                ]) + instance_prompt
-
-        example['instance_images'] = torch.from_numpy(instance_image).permute(
-            2, 0, 1)
-        example['mask'] = torch.from_numpy(mask)
-        example['instance_prompt_ids'] = self.tokenizer(
-            instance_prompt,
-            truncation=True,
-            padding='max_length',
-            max_length=self.tokenizer.model_max_length,
-            return_tensors='pt',
-        ).input_ids
-
-        if self.with_prior_preservation:
-            class_image, class_prompt = self.class_images_path[
-                index % self.num_class_images]
-            class_image = Image.open(class_image)
-            if not class_image.mode == 'RGB':
-                class_image = class_image.convert('RGB')
-            example['class_images'] = self.image_transforms(class_image)
-            example['class_mask'] = torch.ones_like(example['mask'])
-            example['class_prompt_ids'] = self.tokenizer(
-                class_prompt,
-                truncation=True,
-                padding='max_length',
-                max_length=self.tokenizer.model_max_length,
-                return_tensors='pt',
-            ).input_ids
-
-        return example
-
-
-class PromptDataset(Dataset):
-
-    def __init__(self, prompt, num_samples):
-        """Dataset to prepare the prompts to generate class images.
-
-        Args:
-            prompt: Class prompt.
-            num_samples: The number sample for class images.
+        # Prepare optimizer and schedule (linear warmup and decay)
+        no_decay = ['bias', 'norm.weight']
+        optimizer_grouped_parameters = [
+            {
+                'params': [
+                    p for n, p in self.model.named_parameters()
+                    if not any(nd in n for nd in no_decay)
+                ],
+                'weight_decay':
+                self.weight_decay,
+            },
+            {
+                'params': [
+                    p for n, p in self.model.named_parameters()
+                    if any(nd in n for nd in no_decay)
+                ],
+                'weight_decay':
+                0.0,
+            },
+        ]
+        optimizer = AdamW(optimizer_grouped_parameters, lr=self.lr)
+
+        num_training_steps = num_training_steps_per_epoch * self.num_epochs
+        num_warmup_steps = self.warmup_steps if self.warmup_steps >= 0 else int(
+            num_training_steps * 0.1)
+        lr_scheduler = get_linear_schedule_with_warmup(
+            optimizer,
+            num_warmup_steps=num_warmup_steps,
+            num_training_steps=num_training_steps)
+
+        # reset optimizer and lr_scheduler
+        self.optimizer = optimizer
+        self.lr_scheduler = lr_scheduler
+
+        # log info
+        self.logger.info(
+            f'***** Running training: {self.learning_method} *****')
+        self.logger.info('  Num Epochs = %d', self.num_epochs)
+        self.logger.info(
+            '  Num Training steps(one turn in a batch of dialogs) per epoch = %d',
+            num_training_steps_per_epoch)
+        self.logger.info('  Batch size for labeled data = %d',
+                         self.batch_size_label)
+        self.logger.info('  Batch size for unlabeled data = %d',
+                         self.batch_size_nolabel)
+        self.logger.info('  Total optimization steps = %d', num_training_steps)
+        self.logger.info('  Total warmup steps = %d', num_warmup_steps)
+        self.logger.info('************************************')
+
+    def train(self,
+              train_label_iter,
+              train_nolabel_iter=None,
+              valid_label_iter=None,
+              valid_nolabel_iter=None):
+        # begin training
+        num_epochs = self.num_epochs - self.epoch
+        for epoch in range(num_epochs):
+            self.train_epoch(
+                train_label_iter=train_label_iter,
+                train_nolabel_iter=train_nolabel_iter,
+                valid_label_iter=valid_label_iter,
+                valid_nolabel_iter=valid_nolabel_iter)
 
+    def train_epoch(self, train_label_iter, train_nolabel_iter,
+                    valid_label_iter, valid_nolabel_iter):
         """
-        self.prompt = prompt
-        self.num_samples = num_samples
+        Train an epoch.
+        """
+        raise NotImplementedError
 
-    def __len__(self):
-        return self.num_samples
+    def evaluate(self, data_label_iter, data_nolabel_iter, need_save=True):
+        raise NotImplementedError
 
-    def __getitem__(self, index):
-        example = {}
-        example['prompt'] = self.prompt
-        example['index'] = index
-        return example
-
-
-@TRAINERS.register_module(module_name=Trainers.custom_diffusion)
-class CustomDiffusionTrainer(EpochBasedTrainer):
-
-    def __init__(self, *args, **kwargs):
-        super().__init__(*args, **kwargs)
-        """Custom diffusion trainers for fine-tuning stable diffusion.
-
-        Args:
-            with_prior_preservation: a boolean indicating whether to enable prior loss.
-            instance_prompt: a string specifying the instance prompt.
-            class_prompt: a string specifying the class prompt.
-            class_data_dir: the path to the class data directory.
-            num_class_images: the number of class images to generate.
-            prior_loss_weight: the weight of the prior loss.
-            modifier_token: A token to use as a modifier for the concept.
-            initializer_token: A token to use as initializer word.
-            freeze_model: crossattn to enable fine-tuning of all params in the cross attention.
-            sample_batch_size: Batch size (per device) for sampling images.
-            train_batch_size: Batch size (per device) for the training dataloader.
-            center_crop: execute center crop or not.
-            concepts_list: Path to json containing multiple concepts, will overwrite parameters.
-            instance_data_name: The instance data local dir or online ID.
+    def infer(self, data_iter, num_batches=None):
+        raise NotImplementedError
 
-        """
-        self.with_prior_preservation = kwargs.pop('with_prior_preservation',
-                                                  True)
-        instance_prompt = kwargs.pop('instance_prompt', 'a photo of sks dog')
-        class_prompt = kwargs.pop('class_prompt', 'dog')
-        class_data_dir = kwargs.pop('class_data_dir', '/tmp/class_data')
-        self.real_prior = kwargs.pop('real_prior', False)
-        self.num_class_images = kwargs.pop('num_class_images', 200)
-        self.resolution = kwargs.pop('resolution', 512)
-        self.prior_loss_weight = kwargs.pop('prior_loss_weight', 1.0)
-        self.modifier_token = kwargs.pop('modifier_token', '<new1>')
-        self.initializer_token = kwargs.pop('initializer_token', 'ktn+pll+ucd')
-        self.freeze_model = kwargs.pop('freeze_model', 'crossattn_kv')
-        self.sample_batch_size = kwargs.pop('sample_batch_size', 4)
-        self.train_batch_size = kwargs.pop('train_batch_size', 2)
-        self.center_crop = kwargs.pop('center_crop', False)
-        self.concepts_list = kwargs.pop('concepts_list', None)
-        instance_data_name = kwargs.pop(
-            'instance_data_name', 'buptwq/lora-stable-diffusion-finetune-dog')
-
-        # Extract downloaded image folder
-        if self.concepts_list is None:
-            if os.path.isdir(instance_data_name):
-                instance_data_dir = instance_data_name
-            else:
-                ds = MsDataset.load(instance_data_name, split='train')
-                instance_data_dir = os.path.dirname(
-                    next(iter(ds))['Target:FILE'])
-
-        # construct concept list
-        if self.concepts_list is None:
-            self.concepts_list = [{
-                'instance_prompt': instance_prompt,
-                'class_prompt': class_prompt,
-                'instance_data_dir': instance_data_dir,
-                'class_data_dir': class_data_dir,
-            }]
-        else:
-            with open(self.concepts_list, 'r') as f:
-                self.concepts_list = json.load(f)
+    def save(self, is_best=False):
+        """ save """
+        train_state = {
+            'epoch': self.epoch,
+            'batch_num': self.batch_num,
+            'best_valid_metric': self.best_valid_metric,
+            'optimizer': self.optimizer.state_dict()
+        }
+        if self.lr_scheduler is not None:
+            train_state['lr_scheduler'] = self.lr_scheduler.state_dict()
 
-        for concept in self.concepts_list:
-            if not os.path.exists(concept['class_data_dir']):
-                os.makedirs(concept['class_data_dir'])
-            if not os.path.exists(concept['instance_data_dir']):
-                raise Exception(
-                    f"instance dataset {concept['instance_data_dir']} does not exist."
-                )
-
-        # Adding a modifier token which is optimized
-        self.modifier_token_id = []
-        initializer_token_id = []
-        if self.modifier_token is not None:
-            self.modifier_token = self.modifier_token.split('+')
-            self.initializer_token = self.initializer_token.split('+')
-            if len(self.modifier_token) > len(self.initializer_token):
-                raise ValueError(
-                    'You must specify + separated initializer token for each modifier token.'
-                )
-            for modifier_token, initializer_token in zip(
-                    self.modifier_token,
-                    self.initializer_token[:len(self.modifier_token)]):
-                # Add the placeholder token in tokenizer
-                num_added_tokens = self.model.tokenizer.add_tokens(
-                    modifier_token)
-                if num_added_tokens == 0:
-                    raise ValueError(
-                        f'The tokenizer already contains the token {modifier_token}. Please pass a different'
-                        ' `modifier_token` that is not already in the tokenizer.'
-                    )
-
-                # Convert the initializer_token, placeholder_token to ids
-                token_ids = self.model.tokenizer.encode(
-                    [initializer_token], add_special_tokens=False)
-                # Check if initializer_token is a single token or a sequence of tokens
-                if len(token_ids) > 1:
-                    raise ValueError(
-                        'The initializer token must be a single token.')
-
-                initializer_token_id.append(token_ids[0])
-                self.modifier_token_id.append(
-                    self.model.tokenizer.convert_tokens_to_ids(modifier_token))
-
-        # Resize the token embeddings as we are adding new special tokens to the tokenizer
-        self.model.text_encoder.resize_token_embeddings(
-            len(self.model.tokenizer))
-
-        # Resize the token embeddings as we are adding new special tokens to the tokenizer
-        self.model.text_encoder.resize_token_embeddings(
-            len(self.model.tokenizer))
-
-        # Initialise the newly added placeholder token with the embeddings of the initializer token
-        token_embeds = self.model.text_encoder.get_input_embeddings(
-        ).weight.data
-        for x, y in zip(self.modifier_token_id, initializer_token_id):
-            token_embeds[x] = token_embeds[y]
-
-        # Freeze all parameters except for the token embeddings in text encoder
-        params_to_freeze = itertools.chain(
-            self.model.text_encoder.text_model.encoder.parameters(),
-            self.model.text_encoder.text_model.final_layer_norm.parameters(),
-            self.model.text_encoder.text_model.embeddings.position_embedding.
-            parameters(),
-        )
-        self.freeze_params(params_to_freeze)
-
-        # Save checkpoint and configurate files
-        ckpt_hook = list(
-            filter(lambda hook: isinstance(hook, CheckpointHook),
-                   self.hooks))[0]
-        ckpt_hook.set_processor(
-            CustomCheckpointProcessor(self.modifier_token,
-                                      self.modifier_token_id))
-
-        # Add new Custom Diffusion weights to the attention layers
-        attention_class = CustomDiffusionAttnProcessor
-        # Only train key, value projection layers if freeze_model = 'crossattn_kv' else train all params.
-        train_q_out = False if self.freeze_model == 'crossattn_kv' else True
-        custom_diffusion_attn_procs = {}
-
-        st = self.model.unet.state_dict()
-        for name, _ in self.model.unet.attn_processors.items():
-            cross_attention_dim = None if name.endswith(
-                'attn1.processor'
-            ) else self.model.unet.config.cross_attention_dim
-            if name.startswith('mid_block'):
-                hidden_size = self.model.unet.config.block_out_channels[-1]
-            elif name.startswith('up_blocks'):
-                block_id = int(name[len('up_blocks.')])
-                hidden_size = list(
-                    reversed(
-                        self.model.unet.config.block_out_channels))[block_id]
-            elif name.startswith('down_blocks'):
-                block_id = int(name[len('down_blocks.')])
-                hidden_size = self.model.unet.config.block_out_channels[
-                    block_id]
-            layer_name = name.split('.processor')[0]
-            weights = {
-                'to_k_custom_diffusion.weight':
-                st[layer_name + '.to_k.weight'],
-                'to_v_custom_diffusion.weight':
-                st[layer_name + '.to_v.weight'],
+        # Save checkpoint
+        if self.save_checkpoint:
+            model_file = os.path.join(self.save_dir,
+                                      f'state_epoch_{self.epoch}.model')
+            torch.save(self.model.state_dict(), model_file)
+            self.logger.info(f"Saved model state to '{model_file}'")
+
+            train_file = os.path.join(self.save_dir,
+                                      f'state_epoch_{self.epoch}.train')
+            torch.save(train_state, train_file)
+            self.logger.info(f"Saved train state to '{train_file}'")
+
+        # Save current best model
+        if is_best:
+            best_model_file = os.path.join(self.save_dir,
+                                           ModelFile.TORCH_MODEL_BIN_FILE)
+            torch.save(self.model.state_dict(), best_model_file)
+            best_train_file = os.path.join(
+                self.save_dir,
+                '{}.train'.format(ModelFile.TORCH_MODEL_BIN_FILE))
+            torch.save(train_state, best_train_file)
+            self.logger.info(
+                f"Saved best model state to '{best_model_file}' with new best valid metric "
+                f'{self.valid_metric_name.upper()}={self.best_valid_metric:.3f}'
+            )
+
+    def load(self):
+        """ load """
+
+        def _load_model_state():
+            model_state_dict = torch.load(
+                f'{self.func_model.init_checkpoint}',
+                map_location=lambda storage, loc: storage)
+
+            if 'module.' in list(model_state_dict.keys())[0]:
+                new_model_state_dict = OrderedDict()
+                for k, v in model_state_dict.items():
+                    assert k[:7] == 'module.'
+                    new_model_state_dict[k[7:]] = v
+                model_state_dict = new_model_state_dict
+
+            new_model_state_dict = OrderedDict()
+            parameters = {
+                name: param
+                for name, param in self.func_model.named_parameters()
             }
-            if train_q_out:
-                weights['to_q_custom_diffusion.weight'] = st[layer_name
-                                                             + '.to_q.weight']
-                weights['to_out_custom_diffusion.0.weight'] = st[
-                    layer_name + '.to_out.0.weight']
-                weights['to_out_custom_diffusion.0.bias'] = st[
-                    layer_name + '.to_out.0.bias']
-            if cross_attention_dim is not None:
-                custom_diffusion_attn_procs[name] = attention_class(
-                    train_kv=True,
-                    train_q_out=train_q_out,
-                    hidden_size=hidden_size,
-                    cross_attention_dim=cross_attention_dim,
-                ).to(self.model.unet.device)
-                custom_diffusion_attn_procs[name].load_state_dict(weights)
-            else:
-                custom_diffusion_attn_procs[name] = attention_class(
-                    train_kv=False,
-                    train_q_out=False,
-                    hidden_size=hidden_size,
-                    cross_attention_dim=cross_attention_dim,
-                )
-        del st
-        self.model.unet.set_attn_processor(custom_diffusion_attn_procs)
-        self.custom_diffusion_layers = AttnProcsLayers(
-            self.model.unet.attn_processors)
-
-        # Check for conflicts and conflicts
-        if self.with_prior_preservation:
-            for concept in self.concepts_list:
-                if concept['class_data_dir'] is None:
-                    raise ValueError(
-                        'You must specify a data directory for class images.')
-                if concept['class_prompt'] is None:
-                    raise ValueError(
-                        'You must specify prompt for class images.')
-        else:
-            for concept in self.concepts_list:
-                if concept['class_data_dir'] is not None:
-                    warnings.warn(
-                        'You need not use --class_data_dir without --with_prior_preservation.'
-                    )
-                if concept['class_prompt'] is not None:
-                    warnings.warn(
-                        'You need not use --class_prompt without --with_prior_preservation.'
-                    )
-
-        # Generate class images if prior preservation is enabled.
-        if self.with_prior_preservation:
-            self.generate_image()
-
-        # Dataset and DataLoaders creation:
-        train_dataset = CustomDiffusionDataset(
-            concepts_list=self.concepts_list,
-            tokenizer=self.model.tokenizer,
-            with_prior_preservation=self.with_prior_preservation,
-            size=self.resolution,
-            mask_size=self.model.vae.encode(
-                torch.randn(1, 3, self.resolution,
-                            self.resolution).to(dtype=torch.float32).to(
-                                self.device)).latent_dist.sample().size()[-1],
-            center_crop=self.center_crop,
-            num_class_images=self.num_class_images,
-            hflip=False,
-            aug=True,
-        )
-        train_dataloader = torch.utils.data.DataLoader(
-            train_dataset,
-            batch_size=self.train_batch_size,
-            shuffle=True,
-            collate_fn=lambda examples: self.collate_fn(examples),
-            num_workers=2,
-        )
-        self.iter_train_dataloader = itertools.cycle(train_dataloader)
-
-    def freeze_params(self, params):
-        for param in params:
-            param.requires_grad = False
-
-    def collate_fn(self, examples):
-        input_ids = [example['instance_prompt_ids'] for example in examples]
-        pixel_values = [example['instance_images'] for example in examples]
-        mask = [example['mask'] for example in examples]
-        # Concat class and instance examples which avoid doing two forward passes.
-        if self.with_prior_preservation:
-            input_ids += [example['class_prompt_ids'] for example in examples]
-            pixel_values += [example['class_images'] for example in examples]
-            mask += [example['class_mask'] for example in examples]
-
-        input_ids = torch.cat(input_ids, dim=0)
-        pixel_values = torch.stack(pixel_values)
-        mask = torch.stack(mask)
-        pixel_values = pixel_values.to(
-            memory_format=torch.contiguous_format).float()
-        mask = mask.to(memory_format=torch.contiguous_format).float()
-
-        batch = {
-            'input_ids': input_ids,
-            'pixel_values': pixel_values,
-            'mask': mask.unsqueeze(1)
-        }
-        return batch
+            for name, param in model_state_dict.items():
+                if name in parameters:
+                    if param.shape != parameters[name].shape:
+                        assert hasattr(param, 'numpy')
+                        arr = param.numpy()
+                        z = np.random.normal(
+                            scale=self.func_model.initializer_range,
+                            size=parameters[name].shape).astype('float32')
+                        if name == 'embedder.token_embedding.weight':
+                            z[-param.shape[0]:] = arr
+                            print(
+                                f'part of parameter({name}) random normlize initialize'
+                            )
+                        else:
+                            if z.shape[0] < param.shape[0]:
+                                z = arr[:z.shape[0]]
+                                print(f'part of parameter({name}) are dropped')
+                            else:
+                                z[:param.shape[0]] = arr
+                                print(
+                                    f'part of parameter({name}) random normlize initialize'
+                                )
+                        dtype, device = param.dtype, param.device
+                        z = torch.tensor(z, dtype=dtype, device=device)
+                        new_model_state_dict[name] = z
+                    else:
+                        new_model_state_dict[name] = param
+                else:
+                    print(f'parameter({name}) are dropped')
+            model_state_dict = new_model_state_dict
 
-    def generate_image(self):
-        """ Generate class images if prior preservation is enabled.
-        """
-        for i, concept in enumerate(self.concepts_list):
-            class_images_dir = Path(concept['class_data_dir'])
-            if not class_images_dir.exists():
-                class_images_dir.mkdir(parents=True, exist_ok=True)
-
-            cur_class_images = len(list(class_images_dir.iterdir()))
-
-            if cur_class_images < self.num_class_images:
-                pipeline = DiffusionPipeline.from_pretrained(
-                    self.model_dir,
-                    torch_dtype=torch.float32,
-                    safety_checker=None,
-                    revision=None,
-                )
-                pipeline.set_progress_bar_config(disable=True)
-
-                num_new_images = self.num_class_images - cur_class_images
-
-                sample_dataset = PromptDataset(concept['class_prompt'],
-                                               num_new_images)
-                sample_dataloader = torch.utils.data.DataLoader(
-                    sample_dataset, batch_size=self.sample_batch_size)
-
-                pipeline.to(self.device)
-
-                for example in tqdm(
-                        sample_dataloader,
-                        desc='Generating class images',
-                        # disable=not accelerator.is_local_main_process,
-                ):
-                    images = pipeline(example['prompt']).images
-
-                    for i, image in enumerate(images):
-                        hash_image = hashlib.sha1(image.tobytes()).hexdigest()
-                        save_index = example['index'][i] + cur_class_images
-                        image_filename = class_images_dir / f'{save_index}-{hash_image}.jpg'
-                        image.save(image_filename)
-
-                del pipeline
-                if torch.cuda.is_available():
-                    torch.cuda.empty_cache()
-
-    def build_optimizer(self, cfg: ConfigDict, default_args: dict = None):
-        try:
-            return build_optimizer(
-                itertools.chain(
-                    self.model.text_encoder.get_input_embeddings().parameters(
-                    ), self.custom_diffusion_layers.parameters()),
-                cfg=cfg,
-                default_args=default_args)
-        except KeyError as e:
-            self.logger.error(
-                f'Build optimizer error, the optimizer {cfg} is a torch native component, '
-                f'please check if your torch with version: {torch.__version__} matches the config.'
+            for name in parameters:
+                if name not in model_state_dict:
+                    if parameters[name].requires_grad:
+                        print(f'parameter({name}) random normlize initialize')
+                        z = np.random.normal(
+                            scale=self.func_model.initializer_range,
+                            size=parameters[name].shape).astype('float32')
+                        dtype, device = parameters[name].dtype, parameters[
+                            name].device
+                        model_state_dict[name] = torch.tensor(
+                            z, dtype=dtype, device=device)
+                    else:
+                        model_state_dict[name] = parameters[name]
+
+            self.func_model.load_state_dict(model_state_dict)
+            self.logger.info(
+                f"Loaded model state from '{self.func_model.init_checkpoint}.model'"
             )
-            raise e
 
-    def train_loop(self, data_loader):
-        """ Training loop used by `EpochBasedTrainer.train()`
-        """
-        self.invoke_hook(TrainerStages.before_run)
-        self.model.train()
-        for _ in range(self._epoch, self._max_epochs):
-            self.invoke_hook(TrainerStages.before_train_epoch)
-            for i, data_batch in enumerate(data_loader):
-                if i < self.inner_iter:
-                    # inner_iter may be read out from the checkpoint file, so skip the trained iters in the epoch.
-                    continue
-                data_batch = to_device(data_batch, self.device)
-                self.data_batch = data_batch
-                self._inner_iter = i
-                self.invoke_hook(TrainerStages.before_train_iter)
-                self.train_step(self.model, data_batch)
-                self.invoke_hook(TrainerStages.after_train_iter)
-                # Zero out the gradients for all token embeddings except the newly added
-                # embeddings for the concept to optimize the concept embeddings.
-                if self.modifier_token is not None:
-                    grads_text_encoder = self.model.text_encoder.get_input_embeddings(
-                    ).weight.grad
-                    # Get the index for tokens that we want to zero the grads.
-                    index_grads_to_zero = torch.arange(
-                        len(self.model.tokenizer)) != self.modifier_token_id[0]
-                    for i in range(len(self.modifier_token_id[1:])):
-                        modifier_flag = torch.arange(
-                            len(self.model.tokenizer)
-                        ) != self.modifier_token_id[i]
-                        index_grads_to_zero = index_grads_to_zero & modifier_flag
-                    grads_data = grads_text_encoder.data[
-                        index_grads_to_zero, :].fill_(0)
-                    grads_text_encoder.data[
-                        index_grads_to_zero, :] = grads_data
-                # Value changed after the hooks are invoked, do not move them above the invoke_hook code.
-                del self.data_batch
-                self._iter += 1
-                self._mode = ModeKeys.TRAIN
+        def _load_train_state():
+            train_file = f'{self.func_model.init_checkpoint}.train'
+            if os.path.exists(train_file):
+                train_state_dict = torch.load(
+                    train_file, map_location=lambda storage, loc: storage)
+                self.epoch = train_state_dict['epoch']
+                self.best_valid_metric = train_state_dict['best_valid_metric']
+                if self.optimizer is not None and 'optimizer' in train_state_dict:
+                    self.optimizer.load_state_dict(
+                        train_state_dict['optimizer'])
+                if self.lr_scheduler is not None and 'lr_scheduler' in train_state_dict:
+                    self.lr_scheduler.load_state_dict(
+                        train_state_dict['lr_scheduler'])
+                self.logger.info(
+                    f"Loaded train state from '{train_file}' with (epoch-{self.epoch} "
+                    f'best_valid_metric={self.best_valid_metric:.3f})')
+            else:
+                self.logger.info('Loaded no train state')
 
-                if i + 1 >= self.iters_per_epoch:
-                    break
+        if self.func_model.init_checkpoint is None:
+            self.logger.info('Loaded no model !!!')
+            return
+
+        if self.do_train:
+            _load_model_state()
+            return
+
+        if self.do_infer:
+            _load_model_state()
+            _load_train_state()
+
+
+class IntentTrainer(Trainer):
+
+    def __init__(self, model, to_tensor, config, reader=None):
+        super(IntentTrainer, self).__init__(model, to_tensor, config, reader)
+        self.example = config.Model.example
+        self.can_norm = config.Trainer.can_norm
+
+    def can_normalization(self, y_pred, y_true, ex_data_iter):
+        # compute ACC
+        acc_original = np.mean([y_pred.argmax(1) == y_true])
+        message = 'original acc: %s' % acc_original
+
+        # compute uncertainty
+        k = 3
+        y_pred_topk = np.sort(y_pred, axis=1)[:, -k:]
+        y_pred_topk /= y_pred_topk.sum(axis=1, keepdims=True)
+        y_pred_uncertainty =\
+            -(y_pred_topk * np.log(y_pred_topk)).sum(1) / np.log(k)
+
+        # choose threshold
+        # print(np.sort(y_pred_uncertainty)[-100:].tolist())
+        threshold = 0.7
+        y_pred_confident = y_pred[y_pred_uncertainty < threshold]
+        y_pred_unconfident = y_pred[y_pred_uncertainty >= threshold]
+        y_true_confident = y_true[y_pred_uncertainty < threshold]
+        y_true_unconfident = y_true[y_pred_uncertainty >= threshold]
+
+        # compute ACC again for high and low confidence sets
+        acc_confident = (y_pred_confident.argmax(1) == y_true_confident).mean() \
+            if len(y_true_confident) else 0.
+        acc_unconfident = (y_pred_unconfident.argmax(1) == y_true_unconfident).mean() \
+            if len(y_true_unconfident) else 0.
+        message += '   (%s) confident acc: %s' % (len(y_true_confident),
+                                                  acc_confident)
+        message += '   (%s) unconfident acc: %s' % (len(y_true_unconfident),
+                                                    acc_unconfident)
+
+        # get prior distribution from training set
+        prior = np.zeros(self.func_model.num_intent)
+        for _, (batch, batch_size) in ex_data_iter:
+            for intent_label in batch['intent_label']:
+                prior[intent_label] += 1.
+
+        prior /= prior.sum()
+
+        # revise each sample from the low confidence set, and compute new ACC
+        right, alpha, iters = 0, 1, 1
+        for i, y in enumerate(y_pred_unconfident):
+            Y = np.concatenate([y_pred_confident, y[None]], axis=0)
+            for j in range(iters):
+                Y = Y**alpha
+                Y /= Y.mean(axis=0, keepdims=True)
+                Y *= prior[None]
+                Y /= Y.sum(axis=1, keepdims=True)
+            y = Y[-1]
+            if y.argmax() == y_true_unconfident[i]:
+                right += 1
+
+        # get final ACC
+        acc_final = \
+            (acc_confident * len(y_pred_confident) + right) / \
+            len(y_pred)
+        if len(y_pred_unconfident):
+            message += '   new unconfident acc: %s' % (
+                right / len(y_pred_unconfident))
+        else:
+            message += '   no unconfident predictions'
+        message += '   final acc: %s' % acc_final
+        return acc_original, acc_final, message
 
-            self.invoke_hook(TrainerStages.after_train_epoch)
-            # Value changed after the hooks are invoked, do not move them above the invoke_hook code.
-            self._inner_iter = 0
-            self._epoch += 1
-            if self._stop_training:
-                break
-
-        self.invoke_hook(TrainerStages.after_run)
-
-    def train_step(self, model, inputs):
-        """ Perform a training step on a batch of inputs.
-
-        Subclass and override to inject custom behavior.
-
-        Args:
-            model (`TorchModel`): The model to train.
-            inputs (`Dict[str, Union[torch.Tensor, Any]]`):
-                The inputs and targets of the model.
+    def train_epoch(self, train_label_iter, train_nolabel_iter,
+                    valid_label_iter, valid_nolabel_iter):
+        """
+        Train an epoch.
+        """
+        times = []
+        self.epoch += 1
+        self.batch_metrics_tracker_label.clear()
+        self.token_metrics_tracker_label.clear()
+        self.batch_metrics_tracker_nolabel.clear()
+        self.token_metrics_tracker_nolabel.clear()
+
+        num_label_batches = len(train_label_iter)
+        num_nolabel_batches = len(
+            train_nolabel_iter) if train_nolabel_iter is not None else 0
+        num_batches = max(num_label_batches, num_nolabel_batches)
+
+        train_label_iter_loop = iter(train_label_iter)
+        train_nolabel_iter_loop = iter(
+            train_nolabel_iter) if train_nolabel_iter is not None else None
+        report_for_unlabeled_data = True if train_nolabel_iter is not None else False
+
+        for batch_id in range(1, num_batches + 1):
+            # Do a training iteration
+            start_time = time.time()
+            batch_list, batch_size_list, with_label_list, loss_list, metrics_list = [], [], [], [], []
+            data_file_list = []
+
+            # collect batch for labeled data
+            try:
+                data_file_label, (
+                    batch_label,
+                    batch_size_label) = next(train_label_iter_loop)
+            except StopIteration:
+                train_label_iter_loop = iter(train_label_iter)
+                data_file_label, (
+                    batch_label,
+                    batch_size_label) = next(train_label_iter_loop)
+            batch_list.append(batch_label)
+            batch_size_list.append(batch_size_label)
+            with_label_list.append(True)
+            data_file_list.append(data_file_label)
+
+            # collect batch for unlabeled data
+            if train_nolabel_iter is not None:
+                try:
+                    data_file_nolabel, (
+                        batch_nolabel,
+                        batch_size_nolabel) = next(train_nolabel_iter_loop)
+                except StopIteration:
+                    train_nolabel_iter_loop = iter(train_nolabel_iter)
+                    data_file_nolabel, (
+                        batch_nolabel,
+                        batch_size_nolabel) = next(train_nolabel_iter_loop)
+                batch_list.append(batch_nolabel)
+                batch_size_list.append(batch_size_nolabel)
+                with_label_list.append(False)
+                data_file_list.append(data_file_nolabel)
+
+            # forward labeled batch and unlabeled batch and collect outputs, respectively
+            for (batch, batch_size, with_label, data_file) in \
+                    zip(batch_list, batch_size_list, with_label_list, data_file_list):
+                batch = type(batch)(
+                    map(lambda kv: (kv[0], self.to_tensor(kv[1])),
+                        batch.items()))
+                if self.example and with_label:
+                    current_dataset = train_label_iter.data_file_to_dataset[
+                        data_file]
+                    example_batch = self.reader.retrieve_examples(
+                        dataset=current_dataset,
+                        labels=batch['intent_label'],
+                        inds=batch['ids'],
+                        task='intent')
+                    example_batch = type(example_batch)(
+                        map(lambda kv: (kv[0], self.to_tensor(kv[1])),
+                            example_batch.items()))
+                    for k, v in example_batch.items():
+                        batch[k] = v
+                batch['epoch'] = self.epoch
+                batch['num_steps'] = self.batch_num
+                metrics = self.model(
+                    batch,
+                    is_training=True,
+                    with_label=with_label,
+                    data_file=data_file)
+                loss, metrics = self.balance_metrics(
+                    metrics=metrics, batch_size=batch_size)
+                loss_list.append(loss)
+                metrics_list.append(metrics)
+
+            # combine loss for labeled data and unlabeled data
+            # TODO change the computation of combined loss of labeled batch and unlabeled batch
+            loss = loss_list[0] if len(
+                loss_list) == 1 else loss_list[0] + loss_list[1]
+
+            # optimization procedure
+            self.func_model._optimize(
+                loss, optimizer=self.optimizer, lr_scheduler=self.lr_scheduler)
+            elapsed = time.time() - start_time
+            times.append(elapsed)
+            self.batch_num += 1
+
+            # track metrics and log temporary message
+            for (batch_size, metrics,
+                 with_label) in zip(batch_size_list, metrics_list,
+                                    with_label_list):
+                self.track_and_log_message(
+                    metrics=metrics,
+                    batch_id=batch_id,
+                    batch_size=batch_size,
+                    num_batches=num_batches,
+                    times=times,
+                    with_label=with_label)
+
+            # evaluate
+            if self.valid_steps > 0 and valid_label_iter is not None and valid_nolabel_iter is not None \
+                    and batch_id % self.valid_steps == 0:
+                self.evaluate(
+                    data_label_iter=valid_label_iter,
+                    data_nolabel_iter=valid_nolabel_iter)
+
+        # compute accuracy for valid dataset
+        accuracy = self.infer(
+            data_iter=valid_label_iter, ex_data_iter=train_label_iter)
+
+        # report summary message and save checkpoints
+        self.save_and_log_message(
+            report_for_unlabeled_data, cur_valid_metric=-accuracy)
+
+    def forward(self, batch):
+        pred = []
+
+        with torch.no_grad():
+            batch = type(batch)(
+                map(lambda kv: (kv[0], self.to_tensor(kv[1])), batch.items()))
+            result = self.model.infer(inputs=batch)
+            result = {
+                name: result[name].cpu().detach().numpy()
+                for name in result
+            }
+            intent_probs = result['intent_probs']
+            if self.can_norm:
+                pred += [intent_probs]
+            else:
+                pred += np.argmax(intent_probs, axis=1).tolist()
 
-                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the
-                argument `labels`. Check your model's documentation for all accepted arguments.
+        return pred
 
-        Return:
-            `torch.Tensor`: The tensor with training loss on this batch.
+    def infer(self, data_iter, num_batches=None, ex_data_iter=None):
         """
-        self.model.unet.train()
-        if self.modifier_token is not None:
-            self.model.text_encoder.train()
-        self._mode = ModeKeys.TRAIN
-
-        batch = next(self.iter_train_dataloader)
-        # Convert images to latent space
-        latents = self.model.vae.encode(batch['pixel_values'].to(
-            dtype=torch.float32).to(self.device)).latent_dist.sample()
-        latents = latents * self.model.vae.config.scaling_factor
-
-        # Sample noise that we'll add to the latents
-        noise = torch.randn_like(latents)
-        bsz = latents.shape[0]
-        # Sample a random timestep for each image
-        timesteps = torch.randint(
-            0,
-            self.model.noise_scheduler.config.num_train_timesteps, (bsz, ),
-            device=latents.device)
-        timesteps = timesteps.long()
-
-        # Add noise to the latents according to the noise magnitude at each timestep
-        # (this is the forward diffusion process)
-        noisy_latents = self.model.noise_scheduler.add_noise(
-            latents, noise, timesteps)
-
-        # Get the text embedding for conditioning
-        encoder_hidden_states = self.model.text_encoder(batch['input_ids'].to(
-            self.device))[0]
-
-        # Predict the noise residual
-        model_pred = self.model.unet(noisy_latents, timesteps,
-                                     encoder_hidden_states).sample
-
-        # Get the target for loss depending on the prediction type
-        if self.model.noise_scheduler.config.prediction_type == 'epsilon':
-            target = noise
-        elif self.model.noise_scheduler.config.prediction_type == 'v_prediction':
-            target = self.model.noise_scheduler.get_velocity(
-                latents, noise, timesteps)
-        else:
-            raise ValueError(
-                f'Unknown prediction type {self.model.noise_scheduler.config.prediction_type}'
-            )
+        Inference interface.
+        """
+        self.logger.info('Generation starts ...')
+        infer_save_file = os.path.join(self.save_dir,
+                                       f'infer_{self.epoch}.result.json')
+
+        # Inference
+        batch_cnt = 0
+        pred, true = [], []
+        outputs, labels = [], []
+        begin_time = time.time()
+
+        with torch.no_grad():
+            if self.example:
+                for _, (batch, batch_size) in tqdm(
+                        ex_data_iter, desc='Building train memory.'):
+                    batch = type(batch)(
+                        map(lambda kv: (kv[0], self.to_tensor(kv[1])),
+                            batch.items()))
+                    result = self.model.infer(inputs=batch)
+                    result = {
+                        name: result[name].cpu().detach().numpy()
+                        for name in result
+                    }
+                    outputs.append(torch.from_numpy(result['features']))
+                    labels += batch['intent_label'].tolist()
+
+                mem = torch.cat(outputs, dim=0)
+                mem = mem.cuda() if self.func_model.use_gpu else mem
+                labels = torch.LongTensor(labels).unsqueeze(0)
+                labels = labels.cuda() if self.func_model.use_gpu else labels
+                self.logger.info(f'Memory size: {mem.size()}')
+
+            for _, (batch, batch_size) in tqdm(data_iter, total=num_batches):
+                batch = type(batch)(
+                    map(lambda kv: (kv[0], self.to_tensor(kv[1])),
+                        batch.items()))
+                result = self.model.infer(inputs=batch)
+                result = {
+                    name: result[name].cpu().detach().numpy()
+                    for name in result
+                }
+
+                if self.example:
+                    features = torch.from_numpy(result['features'])
+                    features = features.cuda(
+                    ) if self.func_model.use_gpu else features
+                    probs = torch.softmax(features.mm(mem.t()), dim=-1)
+                    intent_probs = torch.zeros(
+                        probs.size(0), self.func_model.num_intent)
+                    intent_probs = intent_probs.cuda(
+                    ) if self.func_model.use_gpu else intent_probs
+                    intent_probs = intent_probs.scatter_add(
+                        -1, labels.repeat(probs.size(0), 1), probs)
+                    intent_probs = intent_probs.cpu().detach().numpy()
+                else:
+                    intent_probs = result['intent_probs']
 
-        if self.with_prior_preservation:
-            # Chunk the noise and model_pred into two parts and compute the loss on each part separately.
-            model_pred, model_pred_prior = torch.chunk(model_pred, 2, dim=0)
-            target, target_prior = torch.chunk(target, 2, dim=0)
-            mask = torch.chunk(batch['mask'].to(self.device), 2, dim=0)[0]
-            # Compute instance loss
-            loss = F.mse_loss(
-                model_pred.float(), target.float(), reduction='none')
-            loss = ((loss * mask).sum([1, 2, 3]) / mask.sum([1, 2, 3])).mean()
-
-            # Compute prior loss
-            prior_loss = F.mse_loss(
-                model_pred_prior.float(),
-                target_prior.float(),
-                reduction='mean')
+                if self.can_norm:
+                    pred += [intent_probs]
+                    true += batch['intent_label'].cpu().detach().tolist()
+                else:
+                    pred += np.argmax(intent_probs, axis=1).tolist()
+                    true += batch['intent_label'].cpu().detach().tolist()
+
+                batch_cnt += 1
+                if batch_cnt == num_batches:
+                    break
 
-            # Add the prior loss to the instance loss.
-            loss = loss + self.prior_loss_weight * prior_loss
+        if self.can_norm:
+            true = np.array(true)
+            pred = np.concatenate(pred, axis=0)
+            acc_original, acc_final, message = self.can_normalization(
+                y_pred=pred, y_true=true, ex_data_iter=ex_data_iter)
+            accuracy = max(acc_original, acc_final)
+            infer_results = {
+                'accuracy': accuracy,
+                'pred_labels': pred.tolist(),
+                'message': message
+            }
+            metrics_message = f'Accuracy: {accuracy}   {message}'
         else:
-            mask = batch['mask'].to(self.device)
-            loss = F.mse_loss(
-                model_pred.float(), target.float(), reduction='none')
-            loss = ((loss * mask).sum([1, 2, 3]) / mask.sum([1, 2, 3])).mean()
-
-        train_outputs = {}
-        train_outputs[OutputKeys.LOSS] = loss
-
-        # add model output info to log
-        if 'log_vars' not in train_outputs:
-            default_keys_pattern = ['loss']
-            match_keys = set([])
-            for key_p in default_keys_pattern:
-                match_keys.update(
-                    [key for key in train_outputs.keys() if key_p in key])
-
-            log_vars = {}
-            for key in match_keys:
-                value = train_outputs.get(key, None)
-                if value is not None:
-                    if is_dist():
-                        value = value.data.clone().to('cuda')
-                        dist.all_reduce(value.div_(dist.get_world_size()))
-                    log_vars.update({key: value.item()})
-            self.log_buffer.update(log_vars)
+            accuracy = sum(p == t for p, t in zip(pred, true)) / len(pred)
+            infer_results = {'accuracy': accuracy, 'pred_labels': pred}
+            metrics_message = f'Accuracy: {accuracy}'
+
+        self.logger.info(f'Saved inference results to {infer_save_file}')
+        with open(infer_save_file, 'w') as fp:
+            json.dump(infer_results, fp, indent=2)
+        message_prefix = f'[Infer][{self.epoch}]'
+        time_cost = f'TIME-{time.time() - begin_time:.3f}'
+        message = '   '.join([message_prefix, metrics_message, time_cost])
+        self.logger.info(message)
+        return accuracy
+
+    def track_and_log_message(self, metrics, batch_id, batch_size, num_batches,
+                              times, with_label):
+        # track metrics
+        batch_metrics_tracker = self.batch_metrics_tracker_label if with_label else self.batch_metrics_tracker_nolabel
+        token_metrics_tracker = self.token_metrics_tracker_label if with_label else self.token_metrics_tracker_nolabel
+
+        metrics = {
+            k: v.cpu().detach().numpy() if isinstance(v, torch.Tensor) else v
+            for k, v in metrics.items()
+        }
+        mlm_num = metrics.pop('mlm_num', 0)
+
+        batch_metrics = {k: v for k, v in metrics.items() if 'token' not in k}
+        token_metrics = {k: v for k, v in metrics.items() if 'token' in k}
+        batch_metrics_tracker.update(batch_metrics, batch_size)
+        token_metrics_tracker.update(token_metrics, mlm_num)
+
+        # log message
+        if self.log_steps > 0 and batch_id % self.log_steps == 0:
+            batch_metrics_message = batch_metrics_tracker.value()
+            token_metrics_message = token_metrics_tracker.value()
+            label_prefix = 'Labeled' if with_label else 'Unlabeled'
+            message_prefix = f'[Train][{self.epoch}][{batch_id}/{num_batches}][{label_prefix}]'
+            avg_time = f'AVG_Time-{sum(times[-self.log_steps:]) / self.log_steps:.3f}'
+            message = '   '.join([
+                message_prefix, batch_metrics_message, token_metrics_message,
+                avg_time
+            ])
+            self.logger.info(message)
+
+    def save_and_log_message(self,
+                             report_for_unlabeled_data,
+                             cur_valid_metric=None):
+        # report message
+        batch_metrics_message = self.batch_metrics_tracker_label.summary()
+        token_metrics_message = self.token_metrics_tracker_label.summary()
+        message_prefix = f'[Valid][{self.epoch}][Labeled]'
+        message = '   '.join(
+            [message_prefix, batch_metrics_message, token_metrics_message])
+        self.logger.info(message)
+        if report_for_unlabeled_data:
+            batch_metrics_message = self.batch_metrics_tracker_nolabel.summary(
+            )
+            token_metrics_message = self.token_metrics_tracker_nolabel.summary(
+            )
+            message_prefix = f'[Valid][{self.epoch}][Unlabeled]'
+            message = '   '.join(
+                [message_prefix, batch_metrics_message, token_metrics_message])
+            self.logger.info(message)
+
+        # save checkpoints
+        assert cur_valid_metric is not None
+        if self.is_decreased_valid_metric:
+            is_best = cur_valid_metric < self.best_valid_metric
         else:
-            self.log_buffer.update(train_outputs['log_vars'])
+            is_best = cur_valid_metric > self.best_valid_metric
+        if is_best:
+            self.best_valid_metric = cur_valid_metric
+        self.save(is_best)
+
+    def balance_metrics(self, metrics, batch_size):
+        if self.gpu > 1:
+            for metric in metrics:
+                if metric is not None:
+                    assert len(metric) == self.gpu
+
+            intent_loss, mlm, token_mlm, mlm_num, kl, con = metrics
+            metrics = {}
+
+            intent_loss = torch.mean(intent_loss)
+            metrics['intent_loss'] = intent_loss
+            loss = intent_loss
+
+            if mlm is not None:
+                mlm_num = torch.sum(mlm_num)
+                token_mlm = torch.sum(mlm) * (batch_size / self.gpu) / mlm_num
+                mlm = torch.mean(mlm)
+                metrics['mlm_num'] = mlm_num
+                metrics['token_mlm'] = token_mlm
+                metrics['mlm'] = mlm
+                loss = loss + (token_mlm if self.func_model.token_loss else
+                               mlm) * self.func_model.mlm_ratio
+
+            if kl is not None:
+                kl = torch.mean(kl)
+                metrics['kl'] = kl
+                loss = loss + kl * self.func_model.kl_ratio
+
+            if con is not None:
+                con = torch.mean(con)
+                metrics['con'] = con
+                loss = loss + con
+
+            metrics['loss'] = loss
 
-        self.train_outputs = train_outputs
+        assert 'loss' in metrics
+        return metrics['loss'], metrics
```

### Comparing `modelscope-1.8.0/modelscope/trainers/multi_modal/dreambooth_diffusion/dreambooth_diffusion_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/multi_modal/dreambooth_diffusion/dreambooth_diffusion_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/multi_modal/efficient_diffusion_tuning/efficient_diffusion_tuning_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/multi_modal/efficient_diffusion_tuning/efficient_diffusion_tuning_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/multi_modal/lora_diffusion/lora_diffusion_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/multi_modal/lora_diffusion/lora_diffusion_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/multi_modal/mgeo_ranking_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/multi_modal/mgeo_ranking_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/multi_modal/mplug/mplug_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/multi_modal/mplug/mplug_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/multi_modal/ofa/ofa_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/multi_modal/ofa/ofa_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/multi_modal/ofa/ofa_trainer_utils.py` & `modelscope-1.8.0rc0/modelscope/trainers/multi_modal/ofa/ofa_trainer_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/multi_modal/stable_diffusion/stable_diffusion_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/multi_modal/stable_diffusion/stable_diffusion_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/multi_modal/team/team_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/multi_modal/team/team_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/multi_modal/team/team_trainer_utils.py` & `modelscope-1.8.0rc0/modelscope/trainers/multi_modal/team/team_trainer_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/nlp/__init__.py` & `modelscope-1.8.0rc0/modelscope/trainers/nlp/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/nlp/csanmt_translation_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/nlp/csanmt_translation_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/nlp/document_grounded_dialog_generate_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/nlp/document_grounded_dialog_generate_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/nlp/document_grounded_dialog_rerank_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/nlp/document_grounded_dialog_rerank_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/nlp/document_grounded_dialog_retrieval_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/nlp/document_grounded_dialog_retrieval_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/nlp/faq_question_answering_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/nlp/faq_question_answering_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/nlp/gpt3_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/nlp/gpt3_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/nlp/gpt_moe_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/nlp/gpt_moe_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/nlp/plug_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/nlp/plug_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/nlp/sentence_embedding_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/nlp/sentence_embedding_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/nlp/sequence_classification_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/nlp/sequence_classification_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/nlp/siamese_uie_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/nlp/siamese_uie_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/nlp/space/dialog_intent_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/nlp/space/dialog_intent_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/nlp/space/dialog_modeling_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/nlp/space/dialog_modeling_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/nlp/space/eval.py` & `modelscope-1.8.0rc0/modelscope/trainers/nlp/space/eval.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/nlp/space/metrics/metrics_tracker.py` & `modelscope-1.8.0rc0/modelscope/trainers/nlp/space/metrics/metrics_tracker.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/nlp/space/trainer/gen_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/nlp/space/trainer/gen_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/nlp/table_question_answering_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/nlp/table_question_answering_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/nlp/text_generation_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/nlp/text_generation_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/nlp/text_ranking_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/nlp/text_ranking_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/nlp/translation_evaluation_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/nlp/translation_evaluation_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/nlp_trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/nlp_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/optimizer/builder.py` & `modelscope-1.8.0rc0/modelscope/trainers/optimizer/builder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/parallel/builder.py` & `modelscope-1.8.0rc0/modelscope/trainers/parallel/builder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/parallel/utils.py` & `modelscope-1.8.0rc0/modelscope/trainers/parallel/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/trainer.py` & `modelscope-1.8.0rc0/modelscope/trainers/trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/training_args.py` & `modelscope-1.8.0rc0/modelscope/trainers/training_args.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/utils/inference.py` & `modelscope-1.8.0rc0/modelscope/trainers/utils/inference.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/trainers/utils/log_buffer.py` & `modelscope-1.8.0rc0/modelscope/trainers/utils/log_buffer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/ast_index_file.py` & `modelscope-1.8.0rc0/modelscope/utils/ast_index_file.py`

 * *Files 16% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.7676688435407567%*

 * *Differences: {"'files_mtime'": "{'TEMPLATE_PATH/models/base/base_model.py': 1690807232.3222961, "*

 * *                  "'TEMPLATE_PATH/models/base/base_head.py': 1690807232.3222961, "*

 * *                  "'TEMPLATE_PATH/models/base/base_torch_model.py': 1690807232.3222961, "*

 * *                  "'TEMPLATE_PATH/models/base/base_torch_head.py': 1690807232.3222961, "*

 * *                  "'TEMPLATE_PATH/models/cv/image_quality_assessment_degradation/image_quality_assessment_degradation.py': "*

 * *                  '1690807232.3502963, ' […]*

```diff
@@ -1,1756 +1,1751 @@
 {
     "files_mtime": {
-        "TEMPLATE_PATH/exporters/audio/ans_dfsmn_exporter.py": 1684224035.4982638,
-        "TEMPLATE_PATH/exporters/base.py": 1691036741.2793348,
-        "TEMPLATE_PATH/exporters/builder.py": 1670492342.9322376,
-        "TEMPLATE_PATH/exporters/cv/cartoon_translation_exporter.py": 1691036741.2793348,
-        "TEMPLATE_PATH/exporters/cv/face_detection_scrfd_exporter.py": 1679578012.59293,
-        "TEMPLATE_PATH/exporters/cv/object_detection_damoyolo_exporter.py": 1687774806.9874692,
-        "TEMPLATE_PATH/exporters/multi_modal/stable_diffusion_exporter.py": 1689760581.5356867,
-        "TEMPLATE_PATH/exporters/nlp/csanmt_for_translation_exporter.py": 1691036741.2793348,
-        "TEMPLATE_PATH/exporters/nlp/model_for_token_classification_exporter.py": 1681971907.399036,
-        "TEMPLATE_PATH/exporters/nlp/sbert_for_sequence_classification_exporter.py": 1679578012.59293,
-        "TEMPLATE_PATH/exporters/nlp/sbert_for_zero_shot_classification_exporter.py": 1679578012.59293,
-        "TEMPLATE_PATH/exporters/tf_model_exporter.py": 1679578012.5939302,
-        "TEMPLATE_PATH/exporters/torch_model_exporter.py": 1679578012.5939302,
-        "TEMPLATE_PATH/metrics/accuracy_metric.py": 1679578012.59593,
-        "TEMPLATE_PATH/metrics/action_detection_evaluator.py": 1679578012.59593,
-        "TEMPLATE_PATH/metrics/audio_noise_metric.py": 1679578012.59593,
-        "TEMPLATE_PATH/metrics/base.py": 1679578012.59593,
-        "TEMPLATE_PATH/metrics/bleu_metric.py": 1679578012.59593,
-        "TEMPLATE_PATH/metrics/builder.py": 1684224035.500264,
-        "TEMPLATE_PATH/metrics/ciderD/ciderD.py": 1670492342.9342375,
-        "TEMPLATE_PATH/metrics/ciderD/ciderD_scorer.py": 1679578012.59593,
-        "TEMPLATE_PATH/metrics/image_color_enhance_metric.py": 1679578012.59593,
-        "TEMPLATE_PATH/metrics/image_colorization_metric.py": 1681971907.401036,
-        "TEMPLATE_PATH/metrics/image_denoise_metric.py": 1679578012.59593,
-        "TEMPLATE_PATH/metrics/image_inpainting_metric.py": 1679578012.59593,
-        "TEMPLATE_PATH/metrics/image_instance_segmentation_metric.py": 1679578012.59593,
-        "TEMPLATE_PATH/metrics/image_portrait_enhancement_metric.py": 1679578012.59593,
-        "TEMPLATE_PATH/metrics/image_quality_assessment_degradation_metric.py": 1679578012.5969303,
-        "TEMPLATE_PATH/metrics/image_quality_assessment_mos_metric.py": 1679578012.5969303,
-        "TEMPLATE_PATH/metrics/inbatch_recall_metric.py": 1679578012.5969303,
-        "TEMPLATE_PATH/metrics/loss_metric.py": 1679578012.5969303,
-        "TEMPLATE_PATH/metrics/map_metric.py": 1679578012.5969303,
-        "TEMPLATE_PATH/metrics/movie_scene_segmentation_metric.py": 1679578012.5969303,
-        "TEMPLATE_PATH/metrics/ned_metric.py": 1679578012.5969303,
-        "TEMPLATE_PATH/metrics/ocr_recognition_metric.py": 1679578012.5969303,
-        "TEMPLATE_PATH/metrics/ppl_metric.py": 1679578012.5969303,
-        "TEMPLATE_PATH/metrics/prediction_saving_wrapper.py": 1679578012.5969303,
-        "TEMPLATE_PATH/metrics/referring_video_object_segmentation_metric.py": 1679578012.5969303,
-        "TEMPLATE_PATH/metrics/sequence_classification_metric.py": 1679578012.5969303,
-        "TEMPLATE_PATH/metrics/text_generation_metric.py": 1689760581.5376868,
-        "TEMPLATE_PATH/metrics/text_ranking_metric.py": 1679578012.5969303,
-        "TEMPLATE_PATH/metrics/token_classification_metric.py": 1679578012.5969303,
-        "TEMPLATE_PATH/metrics/translation_evaluation_metric.py": 1684224035.500264,
-        "TEMPLATE_PATH/metrics/video_frame_interpolation_metric.py": 1679578012.5969303,
-        "TEMPLATE_PATH/metrics/video_stabilization_metric.py": 1679578012.5969303,
-        "TEMPLATE_PATH/metrics/video_summarization_metric.py": 1679578012.5969303,
-        "TEMPLATE_PATH/metrics/video_super_resolution_metric/matlab_functions.py": 1679578012.5979302,
-        "TEMPLATE_PATH/metrics/video_super_resolution_metric/metric_util.py": 1679578012.5979302,
-        "TEMPLATE_PATH/metrics/video_super_resolution_metric/niqe.py": 1679578012.5979302,
-        "TEMPLATE_PATH/metrics/video_super_resolution_metric/video_super_resolution_metric.py": 1679578012.5979302,
-        "TEMPLATE_PATH/models/audio/aec/layers/activations.py": 1670492342.9352376,
-        "TEMPLATE_PATH/models/audio/aec/layers/affine_transform.py": 1670492342.9352376,
-        "TEMPLATE_PATH/models/audio/aec/layers/deep_fsmn.py": 1670492342.9352376,
-        "TEMPLATE_PATH/models/audio/aec/layers/layer_base.py": 1670492342.9352376,
-        "TEMPLATE_PATH/models/audio/aec/layers/uni_deep_fsmn.py": 1670492342.9352376,
-        "TEMPLATE_PATH/models/audio/aec/network/loss.py": 1670492342.9352376,
-        "TEMPLATE_PATH/models/audio/aec/network/modulation_loss.py": 1670492342.9352376,
-        "TEMPLATE_PATH/models/audio/aec/network/se_net.py": 1670492342.9352376,
-        "TEMPLATE_PATH/models/audio/ans/complex_nn.py": 1679578012.5979302,
-        "TEMPLATE_PATH/models/audio/ans/conv_stft.py": 1684224035.500264,
-        "TEMPLATE_PATH/models/audio/ans/denoise_net.py": 1679578012.5979302,
-        "TEMPLATE_PATH/models/audio/ans/frcrn.py": 1679578012.5979302,
-        "TEMPLATE_PATH/models/audio/ans/layers/activations.py": 1679578012.5979302,
-        "TEMPLATE_PATH/models/audio/ans/layers/affine_transform.py": 1679578012.5979302,
-        "TEMPLATE_PATH/models/audio/ans/layers/layer_base.py": 1679578012.5979302,
-        "TEMPLATE_PATH/models/audio/ans/layers/uni_deep_fsmn.py": 1679578012.5979302,
-        "TEMPLATE_PATH/models/audio/ans/se_module_complex.py": 1670492342.9352376,
-        "TEMPLATE_PATH/models/audio/ans/unet.py": 1670492342.9362376,
-        "TEMPLATE_PATH/models/audio/asr/generic_automatic_speech_recognition.py": 1684224035.500264,
-        "TEMPLATE_PATH/models/audio/asr/wenet_automatic_speech_recognition.py": 1670492342.9362376,
-        "TEMPLATE_PATH/models/audio/itn/generic_inverse_text_processing.py": 1679578012.5989301,
-        "TEMPLATE_PATH/models/audio/kws/farfield/fsmn.py": 1670492342.9362376,
-        "TEMPLATE_PATH/models/audio/kws/farfield/fsmn_sele_v2.py": 1681971907.401036,
-        "TEMPLATE_PATH/models/audio/kws/farfield/fsmn_sele_v3.py": 1684224035.500264,
-        "TEMPLATE_PATH/models/audio/kws/farfield/model.py": 1684224035.500264,
-        "TEMPLATE_PATH/models/audio/kws/farfield/model_def.py": 1670492342.9362376,
-        "TEMPLATE_PATH/models/audio/kws/generic_key_word_spotting.py": 1670492342.9362376,
-        "TEMPLATE_PATH/models/audio/kws/nearfield/cmvn.py": 1679578012.5989301,
-        "TEMPLATE_PATH/models/audio/kws/nearfield/fsmn.py": 1681971907.401036,
-        "TEMPLATE_PATH/models/audio/kws/nearfield/model.py": 1681971907.401036,
-        "TEMPLATE_PATH/models/audio/punc/generic_punctuation.py": 1679578012.5989301,
-        "TEMPLATE_PATH/models/audio/separation/layer_norm.py": 1679578012.5989301,
-        "TEMPLATE_PATH/models/audio/separation/mossformer.py": 1679578012.5989301,
-        "TEMPLATE_PATH/models/audio/separation/mossformer_block.py": 1679578012.5989301,
-        "TEMPLATE_PATH/models/audio/separation/mossformer_conv_module.py": 1679578012.5989301,
-        "TEMPLATE_PATH/models/audio/sv/DTDNN.py": 1689760581.5376868,
-        "TEMPLATE_PATH/models/audio/sv/DTDNN_layers.py": 1681971907.401036,
-        "TEMPLATE_PATH/models/audio/sv/ERes2Net.py": 1691036741.280335,
-        "TEMPLATE_PATH/models/audio/sv/ERes2Net_aug.py": 1689760581.5376868,
-        "TEMPLATE_PATH/models/audio/sv/cluster_backend.py": 1691036741.280335,
-        "TEMPLATE_PATH/models/audio/sv/ecapa_tdnn.py": 1689760581.5376868,
-        "TEMPLATE_PATH/models/audio/sv/fusion.py": 1687774806.9894693,
-        "TEMPLATE_PATH/models/audio/sv/generic_speaker_verification.py": 1679578012.5999303,
-        "TEMPLATE_PATH/models/audio/sv/lanuage_recognition_model.py": 1691036741.2813349,
-        "TEMPLATE_PATH/models/audio/sv/pooling_layers.py": 1687774806.9894693,
-        "TEMPLATE_PATH/models/audio/sv/rdino.py": 1684224035.500264,
-        "TEMPLATE_PATH/models/audio/sv/speaker_change_locator.py": 1689760581.5376868,
-        "TEMPLATE_PATH/models/audio/sv/speaker_diarization_dialogue_detection.py": 1691036741.2813349,
-        "TEMPLATE_PATH/models/audio/sv/speaker_diarization_semantic_speaker_turn_detection.py": 1691036741.2813349,
-        "TEMPLATE_PATH/models/audio/tts/sambert_hifi.py": 1679578012.5999303,
-        "TEMPLATE_PATH/models/audio/tts/voice.py": 1684224035.5012639,
-        "TEMPLATE_PATH/models/base/base_head.py": 1679578012.5999303,
-        "TEMPLATE_PATH/models/base/base_model.py": 1691036741.2813349,
-        "TEMPLATE_PATH/models/base/base_torch_head.py": 1670492342.9382377,
-        "TEMPLATE_PATH/models/base/base_torch_model.py": 1679578012.5999303,
-        "TEMPLATE_PATH/models/builder.py": 1679578012.5999303,
-        "TEMPLATE_PATH/models/cv/abnormal_object_detection/mmdet_model.py": 1679578012.5999303,
-        "TEMPLATE_PATH/models/cv/abnormal_object_detection/mmdet_ms/roi_head/mask_scoring_roi_head.py": 1679578012.6009302,
-        "TEMPLATE_PATH/models/cv/abnormal_object_detection/mmdet_ms/roi_head/roi_extractors/single_level_roi_extractor.py": 1679578012.6009302,
-        "TEMPLATE_PATH/models/cv/action_detection/action_detection_onnx.py": 1670492342.9382377,
-        "TEMPLATE_PATH/models/cv/action_detection/modules/action_detection_pytorch.py": 1679578012.6009302,
-        "TEMPLATE_PATH/models/cv/action_detection/modules/resnet.py": 1679578012.6009302,
-        "TEMPLATE_PATH/models/cv/action_recognition/models.py": 1670492342.9382377,
-        "TEMPLATE_PATH/models/cv/action_recognition/s3dg.py": 1670492342.9382377,
-        "TEMPLATE_PATH/models/cv/action_recognition/tada_convnext.py": 1670492342.9382377,
-        "TEMPLATE_PATH/models/cv/action_recognition/temporal_patch_shift_transformer.py": 1681971907.402036,
-        "TEMPLATE_PATH/models/cv/animal_recognition/resnet.py": 1670492342.9392376,
-        "TEMPLATE_PATH/models/cv/animal_recognition/splat.py": 1670492342.9392376,
-        "TEMPLATE_PATH/models/cv/bad_image_detecting/bad_image_detecting.py": 1679578012.6009302,
-        "TEMPLATE_PATH/models/cv/body_2d_keypoints/hrnet_basic_modules.py": 1670492342.9392376,
-        "TEMPLATE_PATH/models/cv/body_2d_keypoints/hrnet_v2.py": 1684224035.5012639,
-        "TEMPLATE_PATH/models/cv/body_2d_keypoints/w48.py": 1670492342.9392376,
-        "TEMPLATE_PATH/models/cv/body_3d_keypoints/cannonical_pose/body_3d_pose.py": 1681971907.402036,
-        "TEMPLATE_PATH/models/cv/body_3d_keypoints/cannonical_pose/canonical_pose_modules.py": 1679578012.6019304,
-        "TEMPLATE_PATH/models/cv/body_3d_keypoints/hdformer/backbone.py": 1679578012.6019304,
-        "TEMPLATE_PATH/models/cv/body_3d_keypoints/hdformer/block.py": 1679578012.6019304,
-        "TEMPLATE_PATH/models/cv/body_3d_keypoints/hdformer/directed_graph.py": 1681971907.402036,
-        "TEMPLATE_PATH/models/cv/body_3d_keypoints/hdformer/hdformer.py": 1679578012.6019304,
-        "TEMPLATE_PATH/models/cv/body_3d_keypoints/hdformer/hdformer_detector.py": 1679578012.6019304,
-        "TEMPLATE_PATH/models/cv/body_3d_keypoints/hdformer/skeleton.py": 1679578012.6019304,
-        "TEMPLATE_PATH/models/cv/cartoon/facelib/LK/lk.py": 1670492342.9392376,
-        "TEMPLATE_PATH/models/cv/cartoon/facelib/config.py": 1670492342.9392376,
-        "TEMPLATE_PATH/models/cv/cartoon/facelib/face_detector.py": 1670492342.9392376,
-        "TEMPLATE_PATH/models/cv/cartoon/facelib/face_landmark.py": 1684224035.5012639,
-        "TEMPLATE_PATH/models/cv/cartoon/facelib/facer.py": 1681971907.402036,
-        "TEMPLATE_PATH/models/cv/cartoon/loss.py": 1679578012.6019304,
-        "TEMPLATE_PATH/models/cv/cartoon/model_tf.py": 1679578012.6019304,
-        "TEMPLATE_PATH/models/cv/cartoon/mtcnn_pytorch/src/align_trans.py": 1670492342.9402375,
-        "TEMPLATE_PATH/models/cv/cartoon/mtcnn_pytorch/src/matlab_cp2tform.py": 1670492342.9402375,
-        "TEMPLATE_PATH/models/cv/cartoon/network.py": 1679578012.6019304,
-        "TEMPLATE_PATH/models/cv/cartoon/utils.py": 1679578012.6019304,
-        "TEMPLATE_PATH/models/cv/cmdssl_video_embedding/c3d.py": 1670492342.9402375,
-        "TEMPLATE_PATH/models/cv/cmdssl_video_embedding/resnet2p1d.py": 1670492342.9402375,
-        "TEMPLATE_PATH/models/cv/cmdssl_video_embedding/resnet3d.py": 1670492342.9402375,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/annotator.py": 1679578012.6029303,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/api.py": 1679578012.6029303,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/midas/base_model.py": 1679578012.6029303,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/midas/blocks.py": 1679578012.6029303,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/midas/dpt_depth.py": 1679578012.6029303,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/midas/midas_net.py": 1679578012.6029303,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/midas/midas_net_custom.py": 1679578012.6029303,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/midas/transforms.py": 1679578012.6029303,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/midas/vit.py": 1679578012.6029303,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/utils.py": 1679578012.6029303,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/mlsd/mbv2_mlsd_large.py": 1679578012.6029303,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/mlsd/utils.py": 1679578012.6039302,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/openpose/body.py": 1679578012.6039302,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/openpose/hand.py": 1679578012.6039302,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/openpose/model.py": 1679578012.6039302,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/openpose/util.py": 1679578012.6039302,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/controlnet.py": 1679578012.6039302,
-        "TEMPLATE_PATH/models/cv/crowd_counting/cc_model.py": 1670492342.9402375,
-        "TEMPLATE_PATH/models/cv/crowd_counting/hrnet_aspp_relu.py": 1684224035.5012639,
-        "TEMPLATE_PATH/models/cv/face_attribute_recognition/fair_face/face_attribute_recognition.py": 1670492342.9412377,
-        "TEMPLATE_PATH/models/cv/face_detection/mogface/models/detectors.py": 1670492342.9412377,
-        "TEMPLATE_PATH/models/cv/face_detection/mogface/models/mogface.py": 1670492342.9412377,
-        "TEMPLATE_PATH/models/cv/face_detection/mogface/models/mogprednet.py": 1670492342.9412377,
-        "TEMPLATE_PATH/models/cv/face_detection/mogface/models/resnet.py": 1670492342.9412377,
-        "TEMPLATE_PATH/models/cv/face_detection/mogface/models/utils.py": 1670492342.9412377,
-        "TEMPLATE_PATH/models/cv/face_detection/mtcnn/models/box_utils.py": 1670492342.9422376,
-        "TEMPLATE_PATH/models/cv/face_detection/mtcnn/models/detector.py": 1670492342.9432375,
-        "TEMPLATE_PATH/models/cv/face_detection/mtcnn/models/first_stage.py": 1670492342.9432375,
-        "TEMPLATE_PATH/models/cv/face_detection/mtcnn/models/get_nets.py": 1670492342.9432375,
-        "TEMPLATE_PATH/models/cv/face_detection/peppa_pig_face/LK/lk.py": 1679578012.6039302,
-        "TEMPLATE_PATH/models/cv/face_detection/peppa_pig_face/face_detector.py": 1679578012.6039302,
-        "TEMPLATE_PATH/models/cv/face_detection/peppa_pig_face/face_landmark.py": 1684224035.5012639,
-        "TEMPLATE_PATH/models/cv/face_detection/peppa_pig_face/facer.py": 1679578012.6039302,
-        "TEMPLATE_PATH/models/cv/face_detection/retinaface/detection.py": 1670492342.9442377,
-        "TEMPLATE_PATH/models/cv/face_detection/retinaface/models/net.py": 1670492342.9442377,
-        "TEMPLATE_PATH/models/cv/face_detection/retinaface/models/retinaface.py": 1670492342.9442377,
-        "TEMPLATE_PATH/models/cv/face_detection/retinaface/utils.py": 1670492342.9442377,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/damofd_detect.py": 1681971907.402036,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/core/bbox/transforms.py": 1679578012.6049304,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/core/post_processing/bbox_nms.py": 1689760581.5376868,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/auto_augment.py": 1679578012.6049304,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/formating.py": 1679578012.6049304,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/loading.py": 1679578012.6049304,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/transforms.py": 1679578012.6049304,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/retinaface.py": 1679578012.6049304,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/master_net.py": 1681971907.402036,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/mobilenet.py": 1679578012.6059303,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/resnet.py": 1679578012.6059303,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/dense_heads/scrfd_head.py": 1679578012.6059303,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/base.py": 1679578012.6059303,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/scrfd.py": 1679578012.6059303,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/single_stage.py": 1679578012.6069303,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/tinymog.py": 1679578012.6069303,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/preprocessor.py": 1679578012.6069303,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/scrfd_detect.py": 1679578012.6069303,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/tinymog_detect.py": 1679578012.6069303,
-        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/detection.py": 1670492342.9482377,
-        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/box_utils.py": 1670492342.9492376,
-        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/mb_tiny.py": 1670492342.9492376,
-        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/ssd/data_preprocessing.py": 1670492342.9492376,
-        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/ssd/fd_config.py": 1670492342.9492376,
-        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/ssd/mb_tiny_fd.py": 1670492342.9502378,
-        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/ssd/predictor.py": 1679578012.6069303,
-        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/ssd/ssd.py": 1670492342.9502378,
-        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/transforms.py": 1670492342.9502378,
-        "TEMPLATE_PATH/models/cv/face_emotion/efficient/model.py": 1679578012.6069303,
-        "TEMPLATE_PATH/models/cv/face_emotion/efficient/utils.py": 1679578012.6069303,
-        "TEMPLATE_PATH/models/cv/face_emotion/emotion_infer.py": 1679578012.6069303,
-        "TEMPLATE_PATH/models/cv/face_emotion/emotion_model.py": 1670492342.9502378,
-        "TEMPLATE_PATH/models/cv/face_emotion/face_alignment/face.py": 1670492342.9502378,
-        "TEMPLATE_PATH/models/cv/face_emotion/face_alignment/face_align.py": 1670492342.9502378,
-        "TEMPLATE_PATH/models/cv/face_generation/op/conv2d_gradfix.py": 1670492342.9512377,
-        "TEMPLATE_PATH/models/cv/face_generation/op/fused_act.py": 1670492342.9512377,
-        "TEMPLATE_PATH/models/cv/face_generation/op/upfirdn2d.py": 1670492342.9512377,
-        "TEMPLATE_PATH/models/cv/face_generation/stylegan2.py": 1670492342.9512377,
-        "TEMPLATE_PATH/models/cv/face_human_hand_detection/det_infer.py": 1670492342.9512377,
-        "TEMPLATE_PATH/models/cv/face_human_hand_detection/ghost_pan.py": 1679578012.6069303,
-        "TEMPLATE_PATH/models/cv/face_human_hand_detection/nanodet_plus_head.py": 1679578012.6079304,
-        "TEMPLATE_PATH/models/cv/face_human_hand_detection/one_stage_detector.py": 1679578012.6079304,
-        "TEMPLATE_PATH/models/cv/face_human_hand_detection/shufflenetv2.py": 1679578012.6079304,
-        "TEMPLATE_PATH/models/cv/face_human_hand_detection/utils.py": 1679578012.6079304,
-        "TEMPLATE_PATH/models/cv/face_recognition/align_face.py": 1679578012.6079304,
-        "TEMPLATE_PATH/models/cv/face_recognition/torchkit/backbone/arcface_backbone.py": 1670492342.9522376,
-        "TEMPLATE_PATH/models/cv/face_recognition/torchkit/backbone/common.py": 1670492342.9522376,
-        "TEMPLATE_PATH/models/cv/face_recognition/torchkit/backbone/facemask_backbone.py": 1670492342.9522376,
-        "TEMPLATE_PATH/models/cv/face_recognition/torchkit/backbone/model_irse.py": 1670492342.9522376,
-        "TEMPLATE_PATH/models/cv/face_recognition/torchkit/backbone/model_resnet.py": 1670492342.9522376,
-        "TEMPLATE_PATH/models/cv/face_recognition/torchkit/rts_backbone.py": 1679578012.6079304,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/models/bfm.py": 1681971907.403036,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/models/de_retouching_module.py": 1681971907.403036,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/models/facelandmark/large_base_lmks_infer.py": 1679578012.6079304,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/models/facelandmark/nets/large_base_lmks_net.py": 1679578012.6079304,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/models/facelandmark/nets/large_eyeball_net.py": 1679578012.6089303,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/models/facerecon_model.py": 1681971907.403036,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/models/losses.py": 1681971907.403036,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/models/networks.py": 1679578012.6089303,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/models/nv_diffrast.py": 1681971907.403036,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/models/opt.py": 1681971907.403036,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/models/pix2pix/networks.py": 1681971907.404036,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/models/pix2pix/pix2pix_model.py": 1681971907.404036,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/models/pix2pix/pix2pix_options.py": 1681971907.404036,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/models/renderer.py": 1681971907.404036,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/models/unet.py": 1681971907.404036,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/utils.py": 1681971907.404036,
-        "TEMPLATE_PATH/models/cv/facial_expression_recognition/fer/facial_expression_recognition.py": 1670492342.9522376,
-        "TEMPLATE_PATH/models/cv/facial_expression_recognition/fer/transforms.py": 1670492342.9522376,
-        "TEMPLATE_PATH/models/cv/facial_expression_recognition/fer/vgg.py": 1670492342.9522376,
-        "TEMPLATE_PATH/models/cv/facial_landmark_confidence/flc/facial_landmark_confidence.py": 1679578012.6089303,
-        "TEMPLATE_PATH/models/cv/facial_landmark_confidence/flc/manual_landmark_net.py": 1670492342.9532378,
-        "TEMPLATE_PATH/models/cv/hand_static/hand_model.py": 1670492342.9532378,
-        "TEMPLATE_PATH/models/cv/hand_static/networks.py": 1679578012.6089303,
-        "TEMPLATE_PATH/models/cv/human_reconstruction/Reconstruction.py": 1679578012.6099303,
-        "TEMPLATE_PATH/models/cv/human_reconstruction/models/Embedding.py": 1679578012.6099303,
-        "TEMPLATE_PATH/models/cv/human_reconstruction/models/PixToMesh.py": 1679578012.6099303,
-        "TEMPLATE_PATH/models/cv/human_reconstruction/models/Res_backbone.py": 1679578012.6099303,
-        "TEMPLATE_PATH/models/cv/human_reconstruction/models/Surface_head.py": 1679578012.6099303,
-        "TEMPLATE_PATH/models/cv/human_reconstruction/models/detectors.py": 1679578012.6099303,
-        "TEMPLATE_PATH/models/cv/human_reconstruction/models/geometry.py": 1679578012.6099303,
-        "TEMPLATE_PATH/models/cv/human_reconstruction/models/human_segmenter.py": 1684224035.5012639,
-        "TEMPLATE_PATH/models/cv/human_reconstruction/models/networks.py": 1679578012.6099303,
-        "TEMPLATE_PATH/models/cv/human_reconstruction/utils.py": 1684224035.502264,
-        "TEMPLATE_PATH/models/cv/image_binary_quant_classification/binary_quant_model.py": 1679578012.6099303,
-        "TEMPLATE_PATH/models/cv/image_binary_quant_classification/bnext.py": 1679578012.6099303,
-        "TEMPLATE_PATH/models/cv/image_body_reshaping/image_body_reshaping.py": 1670492342.9532378,
-        "TEMPLATE_PATH/models/cv/image_body_reshaping/model.py": 1670492342.9532378,
-        "TEMPLATE_PATH/models/cv/image_body_reshaping/person_info.py": 1670492342.9532378,
-        "TEMPLATE_PATH/models/cv/image_body_reshaping/pose_estimator/body.py": 1670492342.9532378,
-        "TEMPLATE_PATH/models/cv/image_body_reshaping/pose_estimator/model.py": 1670492342.9532378,
-        "TEMPLATE_PATH/models/cv/image_body_reshaping/pose_estimator/util.py": 1670492342.9532378,
-        "TEMPLATE_PATH/models/cv/image_body_reshaping/slim_utils.py": 1670492342.9542377,
-        "TEMPLATE_PATH/models/cv/image_classification/backbones/beit_v2.py": 1679578012.6109304,
-        "TEMPLATE_PATH/models/cv/image_classification/backbones/nextvit.py": 1679578012.6109304,
-        "TEMPLATE_PATH/models/cv/image_classification/mmcls_model.py": 1670492342.9542377,
-        "TEMPLATE_PATH/models/cv/image_classification/resnet50_cc.py": 1679578012.6109304,
-        "TEMPLATE_PATH/models/cv/image_classification/utils.py": 1679578012.6109304,
-        "TEMPLATE_PATH/models/cv/image_color_enhance/adaint/adaint.py": 1679578012.6109304,
-        "TEMPLATE_PATH/models/cv/image_color_enhance/csrnet.py": 1670492342.9542377,
-        "TEMPLATE_PATH/models/cv/image_color_enhance/deeplpf/deeplpf_image_color_enhance.py": 1679578012.6109304,
-        "TEMPLATE_PATH/models/cv/image_color_enhance/deeplpf/deeplpfnet.py": 1679578012.6109304,
-        "TEMPLATE_PATH/models/cv/image_color_enhance/image_color_enhance.py": 1679578012.6109304,
-        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/ddcolor.py": 1679578012.6109304,
-        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/ddcolor_for_image_colorization.py": 1679578012.6119304,
-        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/loss.py": 1679578012.6119304,
-        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/utils/convnext.py": 1679578012.6119304,
-        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/utils/position_encoding.py": 1679578012.6119304,
-        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/utils/transformer_utils.py": 1679578012.6119304,
-        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/utils/unet.py": 1679578012.6119304,
-        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/utils/vgg.py": 1679578012.6119304,
-        "TEMPLATE_PATH/models/cv/image_colorization/unet/unet.py": 1679578012.6119304,
-        "TEMPLATE_PATH/models/cv/image_colorization/unet/utils.py": 1679578012.6119304,
-        "TEMPLATE_PATH/models/cv/image_debanding/rrdb/rrdb_image_debanding.py": 1679578012.6119304,
-        "TEMPLATE_PATH/models/cv/image_deblur/nafnet_for_image_deblur.py": 1679578012.6129303,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/defrcn_for_fewshot.py": 1679578012.6129303,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/evaluation/coco_evaluation.py": 1679578012.6129303,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/evaluation/evaluator.py": 1679578012.6129303,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/evaluation/pascal_voc_evaluation.py": 1679578012.6129303,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/models/calibration_layer.py": 1679578012.6129303,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/models/defrcn.py": 1679578012.6129303,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/models/fast_rcnn.py": 1679578012.6129303,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/models/gdl.py": 1679578012.6129303,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/models/resnet.py": 1679578012.6129303,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/models/roi_heads.py": 1679578012.6129303,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/utils/coco_register.py": 1679578012.6129303,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/utils/configuration_mapper.py": 1679578012.6139305,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/utils/model_surgery_op.py": 1679578012.6139305,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/utils/register_data.py": 1679578012.6139305,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/utils/requirements_check.py": 1679578012.6139305,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/utils/voc_register.py": 1684224035.502264,
-        "TEMPLATE_PATH/models/cv/image_denoise/nafnet/NAFNet_arch.py": 1670492342.9542377,
-        "TEMPLATE_PATH/models/cv/image_denoise/nafnet/arch_util.py": 1670492342.9542377,
-        "TEMPLATE_PATH/models/cv/image_denoise/nafnet_for_image_denoise.py": 1679578012.6139305,
-        "TEMPLATE_PATH/models/cv/image_depth_estimation/networks/newcrf_depth.py": 1670492342.9552376,
-        "TEMPLATE_PATH/models/cv/image_depth_estimation/networks/newcrf_layers.py": 1670492342.9552376,
-        "TEMPLATE_PATH/models/cv/image_depth_estimation/networks/newcrf_utils.py": 1670492342.9552376,
-        "TEMPLATE_PATH/models/cv/image_depth_estimation/networks/swin_transformer.py": 1670492342.9552376,
-        "TEMPLATE_PATH/models/cv/image_depth_estimation/networks/uper_crf_head.py": 1670492342.9552376,
-        "TEMPLATE_PATH/models/cv/image_depth_estimation/newcrfs_model.py": 1670492342.9552376,
-        "TEMPLATE_PATH/models/cv/image_depth_estimation_bts/depth_estimation_bts_model.py": 1679578012.6139305,
-        "TEMPLATE_PATH/models/cv/image_depth_estimation_bts/networks/bts_model.py": 1679578012.6139305,
-        "TEMPLATE_PATH/models/cv/image_depth_estimation_bts/networks/decoder.py": 1679578012.6139305,
-        "TEMPLATE_PATH/models/cv/image_depth_estimation_bts/networks/encoder.py": 1679578012.6139305,
-        "TEMPLATE_PATH/models/cv/image_depth_estimation_bts/networks/utils.py": 1679578012.6139305,
-        "TEMPLATE_PATH/models/cv/image_driving_perception/image_driving_percetion_model.py": 1679578012.6139305,
-        "TEMPLATE_PATH/models/cv/image_driving_perception/preprocessor.py": 1679578012.6139305,
-        "TEMPLATE_PATH/models/cv/image_driving_perception/utils.py": 1679578012.6139305,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/facegan/gan_wrap.py": 1679578012.6149304,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/facegan/model.py": 1679578012.6149304,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/facegan/op/conv2d_gradfix.py": 1679578012.6149304,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/facegan/op/fused_act.py": 1679578012.6149304,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/facegan/op/upfirdn2d.py": 1679578012.6149304,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/facelib/align_trans.py": 1679578012.6149304,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/facelib/matlab_cp2tform.py": 1679578012.6149304,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/image_face_fusion.py": 1679578012.6149304,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/network/aad_layer.py": 1679578012.6149304,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/network/aei_flow_net.py": 1679578012.6149304,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/network/bfm.py": 1679578012.6149304,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/network/dense_motion.py": 1679578012.6149304,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/network/facerecon_model.py": 1679578012.6159303,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/network/model_irse.py": 1679578012.6159303,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/network/ops.py": 1679578012.6159303,
-        "TEMPLATE_PATH/models/cv/image_human_parsing/backbone/deeplab_resnet.py": 1679578012.6159303,
-        "TEMPLATE_PATH/models/cv/image_human_parsing/m2fp/m2fp_decoder.py": 1679578012.6159303,
-        "TEMPLATE_PATH/models/cv/image_human_parsing/m2fp/m2fp_encoder.py": 1679578012.6159303,
-        "TEMPLATE_PATH/models/cv/image_human_parsing/m2fp_net.py": 1679578012.6159303,
-        "TEMPLATE_PATH/models/cv/image_human_parsing/parsing_utils.py": 1687774806.9894693,
-        "TEMPLATE_PATH/models/cv/image_inpainting/base.py": 1670492342.9552376,
-        "TEMPLATE_PATH/models/cv/image_inpainting/default.py": 1670492342.9552376,
-        "TEMPLATE_PATH/models/cv/image_inpainting/model.py": 1670492342.9552376,
-        "TEMPLATE_PATH/models/cv/image_inpainting/modules/ade20k/base.py": 1670492342.9562378,
-        "TEMPLATE_PATH/models/cv/image_inpainting/modules/ade20k/resnet.py": 1670492342.9562378,
-        "TEMPLATE_PATH/models/cv/image_inpainting/modules/adversarial.py": 1670492342.9562378,
-        "TEMPLATE_PATH/models/cv/image_inpainting/modules/feature_matching.py": 1670492342.9562378,
-        "TEMPLATE_PATH/models/cv/image_inpainting/modules/ffc.py": 1670492342.9562378,
-        "TEMPLATE_PATH/models/cv/image_inpainting/modules/inception.py": 1670492342.9562378,
-        "TEMPLATE_PATH/models/cv/image_inpainting/modules/perceptual.py": 1670492342.9562378,
-        "TEMPLATE_PATH/models/cv/image_inpainting/modules/pix2pixhd.py": 1670492342.9562378,
-        "TEMPLATE_PATH/models/cv/image_inpainting/refinement.py": 1670492342.9562378,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/backbones/resnet.py": 1684224035.502264,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/backbones/swin_transformer.py": 1679578012.6169305,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/cascade_mask_rcnn_swin.py": 1679578012.6169305,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/datasets/transforms.py": 1670492342.9572377,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/fastinst/fastinst_decoder.py": 1684224035.502264,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/fastinst/fastinst_encoder.py": 1684224035.502264,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/fastinst_model.py": 1684224035.502264,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino/dino_decoder.py": 1679578012.6169305,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino/maskdino_decoder.py": 1679578012.6169305,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino/maskdino_encoder.py": 1679578012.6169305,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino/ms_deform_attn.py": 1679578012.6169305,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino/position_encoding.py": 1679578012.6169305,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino/utils.py": 1679578012.6169305,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino_model.py": 1679578012.6169305,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino_swin.py": 1679578012.6179304,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/model.py": 1670492342.9572377,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/postprocess_utils.py": 1684224035.502264,
-        "TEMPLATE_PATH/models/cv/image_matching/config/default.py": 1679578012.6179304,
-        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/backbone/resnet_fpn.py": 1679578012.6179304,
-        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/loftr.py": 1679578012.6179304,
-        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/loftr_module/fine_preprocess.py": 1679578012.6179304,
-        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/loftr_module/linear_attention.py": 1679578012.6179304,
-        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/loftr_module/quadtree_attention.py": 1679578012.6179304,
-        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/loftr_module/transformer.py": 1679578012.6179304,
-        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/utils/coarse_matching.py": 1679578012.6179304,
-        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/utils/fine_matching.py": 1679578012.6189303,
-        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/utils/position_encoding.py": 1679578012.6189303,
-        "TEMPLATE_PATH/models/cv/image_matching/quadtree_attention_model.py": 1679578012.6189303,
-        "TEMPLATE_PATH/models/cv/image_matching/utils/misc.py": 1679578012.6189303,
-        "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/cas_mvsnet.py": 1679578012.6189303,
-        "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/casmvs_model.py": 1679578012.6189303,
-        "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/colmap2mvsnet.py": 1684224035.503264,
-        "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/depth_filter.py": 1684224035.503264,
-        "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/general_eval_dataset.py": 1679578012.6189303,
-        "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/module.py": 1679578012.6189303,
-        "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/utils.py": 1679578012.6189303,
-        "TEMPLATE_PATH/models/cv/image_paintbyexample/model.py": 1679578012.6199305,
-        "TEMPLATE_PATH/models/cv/image_panoptic_segmentation/panseg_model.py": 1670492342.9572377,
-        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/align_faces.py": 1670492342.9572377,
-        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/eqface/fqa.py": 1670492342.9572377,
-        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/eqface/model_resnet.py": 1670492342.9572377,
-        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/gpen.py": 1670492342.9572377,
-        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/image_portrait_enhancement.py": 1679578012.6199305,
-        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/losses/helpers.py": 1670492342.9572377,
-        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/losses/losses.py": 1670492342.9572377,
-        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/losses/model_irse.py": 1670492342.9572377,
-        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/retinaface/detection.py": 1670492342.958238,
-        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/retinaface/models/net.py": 1670492342.958238,
-        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/retinaface/models/retinaface.py": 1670492342.958238,
-        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/retinaface/utils.py": 1670492342.958238,
-        "TEMPLATE_PATH/models/cv/image_probing_model/backbone.py": 1679578012.6199305,
-        "TEMPLATE_PATH/models/cv/image_probing_model/model.py": 1679578012.6199305,
-        "TEMPLATE_PATH/models/cv/image_probing_model/utils.py": 1679578012.6199305,
-        "TEMPLATE_PATH/models/cv/image_quality_assessment_degradation/degradation_model.py": 1679578012.6199305,
-        "TEMPLATE_PATH/models/cv/image_quality_assessment_degradation/image_quality_assessment_degradation.py": 1679578012.6199305,
-        "TEMPLATE_PATH/models/cv/image_quality_assessment_man/image_quality_assessment_man.py": 1679578012.6199305,
-        "TEMPLATE_PATH/models/cv/image_quality_assessment_man/maniqa.py": 1679578012.6199305,
-        "TEMPLATE_PATH/models/cv/image_quality_assessment_man/swin.py": 1679578012.6199305,
-        "TEMPLATE_PATH/models/cv/image_quality_assessment_mos/backbones/resnet.py": 1679578012.6209304,
-        "TEMPLATE_PATH/models/cv/image_quality_assessment_mos/censeo_ivqa_model.py": 1679578012.6209304,
-        "TEMPLATE_PATH/models/cv/image_quality_assessment_mos/heads/simple_head.py": 1679578012.6209304,
-        "TEMPLATE_PATH/models/cv/image_quality_assessment_mos/image_quality_assessment_mos.py": 1679578012.6209304,
-        "TEMPLATE_PATH/models/cv/image_reid_person/pass_model.py": 1670492342.958238,
-        "TEMPLATE_PATH/models/cv/image_reid_person/transreid_model.py": 1670492342.958238,
-        "TEMPLATE_PATH/models/cv/image_restoration/demoire_models/nets.py": 1679578012.6209304,
-        "TEMPLATE_PATH/models/cv/image_restoration/image_restoration_model.py": 1679578012.6209304,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/ddpm_seg/data_util.py": 1679578012.6209304,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/ddpm_seg/feature_extractors.py": 1679578012.6209304,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/ddpm_seg/pixel_classifier.py": 1679578012.6209304,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/ddpm_seg/utils.py": 1679578012.6209304,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/ddpm_segmentation_model.py": 1679578012.6209304,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/pan_merge/base_panoptic_fusion_head.py": 1670492342.958238,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/pan_merge/maskformer_semantic_head.py": 1670492342.958238,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/semantic_seg_model.py": 1684224035.503264,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/adapter_modules.py": 1670492342.958238,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/base/beit.py": 1670492342.9592378,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/beit_adapter.py": 1670492342.9592378,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/base_decode_head.py": 1670492342.9592378,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/mask2former_head_from_mmseg.py": 1670492342.9592378,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/base_segmentor.py": 1670492342.9592378,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/encoder_decoder_mask2former.py": 1670492342.9592378,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/utils/builder.py": 1670492342.9592378,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/utils/data_process_func.py": 1670492342.9592378,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/utils/seg_func.py": 1670492342.9592378,
-        "TEMPLATE_PATH/models/cv/image_skychange/preprocessor.py": 1679578012.6219304,
-        "TEMPLATE_PATH/models/cv/image_skychange/ptsemseg/BlockModules.py": 1679578012.6219304,
-        "TEMPLATE_PATH/models/cv/image_skychange/ptsemseg/hrnet_backnone.py": 1684224035.503264,
-        "TEMPLATE_PATH/models/cv/image_skychange/ptsemseg/hrnet_super_and_ocr.py": 1684224035.503264,
-        "TEMPLATE_PATH/models/cv/image_skychange/ptsemseg/unet.py": 1679578012.6219304,
-        "TEMPLATE_PATH/models/cv/image_skychange/skychange.py": 1679578012.6219304,
-        "TEMPLATE_PATH/models/cv/image_skychange/skychange_model.py": 1679578012.6219304,
-        "TEMPLATE_PATH/models/cv/image_to_image_generation/data/transforms.py": 1670492342.9602377,
-        "TEMPLATE_PATH/models/cv/image_to_image_generation/model.py": 1670492342.9602377,
-        "TEMPLATE_PATH/models/cv/image_to_image_generation/models/autoencoder.py": 1670492342.9602377,
-        "TEMPLATE_PATH/models/cv/image_to_image_generation/models/clip.py": 1679578012.6219304,
-        "TEMPLATE_PATH/models/cv/image_to_image_generation/ops/diffusion.py": 1670492342.9602377,
-        "TEMPLATE_PATH/models/cv/image_to_image_generation/ops/losses.py": 1670492342.9602377,
-        "TEMPLATE_PATH/models/cv/image_to_image_translation/data/transforms.py": 1670492342.9602377,
-        "TEMPLATE_PATH/models/cv/image_to_image_translation/model_translation.py": 1670492342.9602377,
-        "TEMPLATE_PATH/models/cv/image_to_image_translation/models/autoencoder.py": 1670492342.961238,
-        "TEMPLATE_PATH/models/cv/image_to_image_translation/models/clip.py": 1679578012.6219304,
-        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/apps.py": 1670492342.961238,
-        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/degradation.py": 1670492342.961238,
-        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/diffusion.py": 1679578012.6229305,
-        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/losses.py": 1670492342.961238,
-        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/metrics.py": 1670492342.961238,
-        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/random_color.py": 1670492342.961238,
-        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/random_mask.py": 1670492342.961238,
-        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/svd.py": 1670492342.961238,
-        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/utils.py": 1670492342.961238,
-        "TEMPLATE_PATH/models/cv/image_try_on/generator.py": 1691036741.2813349,
-        "TEMPLATE_PATH/models/cv/image_try_on/landmark.py": 1691036741.2813349,
-        "TEMPLATE_PATH/models/cv/image_try_on/try_on_infer.py": 1691036741.2813349,
-        "TEMPLATE_PATH/models/cv/image_try_on/warping.py": 1691036741.2823348,
-        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/backbone/resnet_DA.py": 1679578012.6229305,
-        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/backbone/vit_horizon_pry_image.py": 1679578012.6229305,
-        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/misc/fourier.py": 1679578012.6229305,
-        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/misc/panostretch.py": 1679578012.6229305,
-        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/misc/post_proc.py": 1679578012.6229305,
-        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/modality/layout.py": 1679578012.6229305,
-        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/panovit.py": 1679578012.6229305,
-        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/utils.py": 1679578012.6229305,
-        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/panovit.py": 1679578012.6229305,
-        "TEMPLATE_PATH/models/cv/language_guided_video_summarization/summarizer.py": 1679578012.6239305,
-        "TEMPLATE_PATH/models/cv/language_guided_video_summarization/transformer/layers.py": 1670492342.9622378,
-        "TEMPLATE_PATH/models/cv/language_guided_video_summarization/transformer/models.py": 1670492342.9622378,
-        "TEMPLATE_PATH/models/cv/language_guided_video_summarization/transformer/modules.py": 1670492342.9622378,
-        "TEMPLATE_PATH/models/cv/language_guided_video_summarization/transformer/sub_layers.py": 1670492342.9622378,
-        "TEMPLATE_PATH/models/cv/motion_generation/model.py": 1679578012.6239305,
-        "TEMPLATE_PATH/models/cv/motion_generation/modules/cfg_sampler.py": 1679578012.6239305,
-        "TEMPLATE_PATH/models/cv/motion_generation/modules/gaussian_diffusion.py": 1679578012.6239305,
-        "TEMPLATE_PATH/models/cv/motion_generation/modules/mdm.py": 1679578012.6239305,
-        "TEMPLATE_PATH/models/cv/motion_generation/modules/respace.py": 1679578012.6239305,
-        "TEMPLATE_PATH/models/cv/motion_generation/modules/rotation2xyz.py": 1679578012.6239305,
-        "TEMPLATE_PATH/models/cv/motion_generation/modules/smpl.py": 1679578012.6239305,
-        "TEMPLATE_PATH/models/cv/movie_scene_segmentation/get_model.py": 1670492342.9622378,
-        "TEMPLATE_PATH/models/cv/movie_scene_segmentation/model.py": 1681973542.0311236,
-        "TEMPLATE_PATH/models/cv/movie_scene_segmentation/utils/head.py": 1679578012.6239305,
-        "TEMPLATE_PATH/models/cv/movie_scene_segmentation/utils/save_op.py": 1681973542.0311236,
-        "TEMPLATE_PATH/models/cv/movie_scene_segmentation/utils/shot_encoder.py": 1670492342.9622378,
-        "TEMPLATE_PATH/models/cv/movie_scene_segmentation/utils/trn.py": 1670492342.9622378,
-        "TEMPLATE_PATH/models/cv/nerf_recon_4k/dataloader/load_blender.py": 1691036741.2823348,
-        "TEMPLATE_PATH/models/cv/nerf_recon_4k/dataloader/load_data.py": 1691036741.2823348,
-        "TEMPLATE_PATH/models/cv/nerf_recon_4k/dataloader/load_llff.py": 1691036741.2823348,
-        "TEMPLATE_PATH/models/cv/nerf_recon_4k/dataloader/load_tankstemple.py": 1691036741.2823348,
-        "TEMPLATE_PATH/models/cv/nerf_recon_4k/dataloader/read_write_model.py": 1691036741.2823348,
-        "TEMPLATE_PATH/models/cv/nerf_recon_4k/nerf_preprocess.py": 1691036741.2823348,
-        "TEMPLATE_PATH/models/cv/nerf_recon_4k/nerf_recon_4k.py": 1691036741.2823348,
-        "TEMPLATE_PATH/models/cv/nerf_recon_4k/network/dvgo.py": 1691036741.283335,
-        "TEMPLATE_PATH/models/cv/nerf_recon_4k/network/utils.py": 1691036741.283335,
-        "TEMPLATE_PATH/models/cv/nerf_recon_acc/dataloader/nerf_dataset.py": 1679578012.6249306,
-        "TEMPLATE_PATH/models/cv/nerf_recon_acc/dataloader/read_write_model.py": 1679578012.6249306,
-        "TEMPLATE_PATH/models/cv/nerf_recon_acc/nerf_preprocess.py": 1679578012.6249306,
-        "TEMPLATE_PATH/models/cv/nerf_recon_acc/nerf_recon_acc.py": 1679578012.6249306,
-        "TEMPLATE_PATH/models/cv/nerf_recon_acc/network/nerf.py": 1679578012.6249306,
-        "TEMPLATE_PATH/models/cv/nerf_recon_acc/network/segmenter.py": 1684224035.503264,
-        "TEMPLATE_PATH/models/cv/nerf_recon_acc/network/utils.py": 1679578012.6249306,
-        "TEMPLATE_PATH/models/cv/nerf_recon_vq_compression/dataloader/blender.py": 1691036741.283335,
-        "TEMPLATE_PATH/models/cv/nerf_recon_vq_compression/dataloader/llff.py": 1691036741.283335,
-        "TEMPLATE_PATH/models/cv/nerf_recon_vq_compression/dataloader/nsvf.py": 1691036741.283335,
-        "TEMPLATE_PATH/models/cv/nerf_recon_vq_compression/dataloader/ray_utils.py": 1691036741.284335,
-        "TEMPLATE_PATH/models/cv/nerf_recon_vq_compression/dataloader/tankstemple.py": 1691036741.284335,
-        "TEMPLATE_PATH/models/cv/nerf_recon_vq_compression/nerf_recon_vq_compression.py": 1691036741.284335,
-        "TEMPLATE_PATH/models/cv/nerf_recon_vq_compression/network/tensoRF.py": 1691036741.284335,
-        "TEMPLATE_PATH/models/cv/nerf_recon_vq_compression/network/tensoRF_VQ.py": 1691036741.2853348,
-        "TEMPLATE_PATH/models/cv/nerf_recon_vq_compression/network/tensorBase.py": 1691036741.2853348,
-        "TEMPLATE_PATH/models/cv/nerf_recon_vq_compression/network/weighted_vq.py": 1691036741.2853348,
-        "TEMPLATE_PATH/models/cv/nerf_recon_vq_compression/renderer.py": 1691036741.2853348,
-        "TEMPLATE_PATH/models/cv/nerf_recon_vq_compression/utils.py": 1691036741.2853348,
-        "TEMPLATE_PATH/models/cv/object_detection/mmdet_model.py": 1670492342.9622378,
-        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/backbones/vit.py": 1670492342.9632378,
-        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/dense_heads/anchor_head.py": 1679578012.6249306,
-        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/dense_heads/rpn_head.py": 1679578012.6249306,
-        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/necks/fpn.py": 1679578012.6259305,
-        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/roi_heads/bbox_heads/convfc_bbox_head.py": 1679578012.6259305,
-        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/roi_heads/mask_heads/fcn_mask_head.py": 1679578012.6259305,
-        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/utils/checkpoint.py": 1679578012.6259305,
-        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/utils/convModule_norm.py": 1679578012.6259305,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/depe_detect.py": 1679578012.6259305,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/assigners/hungarian_assigner_3d.py": 1679578012.6269305,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/coders/nms_free_coder.py": 1679578012.6269305,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/match_costs/match_cost.py": 1679578012.6269305,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/util.py": 1679578012.6269305,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/nuscenes_dataset.py": 1679578012.6269305,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/loading.py": 1679578012.6269305,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/transform_3d.py": 1679578012.6269305,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/backbones/vovnet.py": 1679578012.6269305,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/depth_net.py": 1679578012.6279306,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/petrv2_dednhead.py": 1679578012.6279306,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/detectors/petr3d.py": 1679578012.6279306,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/necks/cp_fpn.py": 1679578012.6279306,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/petr_transformer.py": 1679578012.6279306,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/positional_encoding.py": 1679578012.6279306,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/result_vis.py": 1687774806.9894693,
-        "TEMPLATE_PATH/models/cv/ocr_detection/model.py": 1687774806.9894693,
-        "TEMPLATE_PATH/models/cv/ocr_detection/modules/dbnet.py": 1687774806.9904692,
-        "TEMPLATE_PATH/models/cv/ocr_detection/modules/layers.py": 1687774806.9904692,
-        "TEMPLATE_PATH/models/cv/ocr_detection/modules/mix_ops.py": 1687774806.9904692,
-        "TEMPLATE_PATH/models/cv/ocr_detection/modules/proxyless.py": 1687774806.9904692,
-        "TEMPLATE_PATH/models/cv/ocr_detection/modules/seg_detector_loss.py": 1679578012.6289306,
-        "TEMPLATE_PATH/models/cv/ocr_detection/preprocessor.py": 1684224035.5042639,
-        "TEMPLATE_PATH/models/cv/ocr_detection/utils.py": 1679578012.6289306,
-        "TEMPLATE_PATH/models/cv/ocr_recognition/model.py": 1687774806.9904692,
-        "TEMPLATE_PATH/models/cv/ocr_recognition/modules/CRNN/main_model.py": 1687774806.9914694,
-        "TEMPLATE_PATH/models/cv/ocr_recognition/modules/ConvNextViT/convnext.py": 1687774806.9914694,
-        "TEMPLATE_PATH/models/cv/ocr_recognition/modules/ConvNextViT/main_model.py": 1687774806.9914694,
-        "TEMPLATE_PATH/models/cv/ocr_recognition/modules/ConvNextViT/timm_tinyc.py": 1687774806.9914694,
-        "TEMPLATE_PATH/models/cv/ocr_recognition/modules/ConvNextViT/vitstr.py": 1687774806.9914694,
-        "TEMPLATE_PATH/models/cv/ocr_recognition/modules/LightweightEdge/main_model.py": 1687774806.9914694,
-        "TEMPLATE_PATH/models/cv/ocr_recognition/modules/LightweightEdge/nas_block/layers.py": 1687774806.9924693,
-        "TEMPLATE_PATH/models/cv/ocr_recognition/modules/LightweightEdge/nas_block/mix_ops.py": 1687774806.9924693,
-        "TEMPLATE_PATH/models/cv/ocr_recognition/modules/LightweightEdge/nas_block/proxyless.py": 1687774806.9924693,
-        "TEMPLATE_PATH/models/cv/ocr_recognition/preprocessor.py": 1687774806.9924693,
-        "TEMPLATE_PATH/models/cv/open_vocabulary_detection_vild/vild.py": 1684224035.5042639,
-        "TEMPLATE_PATH/models/cv/panorama_depth_estimation/networks/equi.py": 1679578012.6299305,
-        "TEMPLATE_PATH/models/cv/panorama_depth_estimation/networks/layers.py": 1684224035.5042639,
-        "TEMPLATE_PATH/models/cv/panorama_depth_estimation/networks/mobilenet.py": 1679578012.6299305,
-        "TEMPLATE_PATH/models/cv/panorama_depth_estimation/networks/resnet.py": 1679578012.6299305,
-        "TEMPLATE_PATH/models/cv/panorama_depth_estimation/networks/unifuse.py": 1679578012.6299305,
-        "TEMPLATE_PATH/models/cv/panorama_depth_estimation/networks/util.py": 1679578012.6299305,
-        "TEMPLATE_PATH/models/cv/panorama_depth_estimation/unifuse_model.py": 1679578012.6299305,
-        "TEMPLATE_PATH/models/cv/pedestrian_attribute_recognition/model.py": 1681971907.405036,
-        "TEMPLATE_PATH/models/cv/pointcloud_sceneflow_estimation/common.py": 1679578012.6309307,
-        "TEMPLATE_PATH/models/cv/pointcloud_sceneflow_estimation/pointnet2_utils.py": 1679578012.6309307,
-        "TEMPLATE_PATH/models/cv/pointcloud_sceneflow_estimation/rcp_model.py": 1679578012.6309307,
-        "TEMPLATE_PATH/models/cv/pointcloud_sceneflow_estimation/sf_rcp.py": 1679578012.6309307,
-        "TEMPLATE_PATH/models/cv/product_retrieval_embedding/item_detection.py": 1670492342.964238,
-        "TEMPLATE_PATH/models/cv/product_retrieval_embedding/item_embedding.py": 1670492342.964238,
-        "TEMPLATE_PATH/models/cv/product_retrieval_embedding/item_model.py": 1670492342.964238,
-        "TEMPLATE_PATH/models/cv/product_segmentation/net.py": 1679578012.6309307,
-        "TEMPLATE_PATH/models/cv/product_segmentation/seg_infer.py": 1670492342.964238,
-        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/model.py": 1670492342.9652379,
-        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/backbone.py": 1670492342.9662378,
-        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/criterion.py": 1670492342.9662378,
-        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/matcher.py": 1670492342.9662378,
-        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/misc.py": 1670492342.9662378,
-        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/mttr.py": 1670492342.9662378,
-        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/multimodal_transformer.py": 1670492342.9662378,
-        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/position_encoding_2d.py": 1670492342.9662378,
-        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/postprocessing.py": 1670492342.9662378,
-        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/segmentation.py": 1670492342.9662378,
-        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/swin_transformer.py": 1670492342.9662378,
-        "TEMPLATE_PATH/models/cv/robust_image_classification/easyrobust_model.py": 1679578012.6309307,
-        "TEMPLATE_PATH/models/cv/s2net_panorama_depth_estimation/networks/config.py": 1691036741.286335,
-        "TEMPLATE_PATH/models/cv/s2net_panorama_depth_estimation/networks/decoder.py": 1691036741.286335,
-        "TEMPLATE_PATH/models/cv/s2net_panorama_depth_estimation/networks/model.py": 1691036741.286335,
-        "TEMPLATE_PATH/models/cv/s2net_panorama_depth_estimation/networks/resnet.py": 1691036741.286335,
-        "TEMPLATE_PATH/models/cv/s2net_panorama_depth_estimation/networks/swin_transformer.py": 1691036741.286335,
-        "TEMPLATE_PATH/models/cv/s2net_panorama_depth_estimation/networks/util_helper.py": 1691036741.286335,
-        "TEMPLATE_PATH/models/cv/s2net_panorama_depth_estimation/s2net_model.py": 1691036741.286335,
-        "TEMPLATE_PATH/models/cv/salient_detection/models/backbone/Res2Net_v1b.py": 1679578012.6309307,
-        "TEMPLATE_PATH/models/cv/salient_detection/models/modules.py": 1678888948.7409308,
-        "TEMPLATE_PATH/models/cv/salient_detection/models/senet.py": 1678888948.7409308,
-        "TEMPLATE_PATH/models/cv/salient_detection/models/u2net.py": 1670492342.967238,
-        "TEMPLATE_PATH/models/cv/salient_detection/models/utils.py": 1679578012.6309307,
-        "TEMPLATE_PATH/models/cv/salient_detection/salient_model.py": 1678888948.7419307,
-        "TEMPLATE_PATH/models/cv/shop_segmentation/common.py": 1670492342.967238,
-        "TEMPLATE_PATH/models/cv/shop_segmentation/head_fpn.py": 1670492342.967238,
-        "TEMPLATE_PATH/models/cv/shop_segmentation/models.py": 1670492342.967238,
-        "TEMPLATE_PATH/models/cv/shop_segmentation/neck_fpn.py": 1670492342.967238,
-        "TEMPLATE_PATH/models/cv/shop_segmentation/shop_seg_base.py": 1670492342.967238,
-        "TEMPLATE_PATH/models/cv/shop_segmentation/shop_seg_model.py": 1670492342.967238,
-        "TEMPLATE_PATH/models/cv/shop_segmentation/utils.py": 1670492342.967238,
-        "TEMPLATE_PATH/models/cv/skin_retouching/detection_model/detection_module.py": 1670492342.9682379,
-        "TEMPLATE_PATH/models/cv/skin_retouching/detection_model/detection_unet_in.py": 1670492342.9682379,
-        "TEMPLATE_PATH/models/cv/skin_retouching/inpainting_model/gconv.py": 1670492342.9682379,
-        "TEMPLATE_PATH/models/cv/skin_retouching/inpainting_model/inpainting_unet.py": 1670492342.9682379,
-        "TEMPLATE_PATH/models/cv/skin_retouching/retinaface/box_utils.py": 1670492342.9682379,
-        "TEMPLATE_PATH/models/cv/skin_retouching/retinaface/net.py": 1670492342.9682379,
-        "TEMPLATE_PATH/models/cv/skin_retouching/retinaface/network.py": 1670492342.9682379,
-        "TEMPLATE_PATH/models/cv/skin_retouching/retinaface/predict_single.py": 1670492342.9682379,
-        "TEMPLATE_PATH/models/cv/skin_retouching/retinaface/prior_box.py": 1670492342.9682379,
-        "TEMPLATE_PATH/models/cv/skin_retouching/retinaface/utils.py": 1670492342.9682379,
-        "TEMPLATE_PATH/models/cv/skin_retouching/unet_deploy.py": 1670492342.9682379,
-        "TEMPLATE_PATH/models/cv/skin_retouching/utils.py": 1670492342.9682379,
-        "TEMPLATE_PATH/models/cv/skin_retouching/weights_init.py": 1670492342.9682379,
-        "TEMPLATE_PATH/models/cv/stream_yolo/data/data_augment.py": 1679578012.6309307,
-        "TEMPLATE_PATH/models/cv/stream_yolo/exp/base_exp.py": 1679578012.6309307,
-        "TEMPLATE_PATH/models/cv/stream_yolo/exp/build.py": 1679578012.6309307,
-        "TEMPLATE_PATH/models/cv/stream_yolo/exp/default/streamyolo.py": 1679578012.6319306,
-        "TEMPLATE_PATH/models/cv/stream_yolo/exp/yolox_base.py": 1679578012.6319306,
-        "TEMPLATE_PATH/models/cv/stream_yolo/models/darknet.py": 1679578012.6319306,
-        "TEMPLATE_PATH/models/cv/stream_yolo/models/dfp_pafpn.py": 1679578012.6319306,
-        "TEMPLATE_PATH/models/cv/stream_yolo/models/network_blocks.py": 1679578012.6319306,
-        "TEMPLATE_PATH/models/cv/stream_yolo/models/streamyolo.py": 1679578012.6319306,
-        "TEMPLATE_PATH/models/cv/stream_yolo/models/tal_head.py": 1679578012.6319306,
-        "TEMPLATE_PATH/models/cv/stream_yolo/realtime_video_detector.py": 1679578012.6319306,
-        "TEMPLATE_PATH/models/cv/stream_yolo/utils/boxes.py": 1679578012.6319306,
-        "TEMPLATE_PATH/models/cv/stream_yolo/utils/format.py": 1679578012.6319306,
-        "TEMPLATE_PATH/models/cv/super_resolution/arch_util.py": 1670492342.9682379,
-        "TEMPLATE_PATH/models/cv/super_resolution/ecb.py": 1679578012.6319306,
-        "TEMPLATE_PATH/models/cv/super_resolution/ecbsr_model.py": 1679578012.6319306,
-        "TEMPLATE_PATH/models/cv/super_resolution/rrdbnet_arch.py": 1670492342.9682379,
-        "TEMPLATE_PATH/models/cv/table_recognition/lineless_table_process.py": 1687774806.9924693,
-        "TEMPLATE_PATH/models/cv/table_recognition/model_lore.py": 1679578012.6319306,
-        "TEMPLATE_PATH/models/cv/table_recognition/modules/lore_detector.py": 1679578012.6329305,
-        "TEMPLATE_PATH/models/cv/table_recognition/modules/lore_processor.py": 1679578012.6329305,
-        "TEMPLATE_PATH/models/cv/text_driven_segmentation/clip.py": 1670492342.9692378,
-        "TEMPLATE_PATH/models/cv/text_driven_segmentation/lseg_base.py": 1670492342.9692378,
-        "TEMPLATE_PATH/models/cv/text_driven_segmentation/lseg_blocks.py": 1670492342.9692378,
-        "TEMPLATE_PATH/models/cv/text_driven_segmentation/lseg_model.py": 1670492342.9692378,
-        "TEMPLATE_PATH/models/cv/text_driven_segmentation/lseg_net.py": 1670492342.9692378,
-        "TEMPLATE_PATH/models/cv/text_driven_segmentation/lseg_vit.py": 1670492342.9692378,
-        "TEMPLATE_PATH/models/cv/text_driven_segmentation/model.py": 1670492342.9692378,
-        "TEMPLATE_PATH/models/cv/text_driven_segmentation/simple_tokenizer.py": 1670492342.9692378,
-        "TEMPLATE_PATH/models/cv/text_to_360panorama_image/pipeline_base.py": 1691036741.287335,
-        "TEMPLATE_PATH/models/cv/text_to_360panorama_image/pipeline_sr.py": 1691036741.287335,
-        "TEMPLATE_PATH/models/cv/tinynas_classfication/basic_blocks.py": 1670492342.9692378,
-        "TEMPLATE_PATH/models/cv/tinynas_classfication/global_utils.py": 1670492342.9692378,
-        "TEMPLATE_PATH/models/cv/tinynas_classfication/master_net.py": 1670492342.9692378,
-        "TEMPLATE_PATH/models/cv/tinynas_classfication/model_zoo.py": 1670492342.9692378,
-        "TEMPLATE_PATH/models/cv/tinynas_classfication/plain_net_utils.py": 1670492342.970238,
-        "TEMPLATE_PATH/models/cv/tinynas_classfication/super_blocks.py": 1670492342.970238,
-        "TEMPLATE_PATH/models/cv/tinynas_classfication/super_res_idwexkx.py": 1670492342.970238,
-        "TEMPLATE_PATH/models/cv/tinynas_classfication/super_res_k1kxk1.py": 1670492342.970238,
-        "TEMPLATE_PATH/models/cv/tinynas_classfication/super_res_kxkx.py": 1670492342.970238,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/apis/detector_evaluater.py": 1679578012.6329305,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/apis/detector_inference.py": 1679578012.6329305,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/augmentations/box_level_augs/box_level_augs.py": 1679578012.6329305,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/augmentations/box_level_augs/color_augs.py": 1679578012.6329305,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/augmentations/box_level_augs/gaussian_maps.py": 1679578012.6329305,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/augmentations/box_level_augs/geometric_augs.py": 1681971907.405036,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/augmentations/scale_aware_aug.py": 1679578012.6329305,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/backbones/darknet.py": 1679578012.6329305,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/backbones/tinynas_csp.py": 1679578012.6329305,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/backbones/tinynas_res.py": 1679578012.6339307,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/core/base_ops.py": 1679578012.6339307,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/core/neck_ops.py": 1679578012.6339307,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/core/ops.py": 1679578012.6339307,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/core/ota_assigner.py": 1679578012.6339307,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/core/repvgg_block.py": 1679578012.6339307,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/core/utils.py": 1679578012.6339307,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/core/weight_init.py": 1679578012.6339307,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/heads/gfocal_v2_tiny.py": 1679578012.6339307,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/heads/zero_head.py": 1679578012.6339307,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/losses/distill_loss.py": 1679578012.6349306,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/losses/gfocal_loss.py": 1679578012.6349306,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/necks/giraffe_config.py": 1679578012.6349306,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/necks/giraffe_fpn.py": 1679578012.6349306,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/necks/giraffe_fpn_btn.py": 1679578012.6349306,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/detectors/detector.py": 1679578012.6349306,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/structures/bounding_box.py": 1679578012.6349306,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/structures/boxlist_ops.py": 1679578012.6349306,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/structures/image_list.py": 1679578012.6349306,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/utils/boxes.py": 1679578012.6349306,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/utils/model_utils.py": 1679578012.6359305,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/utils/scheduler.py": 1679578012.6359305,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/detector.py": 1679578012.6359305,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/tinynas_damoyolo.py": 1679578012.6359305,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/tinynas_detector.py": 1670492342.971238,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/utils.py": 1670492342.971238,
-        "TEMPLATE_PATH/models/cv/video_deinterlace/UNet_for_video_deinterlace.py": 1679578012.6359305,
-        "TEMPLATE_PATH/models/cv/video_deinterlace/deinterlace_arch.py": 1679578012.6359305,
-        "TEMPLATE_PATH/models/cv/video_deinterlace/models/archs.py": 1679578012.6359305,
-        "TEMPLATE_PATH/models/cv/video_deinterlace/models/deep_fourier_upsampling.py": 1679578012.6359305,
-        "TEMPLATE_PATH/models/cv/video_deinterlace/models/enh.py": 1679578012.6359305,
-        "TEMPLATE_PATH/models/cv/video_deinterlace/models/fre.py": 1679578012.6359305,
-        "TEMPLATE_PATH/models/cv/video_deinterlace/models/utils.py": 1679578012.6359305,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/configs/default_config.py": 1679578012.6359305,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/dro_model.py": 1679578012.6359305,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/geometry/camera.py": 1679578012.6359305,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/geometry/camera_utils.py": 1679578012.6369307,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/geometry/pose.py": 1679578012.6369307,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/geometry/pose_utils.py": 1679578012.6369307,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/models/model_checkpoint.py": 1679578012.6369307,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/models/model_utils.py": 1679578012.6369307,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/models/model_wrapper.py": 1679578012.6369307,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/models/sfm_model_mf.py": 1679578012.6369307,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/models/sup_model_mf.py": 1679578012.6369307,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/networks/depth_pose/depth_pose_net.py": 1679578012.6369307,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/networks/layers/resnet/depth_decoder.py": 1679578012.6369307,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/networks/layers/resnet/layers.py": 1679578012.6369307,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/networks/layers/resnet/pose_decoder.py": 1679578012.6369307,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/networks/layers/resnet/resnet_encoder.py": 1679578012.6369307,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/networks/optim/extractor.py": 1679578012.6379306,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/networks/optim/update.py": 1679578012.6379306,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/augmentations.py": 1679578012.6379306,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/config.py": 1679578012.6379306,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/depth.py": 1684224035.5042639,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/horovod.py": 1679578012.6379306,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/image.py": 1679578012.6379306,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/image_gt.py": 1679578012.6379306,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/load.py": 1679578012.6379306,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/misc.py": 1679578012.6379306,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/types.py": 1679578012.6379306,
-        "TEMPLATE_PATH/models/cv/video_frame_interpolation/VFINet_arch.py": 1679578012.6379306,
-        "TEMPLATE_PATH/models/cv/video_frame_interpolation/VFINet_for_video_frame_interpolation.py": 1679578012.6379306,
-        "TEMPLATE_PATH/models/cv/video_frame_interpolation/flow_model/corr.py": 1679578012.6389306,
-        "TEMPLATE_PATH/models/cv/video_frame_interpolation/flow_model/extractor.py": 1679578012.6389306,
-        "TEMPLATE_PATH/models/cv/video_frame_interpolation/flow_model/raft.py": 1679578012.6389306,
-        "TEMPLATE_PATH/models/cv/video_frame_interpolation/flow_model/update.py": 1679578012.6389306,
-        "TEMPLATE_PATH/models/cv/video_frame_interpolation/interp_model/IFNet_swin.py": 1679578012.6389306,
-        "TEMPLATE_PATH/models/cv/video_frame_interpolation/interp_model/UNet.py": 1679578012.6389306,
-        "TEMPLATE_PATH/models/cv/video_frame_interpolation/interp_model/flow_reversal.py": 1679578012.6389306,
-        "TEMPLATE_PATH/models/cv/video_frame_interpolation/interp_model/refinenet_arch.py": 1679578012.6389306,
-        "TEMPLATE_PATH/models/cv/video_frame_interpolation/interp_model/transformer_layers.py": 1679578012.6389306,
-        "TEMPLATE_PATH/models/cv/video_frame_interpolation/utils/scene_change_detection.py": 1684224035.5042639,
-        "TEMPLATE_PATH/models/cv/video_frame_interpolation/utils/utils.py": 1679578012.6389306,
-        "TEMPLATE_PATH/models/cv/video_human_matting/model.py": 1670492342.971238,
-        "TEMPLATE_PATH/models/cv/video_human_matting/models/decoder.py": 1670492342.9722378,
-        "TEMPLATE_PATH/models/cv/video_human_matting/models/deep_guided_filter.py": 1670492342.9722378,
-        "TEMPLATE_PATH/models/cv/video_human_matting/models/effv2.py": 1670492342.9722378,
-        "TEMPLATE_PATH/models/cv/video_human_matting/models/lraspp.py": 1670492342.9722378,
-        "TEMPLATE_PATH/models/cv/video_human_matting/models/matting.py": 1670492342.9722378,
-        "TEMPLATE_PATH/models/cv/video_inpainting/inpainting.py": 1679578012.6389306,
-        "TEMPLATE_PATH/models/cv/video_inpainting/inpainting_model.py": 1679578012.6399307,
-        "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_frame_iter_head.py": 1679578012.6399307,
-        "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_head.py": 1679578012.6399307,
-        "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_iter_head.py": 1679578012.6399307,
-        "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_update_head.py": 1679578012.6399307,
-        "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_updator.py": 1679578012.6399307,
-        "TEMPLATE_PATH/models/cv/video_instance_segmentation/neck/msdeformattn_decoder.py": 1679578012.6399307,
-        "TEMPLATE_PATH/models/cv/video_instance_segmentation/track/kernel_update_head.py": 1679578012.6409307,
-        "TEMPLATE_PATH/models/cv/video_instance_segmentation/track/mask_hungarian_assigner.py": 1679578012.6409307,
-        "TEMPLATE_PATH/models/cv/video_instance_segmentation/utils.py": 1679578012.6409307,
-        "TEMPLATE_PATH/models/cv/video_instance_segmentation/video_knet.py": 1679578012.6409307,
-        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/models/common.py": 1679578012.6409307,
-        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/models/decode.py": 1679578012.6409307,
-        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/models/model.py": 1679578012.6409307,
-        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/models/yolo.py": 1679578012.6409307,
-        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/tracker/basetrack.py": 1679578012.6409307,
-        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/tracker/matching.py": 1684224035.505264,
-        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/tracker/multitracker.py": 1684224035.505264,
-        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/utils/image.py": 1679578012.6419306,
-        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/utils/kalman_filter.py": 1679578012.6419306,
-        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/utils/utils.py": 1679578012.6419306,
-        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/utils/visualization.py": 1679578012.6419306,
-        "TEMPLATE_PATH/models/cv/video_object_segmentation/aggregate.py": 1679578012.6419306,
-        "TEMPLATE_PATH/models/cv/video_object_segmentation/cbam.py": 1679578012.6419306,
-        "TEMPLATE_PATH/models/cv/video_object_segmentation/eval_network.py": 1679578012.6419306,
-        "TEMPLATE_PATH/models/cv/video_object_segmentation/inference_core.py": 1679578012.6419306,
-        "TEMPLATE_PATH/models/cv/video_object_segmentation/inference_memory_bank.py": 1679578012.6419306,
-        "TEMPLATE_PATH/models/cv/video_object_segmentation/mod_resnet.py": 1679578012.6419306,
-        "TEMPLATE_PATH/models/cv/video_object_segmentation/model.py": 1679578012.6419306,
-        "TEMPLATE_PATH/models/cv/video_object_segmentation/modules.py": 1679578012.6419306,
-        "TEMPLATE_PATH/models/cv/video_object_segmentation/network.py": 1679578012.6419306,
-        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/backbone/swin_checkpoint.py": 1679578012.6429307,
-        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/backbone/swin_transformer.py": 1679578012.6429307,
-        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/kernel_head.py": 1679578012.6429307,
-        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/kernel_iter_head.py": 1679578012.6429307,
-        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/kernel_update_head.py": 1679578012.6429307,
-        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/kernel_updator.py": 1679578012.6429307,
-        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/mask.py": 1679578012.6429307,
-        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/semantic_fpn_wrapper.py": 1679578012.6429307,
-        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/track_heads.py": 1679578012.6429307,
-        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/neck/fpn.py": 1679578012.6429307,
-        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/track/quasi_dense_embed_tracker.py": 1679578012.6439307,
-        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/video_k_net.py": 1679578012.6439307,
-        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/visualizer.py": 1679578012.6439307,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/config/ostrack.py": 1670492342.9722378,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/layers/attn.py": 1670492342.9722378,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/layers/attn_blocks.py": 1679578012.6439307,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/layers/head.py": 1679578012.6439307,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/layers/patch_embed.py": 1670492342.9722378,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/ostrack/base_backbone.py": 1670492342.9722378,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/ostrack/ostrack.py": 1679578012.6439307,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/ostrack/utils.py": 1670492342.9722378,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/ostrack/vit_ce.py": 1670492342.973238,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/procontext/procontext.py": 1679578012.6439307,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/procontext/utils.py": 1679578012.6439307,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/procontext/vit_ce.py": 1679578012.6439307,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/tracker/ostrack.py": 1670492342.973238,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/tracker/procontext.py": 1679578012.6439307,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/utils/utils.py": 1670492342.973238,
-        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/DUT_raft.py": 1679578012.6439307,
-        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/MotionPro.py": 1679578012.6449306,
-        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/RAFT/corr.py": 1679578012.6449306,
-        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/RAFT/extractor.py": 1679578012.6449306,
-        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/RAFT/raft.py": 1679578012.6449306,
-        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/RAFT/update.py": 1679578012.6449306,
-        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/Smoother.py": 1679578012.6449306,
-        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/config.py": 1679578012.6449306,
-        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/rf_det_module.py": 1679578012.6449306,
-        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/rf_det_so.py": 1679578012.6449306,
-        "TEMPLATE_PATH/models/cv/video_stabilization/DUTRAFTStabilizer.py": 1679578012.6449306,
-        "TEMPLATE_PATH/models/cv/video_stabilization/utils/IterativeSmooth.py": 1679578012.6449306,
-        "TEMPLATE_PATH/models/cv/video_stabilization/utils/MedianFilter.py": 1679578012.6449306,
-        "TEMPLATE_PATH/models/cv/video_stabilization/utils/ProjectionUtils.py": 1679578012.6449306,
-        "TEMPLATE_PATH/models/cv/video_stabilization/utils/RAFTUtils.py": 1679578012.6449306,
-        "TEMPLATE_PATH/models/cv/video_stabilization/utils/WarpUtils.py": 1679578012.6459308,
-        "TEMPLATE_PATH/models/cv/video_stabilization/utils/image_utils.py": 1679578012.6459308,
-        "TEMPLATE_PATH/models/cv/video_stabilization/utils/math_utils.py": 1679578012.6459308,
-        "TEMPLATE_PATH/models/cv/video_streaming_perception/longshortnet/exp/longshortnet_base.py": 1679578012.6459308,
-        "TEMPLATE_PATH/models/cv/video_streaming_perception/longshortnet/longshortnet.py": 1679578012.6459308,
-        "TEMPLATE_PATH/models/cv/video_streaming_perception/longshortnet/models/dfp_pafpn_long.py": 1679578012.6459308,
-        "TEMPLATE_PATH/models/cv/video_streaming_perception/longshortnet/models/dfp_pafpn_short.py": 1679578012.6459308,
-        "TEMPLATE_PATH/models/cv/video_streaming_perception/longshortnet/models/longshort.py": 1679578012.6459308,
-        "TEMPLATE_PATH/models/cv/video_streaming_perception/longshortnet/models/longshort_backbone_neck.py": 1679578012.6459308,
-        "TEMPLATE_PATH/models/cv/video_summarization/base_model.py": 1670492342.973238,
-        "TEMPLATE_PATH/models/cv/video_summarization/kts/cpd_auto.py": 1670492342.973238,
-        "TEMPLATE_PATH/models/cv/video_summarization/kts/cpd_nonlin.py": 1670492342.973238,
-        "TEMPLATE_PATH/models/cv/video_summarization/pgl_sum.py": 1670492342.973238,
-        "TEMPLATE_PATH/models/cv/video_summarization/summarizer.py": 1679578012.6459308,
-        "TEMPLATE_PATH/models/cv/video_super_resolution/basicvsr_net.py": 1679578012.6469307,
-        "TEMPLATE_PATH/models/cv/video_super_resolution/common.py": 1679578012.6469307,
-        "TEMPLATE_PATH/models/cv/video_super_resolution/msrresnet_lite_model.py": 1679578012.6469307,
-        "TEMPLATE_PATH/models/cv/video_super_resolution/real_basicvsr_for_video_super_resolution.py": 1679578012.6469307,
-        "TEMPLATE_PATH/models/cv/video_super_resolution/real_basicvsr_net.py": 1679578012.6469307,
-        "TEMPLATE_PATH/models/cv/vidt/backbone.py": 1679578012.6469307,
-        "TEMPLATE_PATH/models/cv/vidt/deformable_transformer.py": 1679578012.6469307,
-        "TEMPLATE_PATH/models/cv/vidt/fpn_fusion.py": 1679578012.6469307,
-        "TEMPLATE_PATH/models/cv/vidt/head.py": 1679578012.6469307,
-        "TEMPLATE_PATH/models/cv/vidt/model.py": 1679578012.6469307,
-        "TEMPLATE_PATH/models/cv/virual_tryon/sdafnet.py": 1670492342.973238,
-        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/backbone.py": 1689760581.5386868,
-        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/head.py": 1679578012.6479306,
-        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/model.py": 1679578012.6479306,
-        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/petl.py": 1679578012.6479306,
-        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/timm_helpers.py": 1679578012.6479306,
-        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/timm_vision_transformer.py": 1679578012.6479306,
-        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/timm_weight_init.py": 1679578012.6479306,
-        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/vision_efficient_tuning.py": 1679578012.6479306,
-        "TEMPLATE_PATH/models/cv/vision_middleware/backbone.py": 1679578012.6479306,
-        "TEMPLATE_PATH/models/cv/vision_middleware/head.py": 1679578012.6479306,
-        "TEMPLATE_PATH/models/cv/vision_middleware/model.py": 1679578012.6489308,
-        "TEMPLATE_PATH/models/cv/vision_middleware/vim.py": 1679578012.6489308,
-        "TEMPLATE_PATH/models/cv/vop_retrieval/backbone.py": 1679578012.6489308,
-        "TEMPLATE_PATH/models/cv/vop_retrieval/basic_utils.py": 1679578012.6489308,
-        "TEMPLATE_PATH/models/cv/vop_retrieval/model.py": 1679578012.6489308,
-        "TEMPLATE_PATH/models/cv/vop_retrieval/model_se.py": 1679578012.6489308,
-        "TEMPLATE_PATH/models/cv/vop_retrieval/tokenization_clip.py": 1679578012.6489308,
-        "TEMPLATE_PATH/models/multi_modal/clip/bert_tokenizer.py": 1670492342.974238,
-        "TEMPLATE_PATH/models/multi_modal/clip/configuration_bert.py": 1670492342.974238,
-        "TEMPLATE_PATH/models/multi_modal/clip/model.py": 1670492342.974238,
-        "TEMPLATE_PATH/models/multi_modal/clip/modeling_bert.py": 1679578012.6489308,
-        "TEMPLATE_PATH/models/multi_modal/clip_interrogator/model.py": 1684224035.505264,
-        "TEMPLATE_PATH/models/multi_modal/diffusion/diffusion.py": 1679578012.6489308,
-        "TEMPLATE_PATH/models/multi_modal/diffusion/model.py": 1689760581.5386868,
-        "TEMPLATE_PATH/models/multi_modal/diffusion/structbert.py": 1679578012.6499307,
-        "TEMPLATE_PATH/models/multi_modal/diffusion/tokenizer.py": 1679578012.6499307,
-        "TEMPLATE_PATH/models/multi_modal/diffusion/unet_generator.py": 1679578012.6499307,
-        "TEMPLATE_PATH/models/multi_modal/diffusion/unet_upsampler_1024.py": 1679578012.6499307,
-        "TEMPLATE_PATH/models/multi_modal/diffusion/unet_upsampler_256.py": 1679578012.6499307,
-        "TEMPLATE_PATH/models/multi_modal/dpm_solver_pytorch.py": 1679578012.650931,
-        "TEMPLATE_PATH/models/multi_modal/efficient_diffusion_tuning/efficient_stable_diffusion.py": 1691036741.2883348,
-        "TEMPLATE_PATH/models/multi_modal/gemm/gemm_base.py": 1670492342.976238,
-        "TEMPLATE_PATH/models/multi_modal/gemm/gemm_model.py": 1670492342.976238,
-        "TEMPLATE_PATH/models/multi_modal/gemm/tokenizer.py": 1670492342.976238,
-        "TEMPLATE_PATH/models/multi_modal/guided_diffusion/gaussian_diffusion.py": 1679578012.650931,
-        "TEMPLATE_PATH/models/multi_modal/guided_diffusion/respace.py": 1679578012.650931,
-        "TEMPLATE_PATH/models/multi_modal/guided_diffusion/script.py": 1679578012.650931,
-        "TEMPLATE_PATH/models/multi_modal/guided_diffusion/unet.py": 1679578012.650931,
-        "TEMPLATE_PATH/models/multi_modal/mgeo/backbone.py": 1679578012.6519308,
-        "TEMPLATE_PATH/models/multi_modal/mgeo/text_classification.py": 1679578012.6519308,
-        "TEMPLATE_PATH/models/multi_modal/mgeo/text_ranking.py": 1679578012.6519308,
-        "TEMPLATE_PATH/models/multi_modal/mgeo/token_classification.py": 1679578012.6519308,
-        "TEMPLATE_PATH/models/multi_modal/mmr/dataloaders/rawvideo_util.py": 1670492342.976238,
-        "TEMPLATE_PATH/models/multi_modal/mmr/models/clip_for_mm_video_embedding.py": 1684224035.505264,
-        "TEMPLATE_PATH/models/multi_modal/mmr/models/dynamic_inverted_softmax.py": 1670492342.976238,
-        "TEMPLATE_PATH/models/multi_modal/mmr/models/modeling.py": 1670492342.976238,
-        "TEMPLATE_PATH/models/multi_modal/mmr/models/module_clip.py": 1670492342.976238,
-        "TEMPLATE_PATH/models/multi_modal/mmr/models/module_cross.py": 1670492342.977238,
-        "TEMPLATE_PATH/models/multi_modal/mmr/models/tokenization_clip.py": 1670492342.977238,
-        "TEMPLATE_PATH/models/multi_modal/mmr/models/until_module.py": 1670492342.977238,
-        "TEMPLATE_PATH/models/multi_modal/mplug/clip/clip.py": 1670492342.977238,
-        "TEMPLATE_PATH/models/multi_modal/mplug/configuration_mplug.py": 1679578012.6519308,
-        "TEMPLATE_PATH/models/multi_modal/mplug/modeling_mplug.py": 1679578012.6529307,
-        "TEMPLATE_PATH/models/multi_modal/mplug/mvit.py": 1679578012.6529307,
-        "TEMPLATE_PATH/models/multi_modal/mplug/predictor.py": 1670492342.9782379,
-        "TEMPLATE_PATH/models/multi_modal/mplug_for_all_tasks.py": 1679578012.6529307,
-        "TEMPLATE_PATH/models/multi_modal/mplug_owl/configuration_mplug_owl.py": 1691036741.2883348,
-        "TEMPLATE_PATH/models/multi_modal/mplug_owl/modeling_mplug_owl.py": 1691036741.289335,
-        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/clip.py": 1679578012.6529307,
-        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/decoder.py": 1670492342.9782379,
-        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/gaussian_diffusion.py": 1679578012.6529307,
-        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/model.py": 1679578012.6529307,
-        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/prior.py": 1670492342.9782379,
-        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/tokenizer.py": 1679578012.6529307,
-        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/upsampler.py": 1670492342.979238,
-        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/xglm.py": 1679578012.653931,
-        "TEMPLATE_PATH/models/multi_modal/ofa/configuration_mmspeech.py": 1679578012.653931,
-        "TEMPLATE_PATH/models/multi_modal/ofa/configuration_ofa.py": 1679578012.653931,
-        "TEMPLATE_PATH/models/multi_modal/ofa/generate/incremental_decoding_utils.py": 1670492342.979238,
-        "TEMPLATE_PATH/models/multi_modal/ofa/generate/multihead_attention.py": 1670492342.979238,
-        "TEMPLATE_PATH/models/multi_modal/ofa/generate/ngram_repeat_block.py": 1670492342.979238,
-        "TEMPLATE_PATH/models/multi_modal/ofa/generate/search.py": 1679578012.653931,
-        "TEMPLATE_PATH/models/multi_modal/ofa/generate/sequence_generator.py": 1670492342.980238,
-        "TEMPLATE_PATH/models/multi_modal/ofa/generate/token_generation_constraints.py": 1670492342.980238,
-        "TEMPLATE_PATH/models/multi_modal/ofa/generate/utils.py": 1679578012.653931,
-        "TEMPLATE_PATH/models/multi_modal/ofa/modeling_mmspeech.py": 1679578012.6549308,
-        "TEMPLATE_PATH/models/multi_modal/ofa/modeling_ofa.py": 1679578012.6549308,
-        "TEMPLATE_PATH/models/multi_modal/ofa/resnet.py": 1679578012.6549308,
-        "TEMPLATE_PATH/models/multi_modal/ofa/tokenization_ofa.py": 1670492342.981238,
-        "TEMPLATE_PATH/models/multi_modal/ofa/tokenization_ofa_fast.py": 1670492342.981238,
-        "TEMPLATE_PATH/models/multi_modal/ofa/utils/constant.py": 1679578012.6549308,
-        "TEMPLATE_PATH/models/multi_modal/ofa/utils/utils.py": 1679578012.6549308,
-        "TEMPLATE_PATH/models/multi_modal/ofa/vit.py": 1679578012.6549308,
-        "TEMPLATE_PATH/models/multi_modal/ofa_for_all_tasks.py": 1679578012.6559308,
-        "TEMPLATE_PATH/models/multi_modal/ofa_for_text_to_image_synthesis_model.py": 1679578012.6559308,
-        "TEMPLATE_PATH/models/multi_modal/rleg/model.py": 1679578012.6559308,
-        "TEMPLATE_PATH/models/multi_modal/rleg/rleg.py": 1679578012.6559308,
-        "TEMPLATE_PATH/models/multi_modal/soonet/blocks.py": 1679578012.6559308,
-        "TEMPLATE_PATH/models/multi_modal/soonet/clip.py": 1679578012.6559308,
-        "TEMPLATE_PATH/models/multi_modal/soonet/model.py": 1679578012.6559308,
-        "TEMPLATE_PATH/models/multi_modal/soonet/swin_transformer.py": 1679578012.6559308,
-        "TEMPLATE_PATH/models/multi_modal/soonet/tokenizer.py": 1679578012.6559308,
-        "TEMPLATE_PATH/models/multi_modal/soonet/utils.py": 1679578012.6559308,
-        "TEMPLATE_PATH/models/multi_modal/stable_diffusion/stable_diffusion.py": 1689760581.5386868,
-        "TEMPLATE_PATH/models/multi_modal/team/team_model.py": 1670492342.981238,
-        "TEMPLATE_PATH/models/multi_modal/team/utils.py": 1670492342.982238,
-        "TEMPLATE_PATH/models/multi_modal/video_synthesis/autoencoder.py": 1679578012.656931,
-        "TEMPLATE_PATH/models/multi_modal/video_synthesis/diffusion.py": 1679578150.0463462,
-        "TEMPLATE_PATH/models/multi_modal/video_synthesis/text_to_video_synthesis_model.py": 1691036741.289335,
-        "TEMPLATE_PATH/models/multi_modal/video_synthesis/unet_sd.py": 1679578012.656931,
-        "TEMPLATE_PATH/models/multi_modal/vldoc/conv_fpn_trans.py": 1679578012.656931,
-        "TEMPLATE_PATH/models/multi_modal/vldoc/convnext.py": 1679578012.656931,
-        "TEMPLATE_PATH/models/multi_modal/vldoc/model.py": 1679578012.6579309,
-        "TEMPLATE_PATH/models/multi_modal/vldoc/modeling_layout_roberta.py": 1679578012.6579309,
-        "TEMPLATE_PATH/models/multi_modal/vldoc/processing.py": 1679578012.6579309,
-        "TEMPLATE_PATH/models/multi_modal/vldoc/tokenization.py": 1679578012.6579309,
-        "TEMPLATE_PATH/models/multi_modal/vldoc/transformer_local.py": 1679578012.6579309,
-        "TEMPLATE_PATH/models/nlp/T5/backbone.py": 1681971907.4060361,
-        "TEMPLATE_PATH/models/nlp/T5/configuration.py": 1670492342.982238,
-        "TEMPLATE_PATH/models/nlp/T5/text2text_generation.py": 1689760581.5386868,
-        "TEMPLATE_PATH/models/nlp/bart/text_error_correction.py": 1679578012.6589308,
-        "TEMPLATE_PATH/models/nlp/bert/backbone.py": 1679578012.6589308,
-        "TEMPLATE_PATH/models/nlp/bert/configuration.py": 1679578012.6589308,
-        "TEMPLATE_PATH/models/nlp/bert/document_segmentation.py": 1670492342.983238,
-        "TEMPLATE_PATH/models/nlp/bert/fill_mask.py": 1679578012.6589308,
-        "TEMPLATE_PATH/models/nlp/bert/sentence_embedding.py": 1679578012.6589308,
-        "TEMPLATE_PATH/models/nlp/bert/siamese_uie.py": 1679578012.6589308,
-        "TEMPLATE_PATH/models/nlp/bert/text_classification.py": 1679578012.659931,
-        "TEMPLATE_PATH/models/nlp/bert/text_ranking.py": 1679578012.659931,
-        "TEMPLATE_PATH/models/nlp/bert/token_classification.py": 1679578012.659931,
-        "TEMPLATE_PATH/models/nlp/bert/word_alignment.py": 1679578012.659931,
-        "TEMPLATE_PATH/models/nlp/bloom/backbone.py": 1670492342.983238,
-        "TEMPLATE_PATH/models/nlp/canmt/canmt_model.py": 1681971907.4060361,
-        "TEMPLATE_PATH/models/nlp/canmt/canmt_translation.py": 1681971907.4060361,
-        "TEMPLATE_PATH/models/nlp/canmt/sequence_generator.py": 1681971907.4060361,
-        "TEMPLATE_PATH/models/nlp/chatglm/configuration.py": 1691036741.289335,
-        "TEMPLATE_PATH/models/nlp/chatglm/quantization.py": 1691036741.289335,
-        "TEMPLATE_PATH/models/nlp/chatglm/text_generation.py": 1691036741.290335,
-        "TEMPLATE_PATH/models/nlp/chatglm/tokenization.py": 1691036741.290335,
-        "TEMPLATE_PATH/models/nlp/chatglm2/configuration.py": 1691036741.290335,
-        "TEMPLATE_PATH/models/nlp/chatglm2/quantization.py": 1691036741.290335,
-        "TEMPLATE_PATH/models/nlp/chatglm2/text_generation.py": 1691036741.291335,
-        "TEMPLATE_PATH/models/nlp/chatglm2/tokenization.py": 1689760581.5406868,
-        "TEMPLATE_PATH/models/nlp/codegeex/codegeex.py": 1670492342.983238,
-        "TEMPLATE_PATH/models/nlp/codegeex/codegeex_for_code_generation.py": 1670492342.9842381,
-        "TEMPLATE_PATH/models/nlp/codegeex/codegeex_for_code_translation.py": 1670492342.9842381,
-        "TEMPLATE_PATH/models/nlp/codegeex/inference.py": 1670492342.9842381,
-        "TEMPLATE_PATH/models/nlp/codegeex/tokenizer.py": 1670492342.9842381,
-        "TEMPLATE_PATH/models/nlp/csanmt/translation.py": 1679578012.659931,
-        "TEMPLATE_PATH/models/nlp/deberta_v2/backbone.py": 1679578012.659931,
-        "TEMPLATE_PATH/models/nlp/deberta_v2/configuration.py": 1670492342.985238,
-        "TEMPLATE_PATH/models/nlp/deberta_v2/fill_mask.py": 1670492342.985238,
-        "TEMPLATE_PATH/models/nlp/deberta_v2/tokenization.py": 1670492342.985238,
-        "TEMPLATE_PATH/models/nlp/deberta_v2/tokenization_fast.py": 1670492342.985238,
-        "TEMPLATE_PATH/models/nlp/dgds/backbone.py": 1681971907.407036,
-        "TEMPLATE_PATH/models/nlp/dgds/document_grounded_dialog_generate.py": 1679578012.6609309,
-        "TEMPLATE_PATH/models/nlp/dgds/document_grounded_dialog_rerank.py": 1679578012.6609309,
-        "TEMPLATE_PATH/models/nlp/dgds/document_grounded_dialog_retrieval.py": 1679578012.6609309,
-        "TEMPLATE_PATH/models/nlp/fid_T5/text_generation.py": 1681971907.407036,
-        "TEMPLATE_PATH/models/nlp/fid_plug/backbone.py": 1691036741.291335,
-        "TEMPLATE_PATH/models/nlp/fid_plug/configuration.py": 1681971907.407036,
-        "TEMPLATE_PATH/models/nlp/fid_plug/text_generation.py": 1681971907.407036,
-        "TEMPLATE_PATH/models/nlp/glm_130b/generation/strategies.py": 1681971907.408036,
-        "TEMPLATE_PATH/models/nlp/glm_130b/initialize.py": 1681971907.408036,
-        "TEMPLATE_PATH/models/nlp/glm_130b/quantization/functional.py": 1681971907.408036,
-        "TEMPLATE_PATH/models/nlp/glm_130b/quantization/layers.py": 1681971907.408036,
-        "TEMPLATE_PATH/models/nlp/glm_130b/text_generation.py": 1681971907.4090362,
-        "TEMPLATE_PATH/models/nlp/gpt2/backbone.py": 1679578012.6609309,
-        "TEMPLATE_PATH/models/nlp/gpt3/backbone.py": 1687774806.9934692,
-        "TEMPLATE_PATH/models/nlp/gpt3/configuration.py": 1679578012.6609309,
-        "TEMPLATE_PATH/models/nlp/gpt3/distributed_gpt3.py": 1691036741.292335,
-        "TEMPLATE_PATH/models/nlp/gpt3/text_generation.py": 1691036741.292335,
-        "TEMPLATE_PATH/models/nlp/gpt3/tokenizer.py": 1679578012.6619308,
-        "TEMPLATE_PATH/models/nlp/gpt_moe/backbone.py": 1670492342.986238,
-        "TEMPLATE_PATH/models/nlp/gpt_moe/checkpointing.py": 1679578012.6619308,
-        "TEMPLATE_PATH/models/nlp/gpt_moe/configuration.py": 1670492342.986238,
-        "TEMPLATE_PATH/models/nlp/gpt_moe/distributed_gpt_moe.py": 1679578012.6619308,
-        "TEMPLATE_PATH/models/nlp/gpt_moe/moe/experts.py": 1670492342.986238,
-        "TEMPLATE_PATH/models/nlp/gpt_moe/moe/layer.py": 1679578012.6619308,
-        "TEMPLATE_PATH/models/nlp/gpt_moe/moe/mappings.py": 1679578012.6619308,
-        "TEMPLATE_PATH/models/nlp/gpt_moe/moe/sharded_moe.py": 1679578012.662931,
-        "TEMPLATE_PATH/models/nlp/gpt_moe/moe/utils.py": 1670492342.986238,
-        "TEMPLATE_PATH/models/nlp/gpt_moe/text_generation.py": 1679578012.662931,
-        "TEMPLATE_PATH/models/nlp/gpt_moe/tokenizer.py": 1670492342.986238,
-        "TEMPLATE_PATH/models/nlp/gpt_neo/backbone.py": 1670492342.9872382,
-        "TEMPLATE_PATH/models/nlp/heads/crf_head.py": 1679578012.662931,
-        "TEMPLATE_PATH/models/nlp/heads/fill_mask_head.py": 1681971907.4090362,
-        "TEMPLATE_PATH/models/nlp/heads/infromation_extraction_head.py": 1679578012.662931,
-        "TEMPLATE_PATH/models/nlp/heads/text_classification_head.py": 1679578012.662931,
-        "TEMPLATE_PATH/models/nlp/heads/text_generation_head.py": 1679578012.662931,
-        "TEMPLATE_PATH/models/nlp/heads/text_ranking_head.py": 1679578012.662931,
-        "TEMPLATE_PATH/models/nlp/heads/token_classification_head.py": 1679578012.662931,
-        "TEMPLATE_PATH/models/nlp/heads/torch_pretrain_head.py": 1670492342.9872382,
-        "TEMPLATE_PATH/models/nlp/hf_transformers/backbone.py": 1679578012.662931,
-        "TEMPLATE_PATH/models/nlp/llama/backbone.py": 1691036741.293335,
-        "TEMPLATE_PATH/models/nlp/llama/configuration.py": 1681971907.4090362,
-        "TEMPLATE_PATH/models/nlp/llama/convert_llama_weights_to_hf.py": 1681971907.410036,
-        "TEMPLATE_PATH/models/nlp/llama/text_generation.py": 1691036741.293335,
-        "TEMPLATE_PATH/models/nlp/llama/tokenization.py": 1691036741.293335,
-        "TEMPLATE_PATH/models/nlp/llama/tokenization_fast.py": 1691036741.293335,
-        "TEMPLATE_PATH/models/nlp/llama2/backbone.py": 1691036741.293335,
-        "TEMPLATE_PATH/models/nlp/llama2/configuration.py": 1691036741.293335,
-        "TEMPLATE_PATH/models/nlp/llama2/text_generation.py": 1691036741.2943351,
-        "TEMPLATE_PATH/models/nlp/llama2/tokenization.py": 1691036741.2943351,
-        "TEMPLATE_PATH/models/nlp/llama2/tokenization_fast.py": 1691036741.2943351,
-        "TEMPLATE_PATH/models/nlp/lstm/backbone.py": 1679578012.662931,
-        "TEMPLATE_PATH/models/nlp/lstm/token_classification.py": 1679578012.663931,
-        "TEMPLATE_PATH/models/nlp/megatron_bert/backbone.py": 1679578012.663931,
-        "TEMPLATE_PATH/models/nlp/megatron_bert/configuration.py": 1679578012.663931,
-        "TEMPLATE_PATH/models/nlp/megatron_bert/fill_mask.py": 1679578012.663931,
-        "TEMPLATE_PATH/models/nlp/mglm/arguments.py": 1670492342.9872382,
-        "TEMPLATE_PATH/models/nlp/mglm/blocklm_utils.py": 1684224035.506264,
-        "TEMPLATE_PATH/models/nlp/mglm/configure_data.py": 1679578012.663931,
-        "TEMPLATE_PATH/models/nlp/mglm/data_utils/corpora.py": 1670492342.988238,
-        "TEMPLATE_PATH/models/nlp/mglm/data_utils/datasets.py": 1684224035.506264,
-        "TEMPLATE_PATH/models/nlp/mglm/data_utils/extraction.py": 1679578012.6649308,
-        "TEMPLATE_PATH/models/nlp/mglm/data_utils/file_utils.py": 1670492342.988238,
-        "TEMPLATE_PATH/models/nlp/mglm/data_utils/lazy_loader.py": 1679578012.6649308,
-        "TEMPLATE_PATH/models/nlp/mglm/data_utils/samplers.py": 1670492342.988238,
-        "TEMPLATE_PATH/models/nlp/mglm/data_utils/sp_tokenizer.py": 1670492342.988238,
-        "TEMPLATE_PATH/models/nlp/mglm/data_utils/tokenization.py": 1670492342.989238,
-        "TEMPLATE_PATH/models/nlp/mglm/data_utils/tokenization_gpt2.py": 1670492342.989238,
-        "TEMPLATE_PATH/models/nlp/mglm/data_utils/wordpiece.py": 1679578012.6649308,
-        "TEMPLATE_PATH/models/nlp/mglm/generation_utils.py": 1670492342.989238,
-        "TEMPLATE_PATH/models/nlp/mglm/mglm_for_text_summarization.py": 1679578012.6649308,
-        "TEMPLATE_PATH/models/nlp/mglm/model/distributed.py": 1679578012.6649308,
-        "TEMPLATE_PATH/models/nlp/mglm/model/downstream.py": 1670492342.9902382,
-        "TEMPLATE_PATH/models/nlp/mglm/model/modeling_bert.py": 1679578012.665931,
-        "TEMPLATE_PATH/models/nlp/mglm/model/modeling_glm.py": 1679578012.665931,
-        "TEMPLATE_PATH/models/nlp/mglm/model/prompt.py": 1670492342.9902382,
-        "TEMPLATE_PATH/models/nlp/mglm/model/transformer.py": 1679578012.665931,
-        "TEMPLATE_PATH/models/nlp/mglm/process_grid.py": 1670492342.992238,
-        "TEMPLATE_PATH/models/nlp/mglm/run_test.py": 1670492342.992238,
-        "TEMPLATE_PATH/models/nlp/mglm/tasks/data_utils.py": 1679578012.665931,
-        "TEMPLATE_PATH/models/nlp/mglm/tasks/eval_utils.py": 1679578012.665931,
-        "TEMPLATE_PATH/models/nlp/mglm/tasks/language_model/dataset.py": 1670492342.992238,
-        "TEMPLATE_PATH/models/nlp/mglm/tasks/language_model/detokenizer.py": 1670492342.992238,
-        "TEMPLATE_PATH/models/nlp/mglm/tasks/language_model/finetune.py": 1679578012.665931,
-        "TEMPLATE_PATH/models/nlp/mglm/tasks/seq2seq/dataset.py": 1670492342.992238,
-        "TEMPLATE_PATH/models/nlp/mglm/tasks/seq2seq/evaluate.py": 1679578012.666931,
-        "TEMPLATE_PATH/models/nlp/mglm/tasks/seq2seq/finetune.py": 1679578012.666931,
-        "TEMPLATE_PATH/models/nlp/mglm/tasks/superglue/dataset.py": 1670492342.9932382,
-        "TEMPLATE_PATH/models/nlp/mglm/tasks/superglue/evaluate.py": 1670492342.9932382,
-        "TEMPLATE_PATH/models/nlp/mglm/tasks/superglue/finetune.py": 1670492342.9932382,
-        "TEMPLATE_PATH/models/nlp/mglm/tasks/superglue/pvp.py": 1679578012.666931,
-        "TEMPLATE_PATH/models/nlp/mglm/test/test_block.py": 1684224035.507264,
-        "TEMPLATE_PATH/models/nlp/mglm/test/test_rel_shift.py": 1684224035.507264,
-        "TEMPLATE_PATH/models/nlp/mglm/train_utils.py": 1679578012.666931,
-        "TEMPLATE_PATH/models/nlp/mglm/utils.py": 1679578012.666931,
-        "TEMPLATE_PATH/models/nlp/palm_v2/configuration.py": 1679578012.666931,
-        "TEMPLATE_PATH/models/nlp/palm_v2/dureader_eval.py": 1670492342.9942381,
-        "TEMPLATE_PATH/models/nlp/palm_v2/text_generation.py": 1679578012.6679308,
-        "TEMPLATE_PATH/models/nlp/peer/backbone.py": 1691036741.2943351,
-        "TEMPLATE_PATH/models/nlp/peer/configuration.py": 1691036741.2943351,
-        "TEMPLATE_PATH/models/nlp/peer/sas_utils.py": 1679578012.6679308,
-        "TEMPLATE_PATH/models/nlp/peer/text_classification.py": 1679578012.6679308,
-        "TEMPLATE_PATH/models/nlp/plug/AnnealingLR.py": 1670492342.9942381,
-        "TEMPLATE_PATH/models/nlp/plug/backbone.py": 1679578012.668931,
-        "TEMPLATE_PATH/models/nlp/plug/configuration.py": 1679578012.668931,
-        "TEMPLATE_PATH/models/nlp/plug/distributed_plug.py": 1679578012.668931,
-        "TEMPLATE_PATH/models/nlp/plug/generator.py": 1670492342.995238,
-        "TEMPLATE_PATH/models/nlp/plug_mental/adv_utils.py": 1679578012.668931,
-        "TEMPLATE_PATH/models/nlp/plug_mental/backbone.py": 1679578012.668931,
-        "TEMPLATE_PATH/models/nlp/plug_mental/configuration.py": 1679578012.668931,
-        "TEMPLATE_PATH/models/nlp/plug_mental/text_classification.py": 1679578012.668931,
-        "TEMPLATE_PATH/models/nlp/polylm/text_generation.py": 1691036741.295335,
-        "TEMPLATE_PATH/models/nlp/ponet/backbone.py": 1679578012.669931,
-        "TEMPLATE_PATH/models/nlp/ponet/configuration.py": 1670492342.995238,
-        "TEMPLATE_PATH/models/nlp/ponet/document_segmentation.py": 1670492342.995238,
-        "TEMPLATE_PATH/models/nlp/ponet/fill_mask.py": 1679578012.669931,
-        "TEMPLATE_PATH/models/nlp/ponet/tokenization.py": 1670492342.9962382,
-        "TEMPLATE_PATH/models/nlp/qwen/backbone.py": 1691036741.295335,
-        "TEMPLATE_PATH/models/nlp/qwen/configuration.py": 1691036741.295335,
-        "TEMPLATE_PATH/models/nlp/qwen/qwen_generation_utils.py": 1691036741.295335,
-        "TEMPLATE_PATH/models/nlp/qwen/text_generation.py": 1691036741.295335,
-        "TEMPLATE_PATH/models/nlp/qwen/tokenization.py": 1691036741.296335,
-        "TEMPLATE_PATH/models/nlp/space/configuration.py": 1670492342.9962382,
-        "TEMPLATE_PATH/models/nlp/space/dialog_intent_prediction.py": 1670492342.9962382,
-        "TEMPLATE_PATH/models/nlp/space/dialog_modeling.py": 1679578012.669931,
-        "TEMPLATE_PATH/models/nlp/space/dialog_state_tracking.py": 1670492342.9962382,
-        "TEMPLATE_PATH/models/nlp/space/model/gen_unified_transformer.py": 1670492342.9962382,
-        "TEMPLATE_PATH/models/nlp/space/model/generator.py": 1670492342.9962382,
-        "TEMPLATE_PATH/models/nlp/space/model/intent_unified_transformer.py": 1670492342.9962382,
-        "TEMPLATE_PATH/models/nlp/space/model/model_base.py": 1679578012.669931,
-        "TEMPLATE_PATH/models/nlp/space/model/tokenization_space.py": 1670492342.9962382,
-        "TEMPLATE_PATH/models/nlp/space/model/unified_transformer.py": 1679578012.669931,
-        "TEMPLATE_PATH/models/nlp/space/modules/embedder.py": 1670492342.9962382,
-        "TEMPLATE_PATH/models/nlp/space/modules/feedforward.py": 1670492342.9962382,
-        "TEMPLATE_PATH/models/nlp/space/modules/functions.py": 1670492342.9962382,
-        "TEMPLATE_PATH/models/nlp/space/modules/multihead_attention.py": 1670492342.9972382,
-        "TEMPLATE_PATH/models/nlp/space/modules/transformer_block.py": 1670492342.9972382,
-        "TEMPLATE_PATH/models/nlp/space_T_cn/backbone.py": 1679578012.669931,
-        "TEMPLATE_PATH/models/nlp/space_T_cn/configuration.py": 1670492342.9972382,
-        "TEMPLATE_PATH/models/nlp/space_T_cn/table_question_answering.py": 1679578012.6709309,
-        "TEMPLATE_PATH/models/nlp/space_T_en/text_to_sql.py": 1670492342.9972382,
-        "TEMPLATE_PATH/models/nlp/structbert/adv_utils.py": 1679578012.6709309,
-        "TEMPLATE_PATH/models/nlp/structbert/backbone.py": 1679578012.6709309,
-        "TEMPLATE_PATH/models/nlp/structbert/configuration.py": 1670492342.998238,
-        "TEMPLATE_PATH/models/nlp/structbert/faq_question_answering.py": 1679578012.6709309,
-        "TEMPLATE_PATH/models/nlp/structbert/fill_mask.py": 1679578012.6709309,
-        "TEMPLATE_PATH/models/nlp/structbert/text_classification.py": 1679578012.6709309,
-        "TEMPLATE_PATH/models/nlp/structbert/token_classification.py": 1679578012.6709309,
-        "TEMPLATE_PATH/models/nlp/task_models/feature_extraction.py": 1679578012.671931,
-        "TEMPLATE_PATH/models/nlp/task_models/fill_mask.py": 1679578012.671931,
-        "TEMPLATE_PATH/models/nlp/task_models/information_extraction.py": 1679578012.671931,
-        "TEMPLATE_PATH/models/nlp/task_models/task_model.py": 1681971907.410036,
-        "TEMPLATE_PATH/models/nlp/task_models/text_classification.py": 1679578012.671931,
-        "TEMPLATE_PATH/models/nlp/task_models/text_generation.py": 1681971907.410036,
-        "TEMPLATE_PATH/models/nlp/task_models/text_ranking.py": 1679578012.671931,
-        "TEMPLATE_PATH/models/nlp/task_models/token_classification.py": 1679578012.671931,
-        "TEMPLATE_PATH/models/nlp/unite/configuration.py": 1684224035.507264,
-        "TEMPLATE_PATH/models/nlp/unite/translation_evaluation.py": 1684224035.507264,
-        "TEMPLATE_PATH/models/nlp/use/transformer.py": 1679578012.672931,
-        "TEMPLATE_PATH/models/nlp/use/user_satisfaction_estimation.py": 1689760581.5416868,
-        "TEMPLATE_PATH/models/nlp/veco/backbone.py": 1670492342.9992383,
-        "TEMPLATE_PATH/models/nlp/veco/configuration.py": 1670492342.9992383,
-        "TEMPLATE_PATH/models/nlp/veco/fill_mask.py": 1670492342.9992383,
-        "TEMPLATE_PATH/models/nlp/veco/text_classification.py": 1670492342.9992383,
-        "TEMPLATE_PATH/models/nlp/veco/token_classification.py": 1670492342.9992383,
-        "TEMPLATE_PATH/models/nlp/xlm_roberta/backbone.py": 1679578012.672931,
-        "TEMPLATE_PATH/models/nlp/xlm_roberta/configuration.py": 1679578012.672931,
-        "TEMPLATE_PATH/models/science/unifold/config.py": 1670492342.9992383,
-        "TEMPLATE_PATH/models/science/unifold/data/data_ops.py": 1670492343.0002382,
-        "TEMPLATE_PATH/models/science/unifold/data/msa_pairing.py": 1684224035.507264,
-        "TEMPLATE_PATH/models/science/unifold/data/process.py": 1670492343.0002382,
-        "TEMPLATE_PATH/models/science/unifold/data/process_multimer.py": 1670492343.0002382,
-        "TEMPLATE_PATH/models/science/unifold/data/protein.py": 1670492343.0002382,
-        "TEMPLATE_PATH/models/science/unifold/data/residue_constants.py": 1670492343.001238,
-        "TEMPLATE_PATH/models/science/unifold/data/utils.py": 1670492343.001238,
-        "TEMPLATE_PATH/models/science/unifold/dataset.py": 1670492343.001238,
-        "TEMPLATE_PATH/models/science/unifold/model.py": 1670492343.001238,
-        "TEMPLATE_PATH/models/science/unifold/modules/alphafold.py": 1670492343.0022383,
-        "TEMPLATE_PATH/models/science/unifold/modules/attentions.py": 1670492343.0022383,
-        "TEMPLATE_PATH/models/science/unifold/modules/auxillary_heads.py": 1670492343.0022383,
-        "TEMPLATE_PATH/models/science/unifold/modules/common.py": 1670492343.0022383,
-        "TEMPLATE_PATH/models/science/unifold/modules/confidence.py": 1670492343.0032382,
-        "TEMPLATE_PATH/models/science/unifold/modules/embedders.py": 1670492343.0032382,
-        "TEMPLATE_PATH/models/science/unifold/modules/evoformer.py": 1670492343.0032382,
-        "TEMPLATE_PATH/models/science/unifold/modules/featurization.py": 1670492343.0032382,
-        "TEMPLATE_PATH/models/science/unifold/modules/frame.py": 1670492343.0042381,
-        "TEMPLATE_PATH/models/science/unifold/modules/structure_module.py": 1670492343.0042381,
-        "TEMPLATE_PATH/models/science/unifold/modules/template.py": 1670492343.0042381,
-        "TEMPLATE_PATH/models/science/unifold/modules/triangle_multiplication.py": 1670492343.0042381,
-        "TEMPLATE_PATH/models/science/unifold/msa/mmcif.py": 1670492343.0052383,
-        "TEMPLATE_PATH/models/science/unifold/msa/msa_identifiers.py": 1670492343.0052383,
-        "TEMPLATE_PATH/models/science/unifold/msa/parsers.py": 1670492343.0062382,
-        "TEMPLATE_PATH/models/science/unifold/msa/pipeline.py": 1670492343.0062382,
-        "TEMPLATE_PATH/models/science/unifold/msa/templates.py": 1684224035.508264,
-        "TEMPLATE_PATH/models/science/unifold/msa/tools/hhblits.py": 1670492343.0072381,
-        "TEMPLATE_PATH/models/science/unifold/msa/tools/hhsearch.py": 1670492343.0072381,
-        "TEMPLATE_PATH/models/science/unifold/msa/tools/hmmbuild.py": 1670492343.0072381,
-        "TEMPLATE_PATH/models/science/unifold/msa/tools/hmmsearch.py": 1670492343.0082383,
-        "TEMPLATE_PATH/models/science/unifold/msa/tools/jackhmmer.py": 1670492343.0082383,
-        "TEMPLATE_PATH/models/science/unifold/msa/tools/kalign.py": 1670492343.0082383,
-        "TEMPLATE_PATH/models/science/unifold/msa/tools/utils.py": 1670492343.0082383,
-        "TEMPLATE_PATH/models/science/unifold/msa/utils.py": 1670492343.0082383,
-        "TEMPLATE_PATH/msdatasets/audio/asr_dataset.py": 1679578012.672931,
-        "TEMPLATE_PATH/msdatasets/auth/auth_config.py": 1681973542.0311236,
-        "TEMPLATE_PATH/msdatasets/context/dataset_context_config.py": 1689760581.5416868,
-        "TEMPLATE_PATH/msdatasets/data_files/data_files_manager.py": 1689760581.5416868,
-        "TEMPLATE_PATH/msdatasets/data_loader/data_loader.py": 1687774806.9944694,
-        "TEMPLATE_PATH/msdatasets/data_loader/data_loader_manager.py": 1687774806.9944694,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/audio/asr_dataset.py": 1689760581.5416868,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/audio/kws_farfield_dataset.py": 1679578012.6739311,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/audio/kws_nearfield_dataset.py": 1681971907.4110363,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/audio/kws_nearfield_processor.py": 1681971907.4110363,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/bad_image_detecting/bad_image_detecting_dataset.py": 1679578012.6739311,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/builder.py": 1679578012.6739311,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/build.py": 1679578012.6739311,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/collate_batch.py": 1679578012.6739311,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/coco.py": 1679578012.6739311,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/mosaic_wrapper.py": 1679578012.674931,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/coco/coco_eval.py": 1679578012.674931,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/distributed.py": 1679578012.674931,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/grouped_batch_sampler.py": 1679578012.674931,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/iteration_based_batch_sampler.py": 1679578012.674931,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/build.py": 1679578012.674931,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/transforms.py": 1679578012.674931,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/easycv_base.py": 1679578012.674931,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/gopro_image_deblurring_dataset.py": 1679578012.674931,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_colorization/image_colorization_dataset.py": 1679578012.675931,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_inpainting/aug.py": 1679578012.675931,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_inpainting/image_inpainting_dataset.py": 1679578012.675931,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_instance_segmentation_coco_dataset.py": 1679578012.675931,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/data_utils.py": 1679578012.675931,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/image_portrait_enhancement_dataset.py": 1679578012.675931,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_quality_assessment_degradation/image_quality_assessment_degradation_dataset.py": 1679578012.675931,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_quality_assmessment_mos/image_quality_assessment_mos_dataset.py": 1679578012.675931,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/language_guided_video_summarization_dataset.py": 1679578012.675931,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/mgeo_ranking_dataset.py": 1679578012.6769311,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/movie_scene_segmentation_dataset.py": 1679578012.6769311,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/sampler.py": 1679578012.6769311,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/augmenter.py": 1679578012.6769311,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/data_loader.py": 1679578012.6769311,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/image_dataset.py": 1679578012.6769311,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/iou_evaluator.py": 1679578012.6769311,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/quad_measurer.py": 1679578012.6769311,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/augment_data.py": 1679578012.6769311,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/data_process.py": 1679578012.6769311,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_border_map.py": 1679578012.6769311,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_icdar_data.py": 1679578012.6769311,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_seg_detection_data.py": 1679578012.6769311,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/normalize_image.py": 1679578012.677931,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/random_crop_data.py": 1679578012.677931,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_recognition_dataset.py": 1687774806.9944694,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/reds_image_deblurring_dataset.py": 1679578012.677931,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/referring_video_object_segmentation_dataset.py": 1679578012.677931,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/transformers.py": 1679578012.677931,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/data_utils.py": 1679578012.677931,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/sidd_image_denoising_dataset.py": 1679578012.677931,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/transforms.py": 1679578012.677931,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/text_ranking_dataset.py": 1679578012.677931,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/torch_custom_dataset.py": 1679578012.677931,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/veco_dataset.py": 1679578012.677931,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/data_utils.py": 1679578012.677931,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/video_frame_interpolation_dataset.py": 1679578012.678931,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/video_stabilization/video_stabilization_dataset.py": 1679578012.678931,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/video_summarization_dataset.py": 1679578012.678931,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/video_super_resolution/video_super_resolution_dataset.py": 1679578012.678931,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/dataset.py": 1689760581.5416868,
-        "TEMPLATE_PATH/msdatasets/download/dataset_builder.py": 1689760581.542687,
-        "TEMPLATE_PATH/msdatasets/download/download_config.py": 1687774806.9954693,
-        "TEMPLATE_PATH/msdatasets/download/download_manager.py": 1679578012.678931,
-        "TEMPLATE_PATH/msdatasets/meta/data_meta_config.py": 1679578012.678931,
-        "TEMPLATE_PATH/msdatasets/meta/data_meta_manager.py": 1687774806.9954693,
-        "TEMPLATE_PATH/msdatasets/ms_dataset.py": 1689760581.542687,
-        "TEMPLATE_PATH/msdatasets/task_datasets/gopro_image_deblurring_dataset.py": 1679578012.6799312,
-        "TEMPLATE_PATH/msdatasets/task_datasets/reds_image_deblurring_dataset.py": 1679578012.6799312,
-        "TEMPLATE_PATH/msdatasets/task_datasets/sidd_image_denoising.py": 1679578012.6799312,
-        "TEMPLATE_PATH/msdatasets/task_datasets/torch_base_dataset.py": 1679578012.6799312,
-        "TEMPLATE_PATH/msdatasets/task_datasets/video_summarization_dataset.py": 1679578012.6799312,
-        "TEMPLATE_PATH/msdatasets/utils/dataset_utils.py": 1689760581.542687,
-        "TEMPLATE_PATH/msdatasets/utils/delete_utils.py": 1670492343.0132384,
-        "TEMPLATE_PATH/msdatasets/utils/maxcompute_utils.py": 1684224035.509264,
-        "TEMPLATE_PATH/msdatasets/utils/oss_utils.py": 1687774806.9954693,
-        "TEMPLATE_PATH/msdatasets/utils/upload_utils.py": 1679578012.6799312,
-        "TEMPLATE_PATH/pipelines/audio/ans_dfsmn_pipeline.py": 1689760581.542687,
-        "TEMPLATE_PATH/pipelines/audio/ans_pipeline.py": 1679578012.681931,
-        "TEMPLATE_PATH/pipelines/audio/asr_inference_pipeline.py": 1691036741.2973351,
-        "TEMPLATE_PATH/pipelines/audio/asr_wenet_inference_pipeline.py": 1670492343.0132384,
-        "TEMPLATE_PATH/pipelines/audio/inverse_text_processing_pipeline.py": 1679578012.681931,
-        "TEMPLATE_PATH/pipelines/audio/kws_farfield_pipeline.py": 1679578012.681931,
-        "TEMPLATE_PATH/pipelines/audio/kws_kwsbp_pipeline.py": 1679578012.681931,
-        "TEMPLATE_PATH/pipelines/audio/language_recognition_pipeline.py": 1691036741.2973351,
-        "TEMPLATE_PATH/pipelines/audio/linear_aec_pipeline.py": 1689760581.542687,
-        "TEMPLATE_PATH/pipelines/audio/lm_infer_pipeline.py": 1687774806.9964693,
-        "TEMPLATE_PATH/pipelines/audio/punctuation_processing_pipeline.py": 1687774806.9964693,
-        "TEMPLATE_PATH/pipelines/audio/segmentation_clustering_pipeline.py": 1691036741.298335,
-        "TEMPLATE_PATH/pipelines/audio/separation_pipeline.py": 1679578012.6829312,
-        "TEMPLATE_PATH/pipelines/audio/speaker_change_locating_pipeline.py": 1691036741.298335,
-        "TEMPLATE_PATH/pipelines/audio/speaker_diarization_dialogue_detection_pipeline.py": 1691036741.298335,
-        "TEMPLATE_PATH/pipelines/audio/speaker_diarization_pipeline.py": 1691036741.298335,
-        "TEMPLATE_PATH/pipelines/audio/speaker_diarization_semantic_speaker_turn_detection_pipeline.py": 1691036741.298335,
-        "TEMPLATE_PATH/pipelines/audio/speaker_verification_eres2net_pipeline.py": 1687774806.9964693,
-        "TEMPLATE_PATH/pipelines/audio/speaker_verification_light_pipeline.py": 1691036741.298335,
-        "TEMPLATE_PATH/pipelines/audio/speaker_verification_pipeline.py": 1691036741.299335,
-        "TEMPLATE_PATH/pipelines/audio/speaker_verification_rdino_pipeline.py": 1684224035.511264,
-        "TEMPLATE_PATH/pipelines/audio/text_to_speech_pipeline.py": 1679578012.6829312,
-        "TEMPLATE_PATH/pipelines/audio/timestamp_pipeline.py": 1689760581.5436869,
-        "TEMPLATE_PATH/pipelines/audio/voice_activity_detection_pipeline.py": 1687774806.9974694,
-        "TEMPLATE_PATH/pipelines/base.py": 1691036741.299335,
-        "TEMPLATE_PATH/pipelines/builder.py": 1687774806.9974694,
-        "TEMPLATE_PATH/pipelines/cv/action_detection_pipeline.py": 1670492343.0142384,
-        "TEMPLATE_PATH/pipelines/cv/action_recognition_pipeline.py": 1670492343.0142384,
-        "TEMPLATE_PATH/pipelines/cv/animal_recognition_pipeline.py": 1670492343.0142384,
-        "TEMPLATE_PATH/pipelines/cv/arc_face_recognition_pipeline.py": 1681971907.4140363,
-        "TEMPLATE_PATH/pipelines/cv/bad_image_detecting_pipeline.py": 1679578012.683931,
-        "TEMPLATE_PATH/pipelines/cv/body_2d_keypoints_pipeline.py": 1670492343.0142384,
-        "TEMPLATE_PATH/pipelines/cv/body_3d_keypoints_pipeline.py": 1679578012.683931,
-        "TEMPLATE_PATH/pipelines/cv/card_detection_pipeline.py": 1687774806.9974694,
-        "TEMPLATE_PATH/pipelines/cv/cmdssl_video_embedding_pipeline.py": 1670492343.0142384,
-        "TEMPLATE_PATH/pipelines/cv/content_check_pipeline.py": 1679578012.683931,
-        "TEMPLATE_PATH/pipelines/cv/controllable_image_generation_pipeline.py": 1679578012.683931,
-        "TEMPLATE_PATH/pipelines/cv/crowd_counting_pipeline.py": 1670492343.0152383,
-        "TEMPLATE_PATH/pipelines/cv/ddcolor_image_colorization_pipeline.py": 1679578012.683931,
-        "TEMPLATE_PATH/pipelines/cv/ddpm_semantic_segmentation_pipeline.py": 1679578012.683931,
-        "TEMPLATE_PATH/pipelines/cv/face_attribute_recognition_pipeline.py": 1681971907.4140363,
-        "TEMPLATE_PATH/pipelines/cv/face_detection_pipeline.py": 1687774806.9974694,
-        "TEMPLATE_PATH/pipelines/cv/face_emotion_pipeline.py": 1670492343.0152383,
-        "TEMPLATE_PATH/pipelines/cv/face_human_hand_detection_pipeline.py": 1670492343.0152383,
-        "TEMPLATE_PATH/pipelines/cv/face_image_generation_pipeline.py": 1670492343.0152383,
-        "TEMPLATE_PATH/pipelines/cv/face_liveness_ir_pipeline.py": 1681971907.4140363,
-        "TEMPLATE_PATH/pipelines/cv/face_liveness_xc_pipeline.py": 1681971907.4140363,
-        "TEMPLATE_PATH/pipelines/cv/face_processing_base_pipeline.py": 1681971907.4140363,
-        "TEMPLATE_PATH/pipelines/cv/face_quality_assessment_pipeline.py": 1681971907.4140363,
-        "TEMPLATE_PATH/pipelines/cv/face_recognition_onnx_fm_pipeline.py": 1681971907.4140363,
-        "TEMPLATE_PATH/pipelines/cv/face_recognition_onnx_ir_pipeline.py": 1681971907.4140363,
-        "TEMPLATE_PATH/pipelines/cv/face_recognition_ood_pipeline.py": 1681971907.4150362,
-        "TEMPLATE_PATH/pipelines/cv/face_recognition_pipeline.py": 1679578012.684931,
-        "TEMPLATE_PATH/pipelines/cv/face_reconstruction_pipeline.py": 1689760581.5436869,
-        "TEMPLATE_PATH/pipelines/cv/facial_expression_recognition_pipeline.py": 1681971907.4150362,
-        "TEMPLATE_PATH/pipelines/cv/facial_landmark_confidence_pipeline.py": 1681971907.4150362,
-        "TEMPLATE_PATH/pipelines/cv/fast_instance_segmentation_pipeline.py": 1684224035.512264,
-        "TEMPLATE_PATH/pipelines/cv/general_recognition_pipeline.py": 1670492343.0152383,
-        "TEMPLATE_PATH/pipelines/cv/hand_static_pipeline.py": 1670492343.0152383,
-        "TEMPLATE_PATH/pipelines/cv/hicossl_video_embedding_pipeline.py": 1670492343.0152383,
-        "TEMPLATE_PATH/pipelines/cv/human_reconstruction_pipeline.py": 1679578012.684931,
-        "TEMPLATE_PATH/pipelines/cv/image_body_reshaping_pipeline.py": 1670492343.0152383,
-        "TEMPLATE_PATH/pipelines/cv/image_bts_depth_estimation_pipeline.py": 1679578012.684931,
-        "TEMPLATE_PATH/pipelines/cv/image_cartoon_pipeline.py": 1670492343.0162385,
-        "TEMPLATE_PATH/pipelines/cv/image_classification_pipeline.py": 1679578012.684931,
-        "TEMPLATE_PATH/pipelines/cv/image_color_enhance_pipeline.py": 1679578012.684931,
-        "TEMPLATE_PATH/pipelines/cv/image_colorization_pipeline.py": 1670492343.0162385,
-        "TEMPLATE_PATH/pipelines/cv/image_debanding_pipeline.py": 1679578012.684931,
-        "TEMPLATE_PATH/pipelines/cv/image_deblur_pipeline.py": 1679578012.684931,
-        "TEMPLATE_PATH/pipelines/cv/image_defrcn_fewshot_pipeline.py": 1679578012.684931,
-        "TEMPLATE_PATH/pipelines/cv/image_denoise_pipeline.py": 1670492343.0162385,
-        "TEMPLATE_PATH/pipelines/cv/image_depth_estimation_pipeline.py": 1670492343.0162385,
-        "TEMPLATE_PATH/pipelines/cv/image_detection_pipeline.py": 1687774806.9974694,
-        "TEMPLATE_PATH/pipelines/cv/image_driving_perception_pipeline.py": 1679578012.6859312,
-        "TEMPLATE_PATH/pipelines/cv/image_face_fusion_pipeline.py": 1679578012.6859312,
-        "TEMPLATE_PATH/pipelines/cv/image_human_parsing_pipeline.py": 1679578012.6859312,
-        "TEMPLATE_PATH/pipelines/cv/image_inpainting_pipeline.py": 1670492343.0162385,
-        "TEMPLATE_PATH/pipelines/cv/image_inpainting_sdv2_pipeline.py": 1679578012.6859312,
-        "TEMPLATE_PATH/pipelines/cv/image_instance_segmentation_pipeline.py": 1679578012.6859312,
-        "TEMPLATE_PATH/pipelines/cv/image_matching_pipeline.py": 1679578012.6859312,
-        "TEMPLATE_PATH/pipelines/cv/image_matting_pipeline.py": 1684224035.512264,
-        "TEMPLATE_PATH/pipelines/cv/image_mvs_depth_estimation_pipeline.py": 1679578012.6859312,
-        "TEMPLATE_PATH/pipelines/cv/image_open_vocabulary_detection_pipeline.py": 1679578012.6859312,
-        "TEMPLATE_PATH/pipelines/cv/image_paintbyexample_pipeline.py": 1679578012.6859312,
-        "TEMPLATE_PATH/pipelines/cv/image_panoptic_segmentation_pipeline.py": 1689760581.5436869,
-        "TEMPLATE_PATH/pipelines/cv/image_portrait_enhancement_pipeline.py": 1679578012.6859312,
-        "TEMPLATE_PATH/pipelines/cv/image_quality_assessment_degradation_pipeline.py": 1679578012.6859312,
-        "TEMPLATE_PATH/pipelines/cv/image_quality_assessment_man_pipeline.py": 1679578012.6859312,
-        "TEMPLATE_PATH/pipelines/cv/image_quality_assessment_mos_pipeline.py": 1679578012.6859312,
-        "TEMPLATE_PATH/pipelines/cv/image_reid_person_pipeline.py": 1670492343.0162385,
-        "TEMPLATE_PATH/pipelines/cv/image_restoration_pipeline.py": 1679578012.6859312,
-        "TEMPLATE_PATH/pipelines/cv/image_salient_detection_pipeline.py": 1678888948.764931,
-        "TEMPLATE_PATH/pipelines/cv/image_semantic_segmentation_pipeline.py": 1670492343.0162385,
-        "TEMPLATE_PATH/pipelines/cv/image_skychange_pipeline.py": 1679578012.6869311,
-        "TEMPLATE_PATH/pipelines/cv/image_structured_model_probing_pipeline.py": 1679578012.6869311,
-        "TEMPLATE_PATH/pipelines/cv/image_style_transfer_pipeline.py": 1684224035.512264,
-        "TEMPLATE_PATH/pipelines/cv/image_super_resolution_pipeline.py": 1670492343.0162385,
-        "TEMPLATE_PATH/pipelines/cv/image_to_image_generate_pipeline.py": 1670492343.0162385,
-        "TEMPLATE_PATH/pipelines/cv/image_to_image_translation_pipeline.py": 1670492343.0162385,
-        "TEMPLATE_PATH/pipelines/cv/image_try_on_pipeline.py": 1691036741.299335,
-        "TEMPLATE_PATH/pipelines/cv/indoor_layout_estimation_pipeline.py": 1679578012.6869311,
-        "TEMPLATE_PATH/pipelines/cv/language_guided_video_summarization_pipeline.py": 1679578012.6869311,
-        "TEMPLATE_PATH/pipelines/cv/license_plate_detection_pipeline.py": 1670492343.0172384,
-        "TEMPLATE_PATH/pipelines/cv/lineless_table_recognition_pipeline.py": 1679578012.6869311,
-        "TEMPLATE_PATH/pipelines/cv/live_category_pipeline.py": 1670492343.0172384,
-        "TEMPLATE_PATH/pipelines/cv/mask_face_recognition_pipeline.py": 1679578012.6869311,
-        "TEMPLATE_PATH/pipelines/cv/maskdino_instance_segmentation_pipeline.py": 1679578012.6869311,
-        "TEMPLATE_PATH/pipelines/cv/mobile_image_super_resolution_pipeline.py": 1679578012.6869311,
-        "TEMPLATE_PATH/pipelines/cv/mog_face_detection_pipeline.py": 1670492343.0172384,
-        "TEMPLATE_PATH/pipelines/cv/motion_generation_pipeline.py": 1679578012.6869311,
-        "TEMPLATE_PATH/pipelines/cv/movie_scene_segmentation_pipeline.py": 1681973542.0321236,
-        "TEMPLATE_PATH/pipelines/cv/mtcnn_face_detection_pipeline.py": 1670492343.0172384,
-        "TEMPLATE_PATH/pipelines/cv/nerf_recon_4k_pipeline.py": 1691036741.299335,
-        "TEMPLATE_PATH/pipelines/cv/nerf_recon_acc_pipeline.py": 1679578012.6869311,
-        "TEMPLATE_PATH/pipelines/cv/nerf_recon_vq_compression_pipeline.py": 1691036741.299335,
-        "TEMPLATE_PATH/pipelines/cv/object_detection_3d_pipeline.py": 1679578012.6869311,
-        "TEMPLATE_PATH/pipelines/cv/ocr_detection_pipeline.py": 1679578012.6869311,
-        "TEMPLATE_PATH/pipelines/cv/ocr_recognition_pipeline.py": 1687774806.9974694,
-        "TEMPLATE_PATH/pipelines/cv/ocr_utils/model_convnext_transformer.py": 1670492343.0172384,
-        "TEMPLATE_PATH/pipelines/cv/ocr_utils/model_dla34.py": 1670492343.0172384,
-        "TEMPLATE_PATH/pipelines/cv/ocr_utils/model_resnet18_half.py": 1670492343.0172384,
-        "TEMPLATE_PATH/pipelines/cv/ocr_utils/model_resnet_mutex_v4_linewithchar.py": 1670492343.0172384,
-        "TEMPLATE_PATH/pipelines/cv/ocr_utils/model_vlpt.py": 1679578012.6869311,
-        "TEMPLATE_PATH/pipelines/cv/ocr_utils/ocr_modules/convnext.py": 1670492343.0172384,
-        "TEMPLATE_PATH/pipelines/cv/ocr_utils/ocr_modules/timm_tinyc.py": 1670492343.0172384,
-        "TEMPLATE_PATH/pipelines/cv/ocr_utils/ocr_modules/vitstr.py": 1670492343.0182383,
-        "TEMPLATE_PATH/pipelines/cv/ocr_utils/ops.py": 1679578012.687931,
-        "TEMPLATE_PATH/pipelines/cv/ocr_utils/resnet18_v1.py": 1670492343.0182383,
-        "TEMPLATE_PATH/pipelines/cv/ocr_utils/resnet_utils.py": 1670492343.0182383,
-        "TEMPLATE_PATH/pipelines/cv/ocr_utils/table_process.py": 1670492343.0182383,
-        "TEMPLATE_PATH/pipelines/cv/ocr_utils/utils.py": 1679578012.687931,
-        "TEMPLATE_PATH/pipelines/cv/panorama_depth_estimation_pipeline.py": 1679578012.687931,
-        "TEMPLATE_PATH/pipelines/cv/panorama_depth_estimation_s2net_pipeline.py": 1691036741.299335,
-        "TEMPLATE_PATH/pipelines/cv/pedestrian_attribute_recognition_pipeline.py": 1681971907.4150362,
-        "TEMPLATE_PATH/pipelines/cv/pointcloud_sceneflow_estimation_pipeline.py": 1679578012.687931,
-        "TEMPLATE_PATH/pipelines/cv/product_retrieval_embedding_pipeline.py": 1670492343.0182383,
-        "TEMPLATE_PATH/pipelines/cv/product_segmentation_pipeline.py": 1670492343.0182383,
-        "TEMPLATE_PATH/pipelines/cv/realtime_video_object_detection_pipeline.py": 1679578012.687931,
-        "TEMPLATE_PATH/pipelines/cv/referring_video_object_segmentation_pipeline.py": 1691036741.3003352,
-        "TEMPLATE_PATH/pipelines/cv/retina_face_detection_pipeline.py": 1670492343.0182383,
-        "TEMPLATE_PATH/pipelines/cv/shop_segmentation_pipleline.py": 1670492343.0182383,
-        "TEMPLATE_PATH/pipelines/cv/skin_retouching_pipeline.py": 1684224035.512264,
-        "TEMPLATE_PATH/pipelines/cv/table_recognition_pipeline.py": 1670492343.0192385,
-        "TEMPLATE_PATH/pipelines/cv/tbs_detection_pipeline.py": 1684224035.512264,
-        "TEMPLATE_PATH/pipelines/cv/tbs_detection_utils/utils.py": 1679578150.047346,
-        "TEMPLATE_PATH/pipelines/cv/text_driven_segmentation_pipleline.py": 1670492343.0192385,
-        "TEMPLATE_PATH/pipelines/cv/text_to_360panorama_image_pipeline.py": 1691036741.3003352,
-        "TEMPLATE_PATH/pipelines/cv/tinynas_classification_pipeline.py": 1670492343.0192385,
-        "TEMPLATE_PATH/pipelines/cv/tinynas_detection_pipeline.py": 1679578012.687931,
-        "TEMPLATE_PATH/pipelines/cv/ulfd_face_detection_pipeline.py": 1670492343.0192385,
-        "TEMPLATE_PATH/pipelines/cv/video_category_pipeline.py": 1670492343.0192385,
-        "TEMPLATE_PATH/pipelines/cv/video_colorization_pipeline.py": 1679578012.687931,
-        "TEMPLATE_PATH/pipelines/cv/video_deinterlace_pipeline.py": 1679578012.6889312,
-        "TEMPLATE_PATH/pipelines/cv/video_depth_estimation_pipeline.py": 1679578012.6889312,
-        "TEMPLATE_PATH/pipelines/cv/video_frame_interpolation_pipeline.py": 1679578012.6889312,
-        "TEMPLATE_PATH/pipelines/cv/video_human_matting_pipeline.py": 1679578012.6889312,
-        "TEMPLATE_PATH/pipelines/cv/video_inpainting_pipeline.py": 1670492343.0192385,
-        "TEMPLATE_PATH/pipelines/cv/video_instance_segmentation_pipeline.py": 1679578012.6889312,
-        "TEMPLATE_PATH/pipelines/cv/video_multi_object_tracking_pipeline.py": 1679578012.6889312,
-        "TEMPLATE_PATH/pipelines/cv/video_object_segmentation_pipeline.py": 1679578012.6889312,
-        "TEMPLATE_PATH/pipelines/cv/video_panoptic_segmentation_pipeline.py": 1679578012.6889312,
-        "TEMPLATE_PATH/pipelines/cv/video_single_object_tracking_pipeline.py": 1679578012.6889312,
-        "TEMPLATE_PATH/pipelines/cv/video_stabilization_pipeline.py": 1679578012.6889312,
-        "TEMPLATE_PATH/pipelines/cv/video_summarization_pipeline.py": 1670492343.0192385,
-        "TEMPLATE_PATH/pipelines/cv/video_super_resolution_pipeline.py": 1679578012.6889312,
-        "TEMPLATE_PATH/pipelines/cv/vidt_pipeline.py": 1679578012.6889312,
-        "TEMPLATE_PATH/pipelines/cv/virtual_try_on_pipeline.py": 1670492343.0192385,
-        "TEMPLATE_PATH/pipelines/cv/vision_efficient_tuning_pipeline.py": 1679578012.6889312,
-        "TEMPLATE_PATH/pipelines/cv/vision_middleware_pipeline.py": 1679578012.6889312,
-        "TEMPLATE_PATH/pipelines/cv/vop_retrieval_pipeline.py": 1679578012.6889312,
-        "TEMPLATE_PATH/pipelines/cv/vop_retrieval_se_pipeline.py": 1679578012.6899312,
-        "TEMPLATE_PATH/pipelines/multi_modal/asr_pipeline.py": 1679578012.6899312,
-        "TEMPLATE_PATH/pipelines/multi_modal/diffusers_wrapped/diffusers_pipeline.py": 1689760581.5436869,
-        "TEMPLATE_PATH/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/chinese_stable_diffusion_pipeline.py": 1689760581.5446868,
-        "TEMPLATE_PATH/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/stable_diffusion_pipeline.py": 1691036741.3003352,
-        "TEMPLATE_PATH/pipelines/multi_modal/disco_guided_diffusion_pipeline/disco_guided_diffusion.py": 1679578012.6899312,
-        "TEMPLATE_PATH/pipelines/multi_modal/disco_guided_diffusion_pipeline/utils.py": 1679578012.6899312,
-        "TEMPLATE_PATH/pipelines/multi_modal/document_vl_embedding_pipeline.py": 1679578012.6899312,
-        "TEMPLATE_PATH/pipelines/multi_modal/efficient_diffusion_tuning_pipeline.py": 1691036741.3003352,
-        "TEMPLATE_PATH/pipelines/multi_modal/generative_multi_modal_embedding_pipeline.py": 1670492343.0192385,
-        "TEMPLATE_PATH/pipelines/multi_modal/gridvlp_pipeline.py": 1679578012.6899312,
-        "TEMPLATE_PATH/pipelines/multi_modal/image_captioning_pipeline.py": 1684224035.513264,
-        "TEMPLATE_PATH/pipelines/multi_modal/image_text_retrieval_pipeline.py": 1679578012.690931,
-        "TEMPLATE_PATH/pipelines/multi_modal/mgeo_ranking_pipeline.py": 1679578012.690931,
-        "TEMPLATE_PATH/pipelines/multi_modal/multi_modal_embedding_pipeline.py": 1670492343.0192385,
-        "TEMPLATE_PATH/pipelines/multi_modal/multimodal_dialogue_pipeline.py": 1691036741.3003352,
-        "TEMPLATE_PATH/pipelines/multi_modal/ocr_recognition_pipeline.py": 1679578012.690931,
-        "TEMPLATE_PATH/pipelines/multi_modal/soonet_video_temporal_grounding_pipeline.py": 1679578012.690931,
-        "TEMPLATE_PATH/pipelines/multi_modal/sudoku_pipeline.py": 1679578012.690931,
-        "TEMPLATE_PATH/pipelines/multi_modal/team_multi_modal_similarity_pipeline.py": 1670492343.0192385,
-        "TEMPLATE_PATH/pipelines/multi_modal/text2sql_pipeline.py": 1679578012.690931,
-        "TEMPLATE_PATH/pipelines/multi_modal/text_to_image_synthesis_pipeline.py": 1679578012.690931,
-        "TEMPLATE_PATH/pipelines/multi_modal/text_to_video_synthesis_pipeline.py": 1691036741.3003352,
-        "TEMPLATE_PATH/pipelines/multi_modal/video_captioning_pipeline.py": 1679578012.690931,
-        "TEMPLATE_PATH/pipelines/multi_modal/video_multi_modal_embedding_pipeline.py": 1670492343.0192385,
-        "TEMPLATE_PATH/pipelines/multi_modal/video_question_answering_pipeline.py": 1679578012.690931,
-        "TEMPLATE_PATH/pipelines/multi_modal/visual_entailment_pipeline.py": 1679578012.690931,
-        "TEMPLATE_PATH/pipelines/multi_modal/visual_grounding_pipeline.py": 1679578012.690931,
-        "TEMPLATE_PATH/pipelines/multi_modal/visual_question_answering_pipeline.py": 1679578012.690931,
-        "TEMPLATE_PATH/pipelines/nlp/automatic_post_editing_pipeline.py": 1670492343.0202384,
-        "TEMPLATE_PATH/pipelines/nlp/canmt_translation_pipeline.py": 1681971907.4150362,
-        "TEMPLATE_PATH/pipelines/nlp/codegeex_code_generation_pipeline.py": 1670492343.0202384,
-        "TEMPLATE_PATH/pipelines/nlp/codegeex_code_translation_pipeline.py": 1670492343.0202384,
-        "TEMPLATE_PATH/pipelines/nlp/conversational_text_to_sql_pipeline.py": 1670492343.0202384,
-        "TEMPLATE_PATH/pipelines/nlp/dialog_intent_prediction_pipeline.py": 1684224035.513264,
-        "TEMPLATE_PATH/pipelines/nlp/dialog_modeling_pipeline.py": 1670492343.0202384,
-        "TEMPLATE_PATH/pipelines/nlp/dialog_state_tracking_pipeline.py": 1684224035.513264,
-        "TEMPLATE_PATH/pipelines/nlp/distributed_gpt3_pipeline.py": 1691036741.3003352,
-        "TEMPLATE_PATH/pipelines/nlp/distributed_gpt_moe_pipeline.py": 1679578012.690931,
-        "TEMPLATE_PATH/pipelines/nlp/distributed_plug_pipeline.py": 1670492343.0202384,
-        "TEMPLATE_PATH/pipelines/nlp/document_grounded_dialog_generate_pipeline.py": 1684224035.513264,
-        "TEMPLATE_PATH/pipelines/nlp/document_grounded_dialog_rerank_pipeline.py": 1684224035.513264,
-        "TEMPLATE_PATH/pipelines/nlp/document_grounded_dialog_retrieval_pipeline.py": 1684224035.513264,
-        "TEMPLATE_PATH/pipelines/nlp/document_segmentation_pipeline.py": 1691036741.301335,
-        "TEMPLATE_PATH/pipelines/nlp/extractive_summarization_pipeline.py": 1684224035.514264,
-        "TEMPLATE_PATH/pipelines/nlp/faq_question_answering_pipeline.py": 1679578012.6919312,
-        "TEMPLATE_PATH/pipelines/nlp/fasttext_text_classification_pipeline.py": 1670492343.0202384,
-        "TEMPLATE_PATH/pipelines/nlp/feature_extraction_pipeline.py": 1684224035.514264,
-        "TEMPLATE_PATH/pipelines/nlp/fid_dialogue_pipeline.py": 1684224035.514264,
-        "TEMPLATE_PATH/pipelines/nlp/fill_mask_pipeline.py": 1684224035.514264,
-        "TEMPLATE_PATH/pipelines/nlp/glm130b_text_generation_pipeline.py": 1681971907.4160361,
-        "TEMPLATE_PATH/pipelines/nlp/information_extraction_pipeline.py": 1679578012.6919312,
-        "TEMPLATE_PATH/pipelines/nlp/interactive_translation_pipeline.py": 1679578012.6919312,
-        "TEMPLATE_PATH/pipelines/nlp/language_identification_pipline.py": 1679578012.6919312,
-        "TEMPLATE_PATH/pipelines/nlp/llama2_text_generation_pipeline.py": 1691036741.301335,
-        "TEMPLATE_PATH/pipelines/nlp/mglm_text_summarization_pipeline.py": 1679578012.6919312,
-        "TEMPLATE_PATH/pipelines/nlp/named_entity_recognition_pipeline.py": 1684224035.514264,
-        "TEMPLATE_PATH/pipelines/nlp/polylm_text_generation_pipeline.py": 1691036741.301335,
-        "TEMPLATE_PATH/pipelines/nlp/sentence_embedding_pipeline.py": 1684224035.514264,
-        "TEMPLATE_PATH/pipelines/nlp/siamese_uie_pipeline.py": 1684224035.514264,
-        "TEMPLATE_PATH/pipelines/nlp/summarization_pipeline.py": 1679578012.6919312,
-        "TEMPLATE_PATH/pipelines/nlp/table_question_answering_pipeline.py": 1684224035.514264,
-        "TEMPLATE_PATH/pipelines/nlp/text_classification_pipeline.py": 1684224035.514264,
-        "TEMPLATE_PATH/pipelines/nlp/text_error_correction_pipeline.py": 1679578012.6929312,
-        "TEMPLATE_PATH/pipelines/nlp/text_generation_pipeline.py": 1691036741.301335,
-        "TEMPLATE_PATH/pipelines/nlp/text_ranking_pipeline.py": 1684224035.515264,
-        "TEMPLATE_PATH/pipelines/nlp/token_classification_pipeline.py": 1684224035.515264,
-        "TEMPLATE_PATH/pipelines/nlp/translation_evaluation_pipeline.py": 1684224035.515264,
-        "TEMPLATE_PATH/pipelines/nlp/translation_pipeline.py": 1679578012.6929312,
-        "TEMPLATE_PATH/pipelines/nlp/translation_quality_estimation_pipeline.py": 1670492343.0212383,
-        "TEMPLATE_PATH/pipelines/nlp/user_satisfaction_estimation_pipeline.py": 1684224035.515264,
-        "TEMPLATE_PATH/pipelines/nlp/word_alignment_pipeline.py": 1679578012.6929312,
-        "TEMPLATE_PATH/pipelines/nlp/word_segmentation_pipeline.py": 1679578012.6929312,
-        "TEMPLATE_PATH/pipelines/nlp/zero_shot_classification_pipeline.py": 1684224035.515264,
-        "TEMPLATE_PATH/pipelines/pipeline_template.py": 1684224035.515264,
-        "TEMPLATE_PATH/pipelines/science/protein_structure_pipeline.py": 1679578012.6929312,
-        "TEMPLATE_PATH/pipelines/util.py": 1679578012.693931,
-        "TEMPLATE_PATH/preprocessors/asr.py": 1684224035.515264,
-        "TEMPLATE_PATH/preprocessors/audio.py": 1679578012.693931,
-        "TEMPLATE_PATH/preprocessors/base.py": 1691036741.301335,
-        "TEMPLATE_PATH/preprocessors/builder.py": 1670492343.0222385,
-        "TEMPLATE_PATH/preprocessors/common.py": 1679578012.693931,
-        "TEMPLATE_PATH/preprocessors/cv/action_detection_mapper.py": 1679578012.693931,
-        "TEMPLATE_PATH/preprocessors/cv/bad_image_detecting_preprocessor.py": 1679578012.693931,
-        "TEMPLATE_PATH/preprocessors/cv/controllable_image_generation.py": 1679578012.693931,
-        "TEMPLATE_PATH/preprocessors/cv/cv2_transforms.py": 1679578012.6949313,
-        "TEMPLATE_PATH/preprocessors/cv/image_classification_preprocessor.py": 1679578012.6949313,
-        "TEMPLATE_PATH/preprocessors/cv/image_quality_assessment_man.py": 1679578012.6949313,
-        "TEMPLATE_PATH/preprocessors/cv/image_quality_assessment_mos.py": 1679578012.6949313,
-        "TEMPLATE_PATH/preprocessors/cv/image_restoration_preprocessor.py": 1679578012.6949313,
-        "TEMPLATE_PATH/preprocessors/cv/mmcls_preprocessor.py": 1679578012.6949313,
-        "TEMPLATE_PATH/preprocessors/cv/timer.py": 1679578012.6949313,
-        "TEMPLATE_PATH/preprocessors/cv/util.py": 1679578012.6949313,
-        "TEMPLATE_PATH/preprocessors/cv/video_stabilization.py": 1679578012.6949313,
-        "TEMPLATE_PATH/preprocessors/cv/video_super_resolution.py": 1679578012.6949313,
-        "TEMPLATE_PATH/preprocessors/image.py": 1679578012.6949313,
-        "TEMPLATE_PATH/preprocessors/kws.py": 1670492343.0222385,
-        "TEMPLATE_PATH/preprocessors/movie_scene_segmentation/transforms.py": 1679578012.6949313,
-        "TEMPLATE_PATH/preprocessors/multi_modal.py": 1691036741.301335,
-        "TEMPLATE_PATH/preprocessors/nlp/bert_seq_cls_tokenizer.py": 1670492343.0222385,
-        "TEMPLATE_PATH/preprocessors/nlp/canmt_translation.py": 1681971907.4170363,
-        "TEMPLATE_PATH/preprocessors/nlp/dialog_classification_use_preprocessor.py": 1679578012.6959312,
-        "TEMPLATE_PATH/preprocessors/nlp/document_grounded_dialog_generate_preprocessor.py": 1679578012.6959312,
-        "TEMPLATE_PATH/preprocessors/nlp/document_grounded_dialog_rerank_preprocessor.py": 1679578012.6959312,
-        "TEMPLATE_PATH/preprocessors/nlp/document_grounded_dialog_retrieval_preprocessor.py": 1679578012.6959312,
-        "TEMPLATE_PATH/preprocessors/nlp/document_segmentation_preprocessor.py": 1679578012.6959312,
-        "TEMPLATE_PATH/preprocessors/nlp/faq_question_answering_preprocessor.py": 1679578012.6959312,
-        "TEMPLATE_PATH/preprocessors/nlp/feature_extraction_preprocessor.py": 1670492343.0232384,
-        "TEMPLATE_PATH/preprocessors/nlp/fill_mask_preprocessor.py": 1679578012.6959312,
-        "TEMPLATE_PATH/preprocessors/nlp/mgeo_ranking_preprocessor.py": 1679578012.6959312,
-        "TEMPLATE_PATH/preprocessors/nlp/mglm_summarization_preprocessor.py": 1670492343.0232384,
-        "TEMPLATE_PATH/preprocessors/nlp/relation_extraction_preprocessor.py": 1670492343.0232384,
-        "TEMPLATE_PATH/preprocessors/nlp/sentence_embedding_preprocessor.py": 1679578012.6959312,
-        "TEMPLATE_PATH/preprocessors/nlp/siamese_uie_preprocessor.py": 1679578012.6959312,
-        "TEMPLATE_PATH/preprocessors/nlp/space/args.py": 1670492343.0232384,
-        "TEMPLATE_PATH/preprocessors/nlp/space/batch.py": 1689760581.5446868,
-        "TEMPLATE_PATH/preprocessors/nlp/space/data_loader.py": 1670492343.0232384,
-        "TEMPLATE_PATH/preprocessors/nlp/space/dialog_intent_prediction_preprocessor.py": 1670492343.0232384,
-        "TEMPLATE_PATH/preprocessors/nlp/space/dialog_modeling_preprocessor.py": 1670492343.0232384,
-        "TEMPLATE_PATH/preprocessors/nlp/space/dialog_state_tracking_preprocessor.py": 1670492343.0232384,
-        "TEMPLATE_PATH/preprocessors/nlp/space/dst_processors.py": 1670492343.0232384,
-        "TEMPLATE_PATH/preprocessors/nlp/space/fields/gen_field.py": 1679578012.6969314,
-        "TEMPLATE_PATH/preprocessors/nlp/space/fields/intent_field.py": 1689760581.545687,
-        "TEMPLATE_PATH/preprocessors/nlp/space/lazy_dataset.py": 1670492343.0242383,
-        "TEMPLATE_PATH/preprocessors/nlp/space/preprocess.py": 1670492343.0242383,
-        "TEMPLATE_PATH/preprocessors/nlp/space/sampler.py": 1670492343.0242383,
-        "TEMPLATE_PATH/preprocessors/nlp/space/tensorlistdataset.py": 1670492343.0242383,
-        "TEMPLATE_PATH/preprocessors/nlp/space/tokenizer.py": 1679578012.6969314,
-        "TEMPLATE_PATH/preprocessors/nlp/space_T_cn/fields/database.py": 1670492343.0252385,
-        "TEMPLATE_PATH/preprocessors/nlp/space_T_cn/fields/schema_link.py": 1670492343.0252385,
-        "TEMPLATE_PATH/preprocessors/nlp/space_T_cn/fields/struct.py": 1679578012.6969314,
-        "TEMPLATE_PATH/preprocessors/nlp/space_T_cn/table_question_answering_preprocessor.py": 1670492343.0252385,
-        "TEMPLATE_PATH/preprocessors/nlp/space_T_en/conversational_text_to_sql_preprocessor.py": 1670492343.0252385,
-        "TEMPLATE_PATH/preprocessors/nlp/space_T_en/fields/common_utils.py": 1670492343.0252385,
-        "TEMPLATE_PATH/preprocessors/nlp/space_T_en/fields/parse.py": 1670492343.0252385,
-        "TEMPLATE_PATH/preprocessors/nlp/space_T_en/fields/preprocess_dataset.py": 1670492343.0252385,
-        "TEMPLATE_PATH/preprocessors/nlp/space_T_en/fields/process_dataset.py": 1670492343.0252385,
-        "TEMPLATE_PATH/preprocessors/nlp/text_classification_preprocessor.py": 1679578012.6969314,
-        "TEMPLATE_PATH/preprocessors/nlp/text_clean.py": 1681971907.4170363,
-        "TEMPLATE_PATH/preprocessors/nlp/text_error_correction.py": 1679578012.6969314,
-        "TEMPLATE_PATH/preprocessors/nlp/text_generation_preprocessor.py": 1689760581.545687,
-        "TEMPLATE_PATH/preprocessors/nlp/text_ranking_preprocessor.py": 1679578012.6969314,
-        "TEMPLATE_PATH/preprocessors/nlp/token_classification_preprocessor.py": 1687774806.9994693,
-        "TEMPLATE_PATH/preprocessors/nlp/token_classification_thai_preprocessor.py": 1679578012.6969314,
-        "TEMPLATE_PATH/preprocessors/nlp/token_classification_viet_preprocessor.py": 1670492343.0262384,
-        "TEMPLATE_PATH/preprocessors/nlp/transformers_tokenizer.py": 1681971907.4170363,
-        "TEMPLATE_PATH/preprocessors/nlp/translation_evaluation_preprocessor.py": 1684224035.516264,
-        "TEMPLATE_PATH/preprocessors/nlp/utils.py": 1670492343.0262384,
-        "TEMPLATE_PATH/preprocessors/nlp/word_alignment_preprocessor.py": 1679578012.6979313,
-        "TEMPLATE_PATH/preprocessors/nlp/zero_shot_classification_preprocessor.py": 1670492343.0262384,
-        "TEMPLATE_PATH/preprocessors/ofa/asr.py": 1673577236.4163089,
-        "TEMPLATE_PATH/preprocessors/ofa/base.py": 1679578012.6979313,
-        "TEMPLATE_PATH/preprocessors/ofa/image_captioning.py": 1679578012.6979313,
-        "TEMPLATE_PATH/preprocessors/ofa/image_classification.py": 1679578012.6979313,
-        "TEMPLATE_PATH/preprocessors/ofa/ocr_recognition.py": 1679578012.6979313,
-        "TEMPLATE_PATH/preprocessors/ofa/sudoku.py": 1679578012.6979313,
-        "TEMPLATE_PATH/preprocessors/ofa/summarization.py": 1679578012.6979313,
-        "TEMPLATE_PATH/preprocessors/ofa/text2sql.py": 1679578012.6979313,
-        "TEMPLATE_PATH/preprocessors/ofa/text_classification.py": 1679578012.6979313,
-        "TEMPLATE_PATH/preprocessors/ofa/text_to_image_synthesis.py": 1679578012.6979313,
-        "TEMPLATE_PATH/preprocessors/ofa/utils/audio_helper.py": 1670492343.0262384,
-        "TEMPLATE_PATH/preprocessors/ofa/utils/bridge_content_encoder.py": 1679578012.6979313,
-        "TEMPLATE_PATH/preprocessors/ofa/utils/collate.py": 1679578012.6979313,
-        "TEMPLATE_PATH/preprocessors/ofa/utils/constant.py": 1679578012.6989312,
-        "TEMPLATE_PATH/preprocessors/ofa/utils/get_tables.py": 1679578012.6989312,
-        "TEMPLATE_PATH/preprocessors/ofa/utils/random_help.py": 1679578012.6989312,
-        "TEMPLATE_PATH/preprocessors/ofa/utils/text2phone.py": 1670492343.0272384,
-        "TEMPLATE_PATH/preprocessors/ofa/utils/transforms.py": 1670492343.0272384,
-        "TEMPLATE_PATH/preprocessors/ofa/utils/vision_helper.py": 1670492343.0272384,
-        "TEMPLATE_PATH/preprocessors/ofa/visual_entailment.py": 1679578012.6989312,
-        "TEMPLATE_PATH/preprocessors/ofa/visual_grounding.py": 1679578012.6989312,
-        "TEMPLATE_PATH/preprocessors/ofa/visual_question_answering.py": 1679578012.6989312,
-        "TEMPLATE_PATH/preprocessors/science/uni_fold.py": 1679578012.6989312,
-        "TEMPLATE_PATH/preprocessors/speaker.py": 1691036741.302335,
-        "TEMPLATE_PATH/preprocessors/tts.py": 1679578012.6989312,
-        "TEMPLATE_PATH/preprocessors/video.py": 1679578012.6989312,
-        "TEMPLATE_PATH/trainers/audio/ans_trainer.py": 1670492343.0272384,
-        "TEMPLATE_PATH/trainers/audio/asr_trainer.py": 1679578012.6989312,
-        "TEMPLATE_PATH/trainers/audio/kws_farfield_trainer.py": 1679578012.6999314,
-        "TEMPLATE_PATH/trainers/audio/kws_nearfield_trainer.py": 1681971907.4170363,
-        "TEMPLATE_PATH/trainers/audio/kws_utils/batch_utils.py": 1681971907.4170363,
-        "TEMPLATE_PATH/trainers/audio/kws_utils/det_utils.py": 1681971907.4170363,
-        "TEMPLATE_PATH/trainers/audio/kws_utils/file_utils.py": 1681971907.4170363,
-        "TEMPLATE_PATH/trainers/audio/kws_utils/model_utils.py": 1679578012.6999314,
-        "TEMPLATE_PATH/trainers/audio/kws_utils/runtime_utils.py": 1679578012.6999314,
-        "TEMPLATE_PATH/trainers/audio/separation_trainer.py": 1679578012.6999314,
-        "TEMPLATE_PATH/trainers/audio/tts_trainer.py": 1679578012.6999314,
-        "TEMPLATE_PATH/trainers/base.py": 1679578012.7009313,
-        "TEMPLATE_PATH/trainers/builder.py": 1681971907.4180362,
-        "TEMPLATE_PATH/trainers/cli_argument_parser.py": 1684224035.516264,
-        "TEMPLATE_PATH/trainers/cv/action_detection_trainer.py": 1679578012.7009313,
-        "TEMPLATE_PATH/trainers/cv/card_detection_scrfd_trainer.py": 1670492343.0282385,
-        "TEMPLATE_PATH/trainers/cv/cartoon_translation_trainer.py": 1679578012.7009313,
-        "TEMPLATE_PATH/trainers/cv/face_detection_scrfd_trainer.py": 1670492343.0282385,
-        "TEMPLATE_PATH/trainers/cv/image_classifition_trainer.py": 1684224035.516264,
-        "TEMPLATE_PATH/trainers/cv/image_defrcn_fewshot_detection_trainer.py": 1679578012.7009313,
-        "TEMPLATE_PATH/trainers/cv/image_detection_damoyolo_trainer.py": 1679578012.7009313,
-        "TEMPLATE_PATH/trainers/cv/image_inpainting_trainer.py": 1670492343.0282385,
-        "TEMPLATE_PATH/trainers/cv/image_instance_segmentation_trainer.py": 1670492343.0282385,
-        "TEMPLATE_PATH/trainers/cv/image_portrait_enhancement_trainer.py": 1670492343.0282385,
-        "TEMPLATE_PATH/trainers/cv/movie_scene_segmentation_trainer.py": 1670492343.0282385,
-        "TEMPLATE_PATH/trainers/cv/nerf_recon_acc_trainer.py": 1679578012.7009313,
-        "TEMPLATE_PATH/trainers/cv/ocr_detection_db_trainer.py": 1679578012.7009313,
-        "TEMPLATE_PATH/trainers/cv/ocr_recognition_trainer.py": 1679578012.7019312,
-        "TEMPLATE_PATH/trainers/cv/referring_video_object_segmentation_trainer.py": 1679578012.7019312,
-        "TEMPLATE_PATH/trainers/cv/vision_efficient_tuning_trainer.py": 1679578012.7019312,
-        "TEMPLATE_PATH/trainers/default_config.py": 1687774806.9994693,
-        "TEMPLATE_PATH/trainers/hooks/builder.py": 1670492343.0292385,
-        "TEMPLATE_PATH/trainers/hooks/checkpoint/checkpoint_hook.py": 1691036741.302335,
-        "TEMPLATE_PATH/trainers/hooks/checkpoint/checkpoint_processor.py": 1689760581.546687,
-        "TEMPLATE_PATH/trainers/hooks/checkpoint/load_checkpoint_hook.py": 1684224035.5172641,
-        "TEMPLATE_PATH/trainers/hooks/clip_clamp_logit_scale_hook.py": 1670492343.0292385,
-        "TEMPLATE_PATH/trainers/hooks/compression/sparsity_hook.py": 1684224035.5172641,
-        "TEMPLATE_PATH/trainers/hooks/compression/utils.py": 1679578012.7019312,
-        "TEMPLATE_PATH/trainers/hooks/distributed/ddp_hook.py": 1684224035.5172641,
-        "TEMPLATE_PATH/trainers/hooks/distributed/deepspeed_hook.py": 1689760581.546687,
-        "TEMPLATE_PATH/trainers/hooks/distributed/megatron_hook.py": 1689760581.5476868,
-        "TEMPLATE_PATH/trainers/hooks/early_stop_hook.py": 1684224035.5172641,
-        "TEMPLATE_PATH/trainers/hooks/evaluation_hook.py": 1684224035.5172641,
-        "TEMPLATE_PATH/trainers/hooks/hook.py": 1684224035.518264,
-        "TEMPLATE_PATH/trainers/hooks/iter_timer_hook.py": 1670492343.0292385,
-        "TEMPLATE_PATH/trainers/hooks/logger/base.py": 1670492343.0292385,
-        "TEMPLATE_PATH/trainers/hooks/logger/tensorboard_hook.py": 1679578012.7029314,
-        "TEMPLATE_PATH/trainers/hooks/logger/text_logger_hook.py": 1681971907.4180362,
-        "TEMPLATE_PATH/trainers/hooks/lr_scheduler_hook.py": 1689760581.5476868,
-        "TEMPLATE_PATH/trainers/hooks/optimizer/apex_optimizer_hook.py": 1684224035.518264,
-        "TEMPLATE_PATH/trainers/hooks/optimizer/base.py": 1684224035.518264,
-        "TEMPLATE_PATH/trainers/hooks/optimizer/torch_optimizer_hook.py": 1684224035.518264,
-        "TEMPLATE_PATH/trainers/hooks/priority.py": 1670492343.0302384,
-        "TEMPLATE_PATH/trainers/lrscheduler/builder.py": 1679578012.7029314,
-        "TEMPLATE_PATH/trainers/lrscheduler/warmup/base.py": 1670492343.0302384,
-        "TEMPLATE_PATH/trainers/lrscheduler/warmup/warmup.py": 1670492343.0302384,
-        "TEMPLATE_PATH/trainers/multi_modal/clip/clip_trainer.py": 1684224035.518264,
-        "TEMPLATE_PATH/trainers/multi_modal/clip/clip_trainer_utils.py": 1670492343.0302384,
-        "TEMPLATE_PATH/trainers/multi_modal/custom_diffusion/custom_diffusion_trainer.py": 1691036741.3033352,
-        "TEMPLATE_PATH/trainers/multi_modal/dreambooth_diffusion/dreambooth_diffusion_trainer.py": 1691036741.3033352,
-        "TEMPLATE_PATH/trainers/multi_modal/efficient_diffusion_tuning/efficient_diffusion_tuning_trainer.py": 1691036741.3033352,
-        "TEMPLATE_PATH/trainers/multi_modal/lora_diffusion/lora_diffusion_trainer.py": 1691036741.3033352,
-        "TEMPLATE_PATH/trainers/multi_modal/mgeo_ranking_trainer.py": 1679578012.7029314,
-        "TEMPLATE_PATH/trainers/multi_modal/mplug/mplug_trainer.py": 1670492343.0302384,
-        "TEMPLATE_PATH/trainers/multi_modal/ofa/ofa_trainer.py": 1679578012.7039313,
-        "TEMPLATE_PATH/trainers/multi_modal/ofa/ofa_trainer_utils.py": 1679578012.7039313,
-        "TEMPLATE_PATH/trainers/multi_modal/stable_diffusion/stable_diffusion_trainer.py": 1687774807.0004694,
-        "TEMPLATE_PATH/trainers/multi_modal/team/team_trainer.py": 1670492343.0302384,
-        "TEMPLATE_PATH/trainers/multi_modal/team/team_trainer_utils.py": 1670492343.0302384,
-        "TEMPLATE_PATH/trainers/nlp/csanmt_translation_trainer.py": 1670492343.0312386,
-        "TEMPLATE_PATH/trainers/nlp/document_grounded_dialog_generate_trainer.py": 1679578012.7039313,
-        "TEMPLATE_PATH/trainers/nlp/document_grounded_dialog_rerank_trainer.py": 1679578012.7039313,
-        "TEMPLATE_PATH/trainers/nlp/document_grounded_dialog_retrieval_trainer.py": 1679578012.7039313,
-        "TEMPLATE_PATH/trainers/nlp/faq_question_answering_trainer.py": 1679578012.7039313,
-        "TEMPLATE_PATH/trainers/nlp/gpt3_trainer.py": 1679578012.7039313,
-        "TEMPLATE_PATH/trainers/nlp/gpt_moe_trainer.py": 1679578012.7039313,
-        "TEMPLATE_PATH/trainers/nlp/plug_trainer.py": 1679578012.7039313,
-        "TEMPLATE_PATH/trainers/nlp/sentence_embedding_trainer.py": 1679578012.7039313,
-        "TEMPLATE_PATH/trainers/nlp/sequence_classification_trainer.py": 1679578012.7039313,
-        "TEMPLATE_PATH/trainers/nlp/siamese_uie_trainer.py": 1679578012.7049313,
-        "TEMPLATE_PATH/trainers/nlp/space/dialog_intent_trainer.py": 1670492343.0312386,
-        "TEMPLATE_PATH/trainers/nlp/space/dialog_modeling_trainer.py": 1670492343.0312386,
-        "TEMPLATE_PATH/trainers/nlp/space/eval.py": 1670492343.0312386,
-        "TEMPLATE_PATH/trainers/nlp/space/metrics/metrics_tracker.py": 1670492343.0312386,
-        "TEMPLATE_PATH/trainers/nlp/space/trainer/gen_trainer.py": 1670492343.0322385,
-        "TEMPLATE_PATH/trainers/nlp/space/trainer/intent_trainer.py": 1670492343.0322385,
-        "TEMPLATE_PATH/trainers/nlp/table_question_answering_trainer.py": 1679578012.7049313,
-        "TEMPLATE_PATH/trainers/nlp/text_generation_trainer.py": 1691036741.3033352,
-        "TEMPLATE_PATH/trainers/nlp/text_ranking_trainer.py": 1670492343.0322385,
-        "TEMPLATE_PATH/trainers/nlp/translation_evaluation_trainer.py": 1684224035.518264,
-        "TEMPLATE_PATH/trainers/nlp_trainer.py": 1679578012.7049313,
-        "TEMPLATE_PATH/trainers/optimizer/builder.py": 1679578012.7049313,
-        "TEMPLATE_PATH/trainers/parallel/builder.py": 1670492343.0322385,
-        "TEMPLATE_PATH/trainers/parallel/utils.py": 1670492343.0322385,
-        "TEMPLATE_PATH/trainers/trainer.py": 1691036741.304335,
-        "TEMPLATE_PATH/trainers/training_args.py": 1687774807.0014694,
-        "TEMPLATE_PATH/trainers/utils/inference.py": 1679578012.7059314,
-        "TEMPLATE_PATH/trainers/utils/log_buffer.py": 1670492343.0332384
+        "TEMPLATE_PATH/exporters/audio/ans_dfsmn_exporter.py": 1690807232.3142962,
+        "TEMPLATE_PATH/exporters/base.py": 1690807232.3142962,
+        "TEMPLATE_PATH/exporters/builder.py": 1690807232.3142962,
+        "TEMPLATE_PATH/exporters/cv/cartoon_translation_exporter.py": 1690807232.3142962,
+        "TEMPLATE_PATH/exporters/cv/face_detection_scrfd_exporter.py": 1690807232.3142962,
+        "TEMPLATE_PATH/exporters/cv/object_detection_damoyolo_exporter.py": 1690807232.3142962,
+        "TEMPLATE_PATH/exporters/multi_modal/stable_diffusion_exporter.py": 1690807232.3142962,
+        "TEMPLATE_PATH/exporters/nlp/csanmt_for_translation_exporter.py": 1690807232.3142962,
+        "TEMPLATE_PATH/exporters/nlp/model_for_token_classification_exporter.py": 1690807232.3142962,
+        "TEMPLATE_PATH/exporters/nlp/sbert_for_sequence_classification_exporter.py": 1690807232.3142962,
+        "TEMPLATE_PATH/exporters/nlp/sbert_for_zero_shot_classification_exporter.py": 1690807232.3142962,
+        "TEMPLATE_PATH/exporters/tf_model_exporter.py": 1690807232.3142962,
+        "TEMPLATE_PATH/exporters/torch_model_exporter.py": 1690807232.3142962,
+        "TEMPLATE_PATH/metrics/accuracy_metric.py": 1690807232.3182962,
+        "TEMPLATE_PATH/metrics/action_detection_evaluator.py": 1690807232.3182962,
+        "TEMPLATE_PATH/metrics/audio_noise_metric.py": 1690807232.3182962,
+        "TEMPLATE_PATH/metrics/base.py": 1690807232.3182962,
+        "TEMPLATE_PATH/metrics/bleu_metric.py": 1690807232.3182962,
+        "TEMPLATE_PATH/metrics/builder.py": 1690807232.3182962,
+        "TEMPLATE_PATH/metrics/ciderD/ciderD.py": 1690807232.3182962,
+        "TEMPLATE_PATH/metrics/ciderD/ciderD_scorer.py": 1690807232.3182962,
+        "TEMPLATE_PATH/metrics/image_color_enhance_metric.py": 1690807232.3182962,
+        "TEMPLATE_PATH/metrics/image_colorization_metric.py": 1690807232.3182962,
+        "TEMPLATE_PATH/metrics/image_denoise_metric.py": 1690807232.3182962,
+        "TEMPLATE_PATH/metrics/image_inpainting_metric.py": 1690807232.3182962,
+        "TEMPLATE_PATH/metrics/image_instance_segmentation_metric.py": 1690807232.3182962,
+        "TEMPLATE_PATH/metrics/image_portrait_enhancement_metric.py": 1690807232.3182962,
+        "TEMPLATE_PATH/metrics/image_quality_assessment_degradation_metric.py": 1690807232.3182962,
+        "TEMPLATE_PATH/metrics/image_quality_assessment_mos_metric.py": 1690807232.3182962,
+        "TEMPLATE_PATH/metrics/inbatch_recall_metric.py": 1690807232.3182962,
+        "TEMPLATE_PATH/metrics/loss_metric.py": 1690807232.3182962,
+        "TEMPLATE_PATH/metrics/map_metric.py": 1690807232.3182962,
+        "TEMPLATE_PATH/metrics/movie_scene_segmentation_metric.py": 1690807232.3182962,
+        "TEMPLATE_PATH/metrics/ned_metric.py": 1690807232.3182962,
+        "TEMPLATE_PATH/metrics/ocr_recognition_metric.py": 1690807232.3182962,
+        "TEMPLATE_PATH/metrics/ppl_metric.py": 1690807232.3182962,
+        "TEMPLATE_PATH/metrics/prediction_saving_wrapper.py": 1690807232.3182962,
+        "TEMPLATE_PATH/metrics/referring_video_object_segmentation_metric.py": 1690807232.3182962,
+        "TEMPLATE_PATH/metrics/sequence_classification_metric.py": 1690807232.3182962,
+        "TEMPLATE_PATH/metrics/text_generation_metric.py": 1690807232.3182962,
+        "TEMPLATE_PATH/metrics/text_ranking_metric.py": 1690807232.3182962,
+        "TEMPLATE_PATH/metrics/token_classification_metric.py": 1690807232.3182962,
+        "TEMPLATE_PATH/metrics/translation_evaluation_metric.py": 1690807232.3182962,
+        "TEMPLATE_PATH/metrics/video_frame_interpolation_metric.py": 1690807232.3182962,
+        "TEMPLATE_PATH/metrics/video_stabilization_metric.py": 1690807232.3182962,
+        "TEMPLATE_PATH/metrics/video_summarization_metric.py": 1690807232.3182962,
+        "TEMPLATE_PATH/metrics/video_super_resolution_metric/matlab_functions.py": 1690807232.3182962,
+        "TEMPLATE_PATH/metrics/video_super_resolution_metric/metric_util.py": 1690807232.3182962,
+        "TEMPLATE_PATH/metrics/video_super_resolution_metric/niqe.py": 1690807232.3182962,
+        "TEMPLATE_PATH/metrics/video_super_resolution_metric/video_super_resolution_metric.py": 1690807232.3182962,
+        "TEMPLATE_PATH/models/audio/aec/layers/activations.py": 1690807232.3182962,
+        "TEMPLATE_PATH/models/audio/aec/layers/affine_transform.py": 1690807232.3182962,
+        "TEMPLATE_PATH/models/audio/aec/layers/deep_fsmn.py": 1690807232.3182962,
+        "TEMPLATE_PATH/models/audio/aec/layers/layer_base.py": 1690807232.3182962,
+        "TEMPLATE_PATH/models/audio/aec/layers/uni_deep_fsmn.py": 1690807232.3182962,
+        "TEMPLATE_PATH/models/audio/aec/network/loss.py": 1690807232.3182962,
+        "TEMPLATE_PATH/models/audio/aec/network/modulation_loss.py": 1690807232.3182962,
+        "TEMPLATE_PATH/models/audio/aec/network/se_net.py": 1690807232.3182962,
+        "TEMPLATE_PATH/models/audio/ans/complex_nn.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/audio/ans/conv_stft.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/audio/ans/denoise_net.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/audio/ans/frcrn.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/audio/ans/layers/activations.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/audio/ans/layers/affine_transform.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/audio/ans/layers/layer_base.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/audio/ans/layers/uni_deep_fsmn.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/audio/ans/se_module_complex.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/audio/ans/unet.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/audio/asr/generic_automatic_speech_recognition.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/audio/asr/wenet_automatic_speech_recognition.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/audio/itn/generic_inverse_text_processing.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/audio/kws/farfield/fsmn.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/audio/kws/farfield/fsmn_sele_v2.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/audio/kws/farfield/fsmn_sele_v3.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/audio/kws/farfield/model.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/audio/kws/farfield/model_def.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/audio/kws/generic_key_word_spotting.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/audio/kws/nearfield/cmvn.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/audio/kws/nearfield/fsmn.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/audio/kws/nearfield/model.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/audio/punc/generic_punctuation.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/audio/separation/layer_norm.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/audio/separation/mossformer.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/audio/separation/mossformer_block.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/audio/separation/mossformer_conv_module.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/audio/sv/DTDNN.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/audio/sv/DTDNN_layers.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/audio/sv/ERes2Net.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/audio/sv/ERes2Net_aug.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/audio/sv/cluster_backend.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/audio/sv/ecapa_tdnn.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/audio/sv/fusion.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/audio/sv/generic_speaker_verification.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/audio/sv/lanuage_recognition_model.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/audio/sv/pooling_layers.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/audio/sv/rdino.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/audio/sv/speaker_change_locator.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/audio/sv/speaker_diarization_dialogue_detection.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/audio/sv/speaker_diarization_semantic_speaker_turn_detection.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/audio/tts/sambert_hifi.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/audio/tts/voice.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/base/base_head.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/base/base_model.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/base/base_torch_head.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/base/base_torch_model.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/builder.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/cv/abnormal_object_detection/mmdet_model.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/cv/abnormal_object_detection/mmdet_ms/roi_head/mask_scoring_roi_head.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/cv/abnormal_object_detection/mmdet_ms/roi_head/roi_extractors/single_level_roi_extractor.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/cv/action_detection/action_detection_onnx.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/cv/action_detection/modules/action_detection_pytorch.py": 1690807232.3222961,
+        "TEMPLATE_PATH/models/cv/action_detection/modules/resnet.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/action_recognition/models.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/action_recognition/s3dg.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/action_recognition/tada_convnext.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/action_recognition/temporal_patch_shift_transformer.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/animal_recognition/resnet.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/animal_recognition/splat.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/bad_image_detecting/bad_image_detecting.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/body_2d_keypoints/hrnet_basic_modules.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/body_2d_keypoints/hrnet_v2.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/body_2d_keypoints/w48.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/body_3d_keypoints/cannonical_pose/body_3d_pose.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/body_3d_keypoints/cannonical_pose/canonical_pose_modules.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/body_3d_keypoints/hdformer/backbone.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/body_3d_keypoints/hdformer/block.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/body_3d_keypoints/hdformer/directed_graph.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/body_3d_keypoints/hdformer/hdformer.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/body_3d_keypoints/hdformer/hdformer_detector.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/body_3d_keypoints/hdformer/skeleton.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/cartoon/facelib/LK/lk.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/cartoon/facelib/config.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/cartoon/facelib/face_detector.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/cartoon/facelib/face_landmark.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/cartoon/facelib/facer.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/cartoon/loss.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/cartoon/model_tf.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/cartoon/mtcnn_pytorch/src/align_trans.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/cartoon/mtcnn_pytorch/src/matlab_cp2tform.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/cartoon/network.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/cartoon/utils.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/cmdssl_video_embedding/c3d.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/cmdssl_video_embedding/resnet2p1d.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/cmdssl_video_embedding/resnet3d.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/annotator.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/api.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/midas/base_model.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/midas/blocks.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/midas/dpt_depth.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/midas/midas_net.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/midas/midas_net_custom.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/midas/transforms.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/midas/vit.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/utils.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/mlsd/mbv2_mlsd_large.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/mlsd/utils.py": 1690807232.3262963,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/openpose/body.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/openpose/hand.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/openpose/model.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/openpose/util.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/controlnet.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/crowd_counting/cc_model.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/crowd_counting/hrnet_aspp_relu.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/face_attribute_recognition/fair_face/face_attribute_recognition.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/face_detection/mogface/models/detectors.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/face_detection/mogface/models/mogface.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/face_detection/mogface/models/mogprednet.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/face_detection/mogface/models/resnet.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/face_detection/mogface/models/utils.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/face_detection/mtcnn/models/box_utils.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/face_detection/mtcnn/models/detector.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/face_detection/mtcnn/models/first_stage.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/face_detection/mtcnn/models/get_nets.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/face_detection/peppa_pig_face/LK/lk.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/face_detection/peppa_pig_face/face_detector.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/face_detection/peppa_pig_face/face_landmark.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/face_detection/peppa_pig_face/facer.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/face_detection/retinaface/detection.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/face_detection/retinaface/models/net.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/face_detection/retinaface/models/retinaface.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/face_detection/retinaface/utils.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/damofd_detect.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/core/bbox/transforms.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/core/post_processing/bbox_nms.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/auto_augment.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/formating.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/loading.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/transforms.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/retinaface.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/master_net.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/mobilenet.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/resnet.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/dense_heads/scrfd_head.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/base.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/scrfd.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/single_stage.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/tinymog.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/preprocessor.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/scrfd_detect.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/tinymog_detect.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/detection.py": 1690807232.3302963,
+        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/box_utils.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/mb_tiny.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/ssd/data_preprocessing.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/ssd/fd_config.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/ssd/mb_tiny_fd.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/ssd/predictor.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/ssd/ssd.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/transforms.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_emotion/efficient/model.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_emotion/efficient/utils.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_emotion/emotion_infer.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_emotion/emotion_model.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_emotion/face_alignment/face.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_emotion/face_alignment/face_align.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_generation/op/conv2d_gradfix.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_generation/op/fused_act.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_generation/op/upfirdn2d.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_generation/stylegan2.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_human_hand_detection/det_infer.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_human_hand_detection/ghost_pan.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_human_hand_detection/nanodet_plus_head.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_human_hand_detection/one_stage_detector.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_human_hand_detection/shufflenetv2.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_human_hand_detection/utils.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_recognition/align_face.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_recognition/torchkit/backbone/arcface_backbone.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_recognition/torchkit/backbone/common.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_recognition/torchkit/backbone/facemask_backbone.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_recognition/torchkit/backbone/model_irse.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_recognition/torchkit/backbone/model_resnet.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_recognition/torchkit/rts_backbone.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/bfm.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/de_retouching_module.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/facelandmark/large_base_lmks_infer.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/facelandmark/nets/large_base_lmks_net.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/facelandmark/nets/large_eyeball_net.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/facerecon_model.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/losses.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/networks.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/nv_diffrast.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/opt.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/pix2pix/networks.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/pix2pix/pix2pix_model.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/pix2pix/pix2pix_options.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/renderer.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/unet.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/utils.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/facial_expression_recognition/fer/facial_expression_recognition.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/facial_expression_recognition/fer/transforms.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/facial_expression_recognition/fer/vgg.py": 1690807232.3342962,
+        "TEMPLATE_PATH/models/cv/facial_landmark_confidence/flc/facial_landmark_confidence.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/facial_landmark_confidence/flc/manual_landmark_net.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/hand_static/hand_model.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/hand_static/networks.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/human_reconstruction/Reconstruction.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/human_reconstruction/models/Embedding.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/human_reconstruction/models/PixToMesh.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/human_reconstruction/models/Res_backbone.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/human_reconstruction/models/Surface_head.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/human_reconstruction/models/detectors.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/human_reconstruction/models/geometry.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/human_reconstruction/models/human_segmenter.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/human_reconstruction/models/networks.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/human_reconstruction/utils.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/image_binary_quant_classification/binary_quant_model.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/image_binary_quant_classification/bnext.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/image_body_reshaping/image_body_reshaping.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/image_body_reshaping/model.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/image_body_reshaping/person_info.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/image_body_reshaping/pose_estimator/body.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/image_body_reshaping/pose_estimator/model.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/image_body_reshaping/pose_estimator/util.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/image_body_reshaping/slim_utils.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/image_classification/backbones/beit_v2.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/image_classification/backbones/nextvit.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/image_classification/mmcls_model.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/image_classification/resnet50_cc.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/image_classification/utils.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/image_color_enhance/adaint/adaint.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/image_color_enhance/csrnet.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/image_color_enhance/deeplpf/deeplpf_image_color_enhance.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/image_color_enhance/deeplpf/deeplpfnet.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/image_color_enhance/image_color_enhance.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/ddcolor.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/ddcolor_for_image_colorization.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/loss.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/utils/convnext.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/utils/position_encoding.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/utils/transformer_utils.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/utils/unet.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/utils/vgg.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/image_colorization/unet/unet.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/image_colorization/unet/utils.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/image_debanding/rrdb/rrdb_image_debanding.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/image_deblur/nafnet_for_image_deblur.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/defrcn_for_fewshot.py": 1690807232.3382962,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/evaluation/coco_evaluation.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/evaluation/evaluator.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/evaluation/pascal_voc_evaluation.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/models/calibration_layer.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/models/defrcn.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/models/fast_rcnn.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/models/gdl.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/models/resnet.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/models/roi_heads.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/utils/coco_register.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/utils/configuration_mapper.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/utils/model_surgery_op.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/utils/register_data.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/utils/requirements_check.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/utils/voc_register.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_denoise/nafnet/NAFNet_arch.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_denoise/nafnet/arch_util.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_denoise/nafnet_for_image_denoise.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_depth_estimation/networks/newcrf_depth.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_depth_estimation/networks/newcrf_layers.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_depth_estimation/networks/newcrf_utils.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_depth_estimation/networks/swin_transformer.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_depth_estimation/networks/uper_crf_head.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_depth_estimation/newcrfs_model.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_depth_estimation_bts/depth_estimation_bts_model.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_depth_estimation_bts/networks/bts_model.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_depth_estimation_bts/networks/decoder.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_depth_estimation_bts/networks/encoder.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_depth_estimation_bts/networks/utils.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_driving_perception/image_driving_percetion_model.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_driving_perception/preprocessor.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_driving_perception/utils.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/facegan/gan_wrap.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/facegan/model.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/facegan/op/conv2d_gradfix.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/facegan/op/fused_act.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/facegan/op/upfirdn2d.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/facelib/align_trans.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/facelib/matlab_cp2tform.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/image_face_fusion.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/network/aad_layer.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/network/aei_flow_net.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/network/bfm.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/network/dense_motion.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/network/facerecon_model.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/network/model_irse.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/network/ops.py": 1690807232.3422961,
+        "TEMPLATE_PATH/models/cv/image_human_parsing/backbone/deeplab_resnet.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_human_parsing/m2fp/m2fp_decoder.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_human_parsing/m2fp/m2fp_encoder.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_human_parsing/m2fp_net.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_human_parsing/parsing_utils.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_inpainting/base.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_inpainting/default.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_inpainting/model.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_inpainting/modules/ade20k/base.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_inpainting/modules/ade20k/resnet.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_inpainting/modules/adversarial.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_inpainting/modules/feature_matching.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_inpainting/modules/ffc.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_inpainting/modules/inception.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_inpainting/modules/perceptual.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_inpainting/modules/pix2pixhd.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_inpainting/refinement.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/backbones/resnet.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/backbones/swin_transformer.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/cascade_mask_rcnn_swin.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/datasets/transforms.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/fastinst/fastinst_decoder.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/fastinst/fastinst_encoder.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/fastinst_model.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino/dino_decoder.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino/maskdino_decoder.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino/maskdino_encoder.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino/ms_deform_attn.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino/position_encoding.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino/utils.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino_model.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino_swin.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/model.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/postprocess_utils.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_matching/config/default.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/backbone/resnet_fpn.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/loftr.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/loftr_module/fine_preprocess.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/loftr_module/linear_attention.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/loftr_module/quadtree_attention.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/loftr_module/transformer.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/utils/coarse_matching.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/utils/fine_matching.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/utils/position_encoding.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_matching/quadtree_attention_model.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_matching/utils/misc.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/cas_mvsnet.py": 1690807232.3462963,
+        "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/casmvs_model.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/colmap2mvsnet.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/depth_filter.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/general_eval_dataset.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/module.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/utils.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_paintbyexample/model.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_panoptic_segmentation/panseg_model.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/align_faces.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/eqface/fqa.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/eqface/model_resnet.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/gpen.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/image_portrait_enhancement.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/losses/helpers.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/losses/losses.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/losses/model_irse.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/retinaface/detection.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/retinaface/models/net.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/retinaface/models/retinaface.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/retinaface/utils.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_probing_model/backbone.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_probing_model/model.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_probing_model/utils.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_quality_assessment_degradation/degradation_model.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_quality_assessment_degradation/image_quality_assessment_degradation.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_quality_assessment_man/image_quality_assessment_man.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_quality_assessment_man/maniqa.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_quality_assessment_man/swin.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_quality_assessment_mos/backbones/resnet.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_quality_assessment_mos/censeo_ivqa_model.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_quality_assessment_mos/heads/simple_head.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_quality_assessment_mos/image_quality_assessment_mos.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_reid_person/pass_model.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_reid_person/transreid_model.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_restoration/demoire_models/nets.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_restoration/image_restoration_model.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/ddpm_seg/data_util.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/ddpm_seg/feature_extractors.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/ddpm_seg/pixel_classifier.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/ddpm_seg/utils.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/ddpm_segmentation_model.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/pan_merge/base_panoptic_fusion_head.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/pan_merge/maskformer_semantic_head.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/semantic_seg_model.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/adapter_modules.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/base/beit.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/beit_adapter.py": 1690807232.3502963,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/base_decode_head.py": 1690807232.3542962,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/mask2former_head_from_mmseg.py": 1690807232.3542962,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/base_segmentor.py": 1690807232.3542962,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/encoder_decoder_mask2former.py": 1690807232.3542962,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/utils/builder.py": 1690807232.3542962,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/utils/data_process_func.py": 1690807232.3542962,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/utils/seg_func.py": 1690807232.3542962,
+        "TEMPLATE_PATH/models/cv/image_skychange/preprocessor.py": 1690807232.3542962,
+        "TEMPLATE_PATH/models/cv/image_skychange/ptsemseg/BlockModules.py": 1690807232.3542962,
+        "TEMPLATE_PATH/models/cv/image_skychange/ptsemseg/hrnet_backnone.py": 1690807232.3542962,
+        "TEMPLATE_PATH/models/cv/image_skychange/ptsemseg/hrnet_super_and_ocr.py": 1690807232.3542962,
+        "TEMPLATE_PATH/models/cv/image_skychange/ptsemseg/unet.py": 1690807232.3542962,
+        "TEMPLATE_PATH/models/cv/image_skychange/skychange.py": 1690807232.3542962,
+        "TEMPLATE_PATH/models/cv/image_skychange/skychange_model.py": 1690807232.3542962,
+        "TEMPLATE_PATH/models/cv/image_to_image_generation/data/transforms.py": 1690807232.3542962,
+        "TEMPLATE_PATH/models/cv/image_to_image_generation/model.py": 1690807232.3542962,
+        "TEMPLATE_PATH/models/cv/image_to_image_generation/models/autoencoder.py": 1690807232.3542962,
+        "TEMPLATE_PATH/models/cv/image_to_image_generation/models/clip.py": 1690807232.3542962,
+        "TEMPLATE_PATH/models/cv/image_to_image_generation/ops/diffusion.py": 1690807232.3542962,
+        "TEMPLATE_PATH/models/cv/image_to_image_generation/ops/losses.py": 1690807232.3542962,
+        "TEMPLATE_PATH/models/cv/image_to_image_translation/data/transforms.py": 1690807232.3542962,
+        "TEMPLATE_PATH/models/cv/image_to_image_translation/model_translation.py": 1690807232.3542962,
+        "TEMPLATE_PATH/models/cv/image_to_image_translation/models/autoencoder.py": 1690807232.3542962,
+        "TEMPLATE_PATH/models/cv/image_to_image_translation/models/clip.py": 1690807232.3542962,
+        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/apps.py": 1690807232.3542962,
+        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/degradation.py": 1690807232.3542962,
+        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/diffusion.py": 1690807232.3542962,
+        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/losses.py": 1690807232.3542962,
+        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/metrics.py": 1690807232.3542962,
+        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/random_color.py": 1690807232.3542962,
+        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/random_mask.py": 1690807232.3542962,
+        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/svd.py": 1690807232.3542962,
+        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/utils.py": 1690807232.3542962,
+        "TEMPLATE_PATH/models/cv/image_try_on/generator.py": 1690807232.3542962,
+        "TEMPLATE_PATH/models/cv/image_try_on/landmark.py": 1690807232.3542962,
+        "TEMPLATE_PATH/models/cv/image_try_on/try_on_infer.py": 1690807232.3542962,
+        "TEMPLATE_PATH/models/cv/image_try_on/warping.py": 1690807232.3542962,
+        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/backbone/resnet_DA.py": 1690807232.3542962,
+        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/backbone/vit_horizon_pry_image.py": 1690807232.3542962,
+        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/misc/fourier.py": 1690807232.3542962,
+        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/misc/panostretch.py": 1690807232.3542962,
+        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/misc/post_proc.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/modality/layout.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/panovit.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/utils.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/panovit.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/language_guided_video_summarization/summarizer.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/language_guided_video_summarization/transformer/layers.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/language_guided_video_summarization/transformer/models.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/language_guided_video_summarization/transformer/modules.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/language_guided_video_summarization/transformer/sub_layers.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/motion_generation/model.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/motion_generation/modules/cfg_sampler.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/motion_generation/modules/gaussian_diffusion.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/motion_generation/modules/mdm.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/motion_generation/modules/respace.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/motion_generation/modules/rotation2xyz.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/motion_generation/modules/smpl.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/movie_scene_segmentation/get_model.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/movie_scene_segmentation/model.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/movie_scene_segmentation/utils/head.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/movie_scene_segmentation/utils/save_op.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/movie_scene_segmentation/utils/shot_encoder.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/movie_scene_segmentation/utils/trn.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/nerf_recon_4k/dataloader/load_blender.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/nerf_recon_4k/dataloader/load_data.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/nerf_recon_4k/dataloader/load_llff.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/nerf_recon_4k/dataloader/load_tankstemple.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/nerf_recon_4k/dataloader/read_write_model.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/nerf_recon_4k/nerf_preprocess.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/nerf_recon_4k/nerf_recon_4k.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/nerf_recon_4k/network/dvgo.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/nerf_recon_4k/network/utils.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/nerf_recon_acc/dataloader/nerf_dataset.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/nerf_recon_acc/dataloader/read_write_model.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/nerf_recon_acc/nerf_preprocess.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/nerf_recon_acc/nerf_recon_acc.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/nerf_recon_acc/network/nerf.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/nerf_recon_acc/network/segmenter.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/nerf_recon_acc/network/utils.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/nerf_recon_vq_compression/dataloader/blender.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/nerf_recon_vq_compression/dataloader/llff.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/nerf_recon_vq_compression/dataloader/nsvf.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/nerf_recon_vq_compression/dataloader/ray_utils.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/nerf_recon_vq_compression/dataloader/tankstemple.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/nerf_recon_vq_compression/nerf_recon_vq_compression.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/nerf_recon_vq_compression/network/tensoRF.py": 1690807232.3582962,
+        "TEMPLATE_PATH/models/cv/nerf_recon_vq_compression/network/tensoRF_VQ.py": 1690807232.362296,
+        "TEMPLATE_PATH/models/cv/nerf_recon_vq_compression/network/tensorBase.py": 1690807232.362296,
+        "TEMPLATE_PATH/models/cv/nerf_recon_vq_compression/network/weighted_vq.py": 1690807232.362296,
+        "TEMPLATE_PATH/models/cv/nerf_recon_vq_compression/renderer.py": 1690807232.362296,
+        "TEMPLATE_PATH/models/cv/nerf_recon_vq_compression/utils.py": 1690807232.362296,
+        "TEMPLATE_PATH/models/cv/object_detection/mmdet_model.py": 1690807232.362296,
+        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/backbones/vit.py": 1690807232.362296,
+        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/dense_heads/anchor_head.py": 1690807232.362296,
+        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/dense_heads/rpn_head.py": 1690807232.362296,
+        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/necks/fpn.py": 1690807232.362296,
+        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/roi_heads/bbox_heads/convfc_bbox_head.py": 1690807232.362296,
+        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/roi_heads/mask_heads/fcn_mask_head.py": 1690807232.362296,
+        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/utils/checkpoint.py": 1690807232.362296,
+        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/utils/convModule_norm.py": 1690807232.362296,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/depe_detect.py": 1690807232.362296,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/assigners/hungarian_assigner_3d.py": 1690807232.362296,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/coders/nms_free_coder.py": 1690807232.362296,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/match_costs/match_cost.py": 1690807232.362296,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/util.py": 1690807232.362296,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/nuscenes_dataset.py": 1690807232.362296,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/loading.py": 1690807232.362296,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/transform_3d.py": 1690807232.362296,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/backbones/vovnet.py": 1690807232.362296,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/depth_net.py": 1690807232.362296,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/petrv2_dednhead.py": 1690807232.362296,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/detectors/petr3d.py": 1690807232.362296,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/necks/cp_fpn.py": 1690807232.362296,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/petr_transformer.py": 1690807232.362296,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/positional_encoding.py": 1690807232.362296,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/result_vis.py": 1690807232.362296,
+        "TEMPLATE_PATH/models/cv/ocr_detection/model.py": 1690807232.362296,
+        "TEMPLATE_PATH/models/cv/ocr_detection/modules/dbnet.py": 1690807232.362296,
+        "TEMPLATE_PATH/models/cv/ocr_detection/modules/layers.py": 1690807232.362296,
+        "TEMPLATE_PATH/models/cv/ocr_detection/modules/mix_ops.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/ocr_detection/modules/proxyless.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/ocr_detection/modules/seg_detector_loss.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/ocr_detection/preprocessor.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/ocr_detection/utils.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/ocr_recognition/model.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/ocr_recognition/modules/CRNN/main_model.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/ocr_recognition/modules/ConvNextViT/convnext.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/ocr_recognition/modules/ConvNextViT/main_model.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/ocr_recognition/modules/ConvNextViT/timm_tinyc.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/ocr_recognition/modules/ConvNextViT/vitstr.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/ocr_recognition/modules/LightweightEdge/main_model.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/ocr_recognition/modules/LightweightEdge/nas_block/layers.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/ocr_recognition/modules/LightweightEdge/nas_block/mix_ops.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/ocr_recognition/modules/LightweightEdge/nas_block/proxyless.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/ocr_recognition/preprocessor.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/open_vocabulary_detection_vild/vild.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/panorama_depth_estimation/networks/equi.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/panorama_depth_estimation/networks/layers.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/panorama_depth_estimation/networks/mobilenet.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/panorama_depth_estimation/networks/resnet.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/panorama_depth_estimation/networks/unifuse.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/panorama_depth_estimation/networks/util.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/panorama_depth_estimation/unifuse_model.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/pedestrian_attribute_recognition/model.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/pointcloud_sceneflow_estimation/common.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/pointcloud_sceneflow_estimation/pointnet2_utils.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/pointcloud_sceneflow_estimation/rcp_model.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/pointcloud_sceneflow_estimation/sf_rcp.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/product_retrieval_embedding/item_detection.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/product_retrieval_embedding/item_embedding.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/product_retrieval_embedding/item_model.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/product_segmentation/net.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/product_segmentation/seg_infer.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/model.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/backbone.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/criterion.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/matcher.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/misc.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/mttr.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/multimodal_transformer.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/position_encoding_2d.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/postprocessing.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/segmentation.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/swin_transformer.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/robust_image_classification/easyrobust_model.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/s2net_panorama_depth_estimation/networks/config.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/s2net_panorama_depth_estimation/networks/decoder.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/s2net_panorama_depth_estimation/networks/model.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/s2net_panorama_depth_estimation/networks/resnet.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/s2net_panorama_depth_estimation/networks/swin_transformer.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/s2net_panorama_depth_estimation/networks/util_helper.py": 1690807232.3662963,
+        "TEMPLATE_PATH/models/cv/s2net_panorama_depth_estimation/s2net_model.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/salient_detection/models/backbone/Res2Net_v1b.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/salient_detection/models/modules.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/salient_detection/models/senet.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/salient_detection/models/u2net.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/salient_detection/models/utils.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/salient_detection/salient_model.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/shop_segmentation/common.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/shop_segmentation/head_fpn.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/shop_segmentation/models.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/shop_segmentation/neck_fpn.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/shop_segmentation/shop_seg_base.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/shop_segmentation/shop_seg_model.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/shop_segmentation/utils.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/skin_retouching/detection_model/detection_module.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/skin_retouching/detection_model/detection_unet_in.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/skin_retouching/inpainting_model/gconv.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/skin_retouching/inpainting_model/inpainting_unet.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/skin_retouching/retinaface/box_utils.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/skin_retouching/retinaface/net.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/skin_retouching/retinaface/network.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/skin_retouching/retinaface/predict_single.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/skin_retouching/retinaface/prior_box.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/skin_retouching/retinaface/utils.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/skin_retouching/unet_deploy.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/skin_retouching/utils.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/skin_retouching/weights_init.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/stream_yolo/data/data_augment.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/stream_yolo/exp/base_exp.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/stream_yolo/exp/build.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/stream_yolo/exp/default/streamyolo.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/stream_yolo/exp/yolox_base.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/stream_yolo/models/darknet.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/stream_yolo/models/dfp_pafpn.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/stream_yolo/models/network_blocks.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/stream_yolo/models/streamyolo.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/stream_yolo/models/tal_head.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/stream_yolo/realtime_video_detector.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/stream_yolo/utils/boxes.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/stream_yolo/utils/format.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/super_resolution/arch_util.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/super_resolution/ecb.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/super_resolution/ecbsr_model.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/super_resolution/rrdbnet_arch.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/table_recognition/lineless_table_process.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/table_recognition/model_lore.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/table_recognition/modules/lore_detector.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/table_recognition/modules/lore_processor.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/text_driven_segmentation/clip.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/text_driven_segmentation/lseg_base.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/text_driven_segmentation/lseg_blocks.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/text_driven_segmentation/lseg_model.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/text_driven_segmentation/lseg_net.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/text_driven_segmentation/lseg_vit.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/text_driven_segmentation/model.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/text_driven_segmentation/simple_tokenizer.py": 1690807232.3702962,
+        "TEMPLATE_PATH/models/cv/text_to_360panorama_image/pipeline_base.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/text_to_360panorama_image/pipeline_sr.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_classfication/basic_blocks.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_classfication/global_utils.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_classfication/master_net.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_classfication/model_zoo.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_classfication/plain_net_utils.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_classfication/super_blocks.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_classfication/super_res_idwexkx.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_classfication/super_res_k1kxk1.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_classfication/super_res_kxkx.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/apis/detector_evaluater.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/apis/detector_inference.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/augmentations/box_level_augs/box_level_augs.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/augmentations/box_level_augs/color_augs.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/augmentations/box_level_augs/gaussian_maps.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/augmentations/box_level_augs/geometric_augs.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/augmentations/scale_aware_aug.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/backbones/darknet.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/backbones/tinynas_csp.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/backbones/tinynas_res.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/core/base_ops.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/core/neck_ops.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/core/ops.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/core/ota_assigner.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/core/repvgg_block.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/core/utils.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/core/weight_init.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/heads/gfocal_v2_tiny.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/heads/zero_head.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/losses/distill_loss.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/losses/gfocal_loss.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/necks/giraffe_config.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/necks/giraffe_fpn.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/necks/giraffe_fpn_btn.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/detectors/detector.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/structures/bounding_box.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/structures/boxlist_ops.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/structures/image_list.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/utils/boxes.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/utils/model_utils.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/utils/scheduler.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/detector.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/tinynas_damoyolo.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/tinynas_detector.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/utils.py": 1690807232.3742962,
+        "TEMPLATE_PATH/models/cv/video_deinterlace/UNet_for_video_deinterlace.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_deinterlace/deinterlace_arch.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_deinterlace/models/archs.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_deinterlace/models/deep_fourier_upsampling.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_deinterlace/models/enh.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_deinterlace/models/fre.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_deinterlace/models/utils.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/configs/default_config.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/dro_model.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/geometry/camera.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/geometry/camera_utils.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/geometry/pose.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/geometry/pose_utils.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/models/model_checkpoint.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/models/model_utils.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/models/model_wrapper.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/models/sfm_model_mf.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/models/sup_model_mf.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/networks/depth_pose/depth_pose_net.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/networks/layers/resnet/depth_decoder.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/networks/layers/resnet/layers.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/networks/layers/resnet/pose_decoder.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/networks/layers/resnet/resnet_encoder.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/networks/optim/extractor.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/networks/optim/update.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/augmentations.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/config.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/depth.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/horovod.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/image.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/image_gt.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/load.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/misc.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/types.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_frame_interpolation/VFINet_arch.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_frame_interpolation/VFINet_for_video_frame_interpolation.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_frame_interpolation/flow_model/corr.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_frame_interpolation/flow_model/extractor.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_frame_interpolation/flow_model/raft.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_frame_interpolation/flow_model/update.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_frame_interpolation/interp_model/IFNet_swin.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_frame_interpolation/interp_model/UNet.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_frame_interpolation/interp_model/flow_reversal.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_frame_interpolation/interp_model/refinenet_arch.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_frame_interpolation/interp_model/transformer_layers.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_frame_interpolation/utils/scene_change_detection.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_frame_interpolation/utils/utils.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_human_matting/model.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_human_matting/models/decoder.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_human_matting/models/deep_guided_filter.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_human_matting/models/effv2.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_human_matting/models/lraspp.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_human_matting/models/matting.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_inpainting/inpainting.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_inpainting/inpainting_model.py": 1690807232.3782961,
+        "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_frame_iter_head.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_head.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_iter_head.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_update_head.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_updator.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_instance_segmentation/neck/msdeformattn_decoder.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_instance_segmentation/track/kernel_update_head.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_instance_segmentation/track/mask_hungarian_assigner.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_instance_segmentation/utils.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_instance_segmentation/video_knet.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/models/common.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/models/decode.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/models/model.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/models/yolo.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/tracker/basetrack.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/tracker/matching.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/tracker/multitracker.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/utils/image.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/utils/kalman_filter.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/utils/utils.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/utils/visualization.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_object_segmentation/aggregate.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_object_segmentation/cbam.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_object_segmentation/eval_network.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_object_segmentation/inference_core.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_object_segmentation/inference_memory_bank.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_object_segmentation/mod_resnet.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_object_segmentation/model.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_object_segmentation/modules.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_object_segmentation/network.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/backbone/swin_checkpoint.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/backbone/swin_transformer.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/kernel_head.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/kernel_iter_head.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/kernel_update_head.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/kernel_updator.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/mask.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/semantic_fpn_wrapper.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/track_heads.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/neck/fpn.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/track/quasi_dense_embed_tracker.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/video_k_net.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/visualizer.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/config/ostrack.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/layers/attn.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/layers/attn_blocks.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/layers/head.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/layers/patch_embed.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/ostrack/base_backbone.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/ostrack/ostrack.py": 1690807232.3822963,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/ostrack/utils.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/ostrack/vit_ce.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/procontext/procontext.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/procontext/utils.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/procontext/vit_ce.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/tracker/ostrack.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/tracker/procontext.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/utils/utils.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/DUT_raft.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/MotionPro.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/RAFT/corr.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/RAFT/extractor.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/RAFT/raft.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/RAFT/update.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/Smoother.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/config.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/rf_det_module.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/rf_det_so.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/video_stabilization/DUTRAFTStabilizer.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/video_stabilization/utils/IterativeSmooth.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/video_stabilization/utils/MedianFilter.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/video_stabilization/utils/ProjectionUtils.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/video_stabilization/utils/RAFTUtils.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/video_stabilization/utils/WarpUtils.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/video_stabilization/utils/image_utils.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/video_stabilization/utils/math_utils.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/video_streaming_perception/longshortnet/exp/longshortnet_base.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/video_streaming_perception/longshortnet/longshortnet.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/video_streaming_perception/longshortnet/models/dfp_pafpn_long.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/video_streaming_perception/longshortnet/models/dfp_pafpn_short.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/video_streaming_perception/longshortnet/models/longshort.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/video_streaming_perception/longshortnet/models/longshort_backbone_neck.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/video_summarization/base_model.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/video_summarization/kts/cpd_auto.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/video_summarization/kts/cpd_nonlin.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/video_summarization/pgl_sum.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/video_summarization/summarizer.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/video_super_resolution/basicvsr_net.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/video_super_resolution/common.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/video_super_resolution/msrresnet_lite_model.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/video_super_resolution/real_basicvsr_for_video_super_resolution.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/video_super_resolution/real_basicvsr_net.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/vidt/backbone.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/vidt/deformable_transformer.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/vidt/fpn_fusion.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/vidt/head.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/vidt/model.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/virual_tryon/sdafnet.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/backbone.py": 1690807232.3862963,
+        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/head.py": 1690807232.3902962,
+        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/model.py": 1690807232.3902962,
+        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/petl.py": 1690807232.3902962,
+        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/timm_helpers.py": 1690807232.3902962,
+        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/timm_vision_transformer.py": 1690807232.3902962,
+        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/timm_weight_init.py": 1690807232.3902962,
+        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/vision_efficient_tuning.py": 1690807232.3902962,
+        "TEMPLATE_PATH/models/cv/vision_middleware/backbone.py": 1690807232.3902962,
+        "TEMPLATE_PATH/models/cv/vision_middleware/head.py": 1690807232.3902962,
+        "TEMPLATE_PATH/models/cv/vision_middleware/model.py": 1690807232.3902962,
+        "TEMPLATE_PATH/models/cv/vision_middleware/vim.py": 1690807232.3902962,
+        "TEMPLATE_PATH/models/cv/vop_retrieval/backbone.py": 1690807232.3902962,
+        "TEMPLATE_PATH/models/cv/vop_retrieval/basic_utils.py": 1690807232.3902962,
+        "TEMPLATE_PATH/models/cv/vop_retrieval/model.py": 1690807232.3902962,
+        "TEMPLATE_PATH/models/cv/vop_retrieval/model_se.py": 1690807232.3902962,
+        "TEMPLATE_PATH/models/cv/vop_retrieval/tokenization_clip.py": 1690807232.3902962,
+        "TEMPLATE_PATH/models/multi_modal/clip/bert_tokenizer.py": 1690807232.3902962,
+        "TEMPLATE_PATH/models/multi_modal/clip/configuration_bert.py": 1690807232.3902962,
+        "TEMPLATE_PATH/models/multi_modal/clip/model.py": 1690807232.3902962,
+        "TEMPLATE_PATH/models/multi_modal/clip/modeling_bert.py": 1690807232.3902962,
+        "TEMPLATE_PATH/models/multi_modal/clip_interrogator/model.py": 1690807232.3902962,
+        "TEMPLATE_PATH/models/multi_modal/diffusion/diffusion.py": 1690807232.3902962,
+        "TEMPLATE_PATH/models/multi_modal/diffusion/model.py": 1690807232.3902962,
+        "TEMPLATE_PATH/models/multi_modal/diffusion/structbert.py": 1690807232.3902962,
+        "TEMPLATE_PATH/models/multi_modal/diffusion/tokenizer.py": 1690807232.3902962,
+        "TEMPLATE_PATH/models/multi_modal/diffusion/unet_generator.py": 1690807232.3902962,
+        "TEMPLATE_PATH/models/multi_modal/diffusion/unet_upsampler_1024.py": 1690807232.3902962,
+        "TEMPLATE_PATH/models/multi_modal/diffusion/unet_upsampler_256.py": 1690807232.3902962,
+        "TEMPLATE_PATH/models/multi_modal/dpm_solver_pytorch.py": 1690807232.3902962,
+        "TEMPLATE_PATH/models/multi_modal/efficient_diffusion_tuning/efficient_stable_diffusion.py": 1690807232.3902962,
+        "TEMPLATE_PATH/models/multi_modal/gemm/gemm_base.py": 1690807232.3902962,
+        "TEMPLATE_PATH/models/multi_modal/gemm/gemm_model.py": 1690807232.3902962,
+        "TEMPLATE_PATH/models/multi_modal/gemm/tokenizer.py": 1690807232.3902962,
+        "TEMPLATE_PATH/models/multi_modal/guided_diffusion/gaussian_diffusion.py": 1690807232.3902962,
+        "TEMPLATE_PATH/models/multi_modal/guided_diffusion/respace.py": 1690807232.3902962,
+        "TEMPLATE_PATH/models/multi_modal/guided_diffusion/script.py": 1690807232.3902962,
+        "TEMPLATE_PATH/models/multi_modal/guided_diffusion/unet.py": 1690807232.3942962,
+        "TEMPLATE_PATH/models/multi_modal/mgeo/backbone.py": 1690807232.3942962,
+        "TEMPLATE_PATH/models/multi_modal/mgeo/text_classification.py": 1690807232.3942962,
+        "TEMPLATE_PATH/models/multi_modal/mgeo/text_ranking.py": 1690807232.3942962,
+        "TEMPLATE_PATH/models/multi_modal/mgeo/token_classification.py": 1690807232.3942962,
+        "TEMPLATE_PATH/models/multi_modal/mmr/dataloaders/rawvideo_util.py": 1690807232.3942962,
+        "TEMPLATE_PATH/models/multi_modal/mmr/models/clip_for_mm_video_embedding.py": 1690807232.3942962,
+        "TEMPLATE_PATH/models/multi_modal/mmr/models/dynamic_inverted_softmax.py": 1690807232.3942962,
+        "TEMPLATE_PATH/models/multi_modal/mmr/models/modeling.py": 1690807232.3942962,
+        "TEMPLATE_PATH/models/multi_modal/mmr/models/module_clip.py": 1690807232.3942962,
+        "TEMPLATE_PATH/models/multi_modal/mmr/models/module_cross.py": 1690807232.3942962,
+        "TEMPLATE_PATH/models/multi_modal/mmr/models/tokenization_clip.py": 1690807232.3942962,
+        "TEMPLATE_PATH/models/multi_modal/mmr/models/until_module.py": 1690807232.3942962,
+        "TEMPLATE_PATH/models/multi_modal/mplug/clip/clip.py": 1690807232.3942962,
+        "TEMPLATE_PATH/models/multi_modal/mplug/configuration_mplug.py": 1690807232.3942962,
+        "TEMPLATE_PATH/models/multi_modal/mplug/modeling_mplug.py": 1690807232.3942962,
+        "TEMPLATE_PATH/models/multi_modal/mplug/mvit.py": 1690807232.3942962,
+        "TEMPLATE_PATH/models/multi_modal/mplug/predictor.py": 1690807232.3942962,
+        "TEMPLATE_PATH/models/multi_modal/mplug_for_all_tasks.py": 1690807232.3942962,
+        "TEMPLATE_PATH/models/multi_modal/mplug_owl/configuration_mplug_owl.py": 1690807232.3942962,
+        "TEMPLATE_PATH/models/multi_modal/mplug_owl/modeling_mplug_owl.py": 1690807232.3942962,
+        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/clip.py": 1690807232.3942962,
+        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/decoder.py": 1690807232.3942962,
+        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/gaussian_diffusion.py": 1690807232.3942962,
+        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/model.py": 1690807232.3942962,
+        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/prior.py": 1690807232.3942962,
+        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/tokenizer.py": 1690807232.3942962,
+        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/upsampler.py": 1690807232.3942962,
+        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/xglm.py": 1690807232.3942962,
+        "TEMPLATE_PATH/models/multi_modal/ofa/configuration_mmspeech.py": 1690807232.3942962,
+        "TEMPLATE_PATH/models/multi_modal/ofa/configuration_ofa.py": 1690807232.3942962,
+        "TEMPLATE_PATH/models/multi_modal/ofa/generate/incremental_decoding_utils.py": 1690807232.398296,
+        "TEMPLATE_PATH/models/multi_modal/ofa/generate/multihead_attention.py": 1690807232.398296,
+        "TEMPLATE_PATH/models/multi_modal/ofa/generate/ngram_repeat_block.py": 1690807232.398296,
+        "TEMPLATE_PATH/models/multi_modal/ofa/generate/search.py": 1690807232.398296,
+        "TEMPLATE_PATH/models/multi_modal/ofa/generate/sequence_generator.py": 1690807232.398296,
+        "TEMPLATE_PATH/models/multi_modal/ofa/generate/token_generation_constraints.py": 1690807232.398296,
+        "TEMPLATE_PATH/models/multi_modal/ofa/generate/utils.py": 1690807232.398296,
+        "TEMPLATE_PATH/models/multi_modal/ofa/modeling_mmspeech.py": 1690807232.398296,
+        "TEMPLATE_PATH/models/multi_modal/ofa/modeling_ofa.py": 1690807232.398296,
+        "TEMPLATE_PATH/models/multi_modal/ofa/resnet.py": 1690807232.398296,
+        "TEMPLATE_PATH/models/multi_modal/ofa/tokenization_ofa.py": 1690807232.398296,
+        "TEMPLATE_PATH/models/multi_modal/ofa/tokenization_ofa_fast.py": 1690807232.398296,
+        "TEMPLATE_PATH/models/multi_modal/ofa/utils/constant.py": 1690807232.398296,
+        "TEMPLATE_PATH/models/multi_modal/ofa/utils/utils.py": 1690807232.398296,
+        "TEMPLATE_PATH/models/multi_modal/ofa/vit.py": 1690807232.398296,
+        "TEMPLATE_PATH/models/multi_modal/ofa_for_all_tasks.py": 1690807232.398296,
+        "TEMPLATE_PATH/models/multi_modal/ofa_for_text_to_image_synthesis_model.py": 1690807232.398296,
+        "TEMPLATE_PATH/models/multi_modal/rleg/model.py": 1690807232.398296,
+        "TEMPLATE_PATH/models/multi_modal/rleg/rleg.py": 1690807232.398296,
+        "TEMPLATE_PATH/models/multi_modal/soonet/blocks.py": 1690807232.398296,
+        "TEMPLATE_PATH/models/multi_modal/soonet/clip.py": 1690807232.398296,
+        "TEMPLATE_PATH/models/multi_modal/soonet/model.py": 1690807232.398296,
+        "TEMPLATE_PATH/models/multi_modal/soonet/swin_transformer.py": 1690807232.398296,
+        "TEMPLATE_PATH/models/multi_modal/soonet/tokenizer.py": 1690807232.398296,
+        "TEMPLATE_PATH/models/multi_modal/soonet/utils.py": 1690807232.398296,
+        "TEMPLATE_PATH/models/multi_modal/stable_diffusion/stable_diffusion.py": 1690807232.398296,
+        "TEMPLATE_PATH/models/multi_modal/team/team_model.py": 1690807232.398296,
+        "TEMPLATE_PATH/models/multi_modal/team/utils.py": 1690807232.398296,
+        "TEMPLATE_PATH/models/multi_modal/video_synthesis/autoencoder.py": 1690807232.398296,
+        "TEMPLATE_PATH/models/multi_modal/video_synthesis/diffusion.py": 1690807232.398296,
+        "TEMPLATE_PATH/models/multi_modal/video_synthesis/text_to_video_synthesis_model.py": 1690807232.398296,
+        "TEMPLATE_PATH/models/multi_modal/video_synthesis/unet_sd.py": 1690807232.398296,
+        "TEMPLATE_PATH/models/multi_modal/vldoc/conv_fpn_trans.py": 1690807232.398296,
+        "TEMPLATE_PATH/models/multi_modal/vldoc/convnext.py": 1690807232.398296,
+        "TEMPLATE_PATH/models/multi_modal/vldoc/model.py": 1690807232.398296,
+        "TEMPLATE_PATH/models/multi_modal/vldoc/modeling_layout_roberta.py": 1690807232.4022963,
+        "TEMPLATE_PATH/models/multi_modal/vldoc/processing.py": 1690807232.4022963,
+        "TEMPLATE_PATH/models/multi_modal/vldoc/tokenization.py": 1690807232.4022963,
+        "TEMPLATE_PATH/models/multi_modal/vldoc/transformer_local.py": 1690807232.4022963,
+        "TEMPLATE_PATH/models/nlp/T5/backbone.py": 1690807232.4022963,
+        "TEMPLATE_PATH/models/nlp/T5/configuration.py": 1690807232.4022963,
+        "TEMPLATE_PATH/models/nlp/T5/text2text_generation.py": 1690807232.4022963,
+        "TEMPLATE_PATH/models/nlp/bart/text_error_correction.py": 1690807232.4022963,
+        "TEMPLATE_PATH/models/nlp/bert/backbone.py": 1690807232.4022963,
+        "TEMPLATE_PATH/models/nlp/bert/configuration.py": 1690807232.4022963,
+        "TEMPLATE_PATH/models/nlp/bert/document_segmentation.py": 1690807232.4022963,
+        "TEMPLATE_PATH/models/nlp/bert/fill_mask.py": 1690807232.4022963,
+        "TEMPLATE_PATH/models/nlp/bert/sentence_embedding.py": 1690807232.4022963,
+        "TEMPLATE_PATH/models/nlp/bert/siamese_uie.py": 1690807232.4022963,
+        "TEMPLATE_PATH/models/nlp/bert/text_classification.py": 1690807232.4022963,
+        "TEMPLATE_PATH/models/nlp/bert/text_ranking.py": 1690807232.4022963,
+        "TEMPLATE_PATH/models/nlp/bert/token_classification.py": 1690807232.4022963,
+        "TEMPLATE_PATH/models/nlp/bert/word_alignment.py": 1690807232.4022963,
+        "TEMPLATE_PATH/models/nlp/bloom/backbone.py": 1690807232.4022963,
+        "TEMPLATE_PATH/models/nlp/canmt/canmt_model.py": 1690807232.4022963,
+        "TEMPLATE_PATH/models/nlp/canmt/canmt_translation.py": 1690807232.4022963,
+        "TEMPLATE_PATH/models/nlp/canmt/sequence_generator.py": 1690807232.4022963,
+        "TEMPLATE_PATH/models/nlp/chatglm/configuration.py": 1690807232.4022963,
+        "TEMPLATE_PATH/models/nlp/chatglm/quantization.py": 1690807232.4022963,
+        "TEMPLATE_PATH/models/nlp/chatglm/text_generation.py": 1690807232.4022963,
+        "TEMPLATE_PATH/models/nlp/chatglm/tokenization.py": 1690807232.4022963,
+        "TEMPLATE_PATH/models/nlp/chatglm2/configuration.py": 1690807232.4022963,
+        "TEMPLATE_PATH/models/nlp/chatglm2/quantization.py": 1690807232.4022963,
+        "TEMPLATE_PATH/models/nlp/chatglm2/text_generation.py": 1690807232.4062963,
+        "TEMPLATE_PATH/models/nlp/chatglm2/tokenization.py": 1690807232.4062963,
+        "TEMPLATE_PATH/models/nlp/codegeex/codegeex.py": 1690807232.4062963,
+        "TEMPLATE_PATH/models/nlp/codegeex/codegeex_for_code_generation.py": 1690807232.4062963,
+        "TEMPLATE_PATH/models/nlp/codegeex/codegeex_for_code_translation.py": 1690807232.4062963,
+        "TEMPLATE_PATH/models/nlp/codegeex/inference.py": 1690807232.4062963,
+        "TEMPLATE_PATH/models/nlp/codegeex/tokenizer.py": 1690807232.4062963,
+        "TEMPLATE_PATH/models/nlp/csanmt/translation.py": 1690807232.4062963,
+        "TEMPLATE_PATH/models/nlp/deberta_v2/backbone.py": 1690807232.4062963,
+        "TEMPLATE_PATH/models/nlp/deberta_v2/configuration.py": 1690807232.4062963,
+        "TEMPLATE_PATH/models/nlp/deberta_v2/fill_mask.py": 1690807232.4062963,
+        "TEMPLATE_PATH/models/nlp/deberta_v2/tokenization.py": 1690807232.4062963,
+        "TEMPLATE_PATH/models/nlp/deberta_v2/tokenization_fast.py": 1690807232.4062963,
+        "TEMPLATE_PATH/models/nlp/dgds/backbone.py": 1690807232.4062963,
+        "TEMPLATE_PATH/models/nlp/dgds/document_grounded_dialog_generate.py": 1690807232.4062963,
+        "TEMPLATE_PATH/models/nlp/dgds/document_grounded_dialog_rerank.py": 1690807232.4062963,
+        "TEMPLATE_PATH/models/nlp/dgds/document_grounded_dialog_retrieval.py": 1690807232.4062963,
+        "TEMPLATE_PATH/models/nlp/fid_T5/text_generation.py": 1690807232.4062963,
+        "TEMPLATE_PATH/models/nlp/fid_plug/backbone.py": 1690807232.4062963,
+        "TEMPLATE_PATH/models/nlp/fid_plug/configuration.py": 1690807232.4062963,
+        "TEMPLATE_PATH/models/nlp/fid_plug/text_generation.py": 1690807232.4062963,
+        "TEMPLATE_PATH/models/nlp/glm_130b/generation/strategies.py": 1690807232.4062963,
+        "TEMPLATE_PATH/models/nlp/glm_130b/initialize.py": 1690807232.4062963,
+        "TEMPLATE_PATH/models/nlp/glm_130b/quantization/functional.py": 1690807232.4062963,
+        "TEMPLATE_PATH/models/nlp/glm_130b/quantization/layers.py": 1690807232.4062963,
+        "TEMPLATE_PATH/models/nlp/glm_130b/text_generation.py": 1690807232.4062963,
+        "TEMPLATE_PATH/models/nlp/gpt2/backbone.py": 1690807232.4062963,
+        "TEMPLATE_PATH/models/nlp/gpt3/backbone.py": 1690807232.4062963,
+        "TEMPLATE_PATH/models/nlp/gpt3/configuration.py": 1690807232.4062963,
+        "TEMPLATE_PATH/models/nlp/gpt3/distributed_gpt3.py": 1690807232.4062963,
+        "TEMPLATE_PATH/models/nlp/gpt3/text_generation.py": 1690807232.4062963,
+        "TEMPLATE_PATH/models/nlp/gpt3/tokenizer.py": 1690807232.4062963,
+        "TEMPLATE_PATH/models/nlp/gpt_moe/backbone.py": 1690807232.4062963,
+        "TEMPLATE_PATH/models/nlp/gpt_moe/checkpointing.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/gpt_moe/configuration.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/gpt_moe/distributed_gpt_moe.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/gpt_moe/moe/experts.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/gpt_moe/moe/layer.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/gpt_moe/moe/mappings.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/gpt_moe/moe/sharded_moe.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/gpt_moe/moe/utils.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/gpt_moe/text_generation.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/gpt_moe/tokenizer.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/gpt_neo/backbone.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/heads/crf_head.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/heads/fill_mask_head.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/heads/infromation_extraction_head.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/heads/text_classification_head.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/heads/text_generation_head.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/heads/text_ranking_head.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/heads/token_classification_head.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/heads/torch_pretrain_head.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/hf_transformers/backbone.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/llama/backbone.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/llama/configuration.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/llama/convert_llama_weights_to_hf.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/llama/text_generation.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/llama/tokenization.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/llama/tokenization_fast.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/llama2/backbone.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/llama2/configuration.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/llama2/text_generation.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/llama2/tokenization.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/llama2/tokenization_fast.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/lstm/backbone.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/lstm/token_classification.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/megatron_bert/backbone.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/megatron_bert/configuration.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/megatron_bert/fill_mask.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/mglm/arguments.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/mglm/blocklm_utils.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/mglm/configure_data.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/mglm/data_utils/corpora.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/mglm/data_utils/datasets.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/mglm/data_utils/extraction.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/mglm/data_utils/file_utils.py": 1690807232.4102962,
+        "TEMPLATE_PATH/models/nlp/mglm/data_utils/lazy_loader.py": 1690807232.4142962,
+        "TEMPLATE_PATH/models/nlp/mglm/data_utils/samplers.py": 1690807232.4142962,
+        "TEMPLATE_PATH/models/nlp/mglm/data_utils/sp_tokenizer.py": 1690807232.4142962,
+        "TEMPLATE_PATH/models/nlp/mglm/data_utils/tokenization.py": 1690807232.4142962,
+        "TEMPLATE_PATH/models/nlp/mglm/data_utils/tokenization_gpt2.py": 1690807232.4142962,
+        "TEMPLATE_PATH/models/nlp/mglm/data_utils/wordpiece.py": 1690807232.4142962,
+        "TEMPLATE_PATH/models/nlp/mglm/generation_utils.py": 1690807232.4142962,
+        "TEMPLATE_PATH/models/nlp/mglm/mglm_for_text_summarization.py": 1690807232.4142962,
+        "TEMPLATE_PATH/models/nlp/mglm/model/distributed.py": 1690807232.4142962,
+        "TEMPLATE_PATH/models/nlp/mglm/model/downstream.py": 1690807232.4142962,
+        "TEMPLATE_PATH/models/nlp/mglm/model/modeling_bert.py": 1690807232.4142962,
+        "TEMPLATE_PATH/models/nlp/mglm/model/modeling_glm.py": 1690807232.4142962,
+        "TEMPLATE_PATH/models/nlp/mglm/model/prompt.py": 1690807232.4142962,
+        "TEMPLATE_PATH/models/nlp/mglm/model/transformer.py": 1690807232.4142962,
+        "TEMPLATE_PATH/models/nlp/mglm/process_grid.py": 1690807232.4142962,
+        "TEMPLATE_PATH/models/nlp/mglm/run_test.py": 1690807232.4142962,
+        "TEMPLATE_PATH/models/nlp/mglm/tasks/data_utils.py": 1690807232.4142962,
+        "TEMPLATE_PATH/models/nlp/mglm/tasks/eval_utils.py": 1690807232.4142962,
+        "TEMPLATE_PATH/models/nlp/mglm/tasks/language_model/dataset.py": 1690807232.4142962,
+        "TEMPLATE_PATH/models/nlp/mglm/tasks/language_model/detokenizer.py": 1690807232.4142962,
+        "TEMPLATE_PATH/models/nlp/mglm/tasks/language_model/finetune.py": 1690807232.4142962,
+        "TEMPLATE_PATH/models/nlp/mglm/tasks/seq2seq/dataset.py": 1690807232.4142962,
+        "TEMPLATE_PATH/models/nlp/mglm/tasks/seq2seq/evaluate.py": 1690807232.4142962,
+        "TEMPLATE_PATH/models/nlp/mglm/tasks/seq2seq/finetune.py": 1690807232.4142962,
+        "TEMPLATE_PATH/models/nlp/mglm/tasks/superglue/dataset.py": 1690807232.4142962,
+        "TEMPLATE_PATH/models/nlp/mglm/tasks/superglue/evaluate.py": 1690807232.4142962,
+        "TEMPLATE_PATH/models/nlp/mglm/tasks/superglue/finetune.py": 1690807232.4142962,
+        "TEMPLATE_PATH/models/nlp/mglm/tasks/superglue/pvp.py": 1690807232.4142962,
+        "TEMPLATE_PATH/models/nlp/mglm/test/test_block.py": 1690807232.4142962,
+        "TEMPLATE_PATH/models/nlp/mglm/test/test_rel_shift.py": 1690807232.4142962,
+        "TEMPLATE_PATH/models/nlp/mglm/train_utils.py": 1690807232.4142962,
+        "TEMPLATE_PATH/models/nlp/mglm/utils.py": 1690807232.4142962,
+        "TEMPLATE_PATH/models/nlp/palm_v2/configuration.py": 1690807232.4142962,
+        "TEMPLATE_PATH/models/nlp/palm_v2/dureader_eval.py": 1690807232.4142962,
+        "TEMPLATE_PATH/models/nlp/palm_v2/text_generation.py": 1690807232.4142962,
+        "TEMPLATE_PATH/models/nlp/peer/backbone.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/peer/configuration.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/peer/sas_utils.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/peer/text_classification.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/plug/AnnealingLR.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/plug/backbone.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/plug/configuration.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/plug/distributed_plug.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/plug/generator.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/plug_mental/adv_utils.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/plug_mental/backbone.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/plug_mental/configuration.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/plug_mental/text_classification.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/polylm/text_generation.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/ponet/backbone.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/ponet/configuration.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/ponet/document_segmentation.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/ponet/fill_mask.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/ponet/tokenization.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/space/configuration.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/space/dialog_intent_prediction.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/space/dialog_modeling.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/space/dialog_state_tracking.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/space/model/gen_unified_transformer.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/space/model/generator.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/space/model/intent_unified_transformer.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/space/model/model_base.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/space/model/tokenization_space.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/space/model/unified_transformer.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/space/modules/embedder.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/space/modules/feedforward.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/space/modules/functions.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/space/modules/multihead_attention.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/space/modules/transformer_block.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/space_T_cn/backbone.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/space_T_cn/configuration.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/space_T_cn/table_question_answering.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/space_T_en/text_to_sql.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/structbert/adv_utils.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/structbert/backbone.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/structbert/configuration.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/structbert/faq_question_answering.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/structbert/fill_mask.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/structbert/text_classification.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/structbert/token_classification.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/task_models/feature_extraction.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/task_models/fill_mask.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/task_models/information_extraction.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/task_models/task_model.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/task_models/text_classification.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/task_models/text_generation.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/task_models/text_ranking.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/task_models/token_classification.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/unite/configuration.py": 1690807232.418296,
+        "TEMPLATE_PATH/models/nlp/unite/translation_evaluation.py": 1690807232.4222963,
+        "TEMPLATE_PATH/models/nlp/use/transformer.py": 1690807232.4222963,
+        "TEMPLATE_PATH/models/nlp/use/user_satisfaction_estimation.py": 1690807232.4222963,
+        "TEMPLATE_PATH/models/nlp/veco/backbone.py": 1690807232.4222963,
+        "TEMPLATE_PATH/models/nlp/veco/configuration.py": 1690807232.4222963,
+        "TEMPLATE_PATH/models/nlp/veco/fill_mask.py": 1690807232.4222963,
+        "TEMPLATE_PATH/models/nlp/veco/text_classification.py": 1690807232.4222963,
+        "TEMPLATE_PATH/models/nlp/veco/token_classification.py": 1690807232.4222963,
+        "TEMPLATE_PATH/models/nlp/xlm_roberta/backbone.py": 1690807232.4222963,
+        "TEMPLATE_PATH/models/nlp/xlm_roberta/configuration.py": 1690807232.4222963,
+        "TEMPLATE_PATH/models/science/unifold/config.py": 1690807232.4222963,
+        "TEMPLATE_PATH/models/science/unifold/data/data_ops.py": 1690807232.4222963,
+        "TEMPLATE_PATH/models/science/unifold/data/msa_pairing.py": 1690807232.4222963,
+        "TEMPLATE_PATH/models/science/unifold/data/process.py": 1690807232.4222963,
+        "TEMPLATE_PATH/models/science/unifold/data/process_multimer.py": 1690807232.4222963,
+        "TEMPLATE_PATH/models/science/unifold/data/protein.py": 1690807232.4222963,
+        "TEMPLATE_PATH/models/science/unifold/data/residue_constants.py": 1690807232.4222963,
+        "TEMPLATE_PATH/models/science/unifold/data/utils.py": 1690807232.4222963,
+        "TEMPLATE_PATH/models/science/unifold/dataset.py": 1690807232.4222963,
+        "TEMPLATE_PATH/models/science/unifold/model.py": 1690807232.4222963,
+        "TEMPLATE_PATH/models/science/unifold/modules/alphafold.py": 1690807232.4222963,
+        "TEMPLATE_PATH/models/science/unifold/modules/attentions.py": 1690807232.4222963,
+        "TEMPLATE_PATH/models/science/unifold/modules/auxillary_heads.py": 1690807232.4222963,
+        "TEMPLATE_PATH/models/science/unifold/modules/common.py": 1690807232.4222963,
+        "TEMPLATE_PATH/models/science/unifold/modules/confidence.py": 1690807232.4222963,
+        "TEMPLATE_PATH/models/science/unifold/modules/embedders.py": 1690807232.4222963,
+        "TEMPLATE_PATH/models/science/unifold/modules/evoformer.py": 1690807232.4222963,
+        "TEMPLATE_PATH/models/science/unifold/modules/featurization.py": 1690807232.4222963,
+        "TEMPLATE_PATH/models/science/unifold/modules/frame.py": 1690807232.4222963,
+        "TEMPLATE_PATH/models/science/unifold/modules/structure_module.py": 1690807232.4222963,
+        "TEMPLATE_PATH/models/science/unifold/modules/template.py": 1690807232.4222963,
+        "TEMPLATE_PATH/models/science/unifold/modules/triangle_multiplication.py": 1690807232.4222963,
+        "TEMPLATE_PATH/models/science/unifold/msa/mmcif.py": 1690807232.4222963,
+        "TEMPLATE_PATH/models/science/unifold/msa/msa_identifiers.py": 1690807232.4222963,
+        "TEMPLATE_PATH/models/science/unifold/msa/parsers.py": 1690807232.4222963,
+        "TEMPLATE_PATH/models/science/unifold/msa/pipeline.py": 1690807232.4222963,
+        "TEMPLATE_PATH/models/science/unifold/msa/templates.py": 1690807232.4222963,
+        "TEMPLATE_PATH/models/science/unifold/msa/tools/hhblits.py": 1690807232.4222963,
+        "TEMPLATE_PATH/models/science/unifold/msa/tools/hhsearch.py": 1690807232.4222963,
+        "TEMPLATE_PATH/models/science/unifold/msa/tools/hmmbuild.py": 1690807232.4222963,
+        "TEMPLATE_PATH/models/science/unifold/msa/tools/hmmsearch.py": 1690807232.4222963,
+        "TEMPLATE_PATH/models/science/unifold/msa/tools/jackhmmer.py": 1690807232.4262962,
+        "TEMPLATE_PATH/models/science/unifold/msa/tools/kalign.py": 1690807232.4262962,
+        "TEMPLATE_PATH/models/science/unifold/msa/tools/utils.py": 1690807232.4262962,
+        "TEMPLATE_PATH/models/science/unifold/msa/utils.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/audio/asr_dataset.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/auth/auth_config.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/context/dataset_context_config.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/data_files/data_files_manager.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/data_loader/data_loader.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/data_loader/data_loader_manager.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/audio/asr_dataset.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/audio/kws_farfield_dataset.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/audio/kws_nearfield_dataset.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/audio/kws_nearfield_processor.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/bad_image_detecting/bad_image_detecting_dataset.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/builder.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/build.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/collate_batch.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/coco.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/mosaic_wrapper.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/coco/coco_eval.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/distributed.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/grouped_batch_sampler.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/iteration_based_batch_sampler.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/build.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/transforms.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/easycv_base.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/gopro_image_deblurring_dataset.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_colorization/image_colorization_dataset.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_inpainting/aug.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_inpainting/image_inpainting_dataset.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_instance_segmentation_coco_dataset.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/data_utils.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/image_portrait_enhancement_dataset.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_quality_assessment_degradation/image_quality_assessment_degradation_dataset.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_quality_assmessment_mos/image_quality_assessment_mos_dataset.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/language_guided_video_summarization_dataset.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/mgeo_ranking_dataset.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/movie_scene_segmentation_dataset.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/sampler.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/augmenter.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/data_loader.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/image_dataset.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/iou_evaluator.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/quad_measurer.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/augment_data.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/data_process.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_border_map.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_icdar_data.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_seg_detection_data.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/normalize_image.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/random_crop_data.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_recognition_dataset.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/reds_image_deblurring_dataset.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/referring_video_object_segmentation_dataset.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/transformers.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/data_utils.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/sidd_image_denoising_dataset.py": 1690807232.4262962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/transforms.py": 1690807232.4302962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/text_ranking_dataset.py": 1690807232.4302962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/torch_custom_dataset.py": 1690807232.4302962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/veco_dataset.py": 1690807232.4302962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/data_utils.py": 1690807232.4302962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/video_frame_interpolation_dataset.py": 1690807232.4302962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/video_stabilization/video_stabilization_dataset.py": 1690807232.4302962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/video_summarization_dataset.py": 1690807232.4302962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/video_super_resolution/video_super_resolution_dataset.py": 1690807232.4302962,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/dataset.py": 1690807232.4302962,
+        "TEMPLATE_PATH/msdatasets/download/dataset_builder.py": 1690807232.4302962,
+        "TEMPLATE_PATH/msdatasets/download/download_config.py": 1690807232.4302962,
+        "TEMPLATE_PATH/msdatasets/download/download_manager.py": 1690807232.4302962,
+        "TEMPLATE_PATH/msdatasets/meta/data_meta_config.py": 1690807232.4302962,
+        "TEMPLATE_PATH/msdatasets/meta/data_meta_manager.py": 1690807232.4302962,
+        "TEMPLATE_PATH/msdatasets/ms_dataset.py": 1690807232.4302962,
+        "TEMPLATE_PATH/msdatasets/task_datasets/gopro_image_deblurring_dataset.py": 1690807232.4302962,
+        "TEMPLATE_PATH/msdatasets/task_datasets/reds_image_deblurring_dataset.py": 1690807232.4302962,
+        "TEMPLATE_PATH/msdatasets/task_datasets/sidd_image_denoising.py": 1690807232.4302962,
+        "TEMPLATE_PATH/msdatasets/task_datasets/torch_base_dataset.py": 1690807232.4302962,
+        "TEMPLATE_PATH/msdatasets/task_datasets/video_summarization_dataset.py": 1690807232.4302962,
+        "TEMPLATE_PATH/msdatasets/utils/dataset_utils.py": 1690807232.4302962,
+        "TEMPLATE_PATH/msdatasets/utils/delete_utils.py": 1690807232.4302962,
+        "TEMPLATE_PATH/msdatasets/utils/maxcompute_utils.py": 1690807232.4302962,
+        "TEMPLATE_PATH/msdatasets/utils/oss_utils.py": 1690807232.4302962,
+        "TEMPLATE_PATH/msdatasets/utils/upload_utils.py": 1690807232.4302962,
+        "TEMPLATE_PATH/pipelines/audio/ans_dfsmn_pipeline.py": 1690807232.4302962,
+        "TEMPLATE_PATH/pipelines/audio/ans_pipeline.py": 1690807232.4302962,
+        "TEMPLATE_PATH/pipelines/audio/asr_inference_pipeline.py": 1690807232.4302962,
+        "TEMPLATE_PATH/pipelines/audio/asr_wenet_inference_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/audio/inverse_text_processing_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/audio/kws_farfield_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/audio/kws_kwsbp_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/audio/language_recognition_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/audio/linear_aec_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/audio/lm_infer_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/audio/punctuation_processing_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/audio/segmentation_clustering_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/audio/separation_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/audio/speaker_change_locating_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/audio/speaker_diarization_dialogue_detection_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/audio/speaker_diarization_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/audio/speaker_diarization_semantic_speaker_turn_detection_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/audio/speaker_verification_eres2net_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/audio/speaker_verification_light_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/audio/speaker_verification_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/audio/speaker_verification_rdino_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/audio/text_to_speech_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/audio/timestamp_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/audio/voice_activity_detection_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/base.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/builder.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/action_detection_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/action_recognition_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/animal_recognition_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/arc_face_recognition_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/bad_image_detecting_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/body_2d_keypoints_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/body_3d_keypoints_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/card_detection_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/cmdssl_video_embedding_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/content_check_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/controllable_image_generation_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/crowd_counting_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/ddcolor_image_colorization_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/ddpm_semantic_segmentation_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/face_attribute_recognition_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/face_detection_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/face_emotion_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/face_human_hand_detection_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/face_image_generation_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/face_liveness_ir_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/face_liveness_xc_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/face_processing_base_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/face_quality_assessment_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/face_recognition_onnx_fm_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/face_recognition_onnx_ir_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/face_recognition_ood_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/face_recognition_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/face_reconstruction_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/facial_expression_recognition_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/facial_landmark_confidence_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/fast_instance_segmentation_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/general_recognition_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/hand_static_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/hicossl_video_embedding_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/human_reconstruction_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/image_body_reshaping_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/image_bts_depth_estimation_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/image_cartoon_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/image_classification_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/image_color_enhance_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/image_colorization_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/image_debanding_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/image_deblur_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/image_defrcn_fewshot_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/image_denoise_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/image_depth_estimation_pipeline.py": 1690807232.4342961,
+        "TEMPLATE_PATH/pipelines/cv/image_detection_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/image_driving_perception_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/image_face_fusion_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/image_human_parsing_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/image_inpainting_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/image_inpainting_sdv2_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/image_instance_segmentation_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/image_matching_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/image_matting_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/image_mvs_depth_estimation_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/image_open_vocabulary_detection_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/image_paintbyexample_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/image_panoptic_segmentation_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/image_portrait_enhancement_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/image_quality_assessment_degradation_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/image_quality_assessment_man_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/image_quality_assessment_mos_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/image_reid_person_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/image_restoration_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/image_salient_detection_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/image_semantic_segmentation_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/image_skychange_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/image_structured_model_probing_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/image_style_transfer_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/image_super_resolution_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/image_to_image_generate_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/image_to_image_translation_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/image_try_on_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/indoor_layout_estimation_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/language_guided_video_summarization_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/license_plate_detection_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/lineless_table_recognition_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/live_category_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/mask_face_recognition_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/maskdino_instance_segmentation_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/mobile_image_super_resolution_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/mog_face_detection_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/motion_generation_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/movie_scene_segmentation_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/mtcnn_face_detection_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/nerf_recon_4k_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/nerf_recon_acc_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/nerf_recon_vq_compression_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/object_detection_3d_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/ocr_detection_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/ocr_recognition_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/ocr_utils/model_convnext_transformer.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/ocr_utils/model_dla34.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/ocr_utils/model_resnet18_half.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/ocr_utils/model_resnet_mutex_v4_linewithchar.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/ocr_utils/model_vlpt.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/ocr_utils/ocr_modules/convnext.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/ocr_utils/ocr_modules/timm_tinyc.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/ocr_utils/ocr_modules/vitstr.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/ocr_utils/ops.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/ocr_utils/resnet18_v1.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/ocr_utils/resnet_utils.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/ocr_utils/table_process.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/ocr_utils/utils.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/panorama_depth_estimation_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/panorama_depth_estimation_s2net_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/pedestrian_attribute_recognition_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/pointcloud_sceneflow_estimation_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/product_retrieval_embedding_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/product_segmentation_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/realtime_video_object_detection_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/referring_video_object_segmentation_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/retina_face_detection_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/shop_segmentation_pipleline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/skin_retouching_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/table_recognition_pipeline.py": 1690807232.4382963,
+        "TEMPLATE_PATH/pipelines/cv/tbs_detection_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/cv/tbs_detection_utils/utils.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/cv/text_driven_segmentation_pipleline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/cv/text_to_360panorama_image_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/cv/tinynas_classification_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/cv/tinynas_detection_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/cv/ulfd_face_detection_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/cv/video_category_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/cv/video_colorization_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/cv/video_deinterlace_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/cv/video_depth_estimation_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/cv/video_frame_interpolation_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/cv/video_human_matting_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/cv/video_inpainting_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/cv/video_instance_segmentation_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/cv/video_multi_object_tracking_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/cv/video_object_segmentation_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/cv/video_panoptic_segmentation_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/cv/video_single_object_tracking_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/cv/video_stabilization_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/cv/video_summarization_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/cv/video_super_resolution_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/cv/vidt_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/cv/virtual_try_on_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/cv/vision_efficient_tuning_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/cv/vision_middleware_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/cv/vop_retrieval_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/cv/vop_retrieval_se_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/multi_modal/asr_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/multi_modal/diffusers_wrapped/diffusers_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/chinese_stable_diffusion_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/stable_diffusion_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/multi_modal/disco_guided_diffusion_pipeline/disco_guided_diffusion.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/multi_modal/disco_guided_diffusion_pipeline/utils.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/multi_modal/document_vl_embedding_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/multi_modal/efficient_diffusion_tuning_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/multi_modal/generative_multi_modal_embedding_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/multi_modal/gridvlp_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/multi_modal/image_captioning_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/multi_modal/image_text_retrieval_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/multi_modal/mgeo_ranking_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/multi_modal/multi_modal_embedding_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/multi_modal/multimodal_dialogue_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/multi_modal/ocr_recognition_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/multi_modal/soonet_video_temporal_grounding_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/multi_modal/sudoku_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/multi_modal/team_multi_modal_similarity_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/multi_modal/text2sql_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/multi_modal/text_to_image_synthesis_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/multi_modal/text_to_video_synthesis_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/multi_modal/video_captioning_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/multi_modal/video_multi_modal_embedding_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/multi_modal/video_question_answering_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/multi_modal/visual_entailment_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/multi_modal/visual_grounding_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/multi_modal/visual_question_answering_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/nlp/automatic_post_editing_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/nlp/canmt_translation_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/nlp/codegeex_code_generation_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/nlp/codegeex_code_translation_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/nlp/conversational_text_to_sql_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/nlp/dialog_intent_prediction_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/nlp/dialog_modeling_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/nlp/dialog_state_tracking_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/nlp/distributed_gpt3_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/nlp/distributed_gpt_moe_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/nlp/distributed_plug_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/nlp/document_grounded_dialog_generate_pipeline.py": 1690807232.4422963,
+        "TEMPLATE_PATH/pipelines/nlp/document_grounded_dialog_rerank_pipeline.py": 1690807232.4462962,
+        "TEMPLATE_PATH/pipelines/nlp/document_grounded_dialog_retrieval_pipeline.py": 1690807232.4462962,
+        "TEMPLATE_PATH/pipelines/nlp/document_segmentation_pipeline.py": 1690807232.4462962,
+        "TEMPLATE_PATH/pipelines/nlp/extractive_summarization_pipeline.py": 1690807232.4462962,
+        "TEMPLATE_PATH/pipelines/nlp/faq_question_answering_pipeline.py": 1690807232.4462962,
+        "TEMPLATE_PATH/pipelines/nlp/fasttext_text_classification_pipeline.py": 1690807232.4462962,
+        "TEMPLATE_PATH/pipelines/nlp/feature_extraction_pipeline.py": 1690807232.4462962,
+        "TEMPLATE_PATH/pipelines/nlp/fid_dialogue_pipeline.py": 1690807232.4462962,
+        "TEMPLATE_PATH/pipelines/nlp/fill_mask_pipeline.py": 1690807232.4462962,
+        "TEMPLATE_PATH/pipelines/nlp/glm130b_text_generation_pipeline.py": 1690807232.4462962,
+        "TEMPLATE_PATH/pipelines/nlp/information_extraction_pipeline.py": 1690807232.4462962,
+        "TEMPLATE_PATH/pipelines/nlp/interactive_translation_pipeline.py": 1690807232.4462962,
+        "TEMPLATE_PATH/pipelines/nlp/language_identification_pipline.py": 1690807232.4462962,
+        "TEMPLATE_PATH/pipelines/nlp/llama2_text_generation_pipeline.py": 1690807232.4462962,
+        "TEMPLATE_PATH/pipelines/nlp/mglm_text_summarization_pipeline.py": 1690807232.4462962,
+        "TEMPLATE_PATH/pipelines/nlp/named_entity_recognition_pipeline.py": 1690807232.4462962,
+        "TEMPLATE_PATH/pipelines/nlp/polylm_text_generation_pipeline.py": 1690807232.4462962,
+        "TEMPLATE_PATH/pipelines/nlp/sentence_embedding_pipeline.py": 1690807232.4462962,
+        "TEMPLATE_PATH/pipelines/nlp/siamese_uie_pipeline.py": 1690807232.4462962,
+        "TEMPLATE_PATH/pipelines/nlp/summarization_pipeline.py": 1690807232.4462962,
+        "TEMPLATE_PATH/pipelines/nlp/table_question_answering_pipeline.py": 1690807232.4462962,
+        "TEMPLATE_PATH/pipelines/nlp/text_classification_pipeline.py": 1690807232.4462962,
+        "TEMPLATE_PATH/pipelines/nlp/text_error_correction_pipeline.py": 1690807232.4462962,
+        "TEMPLATE_PATH/pipelines/nlp/text_generation_pipeline.py": 1690807232.4462962,
+        "TEMPLATE_PATH/pipelines/nlp/text_ranking_pipeline.py": 1690807232.4462962,
+        "TEMPLATE_PATH/pipelines/nlp/token_classification_pipeline.py": 1690807232.4462962,
+        "TEMPLATE_PATH/pipelines/nlp/translation_evaluation_pipeline.py": 1690807232.4462962,
+        "TEMPLATE_PATH/pipelines/nlp/translation_pipeline.py": 1690807232.4462962,
+        "TEMPLATE_PATH/pipelines/nlp/translation_quality_estimation_pipeline.py": 1690807232.4462962,
+        "TEMPLATE_PATH/pipelines/nlp/user_satisfaction_estimation_pipeline.py": 1690807232.4462962,
+        "TEMPLATE_PATH/pipelines/nlp/word_alignment_pipeline.py": 1690807232.4462962,
+        "TEMPLATE_PATH/pipelines/nlp/word_segmentation_pipeline.py": 1690807232.4462962,
+        "TEMPLATE_PATH/pipelines/nlp/zero_shot_classification_pipeline.py": 1690807232.4462962,
+        "TEMPLATE_PATH/pipelines/pipeline_template.py": 1690807232.4462962,
+        "TEMPLATE_PATH/pipelines/science/protein_structure_pipeline.py": 1690807232.4462962,
+        "TEMPLATE_PATH/pipelines/util.py": 1690807232.4462962,
+        "TEMPLATE_PATH/preprocessors/asr.py": 1690807232.4462962,
+        "TEMPLATE_PATH/preprocessors/audio.py": 1690807232.4462962,
+        "TEMPLATE_PATH/preprocessors/base.py": 1690807232.4462962,
+        "TEMPLATE_PATH/preprocessors/builder.py": 1690807232.4462962,
+        "TEMPLATE_PATH/preprocessors/common.py": 1690807232.4462962,
+        "TEMPLATE_PATH/preprocessors/cv/action_detection_mapper.py": 1690807232.4462962,
+        "TEMPLATE_PATH/preprocessors/cv/bad_image_detecting_preprocessor.py": 1690807232.4462962,
+        "TEMPLATE_PATH/preprocessors/cv/controllable_image_generation.py": 1690807232.4462962,
+        "TEMPLATE_PATH/preprocessors/cv/cv2_transforms.py": 1690807232.4462962,
+        "TEMPLATE_PATH/preprocessors/cv/image_classification_preprocessor.py": 1690807232.4462962,
+        "TEMPLATE_PATH/preprocessors/cv/image_quality_assessment_man.py": 1690807232.4462962,
+        "TEMPLATE_PATH/preprocessors/cv/image_quality_assessment_mos.py": 1690807232.4462962,
+        "TEMPLATE_PATH/preprocessors/cv/image_restoration_preprocessor.py": 1690807232.4462962,
+        "TEMPLATE_PATH/preprocessors/cv/mmcls_preprocessor.py": 1690807232.4462962,
+        "TEMPLATE_PATH/preprocessors/cv/timer.py": 1690807232.4462962,
+        "TEMPLATE_PATH/preprocessors/cv/util.py": 1690807232.4462962,
+        "TEMPLATE_PATH/preprocessors/cv/video_stabilization.py": 1690807232.4462962,
+        "TEMPLATE_PATH/preprocessors/cv/video_super_resolution.py": 1690807232.4462962,
+        "TEMPLATE_PATH/preprocessors/image.py": 1690807232.4462962,
+        "TEMPLATE_PATH/preprocessors/kws.py": 1690807232.4462962,
+        "TEMPLATE_PATH/preprocessors/movie_scene_segmentation/transforms.py": 1690807232.4462962,
+        "TEMPLATE_PATH/preprocessors/multi_modal.py": 1690807232.4462962,
+        "TEMPLATE_PATH/preprocessors/nlp/bert_seq_cls_tokenizer.py": 1690807232.4462962,
+        "TEMPLATE_PATH/preprocessors/nlp/canmt_translation.py": 1690807232.4462962,
+        "TEMPLATE_PATH/preprocessors/nlp/dialog_classification_use_preprocessor.py": 1690807232.4462962,
+        "TEMPLATE_PATH/preprocessors/nlp/document_grounded_dialog_generate_preprocessor.py": 1690807232.4462962,
+        "TEMPLATE_PATH/preprocessors/nlp/document_grounded_dialog_rerank_preprocessor.py": 1690807232.4462962,
+        "TEMPLATE_PATH/preprocessors/nlp/document_grounded_dialog_retrieval_preprocessor.py": 1690807232.4462962,
+        "TEMPLATE_PATH/preprocessors/nlp/document_segmentation_preprocessor.py": 1690807232.4462962,
+        "TEMPLATE_PATH/preprocessors/nlp/faq_question_answering_preprocessor.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/feature_extraction_preprocessor.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/fill_mask_preprocessor.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/mgeo_ranking_preprocessor.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/mglm_summarization_preprocessor.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/relation_extraction_preprocessor.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/sentence_embedding_preprocessor.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/siamese_uie_preprocessor.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/space/args.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/space/batch.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/space/data_loader.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/space/dialog_intent_prediction_preprocessor.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/space/dialog_modeling_preprocessor.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/space/dialog_state_tracking_preprocessor.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/space/dst_processors.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/space/fields/gen_field.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/space/fields/intent_field.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/space/lazy_dataset.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/space/preprocess.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/space/sampler.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/space/tensorlistdataset.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/space/tokenizer.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/space_T_cn/fields/database.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/space_T_cn/fields/schema_link.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/space_T_cn/fields/struct.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/space_T_cn/table_question_answering_preprocessor.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/space_T_en/conversational_text_to_sql_preprocessor.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/space_T_en/fields/common_utils.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/space_T_en/fields/parse.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/space_T_en/fields/preprocess_dataset.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/space_T_en/fields/process_dataset.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/text_classification_preprocessor.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/text_clean.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/text_error_correction.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/text_generation_preprocessor.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/text_ranking_preprocessor.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/token_classification_preprocessor.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/token_classification_thai_preprocessor.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/token_classification_viet_preprocessor.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/transformers_tokenizer.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/translation_evaluation_preprocessor.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/utils.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/word_alignment_preprocessor.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/nlp/zero_shot_classification_preprocessor.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/ofa/asr.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/ofa/base.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/ofa/image_captioning.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/ofa/image_classification.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/ofa/ocr_recognition.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/ofa/sudoku.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/ofa/summarization.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/ofa/text2sql.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/ofa/text_classification.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/ofa/text_to_image_synthesis.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/ofa/utils/audio_helper.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/ofa/utils/bridge_content_encoder.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/ofa/utils/collate.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/ofa/utils/constant.py": 1690807232.4502962,
+        "TEMPLATE_PATH/preprocessors/ofa/utils/get_tables.py": 1690807232.454296,
+        "TEMPLATE_PATH/preprocessors/ofa/utils/random_help.py": 1690807232.454296,
+        "TEMPLATE_PATH/preprocessors/ofa/utils/text2phone.py": 1690807232.454296,
+        "TEMPLATE_PATH/preprocessors/ofa/utils/transforms.py": 1690807232.454296,
+        "TEMPLATE_PATH/preprocessors/ofa/utils/vision_helper.py": 1690807232.454296,
+        "TEMPLATE_PATH/preprocessors/ofa/visual_entailment.py": 1690807232.454296,
+        "TEMPLATE_PATH/preprocessors/ofa/visual_grounding.py": 1690807232.454296,
+        "TEMPLATE_PATH/preprocessors/ofa/visual_question_answering.py": 1690807232.454296,
+        "TEMPLATE_PATH/preprocessors/science/uni_fold.py": 1690807232.454296,
+        "TEMPLATE_PATH/preprocessors/speaker.py": 1690807232.454296,
+        "TEMPLATE_PATH/preprocessors/tts.py": 1690807232.454296,
+        "TEMPLATE_PATH/preprocessors/video.py": 1690807232.454296,
+        "TEMPLATE_PATH/trainers/audio/ans_trainer.py": 1690807232.454296,
+        "TEMPLATE_PATH/trainers/audio/asr_trainer.py": 1690807232.454296,
+        "TEMPLATE_PATH/trainers/audio/kws_farfield_trainer.py": 1690807232.454296,
+        "TEMPLATE_PATH/trainers/audio/kws_nearfield_trainer.py": 1690807232.454296,
+        "TEMPLATE_PATH/trainers/audio/kws_utils/batch_utils.py": 1690807232.454296,
+        "TEMPLATE_PATH/trainers/audio/kws_utils/det_utils.py": 1690807232.454296,
+        "TEMPLATE_PATH/trainers/audio/kws_utils/file_utils.py": 1690807232.454296,
+        "TEMPLATE_PATH/trainers/audio/kws_utils/model_utils.py": 1690807232.454296,
+        "TEMPLATE_PATH/trainers/audio/kws_utils/runtime_utils.py": 1690807232.454296,
+        "TEMPLATE_PATH/trainers/audio/separation_trainer.py": 1690807232.454296,
+        "TEMPLATE_PATH/trainers/audio/tts_trainer.py": 1690807232.454296,
+        "TEMPLATE_PATH/trainers/base.py": 1690807232.454296,
+        "TEMPLATE_PATH/trainers/builder.py": 1690807232.454296,
+        "TEMPLATE_PATH/trainers/cli_argument_parser.py": 1690807232.454296,
+        "TEMPLATE_PATH/trainers/cv/action_detection_trainer.py": 1690807232.454296,
+        "TEMPLATE_PATH/trainers/cv/card_detection_scrfd_trainer.py": 1690807232.454296,
+        "TEMPLATE_PATH/trainers/cv/cartoon_translation_trainer.py": 1690807232.454296,
+        "TEMPLATE_PATH/trainers/cv/face_detection_scrfd_trainer.py": 1690807232.454296,
+        "TEMPLATE_PATH/trainers/cv/image_classifition_trainer.py": 1690807232.454296,
+        "TEMPLATE_PATH/trainers/cv/image_defrcn_fewshot_detection_trainer.py": 1690807232.454296,
+        "TEMPLATE_PATH/trainers/cv/image_detection_damoyolo_trainer.py": 1690807232.454296,
+        "TEMPLATE_PATH/trainers/cv/image_inpainting_trainer.py": 1690807232.454296,
+        "TEMPLATE_PATH/trainers/cv/image_instance_segmentation_trainer.py": 1690807232.454296,
+        "TEMPLATE_PATH/trainers/cv/image_portrait_enhancement_trainer.py": 1690807232.454296,
+        "TEMPLATE_PATH/trainers/cv/movie_scene_segmentation_trainer.py": 1690807232.454296,
+        "TEMPLATE_PATH/trainers/cv/nerf_recon_acc_trainer.py": 1690807232.454296,
+        "TEMPLATE_PATH/trainers/cv/ocr_detection_db_trainer.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/cv/ocr_recognition_trainer.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/cv/referring_video_object_segmentation_trainer.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/cv/vision_efficient_tuning_trainer.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/default_config.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/hooks/builder.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/hooks/checkpoint/checkpoint_hook.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/hooks/checkpoint/checkpoint_processor.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/hooks/checkpoint/load_checkpoint_hook.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/hooks/clip_clamp_logit_scale_hook.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/hooks/compression/sparsity_hook.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/hooks/compression/utils.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/hooks/distributed/ddp_hook.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/hooks/distributed/deepspeed_hook.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/hooks/distributed/megatron_hook.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/hooks/early_stop_hook.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/hooks/evaluation_hook.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/hooks/hook.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/hooks/iter_timer_hook.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/hooks/logger/base.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/hooks/logger/tensorboard_hook.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/hooks/logger/text_logger_hook.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/hooks/lr_scheduler_hook.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/hooks/optimizer/apex_optimizer_hook.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/hooks/optimizer/base.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/hooks/optimizer/torch_optimizer_hook.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/hooks/priority.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/lrscheduler/builder.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/lrscheduler/warmup/base.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/lrscheduler/warmup/warmup.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/multi_modal/clip/clip_trainer.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/multi_modal/clip/clip_trainer_utils.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/multi_modal/custom_diffusion/custom_diffusion_trainer.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/multi_modal/dreambooth_diffusion/dreambooth_diffusion_trainer.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/multi_modal/efficient_diffusion_tuning/efficient_diffusion_tuning_trainer.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/multi_modal/lora_diffusion/lora_diffusion_trainer.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/multi_modal/mgeo_ranking_trainer.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/multi_modal/mplug/mplug_trainer.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/multi_modal/ofa/ofa_trainer.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/multi_modal/ofa/ofa_trainer_utils.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/multi_modal/stable_diffusion/stable_diffusion_trainer.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/multi_modal/team/team_trainer.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/multi_modal/team/team_trainer_utils.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/nlp/csanmt_translation_trainer.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/nlp/document_grounded_dialog_generate_trainer.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/nlp/document_grounded_dialog_rerank_trainer.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/nlp/document_grounded_dialog_retrieval_trainer.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/nlp/faq_question_answering_trainer.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/nlp/gpt3_trainer.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/nlp/gpt_moe_trainer.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/nlp/plug_trainer.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/nlp/sentence_embedding_trainer.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/nlp/sequence_classification_trainer.py": 1690807232.4582963,
+        "TEMPLATE_PATH/trainers/nlp/siamese_uie_trainer.py": 1690807232.4622962,
+        "TEMPLATE_PATH/trainers/nlp/space/dialog_intent_trainer.py": 1690807232.4622962,
+        "TEMPLATE_PATH/trainers/nlp/space/dialog_modeling_trainer.py": 1690807232.4622962,
+        "TEMPLATE_PATH/trainers/nlp/space/eval.py": 1690807232.4622962,
+        "TEMPLATE_PATH/trainers/nlp/space/metrics/metrics_tracker.py": 1690807232.4622962,
+        "TEMPLATE_PATH/trainers/nlp/space/trainer/gen_trainer.py": 1690807232.4622962,
+        "TEMPLATE_PATH/trainers/nlp/space/trainer/intent_trainer.py": 1690807232.4622962,
+        "TEMPLATE_PATH/trainers/nlp/table_question_answering_trainer.py": 1690807232.4622962,
+        "TEMPLATE_PATH/trainers/nlp/text_generation_trainer.py": 1690807232.4622962,
+        "TEMPLATE_PATH/trainers/nlp/text_ranking_trainer.py": 1690807232.4622962,
+        "TEMPLATE_PATH/trainers/nlp/translation_evaluation_trainer.py": 1690807232.4622962,
+        "TEMPLATE_PATH/trainers/nlp_trainer.py": 1690807232.4622962,
+        "TEMPLATE_PATH/trainers/optimizer/builder.py": 1690807232.4622962,
+        "TEMPLATE_PATH/trainers/parallel/builder.py": 1690807232.4622962,
+        "TEMPLATE_PATH/trainers/parallel/utils.py": 1690807232.4622962,
+        "TEMPLATE_PATH/trainers/trainer.py": 1690807232.4622962,
+        "TEMPLATE_PATH/trainers/training_args.py": 1690807232.4622962,
+        "TEMPLATE_PATH/trainers/utils/inference.py": 1690807232.4622962,
+        "TEMPLATE_PATH/trainers/utils/log_buffer.py": 1690807232.4622962
     },
     "index": {
         "('ATTENTION', 'default', 'PETRMultiheadAttention')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/petr_transformer.py",
             "imports": [
-                "math",
+                "torch",
+                "mmcv",
                 "warnings",
+                "mmdet",
                 "copy",
-                "mmcv",
-                "typing",
-                "torch",
-                "mmdet"
+                "math",
+                "typing"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.utils.petr_transformer"
         },
         "('BACKBONES', 'backbone', 'bloom')": {
             "filepath": "TEMPLATE_PATH/models/nlp/bloom/backbone.py",
             "imports": [
                 "transformers"
@@ -1770,139 +1765,139 @@
                 "transformers"
             ],
             "module": "modelscope.models.nlp.gpt2.backbone"
         },
         "('BACKBONES', 'default', 'BASEBEiT')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/base/beit.py",
             "imports": [
-                "math",
-                "mmcv",
                 "functools",
-                "timm",
                 "torch",
-                "mmdet"
+                "mmcv",
+                "mmdet",
+                "timm",
+                "math"
             ],
             "module": "modelscope.models.cv.image_semantic_segmentation.vit_adapter.models.backbone.base.beit"
         },
         "('BACKBONES', 'default', 'BEiTAdapter')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/beit_adapter.py",
             "imports": [
+                "logging",
+                "timm",
                 "math",
                 "torch",
-                "timm",
-                "mmdet",
-                "logging"
+                "mmdet"
             ],
             "module": "modelscope.models.cv.image_semantic_segmentation.vit_adapter.models.backbone.beit_adapter"
         },
         "('BACKBONES', 'default', 'BEiTv2')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_classification/backbones/beit_v2.py",
             "imports": [
+                "collections",
+                "functools",
                 "einops",
-                "math",
-                "itertools",
-                "torch",
+                "mmcv",
                 "mmcls",
-                "collections",
+                "os",
                 "warnings",
-                "mmcv",
+                "math",
                 "typing",
-                "functools",
-                "os"
+                "torch",
+                "itertools"
             ],
             "module": "modelscope.models.cv.image_classification.backbones.beit_v2"
         },
         "('BACKBONES', 'default', 'MasterNet')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/master_net.py",
             "imports": [
-                "torch",
-                "mmdet"
+                "mmdet",
+                "torch"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.backbones.master_net"
         },
         "('BACKBONES', 'default', 'MobileNetV1')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/mobilenet.py",
             "imports": [
-                "mmcv",
+                "mmdet",
                 "torch",
-                "mmdet"
+                "mmcv"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.backbones.mobilenet"
         },
         "('BACKBONES', 'default', 'NextViT')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_classification/backbones/nextvit.py",
             "imports": [
+                "collections",
+                "functools",
                 "einops",
-                "math",
-                "itertools",
-                "torch",
+                "mmcv",
                 "mmcls",
-                "collections",
+                "os",
                 "warnings",
-                "mmcv",
+                "math",
                 "typing",
-                "functools",
-                "os"
+                "torch",
+                "itertools"
             ],
             "module": "modelscope.models.cv.image_classification.backbones.nextvit"
         },
         "('BACKBONES', 'default', 'ResNetV1e')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/resnet.py",
             "imports": [
-                "mmcv",
+                "mmdet",
                 "torch",
-                "mmdet"
+                "mmcv"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.backbones.resnet"
         },
         "('BACKBONES', 'default', 'ViT')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/backbones/vit.py",
             "imports": [
+                "functools",
+                "timm",
                 "math",
                 "torch",
-                "timm",
-                "mmdet",
-                "functools"
+                "mmdet"
             ],
             "module": "modelscope.models.cv.object_detection.mmdet_ms.backbones.vit"
         },
         "('BACKBONES', 'default', 'VoVNet')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/backbones/vovnet.py",
             "imports": [
-                "mmcv",
-                "torch",
+                "collections",
                 "mmdet",
-                "collections"
+                "torch",
+                "mmcv"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.backbones.vovnet"
         },
         "('BBOX_ASSIGNERS', 'default', 'HungarianAssigner3D')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/assigners/hungarian_assigner_3d.py",
             "imports": [
-                "scipy",
+                "mmdet",
                 "torch",
-                "mmdet"
+                "scipy"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.core.bbox.assigners.hungarian_assigner_3d"
         },
         "('BBOX_ASSIGNERS', 'default', 'MaskHungarianAssignerVideo')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_instance_segmentation/track/mask_hungarian_assigner.py",
             "imports": [
-                "scipy",
-                "numpy",
+                "mmdet",
                 "torch",
-                "mmdet"
+                "scipy",
+                "numpy"
             ],
             "module": "modelscope.models.cv.video_instance_segmentation.track.mask_hungarian_assigner"
         },
         "('BBOX_CODERS', 'default', 'NMSFreeCoder')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/coders/nms_free_coder.py",
             "imports": [
-                "torch",
-                "mmdet"
+                "mmdet",
+                "torch"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.core.bbox.coders.nms_free_coder"
         },
         "('CUSTOM_DATASETS', 'bad-image-detecting', 'bad-image-detecting')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/bad_image_detecting/bad_image_detecting_dataset.py",
             "imports": [],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.bad_image_detecting.bad_image_detecting_dataset"
@@ -1939,20 +1934,20 @@
                 "numpy"
             ],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.sidd_image_denoising.sidd_image_denoising_dataset"
         },
         "('CUSTOM_DATASETS', 'image-inpainting', 'FFTInpainting')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_inpainting/image_inpainting_dataset.py",
             "imports": [
-                "glob",
                 "enum",
+                "os",
                 "albumentations",
+                "glob",
                 "cv2",
-                "numpy",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.image_inpainting.image_inpainting_dataset"
         },
         "('CUSTOM_DATASETS', 'image-portrait-enhancement', 'PairedDataset')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/image_portrait_enhancement_dataset.py",
             "imports": [
                 "cv2",
@@ -1971,107 +1966,107 @@
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_quality_assmessment_mos/image_quality_assessment_mos_dataset.py",
             "imports": [],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.image_quality_assmessment_mos.image_quality_assessment_mos_dataset"
         },
         "('CUSTOM_DATASETS', 'image-segmentation', 'cascade_mask_rcnn_swin')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_instance_segmentation_coco_dataset.py",
             "imports": [
-                "numpy",
                 "os",
-                "pycocotools"
+                "pycocotools",
+                "numpy"
             ],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.image_instance_segmentation_coco_dataset"
         },
         "('CUSTOM_DATASETS', 'language-guided-video-summarization', 'clip-it-language-guided-video-summarization')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/language_guided_video_summarization_dataset.py",
             "imports": [
+                "os",
                 "h5py",
                 "torch",
-                "json",
                 "numpy",
-                "os"
+                "json"
             ],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.language_guided_video_summarization_dataset"
         },
         "('CUSTOM_DATASETS', 'movie-scene-segmentation', 'resnet50-bert')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/movie_scene_segmentation_dataset.py",
             "imports": [
-                "torch",
-                "torchvision",
+                "os",
                 "copy",
+                "torch",
                 "random",
-                "os",
+                "torchvision",
                 "json"
             ],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.movie_scene_segmentation.movie_scene_segmentation_dataset"
         },
         "('CUSTOM_DATASETS', 'nli', 'veco')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/veco_dataset.py",
             "imports": [
+                "numpy",
                 "datasets",
-                "typing",
-                "numpy"
+                "typing"
             ],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.veco_dataset"
         },
         "('CUSTOM_DATASETS', 'ocr-recognition', 'OCRRecognition')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_recognition_dataset.py",
             "imports": [
+                "six",
+                "os",
                 "lmdb",
-                "numpy",
                 "torch",
                 "PIL",
                 "cv2",
-                "six",
-                "os",
+                "numpy",
                 "json"
             ],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_recognition_dataset"
         },
         "('CUSTOM_DATASETS', 'referring-video-object-segmentation', 'swinT-referring-video-object-segmentation')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/referring_video_object_segmentation_dataset.py",
             "imports": [
-                "glob",
-                "pandas",
-                "h5py",
-                "torch",
                 "torchvision",
                 "os",
+                "tqdm",
+                "h5py",
+                "torch",
                 "pycocotools",
+                "glob",
+                "pandas",
                 "numpy",
-                "json",
-                "tqdm"
+                "json"
             ],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.referring_video_object_segmentation.referring_video_object_segmentation_dataset"
         },
         "('CUSTOM_DATASETS', 'sentence-embedding', 'bert')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/text_ranking_dataset.py",
             "imports": [
                 "random",
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.text_ranking_dataset"
         },
         "('CUSTOM_DATASETS', 'text-ranking', 'bert')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/text_ranking_dataset.py",
             "imports": [
                 "random",
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.text_ranking_dataset"
         },
         "('CUSTOM_DATASETS', 'text-ranking', 'mgeo')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/mgeo_ranking_dataset.py",
             "imports": [
-                "random",
-                "typing",
                 "torch",
-                "json"
+                "random",
+                "json",
+                "typing"
             ],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.mgeo_ranking_dataset"
         },
         "('CUSTOM_DATASETS', 'video-frame-interpolation', 'video-frame-interpolation')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/video_frame_interpolation_dataset.py",
             "imports": [
                 "cv2",
@@ -2084,581 +2079,581 @@
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/video_stabilization/video_stabilization_dataset.py",
             "imports": [],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.video_stabilization.video_stabilization_dataset"
         },
         "('CUSTOM_DATASETS', 'video-super-resolution', 'real-basicvsr')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/video_super_resolution/video_super_resolution_dataset.py",
             "imports": [
-                "cv2",
-                "numpy",
+                "collections",
                 "torch",
-                "collections"
+                "numpy",
+                "cv2"
             ],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.video_super_resolution.video_super_resolution_dataset"
         },
         "('DATASETS', 'default', 'CustomNuScenesDataset')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/nuscenes_dataset.py",
             "imports": [
+                "mmdet",
                 "mmdet3d",
-                "numpy",
-                "mmdet"
+                "numpy"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.datasets.nuscenes_dataset"
         },
         "('DATASETS', 'default', 'RetinaFaceDataset')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/retinaface.py",
             "imports": [
-                "numpy",
-                "mmdet"
+                "mmdet",
+                "numpy"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.mmdet_patch.datasets.retinaface"
         },
         "('DETECTORS', 'default', 'CustomSingleStageDetector')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/single_stage.py",
             "imports": [
-                "torch",
-                "mmdet"
+                "mmdet",
+                "torch"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.detectors.single_stage"
         },
         "('DETECTORS', 'default', 'EncoderDecoderMask2Former')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/encoder_decoder_mask2former.py",
             "imports": [
-                "torch",
-                "mmdet"
+                "mmdet",
+                "torch"
             ],
             "module": "modelscope.models.cv.image_semantic_segmentation.vit_adapter.models.segmentors.encoder_decoder_mask2former"
         },
         "('DETECTORS', 'default', 'Petr3D')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/detectors/petr3d.py",
             "imports": [
-                "mmdet3d",
+                "mmcv",
                 "torch",
                 "mmdet",
-                "mmcv",
+                "mmdet3d",
                 "numpy"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.detectors.petr3d"
         },
         "('DETECTORS', 'default', 'SCRFD')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/scrfd.py",
             "imports": [
-                "torch",
-                "mmdet"
+                "mmdet",
+                "torch"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.detectors.scrfd"
         },
         "('DETECTORS', 'default', 'TinyMog')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/tinymog.py",
             "imports": [
-                "torch",
-                "mmdet"
+                "mmdet",
+                "torch"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.detectors.tinymog"
         },
         "('EXPORTERS', 'acoustic-noise-suppression', 'speech_dfsmn_ans')": {
             "filepath": "TEMPLATE_PATH/exporters/audio/ans_dfsmn_exporter.py",
             "imports": [
                 "os",
                 "torch"
             ],
             "module": "modelscope.exporters.audio.ans_dfsmn_exporter"
         },
         "('EXPORTERS', 'default', 'cartoon-translation')": {
             "filepath": "TEMPLATE_PATH/exporters/cv/cartoon_translation_exporter.py",
             "imports": [
-                "typing",
-                "tensorflow",
+                "os",
                 "packaging",
-                "os"
+                "tensorflow",
+                "typing"
             ],
             "module": "modelscope.exporters.cv.cartoon_translation_exporter"
         },
         "('EXPORTERS', 'domain-specific-object-detection', 'tinynas-damoyolo')": {
             "filepath": "TEMPLATE_PATH/exporters/cv/object_detection_damoyolo_exporter.py",
             "imports": [
-                "numpy",
-                "onnx",
-                "torch",
                 "functools",
+                "os",
                 "typing",
-                "os"
+                "torch",
+                "onnx",
+                "numpy"
             ],
             "module": "modelscope.exporters.cv.object_detection_damoyolo_exporter"
         },
         "('EXPORTERS', 'face-detection', 'scrfd')": {
             "filepath": "TEMPLATE_PATH/exporters/cv/face_detection_scrfd_exporter.py",
             "imports": [
-                "numpy",
-                "onnx",
-                "torch",
                 "functools",
+                "os",
                 "typing",
-                "os"
+                "torch",
+                "onnx",
+                "numpy"
             ],
             "module": "modelscope.exporters.cv.face_detection_scrfd_exporter"
         },
         "('EXPORTERS', 'image-object-detection', 'tinynas-damoyolo')": {
             "filepath": "TEMPLATE_PATH/exporters/cv/object_detection_damoyolo_exporter.py",
             "imports": [
-                "numpy",
-                "onnx",
-                "torch",
                 "functools",
+                "os",
                 "typing",
-                "os"
+                "torch",
+                "onnx",
+                "numpy"
             ],
             "module": "modelscope.exporters.cv.object_detection_damoyolo_exporter"
         },
         "('EXPORTERS', 'named-entity-recognition', 'transformer-crf')": {
             "filepath": "TEMPLATE_PATH/exporters/nlp/model_for_token_classification_exporter.py",
             "imports": [
-                "typing",
+                "collections",
                 "torch",
-                "collections"
+                "typing"
             ],
             "module": "modelscope.exporters.nlp.model_for_token_classification_exporter"
         },
         "('EXPORTERS', 'nli', 'bert')": {
             "filepath": "TEMPLATE_PATH/exporters/nlp/sbert_for_sequence_classification_exporter.py",
             "imports": [
-                "typing",
+                "collections",
                 "torch",
-                "collections"
+                "typing"
             ],
             "module": "modelscope.exporters.nlp.sbert_for_sequence_classification_exporter"
         },
         "('EXPORTERS', 'nli', 'structbert')": {
             "filepath": "TEMPLATE_PATH/exporters/nlp/sbert_for_sequence_classification_exporter.py",
             "imports": [
-                "typing",
+                "collections",
                 "torch",
-                "collections"
+                "typing"
             ],
             "module": "modelscope.exporters.nlp.sbert_for_sequence_classification_exporter"
         },
         "('EXPORTERS', 'part-of-speech', 'transformer-crf')": {
             "filepath": "TEMPLATE_PATH/exporters/nlp/model_for_token_classification_exporter.py",
             "imports": [
-                "typing",
+                "collections",
                 "torch",
-                "collections"
+                "typing"
             ],
             "module": "modelscope.exporters.nlp.model_for_token_classification_exporter"
         },
         "('EXPORTERS', 'sentence-similarity', 'bert')": {
             "filepath": "TEMPLATE_PATH/exporters/nlp/sbert_for_sequence_classification_exporter.py",
             "imports": [
-                "typing",
+                "collections",
                 "torch",
-                "collections"
+                "typing"
             ],
             "module": "modelscope.exporters.nlp.sbert_for_sequence_classification_exporter"
         },
         "('EXPORTERS', 'sentence-similarity', 'structbert')": {
             "filepath": "TEMPLATE_PATH/exporters/nlp/sbert_for_sequence_classification_exporter.py",
             "imports": [
-                "typing",
+                "collections",
                 "torch",
-                "collections"
+                "typing"
             ],
             "module": "modelscope.exporters.nlp.sbert_for_sequence_classification_exporter"
         },
         "('EXPORTERS', 'sentiment-classification', 'bert')": {
             "filepath": "TEMPLATE_PATH/exporters/nlp/sbert_for_sequence_classification_exporter.py",
             "imports": [
-                "typing",
+                "collections",
                 "torch",
-                "collections"
+                "typing"
             ],
             "module": "modelscope.exporters.nlp.sbert_for_sequence_classification_exporter"
         },
         "('EXPORTERS', 'sentiment-classification', 'structbert')": {
             "filepath": "TEMPLATE_PATH/exporters/nlp/sbert_for_sequence_classification_exporter.py",
             "imports": [
-                "typing",
+                "collections",
                 "torch",
-                "collections"
+                "typing"
             ],
             "module": "modelscope.exporters.nlp.sbert_for_sequence_classification_exporter"
         },
         "('EXPORTERS', 'text-classification', 'bert')": {
             "filepath": "TEMPLATE_PATH/exporters/nlp/sbert_for_sequence_classification_exporter.py",
             "imports": [
-                "typing",
+                "collections",
                 "torch",
-                "collections"
+                "typing"
             ],
             "module": "modelscope.exporters.nlp.sbert_for_sequence_classification_exporter"
         },
         "('EXPORTERS', 'text-classification', 'structbert')": {
             "filepath": "TEMPLATE_PATH/exporters/nlp/sbert_for_sequence_classification_exporter.py",
             "imports": [
-                "typing",
+                "collections",
                 "torch",
-                "collections"
+                "typing"
             ],
             "module": "modelscope.exporters.nlp.sbert_for_sequence_classification_exporter"
         },
         "('EXPORTERS', 'text-to-image-synthesis', 'stable-diffusion')": {
             "filepath": "TEMPLATE_PATH/exporters/multi_modal/stable_diffusion_exporter.py",
             "imports": [
-                "diffusers",
-                "onnx",
-                "torch",
-                "pathlib",
                 "collections",
-                "argparse",
-                "typing",
-                "shutil",
                 "os",
-                "packaging"
+                "typing",
+                "argparse",
+                "pathlib",
+                "torch",
+                "onnx",
+                "diffusers",
+                "packaging",
+                "shutil"
             ],
             "module": "modelscope.exporters.multi_modal.stable_diffusion_exporter"
         },
         "('EXPORTERS', 'token-classification', 'transformer-crf')": {
             "filepath": "TEMPLATE_PATH/exporters/nlp/model_for_token_classification_exporter.py",
             "imports": [
-                "typing",
+                "collections",
                 "torch",
-                "collections"
+                "typing"
             ],
             "module": "modelscope.exporters.nlp.model_for_token_classification_exporter"
         },
         "('EXPORTERS', 'transformer-crf', 'transformer-crf')": {
             "filepath": "TEMPLATE_PATH/exporters/nlp/model_for_token_classification_exporter.py",
             "imports": [
-                "typing",
+                "collections",
                 "torch",
-                "collections"
+                "typing"
             ],
             "module": "modelscope.exporters.nlp.model_for_token_classification_exporter"
         },
         "('EXPORTERS', 'translation', 'csanmt-translation')": {
             "filepath": "TEMPLATE_PATH/exporters/nlp/csanmt_for_translation_exporter.py",
             "imports": [
-                "typing",
                 "os",
-                "tensorflow"
+                "tensorflow",
+                "typing"
             ],
             "module": "modelscope.exporters.nlp.csanmt_for_translation_exporter"
         },
         "('EXPORTERS', 'word-segmentation', 'transformer-crf')": {
             "filepath": "TEMPLATE_PATH/exporters/nlp/model_for_token_classification_exporter.py",
             "imports": [
-                "typing",
+                "collections",
                 "torch",
-                "collections"
+                "typing"
             ],
             "module": "modelscope.exporters.nlp.model_for_token_classification_exporter"
         },
         "('EXPORTERS', 'zero-shot-classification', 'bert')": {
             "filepath": "TEMPLATE_PATH/exporters/nlp/sbert_for_zero_shot_classification_exporter.py",
             "imports": [
-                "typing",
-                "collections"
+                "collections",
+                "typing"
             ],
             "module": "modelscope.exporters.nlp.sbert_for_zero_shot_classification_exporter"
         },
         "('EXPORTERS', 'zero-shot-classification', 'structbert')": {
             "filepath": "TEMPLATE_PATH/exporters/nlp/sbert_for_zero_shot_classification_exporter.py",
             "imports": [
-                "typing",
-                "collections"
+                "collections",
+                "typing"
             ],
             "module": "modelscope.exporters.nlp.sbert_for_zero_shot_classification_exporter"
         },
         "('HEADS', 'default', 'AnchorNHead')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/dense_heads/anchor_head.py",
             "imports": [
                 "mmdet"
             ],
             "module": "modelscope.models.cv.object_detection.mmdet_ms.dense_heads.anchor_head"
         },
         "('HEADS', 'default', 'ConvFCBBoxNHead')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/roi_heads/bbox_heads/convfc_bbox_head.py",
             "imports": [
-                "torch",
-                "mmdet"
+                "mmdet",
+                "torch"
             ],
             "module": "modelscope.models.cv.object_detection.mmdet_ms.roi_heads.bbox_heads.convfc_bbox_head"
         },
         "('HEADS', 'default', 'ConvKernelHeadVideo')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_head.py",
             "imports": [
-                "mmcv",
+                "mmdet",
                 "torch",
-                "mmdet"
+                "mmcv"
             ],
             "module": "modelscope.models.cv.video_instance_segmentation.head.kernel_head"
         },
         "('HEADS', 'default', 'FCNMaskNHead')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/roi_heads/mask_heads/fcn_mask_head.py",
             "imports": [
+                "mmcv",
+                "warnings",
                 "torch",
                 "mmdet",
-                "warnings",
-                "mmcv",
                 "numpy"
             ],
             "module": "modelscope.models.cv.object_detection.mmdet_ms.roi_heads.mask_heads.fcn_mask_head"
         },
         "('HEADS', 'default', 'KernelFrameIterHeadVideo')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_frame_iter_head.py",
             "imports": [
-                "mmcv",
+                "mmdet",
                 "torch",
-                "mmdet"
+                "mmcv"
             ],
             "module": "modelscope.models.cv.video_instance_segmentation.head.kernel_frame_iter_head"
         },
         "('HEADS', 'default', 'KernelIterHeadVideo')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_iter_head.py",
             "imports": [
-                "torch",
-                "mmdet"
+                "mmdet",
+                "torch"
             ],
             "module": "modelscope.models.cv.video_instance_segmentation.head.kernel_iter_head"
         },
         "('HEADS', 'default', 'KernelUpdateHead')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_update_head.py",
             "imports": [
-                "mmcv",
-                "numpy",
+                "mmdet",
                 "torch",
-                "mmdet"
+                "mmcv",
+                "numpy"
             ],
             "module": "modelscope.models.cv.video_instance_segmentation.head.kernel_update_head"
         },
         "('HEADS', 'default', 'KernelUpdateHeadVideo')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_instance_segmentation/track/kernel_update_head.py",
             "imports": [
-                "mmcv",
-                "numpy",
+                "mmdet",
                 "torch",
-                "mmdet"
+                "mmcv",
+                "numpy"
             ],
             "module": "modelscope.models.cv.video_instance_segmentation.track.kernel_update_head"
         },
         "('HEADS', 'default', 'Mask2FormerHeadFromMMSeg')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/mask2former_head_from_mmseg.py",
             "imports": [
-                "mmcv",
-                "torch",
+                "mmdet",
                 "copy",
-                "mmdet"
+                "mmcv",
+                "torch"
             ],
             "module": "modelscope.models.cv.image_semantic_segmentation.vit_adapter.models.decode_heads.mask2former_head_from_mmseg"
         },
         "('HEADS', 'default', 'MaskFormerSemanticHead')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_semantic_segmentation/pan_merge/maskformer_semantic_head.py",
             "imports": [
-                "torch",
-                "mmdet"
+                "mmdet",
+                "torch"
             ],
             "module": "modelscope.models.cv.image_semantic_segmentation.pan_merge.maskformer_semantic_head"
         },
         "('HEADS', 'default', 'MaskScoringNRoIHead')": {
             "filepath": "TEMPLATE_PATH/models/cv/abnormal_object_detection/mmdet_ms/roi_head/mask_scoring_roi_head.py",
             "imports": [
-                "torch",
-                "mmdet"
+                "mmdet",
+                "torch"
             ],
             "module": "modelscope.models.cv.abnormal_object_detection.mmdet_ms.roi_head.mask_scoring_roi_head"
         },
         "('HEADS', 'default', 'PETRv2DEDNHead')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/petrv2_dednhead.py",
             "imports": [
-                "mmdet3d",
+                "mmcv",
+                "copy",
                 "math",
                 "torch",
                 "mmdet",
-                "copy",
-                "mmcv",
+                "mmdet3d",
                 "numpy"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.dense_heads.petrv2_dednhead"
         },
         "('HEADS', 'default', 'RPNNHead')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/dense_heads/rpn_head.py",
             "imports": [
-                "mmcv",
-                "torch",
+                "mmdet",
                 "copy",
-                "mmdet"
+                "mmcv",
+                "torch"
             ],
             "module": "modelscope.models.cv.object_detection.mmdet_ms.dense_heads.rpn_head"
         },
         "('HEADS', 'default', 'SCRFDHead')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/dense_heads/scrfd_head.py",
             "imports": [
-                "mmcv",
-                "numpy",
+                "mmdet",
                 "torch",
-                "mmdet"
+                "mmcv",
+                "numpy"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.dense_heads.scrfd_head"
         },
         "('HEADS', 'default', 'Shared2FCBBoxNHead')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/roi_heads/bbox_heads/convfc_bbox_head.py",
             "imports": [
-                "torch",
-                "mmdet"
+                "mmdet",
+                "torch"
             ],
             "module": "modelscope.models.cv.object_detection.mmdet_ms.roi_heads.bbox_heads.convfc_bbox_head"
         },
         "('HEADS', 'default', 'Shared4Conv1FCBBoxNHead')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/roi_heads/bbox_heads/convfc_bbox_head.py",
             "imports": [
-                "torch",
-                "mmdet"
+                "mmdet",
+                "torch"
             ],
             "module": "modelscope.models.cv.object_detection.mmdet_ms.roi_heads.bbox_heads.convfc_bbox_head"
         },
         "('HEADS', 'default', 'VideoKernelIterHead')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/kernel_iter_head.py",
             "imports": [
-                "torch",
-                "mmdet"
+                "mmdet",
+                "torch"
             ],
             "module": "modelscope.models.cv.video_panoptic_segmentation.head.kernel_iter_head"
         },
         "('HEADS', 'default', 'VideoKernelUpdateHead')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/kernel_update_head.py",
             "imports": [
-                "mmcv",
-                "numpy",
+                "mmdet",
                 "torch",
-                "mmdet"
+                "mmcv",
+                "numpy"
             ],
             "module": "modelscope.models.cv.video_panoptic_segmentation.head.kernel_update_head"
         },
         "('HEADS', 'fill-mask', 'bert-mlm')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/fill_mask_head.py",
             "imports": [
                 "transformers",
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.nlp.heads.fill_mask_head"
         },
         "('HEADS', 'fill-mask', 'fill-mask')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/fill_mask_head.py",
             "imports": [
                 "transformers",
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.nlp.heads.fill_mask_head"
         },
         "('HEADS', 'fill-mask', 'roberta-mlm')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/torch_pretrain_head.py",
             "imports": [
                 "transformers",
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.nlp.heads.torch_pretrain_head"
         },
         "('HEADS', 'fill-mask', 'xlm-roberta-mlm')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/fill_mask_head.py",
             "imports": [
                 "transformers",
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.nlp.heads.fill_mask_head"
         },
         "('HEADS', 'information-extraction', 'information-extraction')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/infromation_extraction_head.py",
             "imports": [
                 "torch"
             ],
             "module": "modelscope.models.nlp.heads.infromation_extraction_head"
         },
         "('HEADS', 'named-entity-recognition', 'lstm-crf')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/crf_head.py",
             "imports": [
                 "transformers",
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.nlp.heads.crf_head"
         },
         "('HEADS', 'named-entity-recognition', 'token-classification')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/token_classification_head.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.nlp.heads.token_classification_head"
         },
         "('HEADS', 'named-entity-recognition', 'transformer-crf')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/crf_head.py",
             "imports": [
                 "transformers",
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.nlp.heads.crf_head"
         },
         "('HEADS', 'nli', 'text-classification')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/text_classification_head.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.nlp.heads.text_classification_head"
         },
         "('HEADS', 'part-of-speech', 'lstm-crf')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/crf_head.py",
             "imports": [
                 "transformers",
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.nlp.heads.crf_head"
         },
         "('HEADS', 'part-of-speech', 'token-classification')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/token_classification_head.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.nlp.heads.token_classification_head"
         },
         "('HEADS', 'part-of-speech', 'transformer-crf')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/crf_head.py",
             "imports": [
                 "transformers",
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.nlp.heads.crf_head"
         },
         "('HEADS', 'relation-extraction', 'information-extraction')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/infromation_extraction_head.py",
             "imports": [
                 "torch"
             ],
             "module": "modelscope.models.nlp.heads.infromation_extraction_head"
         },
         "('HEADS', 'sentence-similarity', 'text-classification')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/text_classification_head.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.nlp.heads.text_classification_head"
         },
         "('HEADS', 'sentiment-classification', 'text-classification')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/text_classification_head.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.nlp.heads.text_classification_head"
         },
         "('HEADS', 'speaker-diarization-dialogue-detection', 'text-classification')": {
             "filepath": "TEMPLATE_PATH/models/audio/sv/speaker_diarization_dialogue_detection.py",
             "imports": [
                 "torch"
@@ -2671,120 +2666,120 @@
                 "torch"
             ],
             "module": "modelscope.models.audio.sv.speaker_diarization_semantic_speaker_turn_detection"
         },
         "('HEADS', 'text-classification', 'text-classification')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/text_classification_head.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.nlp.heads.text_classification_head"
         },
         "('HEADS', 'text-generation', 'text-generation')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/text_generation_head.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.nlp.heads.text_generation_head"
         },
         "('HEADS', 'text-ranking', 'text-ranking')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/text_ranking_head.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.nlp.heads.text_ranking_head"
         },
         "('HEADS', 'token-classification', 'lstm-crf')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/crf_head.py",
             "imports": [
                 "transformers",
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.nlp.heads.crf_head"
         },
         "('HEADS', 'token-classification', 'token-classification')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/token_classification_head.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.nlp.heads.token_classification_head"
         },
         "('HEADS', 'token-classification', 'transformer-crf')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/crf_head.py",
             "imports": [
                 "transformers",
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.nlp.heads.crf_head"
         },
         "('HEADS', 'transformer-crf', 'transformer-crf')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/crf_head.py",
             "imports": [
                 "transformers",
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.nlp.heads.crf_head"
         },
         "('HEADS', 'word-segmentation', 'lstm-crf')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/crf_head.py",
             "imports": [
                 "transformers",
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.nlp.heads.crf_head"
         },
         "('HEADS', 'word-segmentation', 'transformer-crf')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/crf_head.py",
             "imports": [
                 "transformers",
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.nlp.heads.crf_head"
         },
         "('HOOKS', 'default', 'ApexAMPOptimizerHook')": {
             "filepath": "TEMPLATE_PATH/trainers/hooks/optimizer/apex_optimizer_hook.py",
             "imports": [
                 "logging",
-                "torch",
-                "packaging"
+                "packaging",
+                "torch"
             ],
             "module": "modelscope.trainers.hooks.optimizer.apex_optimizer_hook"
         },
         "('HOOKS', 'default', 'BestCkptSaverHook')": {
             "filepath": "TEMPLATE_PATH/trainers/hooks/checkpoint/checkpoint_hook.py",
             "imports": [
-                "shutil",
-                "torch",
-                "json",
+                "os",
                 "typing",
+                "torch",
+                "shutil",
                 "random",
                 "numpy",
-                "os"
+                "json"
             ],
             "module": "modelscope.trainers.hooks.checkpoint.checkpoint_hook"
         },
         "('HOOKS', 'default', 'CheckpointHook')": {
             "filepath": "TEMPLATE_PATH/trainers/hooks/checkpoint/checkpoint_hook.py",
             "imports": [
-                "shutil",
-                "torch",
-                "json",
+                "os",
                 "typing",
+                "torch",
+                "shutil",
                 "random",
                 "numpy",
-                "os"
+                "json"
             ],
             "module": "modelscope.trainers.hooks.checkpoint.checkpoint_hook"
         },
         "('HOOKS', 'default', 'ClipClampLogitScaleHook')": {
             "filepath": "TEMPLATE_PATH/trainers/hooks/clip_clamp_logit_scale_hook.py",
             "imports": [
                 "torch"
@@ -2795,69 +2790,69 @@
             "filepath": "TEMPLATE_PATH/trainers/hooks/distributed/ddp_hook.py",
             "imports": [],
             "module": "modelscope.trainers.hooks.distributed.ddp_hook"
         },
         "('HOOKS', 'default', 'DeepspeedHook')": {
             "filepath": "TEMPLATE_PATH/trainers/hooks/distributed/deepspeed_hook.py",
             "imports": [
-                "math",
                 "megatron_util",
-                "deepspeed",
-                "torch",
                 "functools",
+                "deepspeed",
+                "os",
                 "transformers",
-                "shutil",
-                "os"
+                "math",
+                "torch",
+                "shutil"
             ],
             "module": "modelscope.trainers.hooks.distributed.deepspeed_hook"
         },
         "('HOOKS', 'default', 'EarlyStopHook')": {
             "filepath": "TEMPLATE_PATH/trainers/hooks/early_stop_hook.py",
             "imports": [
                 "numpy"
             ],
             "module": "modelscope.trainers.hooks.early_stop_hook"
         },
         "('HOOKS', 'default', 'EvaluationHook')": {
             "filepath": "TEMPLATE_PATH/trainers/hooks/evaluation_hook.py",
             "imports": [
-                "typing",
-                "collections"
+                "collections",
+                "typing"
             ],
             "module": "modelscope.trainers.hooks.evaluation_hook"
         },
         "('HOOKS', 'default', 'IterTimerHook')": {
             "filepath": "TEMPLATE_PATH/trainers/hooks/iter_timer_hook.py",
             "imports": [
                 "time"
             ],
             "module": "modelscope.trainers.hooks.iter_timer_hook"
         },
         "('HOOKS', 'default', 'LoadCheckpointHook')": {
             "filepath": "TEMPLATE_PATH/trainers/hooks/checkpoint/load_checkpoint_hook.py",
             "imports": [
-                "numpy",
+                "typing",
                 "torch",
+                "packaging",
                 "random",
-                "typing",
-                "packaging"
+                "numpy"
             ],
             "module": "modelscope.trainers.hooks.checkpoint.load_checkpoint_hook"
         },
         "('HOOKS', 'default', 'LrSchedulerHook')": {
             "filepath": "TEMPLATE_PATH/trainers/hooks/lr_scheduler_hook.py",
             "imports": [],
             "module": "modelscope.trainers.hooks.lr_scheduler_hook"
         },
         "('HOOKS', 'default', 'MegatronHook')": {
             "filepath": "TEMPLATE_PATH/trainers/hooks/distributed/megatron_hook.py",
             "imports": [
+                "megatron_util",
                 "shutil",
                 "torch",
-                "megatron_util",
                 "os"
             ],
             "module": "modelscope.trainers.hooks.distributed.megatron_hook"
         },
         "('HOOKS', 'default', 'NoneLrSchedulerHook')": {
             "filepath": "TEMPLATE_PATH/trainers/hooks/lr_scheduler_hook.py",
             "imports": [],
@@ -2890,28 +2885,28 @@
                 "os"
             ],
             "module": "modelscope.trainers.hooks.compression.sparsity_hook"
         },
         "('HOOKS', 'default', 'TensorboardHook')": {
             "filepath": "TEMPLATE_PATH/trainers/hooks/logger/tensorboard_hook.py",
             "imports": [
-                "numpy",
+                "os",
                 "torch",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.trainers.hooks.logger.tensorboard_hook"
         },
         "('HOOKS', 'default', 'TextLoggerHook')": {
             "filepath": "TEMPLATE_PATH/trainers/hooks/logger/text_logger_hook.py",
             "imports": [
-                "torch",
                 "collections",
-                "json",
                 "datetime",
-                "os"
+                "os",
+                "torch",
+                "json"
             ],
             "module": "modelscope.trainers.hooks.logger.text_logger_hook"
         },
         "('HOOKS', 'default', 'TorchAMPOptimizerHook')": {
             "filepath": "TEMPLATE_PATH/trainers/hooks/optimizer/torch_optimizer_hook.py",
             "imports": [
                 "logging"
@@ -2932,490 +2927,477 @@
             "filepath": "TEMPLATE_PATH/trainers/lrscheduler/warmup/warmup.py",
             "imports": [],
             "module": "modelscope.trainers.lrscheduler.warmup.warmup"
         },
         "('MATCH_COST', 'default', 'BBox3DL1Cost')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/match_costs/match_cost.py",
             "imports": [
-                "torch",
-                "mmdet"
+                "mmdet",
+                "torch"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.core.bbox.match_costs.match_cost"
         },
         "('MATCH_COST', 'default', 'MaskCost')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_instance_segmentation/track/mask_hungarian_assigner.py",
             "imports": [
-                "scipy",
-                "numpy",
+                "mmdet",
                 "torch",
-                "mmdet"
+                "scipy",
+                "numpy"
             ],
             "module": "modelscope.models.cv.video_instance_segmentation.track.mask_hungarian_assigner"
         },
         "('METRICS', 'default', 'accuracy')": {
             "filepath": "TEMPLATE_PATH/metrics/accuracy_metric.py",
             "imports": [
-                "typing",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.metrics.accuracy_metric"
         },
         "('METRICS', 'default', 'audio-noise-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/audio_noise_metric.py",
             "imports": [
                 "typing"
             ],
             "module": "modelscope.metrics.audio_noise_metric"
         },
         "('METRICS', 'default', 'bleu')": {
             "filepath": "TEMPLATE_PATH/metrics/bleu_metric.py",
             "imports": [
-                "sacrebleu",
                 "itertools",
+                "sacrebleu",
                 "typing"
             ],
             "module": "modelscope.metrics.bleu_metric"
         },
         "('METRICS', 'default', 'image-color-enhance-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/image_color_enhance_metric.py",
             "imports": [
                 "cv2",
-                "typing",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.metrics.image_color_enhance_metric"
         },
         "('METRICS', 'default', 'image-colorization-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/image_colorization_metric.py",
             "imports": [
-                "torch",
-                "torchvision",
                 "scipy",
-                "cv2",
                 "typing",
+                "torch",
+                "cv2",
+                "torchvision",
                 "numpy"
             ],
             "module": "modelscope.metrics.image_colorization_metric"
         },
         "('METRICS', 'default', 'image-denoise-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/image_denoise_metric.py",
             "imports": [
-                "cv2",
-                "typing",
+                "torch",
                 "numpy",
-                "torch"
+                "cv2",
+                "typing"
             ],
             "module": "modelscope.metrics.image_denoise_metric"
         },
         "('METRICS', 'default', 'image-inpainting-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/image_inpainting_metric.py",
             "imports": [
+                "torch",
                 "scipy",
-                "typing",
                 "numpy",
-                "torch"
+                "typing"
             ],
             "module": "modelscope.metrics.image_inpainting_metric"
         },
         "('METRICS', 'default', 'image-ins-seg-coco-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/image_instance_segmentation_metric.py",
             "imports": [
-                "numpy",
                 "collections",
-                "tempfile",
-                "pycocotools",
+                "os",
                 "typing",
-                "os"
+                "pycocotools",
+                "tempfile",
+                "numpy"
             ],
             "module": "modelscope.metrics.image_instance_segmentation_metric"
         },
         "('METRICS', 'default', 'image-portrait-enhancement-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/image_portrait_enhancement_metric.py",
             "imports": [
                 "cv2",
-                "typing",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.metrics.image_portrait_enhancement_metric"
         },
         "('METRICS', 'default', 'image-quality-assessment-degradation-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/image_quality_assessment_degradation_metric.py",
             "imports": [
-                "numpy",
-                "sys",
-                "torch",
                 "collections",
+                "sys",
+                "os",
+                "tqdm",
                 "scipy",
+                "typing",
+                "torch",
                 "tempfile",
                 "cv2",
-                "typing",
-                "os",
-                "tqdm"
+                "numpy"
             ],
             "module": "modelscope.metrics.image_quality_assessment_degradation_metric"
         },
         "('METRICS', 'default', 'image-quality-assessment-mos-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/image_quality_assessment_mos_metric.py",
             "imports": [
-                "torch",
-                "sys",
+                "numpy",
+                "os",
+                "tqdm",
                 "scipy",
+                "typing",
+                "torch",
                 "tempfile",
                 "cv2",
-                "typing",
-                "numpy",
-                "os",
-                "tqdm"
+                "sys"
             ],
             "module": "modelscope.metrics.image_quality_assessment_mos_metric"
         },
         "('METRICS', 'default', 'inbatch_recall')": {
             "filepath": "TEMPLATE_PATH/metrics/inbatch_recall_metric.py",
             "imports": [
-                "typing",
+                "torch",
                 "numpy",
-                "torch"
+                "typing"
             ],
             "module": "modelscope.metrics.inbatch_recall_metric"
         },
         "('METRICS', 'default', 'loss-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/loss_metric.py",
             "imports": [
                 "sklearn",
-                "typing",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.metrics.loss_metric"
         },
         "('METRICS', 'default', 'mAP')": {
             "filepath": "TEMPLATE_PATH/metrics/map_metric.py",
             "imports": [
-                "typing",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.metrics.map_metric"
         },
         "('METRICS', 'default', 'movie-scene-segmentation-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/movie_scene_segmentation_metric.py",
             "imports": [
-                "typing",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.metrics.movie_scene_segmentation_metric"
         },
         "('METRICS', 'default', 'ned')": {
             "filepath": "TEMPLATE_PATH/metrics/ned_metric.py",
             "imports": [
-                "typing",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.metrics.ned_metric"
         },
         "('METRICS', 'default', 'ocr-recognition-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/ocr_recognition_metric.py",
             "imports": [
-                "typing",
-                "edit_distance",
+                "torch",
                 "numpy",
-                "torch"
+                "edit_distance",
+                "typing"
             ],
             "module": "modelscope.metrics.ocr_recognition_metric"
         },
         "('METRICS', 'default', 'ppl')": {
             "filepath": "TEMPLATE_PATH/metrics/ppl_metric.py",
             "imports": [
                 "torch",
-                "typing",
                 "numpy",
-                "math"
+                "math",
+                "typing"
             ],
             "module": "modelscope.metrics.ppl_metric"
         },
         "('METRICS', 'default', 'prediction-saving-wrapper')": {
             "filepath": "TEMPLATE_PATH/metrics/prediction_saving_wrapper.py",
             "imports": [
                 "sklearn",
-                "typing",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.metrics.prediction_saving_wrapper"
         },
         "('METRICS', 'default', 'referring-video-object-segmentation-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/referring_video_object_segmentation_metric.py",
             "imports": [
-                "numpy",
+                "tqdm",
+                "typing",
                 "torch",
                 "pycocotools",
-                "typing",
-                "tqdm"
+                "numpy"
             ],
             "module": "modelscope.metrics.referring_video_object_segmentation_metric"
         },
         "('METRICS', 'default', 'seq-cls-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/sequence_classification_metric.py",
             "imports": [
                 "sklearn",
-                "typing",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.metrics.sequence_classification_metric"
         },
         "('METRICS', 'default', 'text-gen-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/text_generation_metric.py",
             "imports": [
-                "nltk",
-                "sys",
                 "contextlib",
+                "nltk",
                 "rouge",
-                "typing"
+                "typing",
+                "sys"
             ],
             "module": "modelscope.metrics.text_generation_metric"
         },
         "('METRICS', 'default', 'text-ranking-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/text_ranking_metric.py",
             "imports": [
-                "typing",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.metrics.text_ranking_metric"
         },
         "('METRICS', 'default', 'token-cls-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/token_classification_metric.py",
             "imports": [
                 "importlib",
-                "typing",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.metrics.token_classification_metric"
         },
         "('METRICS', 'default', 'translation-evaluation-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/translation_evaluation_metric.py",
             "imports": [
                 "importlib",
-                "typing",
-                "pandas"
+                "pandas",
+                "typing"
             ],
             "module": "modelscope.metrics.translation_evaluation_metric"
         },
         "('METRICS', 'default', 'video-frame-interpolation-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/video_frame_interpolation_metric.py",
             "imports": [
+                "lpips",
                 "math",
-                "numpy",
+                "typing",
                 "torch",
-                "lpips",
-                "typing"
+                "numpy"
             ],
             "module": "modelscope.metrics.video_frame_interpolation_metric"
         },
         "('METRICS', 'default', 'video-stabilization-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/video_stabilization_metric.py",
             "imports": [
                 "numpy",
-                "sys",
+                "os",
+                "tqdm",
+                "typing",
                 "tempfile",
                 "cv2",
-                "typing",
-                "os",
-                "tqdm"
+                "sys"
             ],
             "module": "modelscope.metrics.video_stabilization_metric"
         },
         "('METRICS', 'default', 'video-summarization-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/video_summarization_metric.py",
             "imports": [
-                "typing",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.metrics.video_summarization_metric"
         },
         "('METRICS', 'default', 'video-super-resolution-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/video_super_resolution_metric/video_super_resolution_metric.py",
             "imports": [
-                "typing",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.metrics.video_super_resolution_metric.video_super_resolution_metric"
         },
         "('MODELS', 'acoustic-noise-suppression', 'speech_dfsmn_ans')": {
             "filepath": "TEMPLATE_PATH/models/audio/ans/denoise_net.py",
             "imports": [
                 "torch"
             ],
             "module": "modelscope.models.audio.ans.denoise_net"
         },
         "('MODELS', 'acoustic-noise-suppression', 'speech_frcrn_ans_cirm_16k')": {
             "filepath": "TEMPLATE_PATH/models/audio/ans/frcrn.py",
             "imports": [
-                "typing",
+                "os",
                 "torch",
-                "os"
+                "typing"
             ],
             "module": "modelscope.models.audio.ans.frcrn"
         },
         "('MODELS', 'auto-speech-recognition', 'generic-asr')": {
             "filepath": "TEMPLATE_PATH/models/audio/asr/generic_automatic_speech_recognition.py",
             "imports": [
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.audio.asr.generic_automatic_speech_recognition"
         },
         "('MODELS', 'auto-speech-recognition', 'ofa')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/ofa_for_all_tasks.py",
             "imports": [
-                "re",
-                "math",
-                "torch",
-                "json",
-                "string",
                 "functools",
+                "string",
+                "os",
                 "typing",
-                "os"
+                "math",
+                "torch",
+                "re",
+                "json"
             ],
             "module": "modelscope.models.multi_modal.ofa_for_all_tasks"
         },
         "('MODELS', 'auto-speech-recognition', 'wenet-asr')": {
             "filepath": "TEMPLATE_PATH/models/audio/asr/wenet_automatic_speech_recognition.py",
             "imports": [
                 "os",
-                "typing",
+                "wenetruntime",
                 "json",
-                "wenetruntime"
+                "typing"
             ],
             "module": "modelscope.models.audio.asr.wenet_automatic_speech_recognition"
         },
         "('MODELS', 'backbone', 'T5')": {
             "filepath": "TEMPLATE_PATH/models/nlp/T5/backbone.py",
             "imports": [
-                "math",
-                "torch",
+                "os",
                 "warnings",
-                "copy",
                 "transformers",
+                "copy",
+                "math",
                 "typing",
-                "os"
+                "torch"
             ],
             "module": "modelscope.models.nlp.T5.backbone"
         },
         "('MODELS', 'backbone', 'bert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/bert/backbone.py",
             "imports": [
-                "torch",
                 "transformers",
-                "math",
-                "packaging"
+                "packaging",
+                "torch",
+                "math"
             ],
             "module": "modelscope.models.nlp.bert.backbone"
         },
         "('MODELS', 'backbone', 'deberta_v2')": {
             "filepath": "TEMPLATE_PATH/models/nlp/deberta_v2/backbone.py",
             "imports": [
-                "transformers",
-                "typing",
+                "collections",
                 "torch",
-                "collections"
+                "transformers",
+                "typing"
             ],
             "module": "modelscope.models.nlp.deberta_v2.backbone"
         },
         "('MODELS', 'backbone', 'llama')": {
             "filepath": "TEMPLATE_PATH/models/nlp/llama/backbone.py",
             "imports": [
-                "torch",
                 "transformers",
-                "typing",
-                "math"
+                "torch",
+                "math",
+                "typing"
             ],
             "module": "modelscope.models.nlp.llama.backbone"
         },
         "('MODELS', 'backbone', 'llama2')": {
             "filepath": "TEMPLATE_PATH/models/nlp/llama2/backbone.py",
             "imports": [
-                "torch",
                 "transformers",
-                "typing",
-                "math"
+                "torch",
+                "math",
+                "typing"
             ],
             "module": "modelscope.models.nlp.llama2.backbone"
         },
         "('MODELS', 'backbone', 'lstm')": {
             "filepath": "TEMPLATE_PATH/models/nlp/lstm/backbone.py",
             "imports": [
                 "torch"
             ],
             "module": "modelscope.models.nlp.lstm.backbone"
         },
         "('MODELS', 'backbone', 'megatron-bert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/megatron_bert/backbone.py",
             "imports": [
-                "torch",
                 "transformers",
+                "torch",
                 "math"
             ],
             "module": "modelscope.models.nlp.megatron_bert.backbone"
         },
         "('MODELS', 'backbone', 'mgeo')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/mgeo/backbone.py",
             "imports": [
-                "math",
-                "dataclasses",
-                "torch",
+                "os",
                 "warnings",
-                "typing",
-                "random",
                 "transformers",
-                "os"
+                "dataclasses",
+                "math",
+                "typing",
+                "torch",
+                "random"
             ],
             "module": "modelscope.models.multi_modal.mgeo.backbone"
         },
         "('MODELS', 'backbone', 'plug-mental')": {
             "filepath": "TEMPLATE_PATH/models/nlp/plug_mental/backbone.py",
             "imports": [
-                "math",
-                "dataclasses",
-                "torch",
                 "transformers",
+                "math",
                 "typing",
-                "packaging"
+                "torch",
+                "packaging",
+                "dataclasses"
             ],
             "module": "modelscope.models.nlp.plug_mental.backbone"
         },
         "('MODELS', 'backbone', 'ponet')": {
             "filepath": "TEMPLATE_PATH/models/nlp/ponet/backbone.py",
             "imports": [
                 "distutils",
+                "transformers",
                 "math",
                 "torch",
-                "transformers",
                 "packaging"
             ],
             "module": "modelscope.models.nlp.ponet.backbone"
         },
-        "('MODELS', 'backbone', 'qwen-7b')": {
-            "filepath": "TEMPLATE_PATH/models/nlp/qwen/backbone.py",
-            "imports": [
-                "math",
-                "importlib",
-                "torch",
-                "typing",
-                "einops",
-                "transformers",
-                "flash_attn"
-            ],
-            "module": "modelscope.models.nlp.qwen.backbone"
-        },
         "('MODELS', 'backbone', 'structbert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/structbert/backbone.py",
             "imports": [
-                "math",
-                "dataclasses",
-                "torch",
                 "transformers",
+                "math",
                 "typing",
-                "packaging"
+                "torch",
+                "packaging",
+                "dataclasses"
             ],
             "module": "modelscope.models.nlp.structbert.backbone"
         },
         "('MODELS', 'backbone', 'transformers')": {
             "filepath": "TEMPLATE_PATH/models/nlp/hf_transformers/backbone.py",
             "imports": [
                 "transformers"
@@ -3428,29 +3410,29 @@
                 "transformers"
             ],
             "module": "modelscope.models.nlp.veco.backbone"
         },
         "('MODELS', 'backbone', 'xlm-roberta')": {
             "filepath": "TEMPLATE_PATH/models/nlp/xlm_roberta/backbone.py",
             "imports": [
-                "torch",
                 "transformers",
-                "math",
-                "packaging"
+                "packaging",
+                "torch",
+                "math"
             ],
             "module": "modelscope.models.nlp.xlm_roberta.backbone"
         },
         "('MODELS', 'bad-image-detecting', 'bad-image-detecting')": {
             "filepath": "TEMPLATE_PATH/models/cv/bad_image_detecting/bad_image_detecting.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
                 "torch",
                 "torchvision",
-                "typing",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.models.cv.bad_image_detecting.bad_image_detecting"
         },
         "('MODELS', 'body-2d-keypoints', 'body-2d-keypoints')": {
             "filepath": "TEMPLATE_PATH/models/cv/body_2d_keypoints/hrnet_v2.py",
             "imports": [
                 "os",
@@ -3458,422 +3440,412 @@
                 "numpy"
             ],
             "module": "modelscope.models.cv.body_2d_keypoints.hrnet_v2"
         },
         "('MODELS', 'body-3d-keypoints', 'body-3d-keypoints')": {
             "filepath": "TEMPLATE_PATH/models/cv/body_3d_keypoints/cannonical_pose/body_3d_pose.py",
             "imports": [
-                "numpy",
-                "torch",
                 "logging",
+                "os",
                 "typing",
-                "os"
+                "torch",
+                "numpy"
             ],
             "module": "modelscope.models.cv.body_3d_keypoints.cannonical_pose.body_3d_pose"
         },
         "('MODELS', 'body-3d-keypoints', 'hdformer')": {
             "filepath": "TEMPLATE_PATH/models/cv/body_3d_keypoints/hdformer/hdformer_detector.py",
             "imports": [
-                "typing",
-                "numpy",
+                "os",
                 "torch",
-                "os"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.models.cv.body_3d_keypoints.hdformer.hdformer_detector"
         },
         "('MODELS', 'card-detection', 'scrfd')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/scrfd_detect.py",
             "imports": [
-                "numpy",
-                "torch",
+                "os",
                 "copy",
                 "typing",
-                "os"
+                "torch",
+                "numpy"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.scrfd_detect"
         },
         "('MODELS', 'chat', 'chatglm2-6b')": {
             "filepath": "TEMPLATE_PATH/models/nlp/chatglm2/text_generation.py",
             "imports": [
-                "math",
-                "torch",
-                "sys",
+                "transformers",
                 "warnings",
                 "copy",
-                "transformers",
-                "typing"
+                "typing",
+                "math",
+                "torch",
+                "sys"
             ],
             "module": "modelscope.models.nlp.chatglm2.text_generation"
         },
         "('MODELS', 'chat', 'chatglm6b')": {
             "filepath": "TEMPLATE_PATH/models/nlp/chatglm/text_generation.py",
             "imports": [
-                "re",
-                "math",
-                "torch",
-                "sys",
+                "os",
                 "warnings",
-                "copy",
                 "transformers",
+                "copy",
+                "math",
                 "typing",
-                "os"
+                "torch",
+                "re",
+                "sys"
             ],
             "module": "modelscope.models.nlp.chatglm.text_generation"
         },
-        "('MODELS', 'chat', 'qwen-7b')": {
-            "filepath": "TEMPLATE_PATH/models/nlp/qwen/text_generation.py",
-            "imports": [
-                "typing",
-                "transformers",
-                "warnings",
-                "torch"
-            ],
-            "module": "modelscope.models.nlp.qwen.text_generation"
-        },
         "('MODELS', 'code-generation', 'codegeex')": {
             "filepath": "TEMPLATE_PATH/models/nlp/codegeex/codegeex_for_code_generation.py",
             "imports": [
-                "typing",
+                "copy",
                 "torch",
-                "copy"
+                "typing"
             ],
             "module": "modelscope.models.nlp.codegeex.codegeex_for_code_generation"
         },
         "('MODELS', 'code-translation', 'codegeex')": {
             "filepath": "TEMPLATE_PATH/models/nlp/codegeex/codegeex_for_code_translation.py",
             "imports": [
-                "typing",
+                "copy",
                 "torch",
-                "copy"
+                "typing"
             ],
             "module": "modelscope.models.nlp.codegeex.codegeex_for_code_translation"
         },
         "('MODELS', 'competency-aware-translation', 'canmt')": {
             "filepath": "TEMPLATE_PATH/models/nlp/canmt/canmt_translation.py",
             "imports": [
+                "os",
                 "math",
-                "numpy",
-                "torch",
                 "typing",
-                "os"
+                "torch",
+                "numpy"
             ],
             "module": "modelscope.models.nlp.canmt.canmt_translation"
         },
         "('MODELS', 'controllable-image-generation', 'controllable-image-generation')": {
             "filepath": "TEMPLATE_PATH/models/cv/controllable_image_generation/controlnet.py",
             "imports": [
+                "numpy",
                 "einops",
+                "os",
+                "cv2",
                 "math",
-                "numpy",
-                "sys",
+                "typing",
                 "torch",
                 "PIL",
-                "tempfile",
                 "control_ldm",
-                "cv2",
-                "typing",
+                "tempfile",
                 "random",
-                "os"
+                "sys"
             ],
             "module": "modelscope.models.cv.controllable_image_generation.controlnet"
         },
         "('MODELS', 'crowd-counting', 'HRNetCrowdCounting')": {
             "filepath": "TEMPLATE_PATH/models/cv/crowd_counting/cc_model.py",
             "imports": [
+                "os",
                 "torch",
-                "typing",
-                "os"
+                "typing"
             ],
             "module": "modelscope.models.cv.crowd_counting.cc_model"
         },
         "('MODELS', 'document-grounded-dialog-generate', 'doc2bot')": {
             "filepath": "TEMPLATE_PATH/models/nlp/dgds/document_grounded_dialog_generate.py",
             "imports": [
-                "typing",
+                "os",
                 "torch",
-                "os"
+                "typing"
             ],
             "module": "modelscope.models.nlp.dgds.document_grounded_dialog_generate"
         },
         "('MODELS', 'document-grounded-dialog-rerank', 'doc2bot')": {
             "filepath": "TEMPLATE_PATH/models/nlp/dgds/document_grounded_dialog_rerank.py",
             "imports": [
-                "typing",
+                "os",
                 "torch",
-                "os"
+                "typing"
             ],
             "module": "modelscope.models.nlp.dgds.document_grounded_dialog_rerank"
         },
         "('MODELS', 'document-grounded-dialog-retrieval', 'doc2bot')": {
             "filepath": "TEMPLATE_PATH/models/nlp/dgds/document_grounded_dialog_retrieval.py",
             "imports": [
-                "typing",
+                "os",
                 "torch",
-                "os"
+                "typing"
             ],
             "module": "modelscope.models.nlp.dgds.document_grounded_dialog_retrieval"
         },
         "('MODELS', 'document-segmentation', 'bert-for-document-segmentation')": {
             "filepath": "TEMPLATE_PATH/models/nlp/bert/document_segmentation.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.nlp.bert.document_segmentation"
         },
         "('MODELS', 'document-segmentation', 'ponet-for-document-segmentation')": {
             "filepath": "TEMPLATE_PATH/models/nlp/ponet/document_segmentation.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.nlp.ponet.document_segmentation"
         },
         "('MODELS', 'document-vl-embedding', 'vldoc')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/vldoc/model.py",
             "imports": [
-                "re",
+                "logging",
+                "os",
+                "copy",
                 "math",
-                "sys",
                 "torch",
+                "re",
                 "torchvision",
-                "copy",
-                "logging",
-                "os",
+                "sys",
                 "json"
             ],
             "module": "modelscope.models.multi_modal.vldoc.model"
         },
         "('MODELS', 'domain-specific-object-detection', 'tinynas-damoyolo')": {
             "filepath": "TEMPLATE_PATH/models/cv/tinynas_detection/tinynas_damoyolo.py",
             "imports": [],
             "module": "modelscope.models.cv.tinynas_detection.tinynas_damoyolo"
         },
         "('MODELS', 'efficient-diffusion-tuning', 'efficient-diffusion-tuning')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/efficient_diffusion_tuning/efficient_stable_diffusion.py",
             "imports": [
-                "diffusers",
-                "torch",
                 "functools",
+                "os",
                 "transformers",
                 "typing",
-                "os"
+                "torch",
+                "diffusers"
             ],
             "module": "modelscope.models.multi_modal.efficient_diffusion_tuning.efficient_stable_diffusion"
         },
         "('MODELS', 'extractive-summarization', 'ponet-for-document-segmentation')": {
             "filepath": "TEMPLATE_PATH/models/nlp/ponet/document_segmentation.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.nlp.ponet.document_segmentation"
         },
         "('MODELS', 'face-2d-keypoints', 'flc')": {
             "filepath": "TEMPLATE_PATH/models/cv/facial_landmark_confidence/flc/facial_landmark_confidence.py",
             "imports": [
+                "os",
                 "torch",
                 "PIL",
                 "cv2",
-                "numpy",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.models.cv.facial_landmark_confidence.flc.facial_landmark_confidence"
         },
         "('MODELS', 'face-attribute-recognition', 'fairface')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_attribute_recognition/fair_face/face_attribute_recognition.py",
             "imports": [
+                "os",
                 "torch",
-                "torchvision",
                 "PIL",
                 "cv2",
-                "numpy",
-                "os"
+                "torchvision",
+                "numpy"
             ],
             "module": "modelscope.models.cv.face_attribute_recognition.fair_face.face_attribute_recognition"
         },
         "('MODELS', 'face-detection', 'damofd')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/damofd_detect.py",
             "imports": [
-                "typing",
-                "torch",
+                "os",
                 "copy",
-                "os"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.damofd_detect"
         },
         "('MODELS', 'face-detection', 'mogface')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/mogface/models/detectors.py",
             "imports": [
-                "cv2",
-                "numpy",
+                "os",
                 "torch",
-                "os"
+                "numpy",
+                "cv2"
             ],
             "module": "modelscope.models.cv.face_detection.mogface.models.detectors"
         },
         "('MODELS', 'face-detection', 'mtcnn')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/mtcnn/models/detector.py",
             "imports": [
-                "PIL",
-                "numpy",
+                "os",
                 "torch",
-                "os"
+                "numpy",
+                "PIL"
             ],
             "module": "modelscope.models.cv.face_detection.mtcnn.models.detector"
         },
         "('MODELS', 'face-detection', 'retinaface')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/retinaface/detection.py",
             "imports": [
-                "cv2",
+                "torch",
                 "numpy",
-                "torch"
+                "cv2"
             ],
             "module": "modelscope.models.cv.face_detection.retinaface.detection"
         },
         "('MODELS', 'face-detection', 'scrfd')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/scrfd_detect.py",
             "imports": [
-                "numpy",
-                "torch",
+                "os",
                 "copy",
                 "typing",
-                "os"
+                "torch",
+                "numpy"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.scrfd_detect"
         },
         "('MODELS', 'face-detection', 'tinymog')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/tinymog_detect.py",
             "imports": [
-                "typing",
-                "torch",
+                "os",
                 "copy",
-                "os"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.tinymog_detect"
         },
         "('MODELS', 'face-detection', 'ulfd')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/detection.py",
             "imports": [
-                "cv2",
-                "numpy",
+                "os",
                 "torch",
-                "os"
+                "numpy",
+                "cv2"
             ],
             "module": "modelscope.models.cv.face_detection.ulfd_slim.detection"
         },
         "('MODELS', 'face-emotion', 'face-emotion')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_emotion/emotion_model.py",
             "imports": [
-                "sys",
                 "os",
-                "torch"
+                "torch",
+                "sys"
             ],
             "module": "modelscope.models.cv.face_emotion.emotion_model"
         },
         "('MODELS', 'face-human-hand-detection', 'face-human-hand-detection')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_human_hand_detection/det_infer.py",
             "imports": [
-                "cv2",
+                "torch",
                 "numpy",
-                "torch"
+                "cv2"
             ],
             "module": "modelscope.models.cv.face_human_hand_detection.det_infer"
         },
         "('MODELS', 'face-recognition', 'rts-backbone')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_recognition/torchkit/rts_backbone.py",
             "imports": [
+                "collections",
                 "torch",
                 "os",
-                "math",
-                "collections"
+                "math"
             ],
             "module": "modelscope.models.cv.face_recognition.torchkit.rts_backbone"
         },
         "('MODELS', 'face-reconstruction', 'face_reconstruction')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_reconstruction/models/facerecon_model.py",
             "imports": [
-                "numpy",
-                "torch",
                 "collections",
+                "os",
+                "torch",
                 "cv2",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.models.cv.face_reconstruction.models.facerecon_model"
         },
         "('MODELS', 'facial-expression-recognition', 'fer')": {
             "filepath": "TEMPLATE_PATH/models/cv/facial_expression_recognition/fer/facial_expression_recognition.py",
             "imports": [
+                "os",
                 "torch",
                 "PIL",
                 "cv2",
-                "numpy",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.models.cv.facial_expression_recognition.fer.facial_expression_recognition"
         },
         "('MODELS', 'faq-question-answering', 'structbert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/structbert/faq_question_answering.py",
             "imports": [
-                "math",
-                "torch",
                 "collections",
+                "os",
+                "math",
                 "typing",
-                "os"
+                "torch"
             ],
             "module": "modelscope.models.nlp.structbert.faq_question_answering"
         },
         "('MODELS', 'feature-extraction', 'feature-extraction')": {
             "filepath": "TEMPLATE_PATH/models/nlp/task_models/feature_extraction.py",
             "imports": [
-                "typing",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.models.nlp.task_models.feature_extraction"
         },
         "('MODELS', 'fid-dialogue', 'fid-T5')": {
             "filepath": "TEMPLATE_PATH/models/nlp/fid_T5/text_generation.py",
             "imports": [
-                "transformers",
                 "os",
                 "torch",
+                "transformers",
                 "io"
             ],
             "module": "modelscope.models.nlp.fid_T5.text_generation"
         },
         "('MODELS', 'fid-dialogue', 'fid-plug')": {
             "filepath": "TEMPLATE_PATH/models/nlp/fid_plug/text_generation.py",
             "imports": [
-                "transformers",
                 "os",
                 "torch",
+                "transformers",
                 "io"
             ],
             "module": "modelscope.models.nlp.fid_plug.text_generation"
         },
         "('MODELS', 'fill-mask', 'bert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/bert/fill_mask.py",
             "imports": [],
             "module": "modelscope.models.nlp.bert.fill_mask"
         },
         "('MODELS', 'fill-mask', 'deberta_v2')": {
             "filepath": "TEMPLATE_PATH/models/nlp/deberta_v2/fill_mask.py",
             "imports": [
                 "transformers",
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.nlp.deberta_v2.fill_mask"
         },
         "('MODELS', 'fill-mask', 'fill-mask')": {
             "filepath": "TEMPLATE_PATH/models/nlp/task_models/fill_mask.py",
             "imports": [
-                "typing",
                 "torch",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.models.nlp.task_models.fill_mask"
         },
         "('MODELS', 'fill-mask', 'megatron-bert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/megatron_bert/fill_mask.py",
             "imports": [
                 "transformers",
@@ -3903,120 +3875,120 @@
                 "transformers"
             ],
             "module": "modelscope.models.nlp.veco.fill_mask"
         },
         "('MODELS', 'generative-multi-modal-embedding', 'gemm-generative-multi-modal')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/gemm/gemm_model.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
                 "torch",
                 "PIL",
                 "torchvision",
-                "json",
-                "typing",
-                "os"
+                "numpy",
+                "json"
             ],
             "module": "modelscope.models.multi_modal.gemm.gemm_model"
         },
         "('MODELS', 'generative-multi-modal-embedding', 'rleg-generative-multi-modal')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/rleg/rleg.py",
             "imports": [
+                "torch",
                 "torchvision",
-                "typing",
-                "torch"
+                "typing"
             ],
             "module": "modelscope.models.multi_modal.rleg.rleg"
         },
         "('MODELS', 'hand-static', 'hand-static')": {
             "filepath": "TEMPLATE_PATH/models/cv/hand_static/hand_model.py",
             "imports": [
+                "numpy",
+                "os",
                 "torch",
-                "sys",
-                "torchvision",
                 "PIL",
                 "cv2",
-                "numpy",
-                "os"
+                "torchvision",
+                "sys"
             ],
             "module": "modelscope.models.cv.hand_static.hand_model"
         },
         "('MODELS', 'human-detection', 'detection')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection/mmdet_model.py",
             "imports": [
-                "numpy",
+                "os",
                 "torch",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.models.cv.object_detection.mmdet_model"
         },
         "('MODELS', 'human-reconstruction', 'human-reconstruction')": {
             "filepath": "TEMPLATE_PATH/models/cv/human_reconstruction/Reconstruction.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
+                "skimage",
                 "torch",
                 "PIL",
-                "torchvision",
                 "cv2",
-                "typing",
-                "os",
-                "skimage"
+                "torchvision",
+                "numpy"
             ],
             "module": "modelscope.models.cv.human_reconstruction.Reconstruction"
         },
         "('MODELS', 'image-body-reshaping', 'image-body-reshaping')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_body_reshaping/image_body_reshaping.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
                 "torch",
                 "cv2",
-                "typing",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.models.cv.image_body_reshaping.image_body_reshaping"
         },
         "('MODELS', 'image-captioning', 'clip-interrogator')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/clip_interrogator/model.py",
             "imports": [
-                "torch",
-                "requests",
-                "numpy",
-                "typing",
-                "os",
+                "hashlib",
+                "safetensors",
+                "open_clip",
                 "tqdm",
                 "math",
+                "time",
+                "torch",
+                "PIL",
                 "dataclasses",
                 "torchvision",
-                "PIL",
-                "open_clip",
-                "safetensors",
-                "time",
-                "hashlib",
-                "transformers"
+                "numpy",
+                "os",
+                "requests",
+                "transformers",
+                "typing"
             ],
             "module": "modelscope.models.multi_modal.clip_interrogator.model"
         },
         "('MODELS', 'image-captioning', 'mplug')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/mplug_for_all_tasks.py",
             "imports": [
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.multi_modal.mplug_for_all_tasks"
         },
         "('MODELS', 'image-captioning', 'ofa')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/ofa_for_all_tasks.py",
             "imports": [
-                "re",
-                "math",
-                "torch",
-                "json",
-                "string",
                 "functools",
+                "string",
+                "os",
                 "typing",
-                "os"
+                "math",
+                "torch",
+                "re",
+                "json"
             ],
             "module": "modelscope.models.multi_modal.ofa_for_all_tasks"
         },
         "('MODELS', 'image-classification', 'ClassificationModel')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_classification/mmcls_model.py",
             "imports": [
                 "os"
@@ -4030,129 +4002,129 @@
                 "torch"
             ],
             "module": "modelscope.models.cv.robust_image_classification.easyrobust_model"
         },
         "('MODELS', 'image-classification', 'bnext')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_binary_quant_classification/binary_quant_model.py",
             "imports": [
-                "os",
+                "collections",
                 "torch",
-                "collections"
+                "os"
             ],
             "module": "modelscope.models.cv.image_binary_quant_classification.binary_quant_model"
         },
         "('MODELS', 'image-classification', 'content-check')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_classification/resnet50_cc.py",
             "imports": [
+                "collections",
+                "os",
                 "math",
                 "torch",
-                "collections",
-                "torchvision",
-                "os"
+                "torchvision"
             ],
             "module": "modelscope.models.cv.image_classification.resnet50_cc"
         },
         "('MODELS', 'image-classification', 'image-probing-model')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_probing_model/model.py",
             "imports": [
-                "typing",
-                "json",
+                "os",
                 "torch",
-                "os"
+                "json",
+                "typing"
             ],
             "module": "modelscope.models.cv.image_probing_model.model"
         },
         "('MODELS', 'image-classification', 'ofa')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/ofa_for_all_tasks.py",
             "imports": [
-                "re",
-                "math",
-                "torch",
-                "json",
-                "string",
                 "functools",
+                "string",
+                "os",
                 "typing",
-                "os"
+                "math",
+                "torch",
+                "re",
+                "json"
             ],
             "module": "modelscope.models.multi_modal.ofa_for_all_tasks"
         },
         "('MODELS', 'image-color-enhancement', 'adaint')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_color_enhance/adaint/adaint.py",
             "imports": [
-                "numbers",
+                "os",
+                "typing",
                 "torch",
                 "torchvision",
-                "typing",
-                "os"
+                "numbers"
             ],
             "module": "modelscope.models.cv.image_color_enhance.adaint.adaint"
         },
         "('MODELS', 'image-color-enhancement', 'csrnet')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_color_enhance/image_color_enhance.py",
             "imports": [
-                "typing",
+                "os",
                 "torch",
-                "os"
+                "typing"
             ],
             "module": "modelscope.models.cv.image_color_enhance.image_color_enhance"
         },
         "('MODELS', 'image-color-enhancement', 'deeplpfnet')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_color_enhance/deeplpf/deeplpf_image_color_enhance.py",
             "imports": [
-                "typing",
+                "os",
                 "torch",
-                "os"
+                "typing"
             ],
             "module": "modelscope.models.cv.image_color_enhance.deeplpf.deeplpf_image_color_enhance"
         },
         "('MODELS', 'image-colorization', 'ddcolor')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/ddcolor_for_image_colorization.py",
             "imports": [
-                "numpy",
-                "torch",
+                "os",
                 "copy",
                 "typing",
-                "os"
+                "torch",
+                "numpy"
             ],
             "module": "modelscope.models.cv.image_colorization.ddcolor.ddcolor_for_image_colorization"
         },
         "('MODELS', 'image-debanding', 'rrdb')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_debanding/rrdb/rrdb_image_debanding.py",
             "imports": [
+                "os",
                 "torch",
-                "typing",
-                "os"
+                "typing"
             ],
             "module": "modelscope.models.cv.image_debanding.rrdb.rrdb_image_debanding"
         },
         "('MODELS', 'image-deblurring', 'nafnet')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_deblur/nafnet_for_image_deblur.py",
             "imports": [
+                "os",
                 "torch",
-                "typing",
-                "os"
+                "typing"
             ],
             "module": "modelscope.models.cv.image_deblur.nafnet_for_image_deblur"
         },
         "('MODELS', 'image-demoireing', 'image-restoration')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_restoration/image_restoration_model.py",
             "imports": [
-                "cv2",
-                "numpy",
+                "os",
                 "torch",
-                "os"
+                "numpy",
+                "cv2"
             ],
             "module": "modelscope.models.cv.image_restoration.image_restoration_model"
         },
         "('MODELS', 'image-denoising', 'nafnet')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_denoise/nafnet_for_image_denoise.py",
             "imports": [
-                "typing",
+                "os",
                 "torch",
-                "os"
+                "typing"
             ],
             "module": "modelscope.models.cv.image_denoise.nafnet_for_image_denoise"
         },
         "('MODELS', 'image-depth-estimation', 'bts-depth-estimation')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_depth_estimation_bts/depth_estimation_bts_model.py",
             "imports": [
                 "os",
@@ -4168,91 +4140,91 @@
                 "numpy"
             ],
             "module": "modelscope.models.cv.image_depth_estimation.newcrfs_model"
         },
         "('MODELS', 'image-driving-perception', 'yolopv2')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_driving_perception/image_driving_percetion_model.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
                 "torch",
                 "cv2",
-                "typing",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.models.cv.image_driving_perception.image_driving_percetion_model"
         },
         "('MODELS', 'image-face-fusion', 'image-face-fusion')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_face_fusion/image_face_fusion.py",
             "imports": [
-                "torch",
                 "collections",
+                "os",
+                "typing",
+                "torch",
                 "PIL",
-                "torchvision",
                 "cv2",
-                "typing",
-                "numpy",
-                "os"
+                "torchvision",
+                "numpy"
             ],
             "module": "modelscope.models.cv.image_face_fusion.image_face_fusion"
         },
         "('MODELS', 'image-fewshot-detection', 'defrcn')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/defrcn_for_fewshot.py",
             "imports": [
-                "typing",
+                "os",
                 "torch",
-                "os"
+                "typing"
             ],
             "module": "modelscope.models.cv.image_defrcn_fewshot.defrcn_for_fewshot"
         },
         "('MODELS', 'image-inpainting', 'FFTInpainting')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_inpainting/model.py",
             "imports": [
+                "os",
                 "torch",
-                "typing",
-                "os"
+                "typing"
             ],
             "module": "modelscope.models.cv.image_inpainting.model"
         },
         "('MODELS', 'image-matching', 'quadtree-attention-image-matching')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_matching/quadtree_attention_model.py",
             "imports": [
-                "torch",
+                "os",
                 "pathlib",
+                "torch",
                 "cv2",
-                "numpy",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.models.cv.image_matching.quadtree_attention_model"
         },
         "('MODELS', 'image-multi-view-depth-estimation', 'image-casmvs-depth-estimation')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/casmvs_model.py",
             "imports": [
-                "torch",
+                "os",
                 "easydict",
+                "torch",
                 "cv2",
-                "numpy",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.models.cv.image_mvs_depth_estimation.casmvs_model"
         },
         "('MODELS', 'image-object-detection', 'MaskScoring')": {
             "filepath": "TEMPLATE_PATH/models/cv/abnormal_object_detection/mmdet_model.py",
             "imports": [
-                "numpy",
+                "os",
                 "torch",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.models.cv.abnormal_object_detection.mmdet_model"
         },
         "('MODELS', 'image-object-detection', 'detection')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection/mmdet_model.py",
             "imports": [
-                "numpy",
+                "os",
                 "torch",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.models.cv.object_detection.mmdet_model"
         },
         "('MODELS', 'image-object-detection', 'tinynas-damoyolo')": {
             "filepath": "TEMPLATE_PATH/models/cv/tinynas_detection/tinynas_damoyolo.py",
             "imports": [],
             "module": "modelscope.models.cv.tinynas_detection.tinynas_damoyolo"
@@ -4269,101 +4241,101 @@
                 "torch"
             ],
             "module": "modelscope.models.cv.vidt.model"
         },
         "('MODELS', 'image-paintbyexample', 'Stablediffusion-Paintbyexample')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_paintbyexample/model.py",
             "imports": [
-                "paint_ldm",
-                "torch",
                 "omegaconf",
+                "os",
                 "typing",
-                "os"
+                "torch",
+                "paint_ldm"
             ],
             "module": "modelscope.models.cv.image_paintbyexample.model"
         },
         "('MODELS', 'image-portrait-enhancement', 'gpen')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_portrait_enhancement/image_portrait_enhancement.py",
             "imports": [
+                "os",
                 "torch",
-                "typing",
                 "math",
-                "os"
+                "typing"
             ],
             "module": "modelscope.models.cv.image_portrait_enhancement.image_portrait_enhancement"
         },
         "('MODELS', 'image-quality-assessment-degradation', 'image-quality-assessment-degradation')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_quality_assessment_degradation/image_quality_assessment_degradation.py",
             "imports": [
+                "os",
                 "torch",
-                "typing",
-                "os"
+                "typing"
             ],
             "module": "modelscope.models.cv.image_quality_assessment_degradation.image_quality_assessment_degradation"
         },
         "('MODELS', 'image-quality-assessment-mos', 'image-quality-assessment-man')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_quality_assessment_man/image_quality_assessment_man.py",
             "imports": [
+                "os",
                 "torch",
-                "typing",
-                "os"
+                "typing"
             ],
             "module": "modelscope.models.cv.image_quality_assessment_man.image_quality_assessment_man"
         },
         "('MODELS', 'image-quality-assessment-mos', 'image-quality-assessment-mos')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_quality_assessment_mos/image_quality_assessment_mos.py",
             "imports": [
+                "os",
                 "torch",
-                "typing",
-                "os"
+                "typing"
             ],
             "module": "modelscope.models.cv.image_quality_assessment_mos.image_quality_assessment_mos"
         },
         "('MODELS', 'image-reid-person', 'passvitb')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_reid_person/pass_model.py",
             "imports": [
-                "enum",
                 "os",
+                "enum",
                 "torch"
             ],
             "module": "modelscope.models.cv.image_reid_person.pass_model"
         },
         "('MODELS', 'image-segmentation', 'cascade_mask_rcnn_swin')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_instance_segmentation/model.py",
             "imports": [
+                "os",
                 "torch",
-                "typing",
-                "os"
+                "typing"
             ],
             "module": "modelscope.models.cv.image_instance_segmentation.model"
         },
         "('MODELS', 'image-segmentation', 'fastinst')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_instance_segmentation/fastinst_model.py",
             "imports": [
-                "typing",
+                "os",
                 "torch",
-                "os"
+                "typing"
             ],
             "module": "modelscope.models.cv.image_instance_segmentation.fastinst_model"
         },
         "('MODELS', 'image-segmentation', 'm2fp')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_human_parsing/m2fp_net.py",
             "imports": [
-                "typing",
+                "os",
                 "torch",
-                "os"
+                "typing"
             ],
             "module": "modelscope.models.cv.image_human_parsing.m2fp_net"
         },
         "('MODELS', 'image-segmentation', 'maskdino_swin')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino_model.py",
             "imports": [
+                "os",
                 "torch",
-                "typing",
-                "os"
+                "typing"
             ],
             "module": "modelscope.models.cv.image_instance_segmentation.maskdino_model"
         },
         "('MODELS', 'image-segmentation', 'swinL-panoptic-segmentation')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_panoptic_segmentation/panseg_model.py",
             "imports": [
                 "os",
@@ -4379,18 +4351,18 @@
                 "numpy"
             ],
             "module": "modelscope.models.cv.image_semantic_segmentation.semantic_seg_model"
         },
         "('MODELS', 'image-segmentation', 'vision-middleware')": {
             "filepath": "TEMPLATE_PATH/models/cv/vision_middleware/model.py",
             "imports": [
-                "typing",
-                "json",
+                "os",
                 "torch",
-                "os"
+                "json",
+                "typing"
             ],
             "module": "modelscope.models.cv.vision_middleware.model"
         },
         "('MODELS', 'image-segmentation', 'vitadapter-semantic-segmentation')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_semantic_segmentation/semantic_seg_model.py",
             "imports": [
                 "os",
@@ -4398,54 +4370,54 @@
                 "numpy"
             ],
             "module": "modelscope.models.cv.image_semantic_segmentation.semantic_seg_model"
         },
         "('MODELS', 'image-skychange', 'image-skychange')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_skychange/skychange_model.py",
             "imports": [
-                "math",
-                "torch",
                 "collections",
-                "json",
-                "time",
-                "cv2",
                 "pdb",
+                "os",
+                "math",
+                "time",
                 "typing",
-                "os"
+                "torch",
+                "cv2",
+                "json"
             ],
             "module": "modelscope.models.cv.image_skychange.skychange_model"
         },
         "('MODELS', 'image-super-resolution', 'ecbsr')": {
             "filepath": "TEMPLATE_PATH/models/cv/super_resolution/ecbsr_model.py",
             "imports": [
-                "typing",
+                "os",
                 "torch",
-                "os"
+                "typing"
             ],
             "module": "modelscope.models.cv.super_resolution.ecbsr_model"
         },
         "('MODELS', 'image-text-retrieval', 'mplug')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/mplug_for_all_tasks.py",
             "imports": [
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.multi_modal.mplug_for_all_tasks"
         },
         "('MODELS', 'image-try-on', 'image-try-on')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_try_on/try_on_infer.py",
             "imports": [
+                "os",
                 "yaml",
+                "argparse",
                 "torch",
-                "torchvision",
                 "PIL",
-                "argparse",
                 "cv2",
-                "numpy",
-                "os"
+                "torchvision",
+                "numpy"
             ],
             "module": "modelscope.models.cv.image_try_on.try_on_infer"
         },
         "('MODELS', 'indoor-layout-estimation', 'panovit-layout-estimation')": {
             "filepath": "TEMPLATE_PATH/models/cv/indoor_layout_estimation/panovit.py",
             "imports": [
                 "os",
@@ -4454,214 +4426,214 @@
                 "yacs"
             ],
             "module": "modelscope.models.cv.indoor_layout_estimation.panovit"
         },
         "('MODELS', 'information-extraction', 'information-extraction')": {
             "filepath": "TEMPLATE_PATH/models/nlp/task_models/information_extraction.py",
             "imports": [
-                "typing",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.models.nlp.task_models.information_extraction"
         },
         "('MODELS', 'inverse-text-processing', 'generic-itn')": {
             "filepath": "TEMPLATE_PATH/models/audio/itn/generic_inverse_text_processing.py",
             "imports": [
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.audio.itn.generic_inverse_text_processing"
         },
         "('MODELS', 'keyword-spotting', 'kws-kwsbp')": {
             "filepath": "TEMPLATE_PATH/models/audio/kws/generic_key_word_spotting.py",
             "imports": [
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.audio.kws.generic_key_word_spotting"
         },
         "('MODELS', 'keyword-spotting', 'speech_dfsmn_kws_char_farfield')": {
             "filepath": "TEMPLATE_PATH/models/audio/kws/farfield/model.py",
             "imports": [
+                "os",
                 "tempfile",
-                "typing",
-                "os"
+                "typing"
             ],
             "module": "modelscope.models.audio.kws.farfield.model"
         },
         "('MODELS', 'keyword-spotting', 'speech_dfsmn_kws_char_farfield_iot')": {
             "filepath": "TEMPLATE_PATH/models/audio/kws/farfield/model.py",
             "imports": [
+                "os",
                 "tempfile",
-                "typing",
-                "os"
+                "typing"
             ],
             "module": "modelscope.models.audio.kws.farfield.model"
         },
         "('MODELS', 'keyword-spotting', 'speech_kws_fsmn_char_ctc_nearfield')": {
             "filepath": "TEMPLATE_PATH/models/audio/kws/nearfield/model.py",
             "imports": [
-                "sys",
+                "os",
+                "typing",
                 "torch",
                 "tempfile",
-                "typing",
-                "os"
+                "sys"
             ],
             "module": "modelscope.models.audio.kws.nearfield.model"
         },
         "('MODELS', 'language-guided-video-summarization', 'clip-it-language-guided-video-summarization')": {
             "filepath": "TEMPLATE_PATH/models/cv/language_guided_video_summarization/summarizer.py",
             "imports": [
-                "torch",
-                "bmt_clipit",
-                "videofeatures_clipit",
-                "argparse",
+                "os",
                 "typing",
+                "argparse",
+                "videofeatures_clipit",
+                "torch",
                 "numpy",
-                "os"
+                "bmt_clipit"
             ],
             "module": "modelscope.models.cv.language_guided_video_summarization.summarizer"
         },
         "('MODELS', 'language-score-prediction', 'generic-asr')": {
             "filepath": "TEMPLATE_PATH/models/audio/asr/generic_automatic_speech_recognition.py",
             "imports": [
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.audio.asr.generic_automatic_speech_recognition"
         },
         "('MODELS', 'lineless-table-recognition', 'LoreModel')": {
             "filepath": "TEMPLATE_PATH/models/cv/table_recognition/model_lore.py",
             "imports": [
-                "math",
-                "numpy",
-                "torch",
+                "os",
                 "copy",
                 "typing",
-                "os"
+                "math",
+                "torch",
+                "numpy"
             ],
             "module": "modelscope.models.cv.table_recognition.model_lore"
         },
         "('MODELS', 'movie-scene-segmentation', 'resnet50-bert')": {
             "filepath": "TEMPLATE_PATH/models/cv/movie_scene_segmentation/model.py",
             "imports": [
+                "einops",
+                "os",
+                "tqdm",
                 "math",
-                "numpy",
-                "torch",
+                "typing",
                 "shotdetect_scenedetect_lgss",
-                "torchvision",
+                "torch",
                 "PIL",
-                "typing",
-                "einops",
-                "os",
-                "tqdm"
+                "torchvision",
+                "numpy"
             ],
             "module": "modelscope.models.cv.movie_scene_segmentation.model"
         },
         "('MODELS', 'multi-modal-embedding', 'clip-multi-modal-embedding')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/clip/model.py",
             "imports": [
-                "numpy",
-                "torch",
                 "collections",
-                "json",
+                "os",
                 "typing",
-                "os"
+                "torch",
+                "numpy",
+                "json"
             ],
             "module": "modelscope.models.multi_modal.clip.model"
         },
         "('MODELS', 'multi-modal-similarity', 'team-multi-modal-similarity')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/team/team_model.py",
             "imports": [
                 "tokenizers",
+                "typing",
                 "torch",
-                "torchvision",
                 "PIL",
                 "cv2",
-                "typing",
+                "torchvision",
                 "numpy"
             ],
             "module": "modelscope.models.multi_modal.team.team_model"
         },
         "('MODELS', 'multimodal-dialogue', 'mplug-owl')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/mplug_owl/modeling_mplug_owl.py",
             "imports": [
-                "typing",
-                "math",
-                "dataclasses",
-                "torch",
-                "copy",
                 "logging",
-                "random",
-                "transformers",
                 "os",
-                "io"
+                "transformers",
+                "copy",
+                "dataclasses",
+                "math",
+                "typing",
+                "torch",
+                "io",
+                "random"
             ],
             "module": "modelscope.models.multi_modal.mplug_owl.modeling_mplug_owl"
         },
         "('MODELS', 'named-entity-recognition', 'lstm-crf')": {
             "filepath": "TEMPLATE_PATH/models/nlp/lstm/token_classification.py",
             "imports": [],
             "module": "modelscope.models.nlp.lstm.token_classification"
         },
         "('MODELS', 'named-entity-recognition', 'token-classification-for-ner')": {
             "filepath": "TEMPLATE_PATH/models/nlp/task_models/token_classification.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.nlp.task_models.token_classification"
         },
         "('MODELS', 'named-entity-recognition', 'transformer-crf')": {
             "filepath": "TEMPLATE_PATH/models/nlp/task_models/token_classification.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.nlp.task_models.token_classification"
         },
         "('MODELS', 'nerf-recon-4k', 'nerf-recon-4k')": {
             "filepath": "TEMPLATE_PATH/models/cv/nerf_recon_4k/nerf_recon_4k.py",
             "imports": [
-                "torch",
-                "argparse",
+                "mmcv",
+                "os",
+                "tqdm",
                 "time",
+                "argparse",
+                "torch",
                 "imageio",
-                "mmcv",
                 "random",
-                "numpy",
-                "os",
-                "tqdm"
+                "numpy"
             ],
             "module": "modelscope.models.cv.nerf_recon_4k.nerf_recon_4k"
         },
         "('MODELS', 'nerf-recon-acc', 'nerf-recon-acc')": {
             "filepath": "TEMPLATE_PATH/models/cv/nerf_recon_acc/nerf_recon_acc.py",
             "imports": [
-                "glob",
-                "torch",
+                "os",
+                "tqdm",
                 "time",
+                "torch",
+                "glob",
                 "cv2",
-                "numpy",
-                "os",
-                "tqdm"
+                "numpy"
             ],
             "module": "modelscope.models.cv.nerf_recon_acc.nerf_recon_acc"
         },
         "('MODELS', 'nerf-recon-vq-compression', 'nerf-recon-vq-compression')": {
             "filepath": "TEMPLATE_PATH/models/cv/nerf_recon_vq_compression/nerf_recon_vq_compression.py",
             "imports": [
-                "glob",
-                "torch",
-                "time",
-                "cv2",
                 "functools",
-                "numpy",
                 "os",
-                "tqdm"
+                "tqdm",
+                "time",
+                "torch",
+                "glob",
+                "cv2",
+                "numpy"
             ],
             "module": "modelscope.models.cv.nerf_recon_vq_compression.nerf_recon_vq_compression"
         },
         "('MODELS', 'nli', 'bert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/bert/text_classification.py",
             "imports": [],
             "module": "modelscope.models.nlp.bert.text_classification"
@@ -4672,16 +4644,16 @@
                 "torch"
             ],
             "module": "modelscope.models.multi_modal.mgeo.text_classification"
         },
         "('MODELS', 'nli', 'peer')": {
             "filepath": "TEMPLATE_PATH/models/nlp/peer/text_classification.py",
             "imports": [
-                "torch",
-                "copy"
+                "copy",
+                "torch"
             ],
             "module": "modelscope.models.nlp.peer.text_classification"
         },
         "('MODELS', 'nli', 'plug-mental')": {
             "filepath": "TEMPLATE_PATH/models/nlp/plug_mental/text_classification.py",
             "imports": [
                 "torch"
@@ -4701,83 +4673,83 @@
                 "transformers"
             ],
             "module": "modelscope.models.nlp.veco.text_classification"
         },
         "('MODELS', 'object-detection-3d', 'depe')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/depe_detect.py",
             "imports": [
-                "typing",
-                "numpy",
+                "os",
                 "torch",
-                "os"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.depe_detect"
         },
         "('MODELS', 'ocr-detection', 'OCRDetection')": {
             "filepath": "TEMPLATE_PATH/models/cv/ocr_detection/model.py",
             "imports": [
-                "typing",
-                "numpy",
+                "os",
                 "torch",
-                "os"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.models.cv.ocr_detection.model"
         },
         "('MODELS', 'ocr-recognition', 'OCRRecognition')": {
             "filepath": "TEMPLATE_PATH/models/cv/ocr_recognition/model.py",
             "imports": [
                 "os",
                 "torch"
             ],
             "module": "modelscope.models.cv.ocr_recognition.model"
         },
         "('MODELS', 'ocr-recognition', 'ofa')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/ofa_for_all_tasks.py",
             "imports": [
-                "re",
-                "math",
-                "torch",
-                "json",
-                "string",
                 "functools",
+                "string",
+                "os",
                 "typing",
-                "os"
+                "math",
+                "torch",
+                "re",
+                "json"
             ],
             "module": "modelscope.models.multi_modal.ofa_for_all_tasks"
         },
         "('MODELS', 'open-vocabulary-detection', 'open-vocabulary-detection-vild')": {
             "filepath": "TEMPLATE_PATH/models/cv/open_vocabulary_detection_vild/vild.py",
             "imports": [
-                "numpy",
+                "os",
                 "clip",
-                "torch",
                 "scipy",
                 "typing",
-                "os",
-                "tensorflow"
+                "tensorflow",
+                "torch",
+                "numpy"
             ],
             "module": "modelscope.models.cv.open_vocabulary_detection_vild.vild"
         },
         "('MODELS', 'panorama-depth-estimation', 's2net-depth-estimation')": {
             "filepath": "TEMPLATE_PATH/models/cv/s2net_panorama_depth_estimation/s2net_model.py",
             "imports": [
-                "torchvision",
-                "numpy",
+                "os",
                 "torch",
-                "os"
+                "torchvision",
+                "numpy"
             ],
             "module": "modelscope.models.cv.s2net_panorama_depth_estimation.s2net_model"
         },
         "('MODELS', 'panorama-depth-estimation', 'unifuse-depth-estimation')": {
             "filepath": "TEMPLATE_PATH/models/cv/panorama_depth_estimation/unifuse_model.py",
             "imports": [
-                "torchvision",
-                "numpy",
+                "os",
                 "torch",
-                "os"
+                "torchvision",
+                "numpy"
             ],
             "module": "modelscope.models.cv.panorama_depth_estimation.unifuse_model"
         },
         "('MODELS', 'part-of-speech', 'bert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/bert/token_classification.py",
             "imports": [],
             "module": "modelscope.models.nlp.bert.token_classification"
@@ -4800,119 +4772,119 @@
                 "torch"
             ],
             "module": "modelscope.models.nlp.structbert.token_classification"
         },
         "('MODELS', 'part-of-speech', 'token-classification')": {
             "filepath": "TEMPLATE_PATH/models/nlp/task_models/token_classification.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.nlp.task_models.token_classification"
         },
         "('MODELS', 'part-of-speech', 'transformer-crf')": {
             "filepath": "TEMPLATE_PATH/models/nlp/task_models/token_classification.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.nlp.task_models.token_classification"
         },
         "('MODELS', 'pedestrian-attribute-recognition', 'pedestrian-attribute-recognition')": {
             "filepath": "TEMPLATE_PATH/models/cv/pedestrian_attribute_recognition/model.py",
             "imports": [
-                "torchvision",
-                "numpy",
+                "os",
                 "torch",
-                "os"
+                "torchvision",
+                "numpy"
             ],
             "module": "modelscope.models.cv.pedestrian_attribute_recognition.model"
         },
         "('MODELS', 'pointcloud-sceneflow-estimation', 'rcp-sceneflow-estimation')": {
             "filepath": "TEMPLATE_PATH/models/cv/pointcloud_sceneflow_estimation/rcp_model.py",
             "imports": [
-                "numpy",
+                "os",
                 "torch",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.models.cv.pointcloud_sceneflow_estimation.rcp_model"
         },
         "('MODELS', 'product-retrieval-embedding', 'product-retrieval-embedding')": {
             "filepath": "TEMPLATE_PATH/models/cv/product_retrieval_embedding/item_model.py",
             "imports": [
-                "typing",
-                "numpy",
+                "os",
                 "torch",
-                "os"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.models.cv.product_retrieval_embedding.item_model"
         },
         "('MODELS', 'product-segmentation', 'product-segmentation')": {
             "filepath": "TEMPLATE_PATH/models/cv/product_segmentation/seg_infer.py",
             "imports": [
-                "PIL",
                 "numpy",
                 "torch",
-                "cv2"
+                "cv2",
+                "PIL"
             ],
             "module": "modelscope.models.cv.product_segmentation.seg_infer"
         },
         "('MODELS', 'protein-structure', 'unifold')": {
             "filepath": "TEMPLATE_PATH/models/science/unifold/model.py",
             "imports": [
                 "os",
-                "typing",
                 "torch",
-                "argparse"
+                "argparse",
+                "typing"
             ],
             "module": "modelscope.models.science.unifold.model"
         },
         "('MODELS', 'punctuation', 'generic-punc')": {
             "filepath": "TEMPLATE_PATH/models/audio/punc/generic_punctuation.py",
             "imports": [
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.audio.punc.generic_punctuation"
         },
         "('MODELS', 'referring-video-object-segmentation', 'swinT-referring-video-object-segmentation')": {
             "filepath": "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/model.py",
             "imports": [
-                "typing",
+                "os",
                 "torch",
-                "os"
+                "typing"
             ],
             "module": "modelscope.models.cv.referring_video_object_segmentation.model"
         },
         "('MODELS', 'relation-extraction', 'information-extraction')": {
             "filepath": "TEMPLATE_PATH/models/nlp/task_models/information_extraction.py",
             "imports": [
-                "typing",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.models.nlp.task_models.information_extraction"
         },
         "('MODELS', 'semantic-segmentation', 'ddpm')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_semantic_segmentation/ddpm_segmentation_model.py",
             "imports": [
-                "ddpm_guided_diffusion",
-                "typing",
+                "os",
                 "torch",
-                "os"
+                "ddpm_guided_diffusion",
+                "typing"
             ],
             "module": "modelscope.models.cv.image_semantic_segmentation.ddpm_segmentation_model"
         },
         "('MODELS', 'semantic-segmentation', 'detection')": {
             "filepath": "TEMPLATE_PATH/models/cv/salient_detection/salient_model.py",
             "imports": [
+                "os",
                 "torch",
-                "torchvision",
                 "PIL",
                 "cv2",
-                "os"
+                "torchvision"
             ],
             "module": "modelscope.models.cv.salient_detection.salient_model"
         },
         "('MODELS', 'sentence-embedding', 'bert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/bert/sentence_embedding.py",
             "imports": [
                 "torch"
@@ -4930,16 +4902,16 @@
                 "torch"
             ],
             "module": "modelscope.models.multi_modal.mgeo.text_classification"
         },
         "('MODELS', 'sentence-similarity', 'peer')": {
             "filepath": "TEMPLATE_PATH/models/nlp/peer/text_classification.py",
             "imports": [
-                "torch",
-                "copy"
+                "copy",
+                "torch"
             ],
             "module": "modelscope.models.nlp.peer.text_classification"
         },
         "('MODELS', 'sentence-similarity', 'plug-mental')": {
             "filepath": "TEMPLATE_PATH/models/nlp/plug_mental/text_classification.py",
             "imports": [
                 "torch"
@@ -4971,16 +4943,16 @@
                 "torch"
             ],
             "module": "modelscope.models.multi_modal.mgeo.text_classification"
         },
         "('MODELS', 'sentiment-classification', 'peer')": {
             "filepath": "TEMPLATE_PATH/models/nlp/peer/text_classification.py",
             "imports": [
-                "torch",
-                "copy"
+                "copy",
+                "torch"
             ],
             "module": "modelscope.models.nlp.peer.text_classification"
         },
         "('MODELS', 'sentiment-classification', 'plug-mental')": {
             "filepath": "TEMPLATE_PATH/models/nlp/plug_mental/text_classification.py",
             "imports": [
                 "torch"
@@ -5000,60 +4972,60 @@
                 "transformers"
             ],
             "module": "modelscope.models.nlp.veco.text_classification"
         },
         "('MODELS', 'shop-segmentation', 'shop-segmentation')": {
             "filepath": "TEMPLATE_PATH/models/cv/shop_segmentation/shop_seg_model.py",
             "imports": [
-                "numpy",
-                "torch",
-                "PIL",
                 "os",
                 "typing",
+                "torch",
+                "PIL",
+                "numpy",
                 "json"
             ],
             "module": "modelscope.models.cv.shop_segmentation.shop_seg_model"
         },
         "('MODELS', 'siamese-uie', 'bert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/bert/siamese_uie.py",
             "imports": [
-                "torch",
-                "copy"
+                "copy",
+                "torch"
             ],
             "module": "modelscope.models.nlp.bert.siamese_uie"
         },
         "('MODELS', 'speaker-diarization', 'cluster-backend')": {
             "filepath": "TEMPLATE_PATH/models/audio/sv/cluster_backend.py",
             "imports": [
-                "umap",
-                "numpy",
-                "sklearn",
                 "hdbscan",
+                "sklearn",
                 "scipy",
-                "typing"
+                "umap",
+                "typing",
+                "numpy"
             ],
             "module": "modelscope.models.audio.sv.cluster_backend"
         },
         "('MODELS', 'speaker-diarization', 'generic-sv')": {
             "filepath": "TEMPLATE_PATH/models/audio/sv/generic_speaker_verification.py",
             "imports": [
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.audio.sv.generic_speaker_verification"
         },
         "('MODELS', 'speaker-diarization', 'scl-sd')": {
             "filepath": "TEMPLATE_PATH/models/audio/sv/speaker_change_locator.py",
             "imports": [
-                "numpy",
-                "torchaudio",
-                "torch",
                 "collections",
+                "os",
                 "typing",
-                "os"
+                "torch",
+                "torchaudio",
+                "numpy"
             ],
             "module": "modelscope.models.audio.sv.speaker_change_locator"
         },
         "('MODELS', 'speaker-diarization-dialogue-detection', 'bert')": {
             "filepath": "TEMPLATE_PATH/models/audio/sv/speaker_diarization_dialogue_detection.py",
             "imports": [
                 "torch"
@@ -5080,162 +5052,162 @@
                 "torch"
             ],
             "module": "modelscope.models.audio.sv.speaker_diarization_semantic_speaker_turn_detection"
         },
         "('MODELS', 'speaker-verification', 'cam++-sv')": {
             "filepath": "TEMPLATE_PATH/models/audio/sv/DTDNN.py",
             "imports": [
-                "numpy",
-                "torchaudio",
-                "torch",
                 "collections",
+                "os",
                 "typing",
-                "os"
+                "torch",
+                "torchaudio",
+                "numpy"
             ],
             "module": "modelscope.models.audio.sv.DTDNN"
         },
         "('MODELS', 'speaker-verification', 'ecapa-tdnn-sv')": {
             "filepath": "TEMPLATE_PATH/models/audio/sv/ecapa_tdnn.py",
             "imports": [
+                "os",
+                "typing",
                 "math",
-                "numpy",
-                "torchaudio",
                 "torch",
-                "typing",
-                "os"
+                "torchaudio",
+                "numpy"
             ],
             "module": "modelscope.models.audio.sv.ecapa_tdnn"
         },
         "('MODELS', 'speaker-verification', 'eres2net-aug-sv')": {
             "filepath": "TEMPLATE_PATH/models/audio/sv/ERes2Net_aug.py",
             "imports": [
+                "os",
+                "typing",
                 "math",
-                "torchaudio",
                 "torch",
-                "typing",
-                "os"
+                "torchaudio"
             ],
             "module": "modelscope.models.audio.sv.ERes2Net_aug"
         },
         "('MODELS', 'speaker-verification', 'eres2net-sv')": {
             "filepath": "TEMPLATE_PATH/models/audio/sv/ERes2Net.py",
             "imports": [
+                "os",
+                "typing",
                 "math",
-                "torchaudio",
                 "torch",
-                "typing",
-                "os"
+                "torchaudio"
             ],
             "module": "modelscope.models.audio.sv.ERes2Net"
         },
         "('MODELS', 'speaker-verification', 'generic-sv')": {
             "filepath": "TEMPLATE_PATH/models/audio/sv/generic_speaker_verification.py",
             "imports": [
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.audio.sv.generic_speaker_verification"
         },
         "('MODELS', 'speaker-verification', 'rdino_ecapa-tdnn-sv')": {
             "filepath": "TEMPLATE_PATH/models/audio/sv/rdino.py",
             "imports": [
+                "os",
                 "math",
-                "torchaudio",
-                "torch",
                 "typing",
-                "os"
+                "torch",
+                "torchaudio"
             ],
             "module": "modelscope.models.audio.sv.rdino"
         },
         "('MODELS', 'speech-language-recognition', 'cam++-lre')": {
             "filepath": "TEMPLATE_PATH/models/audio/sv/lanuage_recognition_model.py",
             "imports": [
-                "numpy",
-                "torchaudio",
-                "torch",
+                "os",
                 "typing",
-                "os"
+                "torch",
+                "torchaudio",
+                "numpy"
             ],
             "module": "modelscope.models.audio.sv.lanuage_recognition_model"
         },
         "('MODELS', 'speech-separation', 'speech_mossformer_separation_temporal_8k')": {
             "filepath": "TEMPLATE_PATH/models/audio/separation/mossformer.py",
             "imports": [
-                "typing",
-                "torch",
+                "os",
                 "copy",
-                "os"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.audio.separation.mossformer"
         },
         "('MODELS', 'speech-timestamp', 'generic-asr')": {
             "filepath": "TEMPLATE_PATH/models/audio/asr/generic_automatic_speech_recognition.py",
             "imports": [
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.audio.asr.generic_automatic_speech_recognition"
         },
         "('MODELS', 'sudoku', 'ofa')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/ofa_for_all_tasks.py",
             "imports": [
-                "re",
-                "math",
-                "torch",
-                "json",
-                "string",
                 "functools",
+                "string",
+                "os",
                 "typing",
-                "os"
+                "math",
+                "torch",
+                "re",
+                "json"
             ],
             "module": "modelscope.models.multi_modal.ofa_for_all_tasks"
         },
         "('MODELS', 'table-question-answering', 'space-T-cn')": {
             "filepath": "TEMPLATE_PATH/models/nlp/space_T_cn/table_question_answering.py",
             "imports": [
-                "numpy",
-                "torch",
+                "os",
                 "transformers",
                 "typing",
-                "os"
+                "torch",
+                "numpy"
             ],
             "module": "modelscope.models.nlp.space_T_cn.table_question_answering"
         },
         "('MODELS', 'table-question-answering', 'space-T-en')": {
             "filepath": "TEMPLATE_PATH/models/nlp/space_T_en/text_to_sql.py",
             "imports": [
                 "os",
-                "typing",
                 "torch",
-                "text2sql_lgesql"
+                "text2sql_lgesql",
+                "typing"
             ],
             "module": "modelscope.models.nlp.space_T_en.text_to_sql"
         },
         "('MODELS', 'task-oriented-conversation', 'space-dst')": {
             "filepath": "TEMPLATE_PATH/models/nlp/space/dialog_state_tracking.py",
             "imports": [
                 "transformers",
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.nlp.space.dialog_state_tracking"
         },
         "('MODELS', 'task-oriented-conversation', 'space-intent')": {
             "filepath": "TEMPLATE_PATH/models/nlp/space/dialog_intent_prediction.py",
             "imports": [
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.nlp.space.dialog_intent_prediction"
         },
         "('MODELS', 'task-oriented-conversation', 'space-modeling')": {
             "filepath": "TEMPLATE_PATH/models/nlp/space/dialog_modeling.py",
             "imports": [
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.nlp.space.dialog_modeling"
         },
         "('MODELS', 'text-classification', 'bert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/bert/text_classification.py",
             "imports": [],
             "module": "modelscope.models.nlp.bert.text_classification"
@@ -5246,30 +5218,30 @@
                 "torch"
             ],
             "module": "modelscope.models.multi_modal.mgeo.text_classification"
         },
         "('MODELS', 'text-classification', 'ofa')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/ofa_for_all_tasks.py",
             "imports": [
-                "re",
-                "math",
-                "torch",
-                "json",
-                "string",
                 "functools",
+                "string",
+                "os",
                 "typing",
-                "os"
+                "math",
+                "torch",
+                "re",
+                "json"
             ],
             "module": "modelscope.models.multi_modal.ofa_for_all_tasks"
         },
         "('MODELS', 'text-classification', 'peer')": {
             "filepath": "TEMPLATE_PATH/models/nlp/peer/text_classification.py",
             "imports": [
-                "torch",
-                "copy"
+                "copy",
+                "torch"
             ],
             "module": "modelscope.models.nlp.peer.text_classification"
         },
         "('MODELS', 'text-classification', 'plug-mental')": {
             "filepath": "TEMPLATE_PATH/models/nlp/plug_mental/text_classification.py",
             "imports": [
                 "torch"
@@ -5282,154 +5254,144 @@
                 "torch"
             ],
             "module": "modelscope.models.nlp.structbert.text_classification"
         },
         "('MODELS', 'text-classification', 'text-classification')": {
             "filepath": "TEMPLATE_PATH/models/nlp/task_models/text_classification.py",
             "imports": [
-                "typing",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.models.nlp.task_models.text_classification"
         },
         "('MODELS', 'text-classification', 'user-satisfaction-estimation')": {
             "filepath": "TEMPLATE_PATH/models/nlp/use/user_satisfaction_estimation.py",
             "imports": [
-                "numpy",
-                "torch",
+                "os",
                 "transformers",
                 "typing",
-                "os"
+                "torch",
+                "numpy"
             ],
             "module": "modelscope.models.nlp.use.user_satisfaction_estimation"
         },
         "('MODELS', 'text-classification', 'veco')": {
             "filepath": "TEMPLATE_PATH/models/nlp/veco/text_classification.py",
             "imports": [
                 "transformers"
             ],
             "module": "modelscope.models.nlp.veco.text_classification"
         },
         "('MODELS', 'text-driven-segmentation', 'text-driven-segmentation')": {
             "filepath": "TEMPLATE_PATH/models/cv/text_driven_segmentation/lseg_model.py",
             "imports": [
-                "numpy",
-                "torch",
-                "PIL",
                 "os",
                 "typing",
+                "torch",
+                "PIL",
+                "numpy",
                 "json"
             ],
             "module": "modelscope.models.cv.text_driven_segmentation.lseg_model"
         },
         "('MODELS', 'text-error-correction', 'bart')": {
             "filepath": "TEMPLATE_PATH/models/nlp/bart/text_error_correction.py",
             "imports": [
+                "os",
                 "torch",
-                "typing",
-                "os"
+                "typing"
             ],
             "module": "modelscope.models.nlp.bart.text_error_correction"
         },
         "('MODELS', 'text-generation', 'glm130b')": {
             "filepath": "TEMPLATE_PATH/models/nlp/glm_130b/text_generation.py",
             "imports": [
-                "re",
-                "sys",
-                "torch",
-                "SwissArmyTransformer",
                 "functools",
+                "os",
                 "copy",
-                "time",
                 "typing",
+                "time",
+                "torch",
+                "stat",
+                "re",
+                "SwissArmyTransformer",
                 "random",
-                "os",
-                "stat"
+                "sys"
             ],
             "module": "modelscope.models.nlp.glm_130b.text_generation"
         },
         "('MODELS', 'text-generation', 'gpt-moe')": {
             "filepath": "TEMPLATE_PATH/models/nlp/gpt_moe/text_generation.py",
             "imports": [
                 "transformers",
                 "typing"
             ],
             "module": "modelscope.models.nlp.gpt_moe.text_generation"
         },
         "('MODELS', 'text-generation', 'gpt3')": {
             "filepath": "TEMPLATE_PATH/models/nlp/gpt3/text_generation.py",
             "imports": [
-                "transformers",
-                "typing",
+                "collections",
                 "torch",
-                "collections"
+                "transformers",
+                "typing"
             ],
             "module": "modelscope.models.nlp.gpt3.text_generation"
         },
         "('MODELS', 'text-generation', 'llama')": {
             "filepath": "TEMPLATE_PATH/models/nlp/llama/text_generation.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.nlp.llama.text_generation"
         },
         "('MODELS', 'text-generation', 'llama2')": {
             "filepath": "TEMPLATE_PATH/models/nlp/llama2/text_generation.py",
             "imports": [
                 "transformers",
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.nlp.llama2.text_generation"
         },
         "('MODELS', 'text-generation', 'palm-v2')": {
             "filepath": "TEMPLATE_PATH/models/nlp/palm_v2/text_generation.py",
             "imports": [
-                "math",
-                "numpy",
-                "dataclasses",
-                "torch",
+                "os",
+                "transformers",
                 "copy",
+                "math",
                 "typing",
-                "transformers",
-                "os",
-                "json",
+                "codecs",
+                "torch",
+                "dataclasses",
                 "subprocess",
-                "codecs"
+                "numpy",
+                "json"
             ],
             "module": "modelscope.models.nlp.palm_v2.text_generation"
         },
         "('MODELS', 'text-generation', 'polylm')": {
             "filepath": "TEMPLATE_PATH/models/nlp/polylm/text_generation.py",
             "imports": [
-                "transformers",
-                "typing",
+                "collections",
                 "torch",
-                "collections"
-            ],
-            "module": "modelscope.models.nlp.polylm.text_generation"
-        },
-        "('MODELS', 'text-generation', 'qwen-7b')": {
-            "filepath": "TEMPLATE_PATH/models/nlp/qwen/text_generation.py",
-            "imports": [
-                "typing",
                 "transformers",
-                "warnings",
-                "torch"
+                "typing"
             ],
-            "module": "modelscope.models.nlp.qwen.text_generation"
+            "module": "modelscope.models.nlp.polylm.text_generation"
         },
         "('MODELS', 'text-generation', 'text-generation')": {
             "filepath": "TEMPLATE_PATH/models/nlp/task_models/text_generation.py",
             "imports": [
-                "typing",
                 "transformers",
+                "torch",
                 "numpy",
-                "torch"
+                "typing"
             ],
             "module": "modelscope.models.nlp.task_models.text_generation"
         },
         "('MODELS', 'text-ranking', 'bert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/bert/text_ranking.py",
             "imports": [],
             "module": "modelscope.models.nlp.bert.text_ranking"
@@ -5440,146 +5402,146 @@
                 "torch"
             ],
             "module": "modelscope.models.multi_modal.mgeo.text_ranking"
         },
         "('MODELS', 'text-ranking', 'text-ranking')": {
             "filepath": "TEMPLATE_PATH/models/nlp/task_models/text_ranking.py",
             "imports": [
-                "typing",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.models.nlp.task_models.text_ranking"
         },
         "('MODELS', 'text-summarization', 'mglm')": {
             "filepath": "TEMPLATE_PATH/models/nlp/mglm/mglm_for_text_summarization.py",
             "imports": [
                 "megatron_util",
-                "numpy",
+                "os",
+                "typing",
                 "torch",
                 "random",
-                "typing",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.models.nlp.mglm.mglm_for_text_summarization"
         },
         "('MODELS', 'text-summarization', 'ofa')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/ofa_for_all_tasks.py",
             "imports": [
-                "re",
-                "math",
-                "torch",
-                "json",
-                "string",
                 "functools",
+                "string",
+                "os",
                 "typing",
-                "os"
+                "math",
+                "torch",
+                "re",
+                "json"
             ],
             "module": "modelscope.models.multi_modal.ofa_for_all_tasks"
         },
         "('MODELS', 'text-to-image-synthesis', 'diffusion-text-to-image-synthesis')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/diffusion/model.py",
             "imports": [
-                "numpy",
-                "torch",
                 "os",
                 "typing",
+                "torch",
+                "numpy",
                 "json"
             ],
             "module": "modelscope.models.multi_modal.diffusion.model"
         },
         "('MODELS', 'text-to-image-synthesis', 'multi-stage-diffusion-text-to-image-synthesis')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/model.py",
             "imports": [
+                "os",
                 "math",
-                "numpy",
+                "typing",
                 "torch",
                 "PIL",
-                "typing",
-                "os",
+                "numpy",
                 "json"
             ],
             "module": "modelscope.models.multi_modal.multi_stage_diffusion.model"
         },
         "('MODELS', 'text-to-image-synthesis', 'ofa')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/ofa_for_text_to_image_synthesis_model.py",
             "imports": [
                 "taming",
-                "numpy",
+                "os",
+                "typing",
+                "pkg_resources",
                 "torch",
                 "PIL",
                 "torchvision",
-                "json",
-                "pkg_resources",
-                "typing",
-                "os"
+                "numpy",
+                "json"
             ],
             "module": "modelscope.models.multi_modal.ofa_for_text_to_image_synthesis_model"
         },
         "('MODELS', 'text-to-image-synthesis', 'stable-diffusion')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/stable_diffusion/stable_diffusion.py",
             "imports": [
-                "diffusers",
-                "torch",
                 "functools",
+                "os",
                 "transformers",
                 "typing",
-                "os",
+                "torch",
+                "diffusers",
                 "packaging"
             ],
             "module": "modelscope.models.multi_modal.stable_diffusion.stable_diffusion"
         },
         "('MODELS', 'text-to-speech', 'sambert-hifigan')": {
             "filepath": "TEMPLATE_PATH/models/audio/tts/sambert_hifi.py",
             "imports": [
-                "yaml",
-                "__future__",
                 "wave",
-                "numpy",
                 "zipfile",
-                "json",
-                "matplotlib",
                 "datetime",
+                "os",
+                "__future__",
+                "matplotlib",
+                "yaml",
                 "shutil",
-                "os"
+                "numpy",
+                "json"
             ],
             "module": "modelscope.models.audio.tts.sambert_hifi"
         },
         "('MODELS', 'text-to-video-synthesis', 'latent-text-to-video-synthesis')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/video_synthesis/text_to_video_synthesis_model.py",
             "imports": [
-                "torch",
-                "open_clip",
                 "einops",
+                "os",
+                "open_clip",
                 "typing",
-                "os"
+                "torch"
             ],
             "module": "modelscope.models.multi_modal.video_synthesis.text_to_video_synthesis_model"
         },
         "('MODELS', 'text2sql', 'ofa')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/ofa_for_all_tasks.py",
             "imports": [
-                "re",
-                "math",
-                "torch",
-                "json",
-                "string",
                 "functools",
+                "string",
+                "os",
                 "typing",
-                "os"
+                "math",
+                "torch",
+                "re",
+                "json"
             ],
             "module": "modelscope.models.multi_modal.ofa_for_all_tasks"
         },
         "('MODELS', 'text2text-generation', 'T5')": {
             "filepath": "TEMPLATE_PATH/models/nlp/T5/text2text_generation.py",
             "imports": [
-                "torch",
+                "transformers",
                 "warnings",
                 "copy",
-                "transformers",
-                "typing"
+                "typing",
+                "torch"
             ],
             "module": "modelscope.models.nlp.T5.text2text_generation"
         },
         "('MODELS', 'token-classification', 'bert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/bert/token_classification.py",
             "imports": [],
             "module": "modelscope.models.nlp.bert.token_classification"
@@ -5602,258 +5564,258 @@
                 "torch"
             ],
             "module": "modelscope.models.nlp.structbert.token_classification"
         },
         "('MODELS', 'token-classification', 'token-classification')": {
             "filepath": "TEMPLATE_PATH/models/nlp/task_models/token_classification.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.nlp.task_models.token_classification"
         },
         "('MODELS', 'token-classification', 'transformer-crf')": {
             "filepath": "TEMPLATE_PATH/models/nlp/task_models/token_classification.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.nlp.task_models.token_classification"
         },
         "('MODELS', 'token-classification', 'transformer-crf-for-word-segmentation')": {
             "filepath": "TEMPLATE_PATH/models/nlp/task_models/token_classification.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.nlp.task_models.token_classification"
         },
         "('MODELS', 'token-classification', 'veco')": {
             "filepath": "TEMPLATE_PATH/models/nlp/veco/token_classification.py",
             "imports": [
                 "transformers",
                 "torch"
             ],
             "module": "modelscope.models.nlp.veco.token_classification"
         },
         "('MODELS', 'transformer-crf', 'transformer-crf')": {
             "filepath": "TEMPLATE_PATH/models/nlp/task_models/token_classification.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.nlp.task_models.token_classification"
         },
         "('MODELS', 'translation', 'csanmt-translation')": {
             "filepath": "TEMPLATE_PATH/models/nlp/csanmt/translation.py",
             "imports": [
+                "collections",
                 "tensorflow",
-                "typing",
                 "math",
-                "collections"
+                "typing"
             ],
             "module": "modelscope.models.nlp.csanmt.translation"
         },
         "('MODELS', 'translation-evaluation', 'unite')": {
             "filepath": "TEMPLATE_PATH/models/nlp/unite/translation_evaluation.py",
             "imports": [
-                "math",
-                "dataclasses",
-                "torch",
+                "transformers",
                 "warnings",
+                "math",
                 "typing",
-                "transformers",
-                "numpy",
-                "packaging"
+                "torch",
+                "packaging",
+                "dataclasses",
+                "numpy"
             ],
             "module": "modelscope.models.nlp.unite.translation_evaluation"
         },
         "('MODELS', 'video-captioning', 'hitea')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/mplug_for_all_tasks.py",
             "imports": [
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.multi_modal.mplug_for_all_tasks"
         },
         "('MODELS', 'video-deinterlace', 'video-deinterlace')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_deinterlace/UNet_for_video_deinterlace.py",
             "imports": [
-                "typing",
-                "torch",
+                "os",
                 "copy",
-                "os"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.cv.video_deinterlace.UNet_for_video_deinterlace"
         },
         "('MODELS', 'video-depth-estimation', 'dro-resnet18-depth-estimation')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_depth_estimation/dro_model.py",
             "imports": [
-                "glob",
+                "os",
+                "tqdm",
                 "torch",
+                "glob",
                 "cv2",
-                "numpy",
-                "os",
-                "tqdm"
+                "numpy"
             ],
             "module": "modelscope.models.cv.video_depth_estimation.dro_model"
         },
         "('MODELS', 'video-frame-interpolation', 'video-frame-interpolation')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_frame_interpolation/VFINet_for_video_frame_interpolation.py",
             "imports": [
-                "typing",
-                "torch",
+                "os",
                 "copy",
-                "os"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.cv.video_frame_interpolation.VFINet_for_video_frame_interpolation"
         },
         "('MODELS', 'video-human-matting', 'video-human-matting')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_human_matting/model.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
                 "torch",
                 "torchvision",
-                "typing",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.models.cv.video_human_matting.model"
         },
         "('MODELS', 'video-inpainting', 'video-inpainting')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_inpainting/inpainting_model.py",
             "imports": [
                 "torch",
                 "torchvision",
-                "math",
-                "numpy"
+                "numpy",
+                "math"
             ],
             "module": "modelscope.models.cv.video_inpainting.inpainting_model"
         },
         "('MODELS', 'video-instance-segmentation', 'swinb-video-instance-segmentation')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_instance_segmentation/video_knet.py",
             "imports": [
-                "torch",
-                "mmdet"
+                "mmdet",
+                "torch"
             ],
             "module": "modelscope.models.cv.video_instance_segmentation.video_knet"
         },
         "('MODELS', 'video-multi-modal-embedding', 'video-clip-multi-modal-embedding')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/mmr/models/clip_for_mm_video_embedding.py",
             "imports": [
-                "uuid",
-                "numpy",
-                "decord",
                 "urllib",
+                "os",
+                "typing",
+                "decord",
                 "torch",
+                "uuid",
                 "PIL",
                 "tempfile",
-                "typing",
                 "random",
-                "os",
+                "numpy",
                 "json"
             ],
             "module": "modelscope.models.multi_modal.mmr.models.clip_for_mm_video_embedding"
         },
         "('MODELS', 'video-object-detection', 'longshortnet')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_streaming_perception/longshortnet/longshortnet.py",
             "imports": [
-                "torch",
-                "json",
+                "logging",
+                "os",
+                "tqdm",
                 "time",
                 "argparse",
+                "torch",
                 "cv2",
-                "logging",
                 "numpy",
-                "os",
-                "tqdm"
+                "json"
             ],
             "module": "modelscope.models.cv.video_streaming_perception.longshortnet.longshortnet"
         },
         "('MODELS', 'video-object-detection', 'realtime-video-object-detection')": {
             "filepath": "TEMPLATE_PATH/models/cv/stream_yolo/realtime_video_detector.py",
             "imports": [
-                "torch",
-                "json",
+                "logging",
+                "os",
+                "tqdm",
                 "time",
                 "argparse",
+                "torch",
                 "cv2",
-                "logging",
                 "numpy",
-                "os",
-                "tqdm"
+                "json"
             ],
             "module": "modelscope.models.cv.stream_yolo.realtime_video_detector"
         },
         "('MODELS', 'video-object-segmentation', 'video-object-segmentation')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_object_segmentation/model.py",
             "imports": [
+                "os",
                 "torch",
-                "typing",
-                "os"
+                "typing"
             ],
             "module": "modelscope.models.cv.video_object_segmentation.model"
         },
         "('MODELS', 'video-panoptic-segmentation', 'swinb-video-panoptic-segmentation')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/video_k_net.py",
             "imports": [
-                "mmcv",
-                "numpy",
+                "mmdet",
                 "torch",
-                "mmdet"
+                "mmcv",
+                "numpy"
             ],
             "module": "modelscope.models.cv.video_panoptic_segmentation.video_k_net"
         },
         "('MODELS', 'video-question-answering', 'hitea')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/mplug_for_all_tasks.py",
             "imports": [
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.multi_modal.mplug_for_all_tasks"
         },
         "('MODELS', 'video-stabilization', 'video-stabilization')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_stabilization/DUTRAFTStabilizer.py",
             "imports": [
-                "math",
                 "numpy",
-                "sys",
+                "os",
+                "math",
+                "typing",
                 "torch",
                 "tempfile",
                 "cv2",
-                "typing",
-                "os"
+                "sys"
             ],
             "module": "modelscope.models.cv.video_stabilization.DUTRAFTStabilizer"
         },
         "('MODELS', 'video-summarization', 'pgl-video-summarization')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_summarization/summarizer.py",
             "imports": [
-                "typing",
-                "numpy",
+                "os",
                 "torch",
-                "os"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.models.cv.video_summarization.summarizer"
         },
         "('MODELS', 'video-super-resolution', 'msrresnet-lite')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_super_resolution/msrresnet_lite_model.py",
             "imports": [
+                "os",
                 "functools",
-                "typing",
                 "torch",
-                "os"
+                "typing"
             ],
             "module": "modelscope.models.cv.video_super_resolution.msrresnet_lite_model"
         },
         "('MODELS', 'video-super-resolution', 'real-basicvsr')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_super_resolution/real_basicvsr_for_video_super_resolution.py",
             "imports": [
+                "os",
                 "torch",
-                "typing",
-                "os"
+                "typing"
             ],
             "module": "modelscope.models.cv.video_super_resolution.real_basicvsr_for_video_super_resolution"
         },
         "('MODELS', 'video-temporal-grounding', 'soonet')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/soonet/model.py",
             "imports": [
                 "os",
@@ -5876,74 +5838,74 @@
                 "torch"
             ],
             "module": "modelscope.models.cv.vop_retrieval.model_se"
         },
         "('MODELS', 'vision-efficient-tuning', 'vision-efficient-tuning')": {
             "filepath": "TEMPLATE_PATH/models/cv/vision_efficient_tuning/model.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.cv.vision_efficient_tuning.model"
         },
         "('MODELS', 'visual-entailment', 'ofa')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/ofa_for_all_tasks.py",
             "imports": [
-                "re",
-                "math",
-                "torch",
-                "json",
-                "string",
                 "functools",
+                "string",
+                "os",
                 "typing",
-                "os"
+                "math",
+                "torch",
+                "re",
+                "json"
             ],
             "module": "modelscope.models.multi_modal.ofa_for_all_tasks"
         },
         "('MODELS', 'visual-grounding', 'ofa')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/ofa_for_all_tasks.py",
             "imports": [
-                "re",
-                "math",
-                "torch",
-                "json",
-                "string",
                 "functools",
+                "string",
+                "os",
                 "typing",
-                "os"
+                "math",
+                "torch",
+                "re",
+                "json"
             ],
             "module": "modelscope.models.multi_modal.ofa_for_all_tasks"
         },
         "('MODELS', 'visual-question-answering', 'mplug')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/mplug_for_all_tasks.py",
             "imports": [
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.multi_modal.mplug_for_all_tasks"
         },
         "('MODELS', 'visual-question-answering', 'ofa')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/ofa_for_all_tasks.py",
             "imports": [
-                "re",
-                "math",
-                "torch",
-                "json",
-                "string",
                 "functools",
+                "string",
+                "os",
                 "typing",
-                "os"
+                "math",
+                "torch",
+                "re",
+                "json"
             ],
             "module": "modelscope.models.multi_modal.ofa_for_all_tasks"
         },
         "('MODELS', 'voice-activity-detection', 'generic-asr')": {
             "filepath": "TEMPLATE_PATH/models/audio/asr/generic_automatic_speech_recognition.py",
             "imports": [
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.audio.asr.generic_automatic_speech_recognition"
         },
         "('MODELS', 'word-alignment', 'bert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/bert/word_alignment.py",
             "imports": [
                 "torch"
@@ -5978,24 +5940,24 @@
                 "torch"
             ],
             "module": "modelscope.models.nlp.structbert.token_classification"
         },
         "('MODELS', 'word-segmentation', 'transformer-crf')": {
             "filepath": "TEMPLATE_PATH/models/nlp/task_models/token_classification.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.nlp.task_models.token_classification"
         },
         "('MODELS', 'word-segmentation', 'transformer-crf-for-word-segmentation')": {
             "filepath": "TEMPLATE_PATH/models/nlp/task_models/token_classification.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.nlp.task_models.token_classification"
         },
         "('MODELS', 'zero-shot-classification', 'bert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/bert/text_classification.py",
             "imports": [],
             "module": "modelscope.models.nlp.bert.text_classification"
@@ -6006,16 +5968,16 @@
                 "torch"
             ],
             "module": "modelscope.models.multi_modal.mgeo.text_classification"
         },
         "('MODELS', 'zero-shot-classification', 'peer')": {
             "filepath": "TEMPLATE_PATH/models/nlp/peer/text_classification.py",
             "imports": [
-                "torch",
-                "copy"
+                "copy",
+                "torch"
             ],
             "module": "modelscope.models.nlp.peer.text_classification"
         },
         "('MODELS', 'zero-shot-classification', 'plug-mental')": {
             "filepath": "TEMPLATE_PATH/models/nlp/plug_mental/text_classification.py",
             "imports": [
                 "torch"
@@ -6028,233 +5990,221 @@
                 "torch"
             ],
             "module": "modelscope.models.nlp.structbert.text_classification"
         },
         "('NECKS', 'default', 'CPFPN')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/necks/cp_fpn.py",
             "imports": [
-                "mmcv",
+                "mmdet",
                 "torch",
-                "mmdet"
+                "mmcv"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.necks.cp_fpn"
         },
         "('NECKS', 'default', 'FPNF')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/necks/fpn.py",
             "imports": [
-                "mmcv",
+                "mmdet",
                 "torch",
-                "mmdet"
+                "mmcv"
             ],
             "module": "modelscope.models.cv.object_detection.mmdet_ms.necks.fpn"
         },
         "('NECKS', 'default', 'MSDeformAttnPixelDecoder')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_instance_segmentation/neck/msdeformattn_decoder.py",
             "imports": [
-                "mmcv",
+                "mmdet",
                 "torch",
-                "mmdet"
+                "mmcv"
             ],
             "module": "modelscope.models.cv.video_instance_segmentation.neck.msdeformattn_decoder"
         },
         "('NECKS', 'default', 'SemanticFPNWrapper')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/semantic_fpn_wrapper.py",
             "imports": [
-                "mmcv",
+                "mmdet",
                 "torch",
-                "mmdet"
+                "mmcv"
             ],
             "module": "modelscope.models.cv.video_panoptic_segmentation.head.semantic_fpn_wrapper"
         },
         "('PARALLEL', 'default', 'DistributedDataParallel')": {
             "filepath": "TEMPLATE_PATH/trainers/parallel/builder.py",
             "imports": [
                 "torch"
             ],
             "module": "modelscope.trainers.parallel.builder"
         },
         "('PIPELINES', 'acoustic-echo-cancellation', 'speech-dfsmn-aec-psm-16k')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/linear_aec_pipeline.py",
             "imports": [
+                "os",
+                "scipy",
+                "typing",
                 "yaml",
-                "numpy",
                 "importlib",
                 "torch",
-                "scipy",
-                "typing",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.pipelines.audio.linear_aec_pipeline"
         },
         "('PIPELINES', 'acoustic-noise-suppression', 'speech_dfsmn_ans_psm_48k_causal')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/ans_dfsmn_pipeline.py",
             "imports": [
-                "numpy",
-                "sys",
-                "torch",
                 "collections",
+                "librosa",
+                "numpy",
                 "soundfile",
-                "typing",
                 "os",
+                "typing",
+                "torch",
                 "io",
-                "librosa"
+                "sys"
             ],
             "module": "modelscope.pipelines.audio.ans_dfsmn_pipeline"
         },
         "('PIPELINES', 'acoustic-noise-suppression', 'speech_frcrn_ans_cirm_16k')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/ans_pipeline.py",
             "imports": [
-                "torch",
+                "librosa",
                 "soundfile",
                 "typing",
-                "numpy",
+                "torch",
                 "io",
-                "librosa"
+                "numpy"
             ],
             "module": "modelscope.pipelines.audio.ans_pipeline"
         },
         "('PIPELINES', 'action-detection', 'ResNetC3D-action-detection')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/action_detection_pipeline.py",
             "imports": [
+                "os",
                 "math",
-                "typing",
-                "os"
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.action_detection_pipeline"
         },
         "('PIPELINES', 'action-recognition', 'TAdaConv_action-recognition')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/action_recognition_pipeline.py",
             "imports": [
+                "os",
                 "torch",
-                "typing",
                 "math",
-                "os"
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.action_recognition_pipeline"
         },
         "('PIPELINES', 'action-recognition', 'patchshift-action-recognition')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/action_recognition_pipeline.py",
             "imports": [
+                "os",
                 "torch",
-                "typing",
                 "math",
-                "os"
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.action_recognition_pipeline"
         },
         "('PIPELINES', 'animal-recognition', 'resnet101-animal-recognition')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/animal_recognition_pipeline.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
                 "torch",
                 "PIL",
-                "torchvision",
                 "cv2",
-                "typing",
-                "os"
+                "torchvision",
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.animal_recognition_pipeline"
         },
         "('PIPELINES', 'auto-speech-recognition', 'asr-inference')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/asr_inference_pipeline.py",
             "imports": [
+                "os",
                 "yaml",
-                "typing",
                 "json",
-                "os"
+                "typing"
             ],
             "module": "modelscope.pipelines.audio.asr_inference_pipeline"
         },
         "('PIPELINES', 'auto-speech-recognition', 'asr-wenet-inference')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/asr_wenet_inference_pipeline.py",
             "imports": [
                 "typing"
             ],
             "module": "modelscope.pipelines.audio.asr_wenet_inference_pipeline"
         },
         "('PIPELINES', 'auto-speech-recognition', 'ofa-asr')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/asr_pipeline.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.pipelines.multi_modal.asr_pipeline"
         },
         "('PIPELINES', 'bad-image-detecting', 'bad-image-detecting')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/bad_image_detecting_pipeline.py",
             "imports": [
-                "typing",
                 "torch",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.bad_image_detecting_pipeline"
         },
         "('PIPELINES', 'body-2d-keypoints', 'hrnetv2w32_body-2d-keypoints_image')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/body_2d_keypoints_pipeline.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
                 "torch",
                 "PIL",
-                "torchvision",
                 "cv2",
-                "typing",
-                "os",
+                "torchvision",
+                "numpy",
                 "json"
             ],
             "module": "modelscope.pipelines.cv.body_2d_keypoints_pipeline"
         },
         "('PIPELINES', 'body-3d-keypoints', 'canonical_body-3d-keypoints_video')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/body_3d_keypoints_pipeline.py",
             "imports": [
-                "numpy",
+                "datetime",
+                "os",
                 "mpl_toolkits",
+                "matplotlib",
+                "typing",
                 "torch",
                 "tempfile",
-                "matplotlib",
                 "cv2",
-                "typing",
-                "datetime",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.body_3d_keypoints_pipeline"
         },
         "('PIPELINES', 'card-detection', 'resnet-card-detection-scrfd34gkps')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/card_detection_pipeline.py",
             "imports": [
                 "typing"
             ],
             "module": "modelscope.pipelines.cv.card_detection_pipeline"
         },
         "('PIPELINES', 'chat', 'chatglm2_6b-text-generation')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/text_generation_pipeline.py",
             "imports": [
-                "transformers",
-                "typing",
+                "os",
                 "torch",
-                "os"
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.text_generation_pipeline"
         },
         "('PIPELINES', 'chat', 'chatglm6b-text-generation')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/text_generation_pipeline.py",
             "imports": [
-                "transformers",
-                "typing",
-                "torch",
-                "os"
-            ],
-            "module": "modelscope.pipelines.nlp.text_generation_pipeline"
-        },
-        "('PIPELINES', 'chat', 'qwen-chat')": {
-            "filepath": "TEMPLATE_PATH/pipelines/nlp/text_generation_pipeline.py",
-            "imports": [
-                "transformers",
-                "typing",
+                "os",
                 "torch",
-                "os"
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.text_generation_pipeline"
         },
         "('PIPELINES', 'code-generation', 'codegeex-code-generation')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/codegeex_code_generation_pipeline.py",
             "imports": [
                 "typing"
@@ -6268,54 +6218,54 @@
             ],
             "module": "modelscope.pipelines.nlp.codegeex_code_translation_pipeline"
         },
         "('PIPELINES', 'competency-aware-translation', 'canmt-translation')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/canmt_translation_pipeline.py",
             "imports": [
                 "os",
-                "typing",
+                "sacremoses",
                 "torch",
-                "sacremoses"
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.canmt_translation_pipeline"
         },
         "('PIPELINES', 'controllable-image-generation', 'controllable-image-generation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/controllable_image_generation_pipeline.py",
             "imports": [
-                "glob",
+                "os",
+                "tempfile",
                 "math",
-                "numpy",
+                "typing",
                 "torch",
-                "tempfile",
+                "glob",
                 "cv2",
-                "typing",
-                "os",
-                "subprocess"
+                "subprocess",
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.controllable_image_generation_pipeline"
         },
         "('PIPELINES', 'crowd-counting', 'hrnet-crowd-counting')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/crowd_counting_pipeline.py",
             "imports": [
                 "math",
+                "typing",
                 "torch",
-                "torchvision",
                 "PIL",
-                "typing",
+                "torchvision",
                 "numpy"
             ],
             "module": "modelscope.pipelines.cv.crowd_counting_pipeline"
         },
         "('PIPELINES', 'default', 'DefaultFormatBundleV2')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/formating.py",
             "imports": [
-                "mmcv",
-                "numpy",
+                "mmdet",
                 "torch",
-                "mmdet"
+                "mmcv",
+                "numpy"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.mmdet_patch.datasets.pipelines.formating"
         },
         "('PIPELINES', 'default', 'LoadAnnotationsV2')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/loading.py",
             "imports": [
                 "os",
@@ -6324,1279 +6274,1279 @@
                 "numpy"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.mmdet_patch.datasets.pipelines.loading"
         },
         "('PIPELINES', 'default', 'LoadMultiViewImageFromMultiSweepsFiles')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/loading.py",
             "imports": [
+                "mmdet",
                 "mmcv",
-                "numpy",
-                "mmdet"
+                "numpy"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.datasets.pipelines.loading"
         },
         "('PIPELINES', 'default', 'NormalizeMultiviewImage')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/transform_3d.py",
             "imports": [
-                "mmdet3d",
+                "torch",
+                "mmcv",
                 "PIL",
+                "mmdet",
                 "copy",
-                "mmcv",
-                "numpy",
-                "torch",
-                "mmdet"
+                "mmdet3d",
+                "numpy"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.datasets.pipelines.transform_3d"
         },
         "('PIPELINES', 'default', 'PadMultiViewImage')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/transform_3d.py",
             "imports": [
-                "mmdet3d",
+                "torch",
+                "mmcv",
                 "PIL",
+                "mmdet",
                 "copy",
-                "mmcv",
-                "numpy",
-                "torch",
-                "mmdet"
+                "mmdet3d",
+                "numpy"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.datasets.pipelines.transform_3d"
         },
         "('PIPELINES', 'default', 'RandomFlipV2')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/transforms.py",
             "imports": [
+                "mmdet",
                 "mmcv",
-                "numpy",
-                "mmdet"
+                "numpy"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.mmdet_patch.datasets.pipelines.transforms"
         },
         "('PIPELINES', 'default', 'RandomSquareCrop')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/transforms.py",
             "imports": [
+                "mmdet",
                 "mmcv",
-                "numpy",
-                "mmdet"
+                "numpy"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.mmdet_patch.datasets.pipelines.transforms"
         },
         "('PIPELINES', 'default', 'ResizeCropFlipImage')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/transform_3d.py",
             "imports": [
-                "mmdet3d",
+                "torch",
+                "mmcv",
                 "PIL",
+                "mmdet",
                 "copy",
-                "mmcv",
-                "numpy",
-                "torch",
-                "mmdet"
+                "mmdet3d",
+                "numpy"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.datasets.pipelines.transform_3d"
         },
         "('PIPELINES', 'default', 'ResizeToMultiple')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/utils/data_process_func.py",
             "imports": [
-                "mmcv",
-                "mmdet"
+                "mmdet",
+                "mmcv"
             ],
             "module": "modelscope.models.cv.image_semantic_segmentation.vit_adapter.utils.data_process_func"
         },
         "('PIPELINES', 'default', 'ResizeV2')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/transforms.py",
             "imports": [
+                "mmdet",
                 "mmcv",
-                "numpy",
-                "mmdet"
+                "numpy"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.mmdet_patch.datasets.pipelines.transforms"
         },
         "('PIPELINES', 'default', 'RotateV2')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/auto_augment.py",
             "imports": [
-                "copy",
-                "cv2",
                 "mmcv",
+                "mmdet",
+                "copy",
                 "numpy",
-                "mmdet"
+                "cv2"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.mmdet_patch.datasets.pipelines.auto_augment"
         },
         "('PIPELINES', 'document-grounded-dialog-generate', 'document-grounded-dialog-generate')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/document_grounded_dialog_generate_pipeline.py",
             "imports": [
                 "typing"
             ],
             "module": "modelscope.pipelines.nlp.document_grounded_dialog_generate_pipeline"
         },
         "('PIPELINES', 'document-grounded-dialog-rerank', 'document-grounded-dialog-rerank')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/document_grounded_dialog_rerank_pipeline.py",
             "imports": [
-                "typing",
-                "re",
+                "collections",
                 "numpy",
+                "pprint",
                 "ujson",
-                "sys",
-                "torch",
-                "collections",
+                "os",
+                "transformers",
+                "typing",
                 "time",
-                "pprint",
+                "torch",
+                "re",
                 "random",
-                "transformers",
-                "os"
+                "sys"
             ],
             "module": "modelscope.pipelines.nlp.document_grounded_dialog_rerank_pipeline"
         },
         "('PIPELINES', 'document-grounded-dialog-retrieval', 'document-grounded-dialog-retrieval')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/document_grounded_dialog_retrieval_pipeline.py",
             "imports": [
-                "faiss",
-                "numpy",
                 "os",
                 "typing",
+                "faiss",
+                "numpy",
                 "json"
             ],
             "module": "modelscope.pipelines.nlp.document_grounded_dialog_retrieval_pipeline"
         },
         "('PIPELINES', 'document-segmentation', 'document-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/document_segmentation_pipeline.py",
             "imports": [
-                "re",
                 "numpy",
+                "typing",
                 "torch",
-                "datasets",
-                "typing"
+                "re",
+                "datasets"
             ],
             "module": "modelscope.pipelines.nlp.document_segmentation_pipeline"
         },
         "('PIPELINES', 'document-vl-embedding', 'document-vl-embedding')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/document_vl_embedding_pipeline.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.pipelines.multi_modal.document_vl_embedding_pipeline"
         },
         "('PIPELINES', 'domain-specific-object-detection', 'tinynas-detection')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/tinynas_detection_pipeline.py",
             "imports": [
                 "typing"
             ],
             "module": "modelscope.pipelines.cv.tinynas_detection_pipeline"
         },
         "('PIPELINES', 'efficient-diffusion-tuning', 'efficient-diffusion-tuning')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/efficient_diffusion_tuning_pipeline.py",
             "imports": [
+                "typing",
                 "torch",
-                "torchvision",
                 "PIL",
                 "cv2",
-                "typing",
+                "torchvision",
                 "numpy"
             ],
             "module": "modelscope.pipelines.multi_modal.efficient_diffusion_tuning_pipeline"
         },
         "('PIPELINES', 'extractive-summarization', 'extractive-summarization')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/extractive_summarization_pipeline.py",
             "imports": [
-                "re",
                 "numpy",
+                "typing",
                 "torch",
-                "datasets",
-                "typing"
+                "re",
+                "datasets"
             ],
             "module": "modelscope.pipelines.nlp.extractive_summarization_pipeline"
         },
         "('PIPELINES', 'face-2d-keypoints', 'manual-facial-landmark-confidence-flcm')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/facial_landmark_confidence_pipeline.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
                 "torch",
                 "PIL",
                 "cv2",
-                "typing",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.facial_landmark_confidence_pipeline"
         },
         "('PIPELINES', 'face-attribute-recognition', 'resnet34-face-attribute-recognition-fairface')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/face_attribute_recognition_pipeline.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
                 "torch",
                 "PIL",
                 "cv2",
-                "typing",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.face_attribute_recognition_pipeline"
         },
         "('PIPELINES', 'face-detection', 'manual-face-detection-mtcnn')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/mtcnn_face_detection_pipeline.py",
             "imports": [
+                "os",
                 "torch",
-                "typing",
-                "os"
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.mtcnn_face_detection_pipeline"
         },
         "('PIPELINES', 'face-detection', 'manual-face-detection-ulfd')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/ulfd_face_detection_pipeline.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
                 "torch",
                 "PIL",
                 "cv2",
-                "typing",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.ulfd_face_detection_pipeline"
         },
         "('PIPELINES', 'face-detection', 'resnet-face-detection-scrfd10gkps')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/face_detection_pipeline.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
                 "torch",
                 "PIL",
                 "cv2",
-                "typing",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.face_detection_pipeline"
         },
         "('PIPELINES', 'face-detection', 'resnet101-face-detection-cvpr22papermogface')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/mog_face_detection_pipeline.py",
             "imports": [
-                "typing",
                 "os",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.mog_face_detection_pipeline"
         },
         "('PIPELINES', 'face-detection', 'resnet50-face-detection-retinaface')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/retina_face_detection_pipeline.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
                 "torch",
                 "PIL",
                 "cv2",
-                "typing",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.retina_face_detection_pipeline"
         },
         "('PIPELINES', 'face-emotion', 'face-emotion')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/face_emotion_pipeline.py",
             "imports": [
-                "typing",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.face_emotion_pipeline"
         },
         "('PIPELINES', 'face-human-hand-detection', 'face-human-hand-detection')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/face_human_hand_detection_pipeline.py",
             "imports": [
-                "typing",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.face_human_hand_detection_pipeline"
         },
         "('PIPELINES', 'face-image-generation', 'gan-face-image-generation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/face_image_generation_pipeline.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
                 "torch",
                 "PIL",
                 "cv2",
-                "typing",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.face_image_generation_pipeline"
         },
         "('PIPELINES', 'face-liveness', 'manual-face-liveness-flir')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/face_liveness_ir_pipeline.py",
             "imports": [
-                "onnxruntime",
+                "os",
+                "typing",
                 "torch",
                 "PIL",
+                "onnxruntime",
                 "cv2",
-                "typing",
-                "numpy",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.face_liveness_ir_pipeline"
         },
         "('PIPELINES', 'face-liveness', 'manual-face-liveness-flxc')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/face_liveness_xc_pipeline.py",
             "imports": [
-                "onnxruntime",
+                "os",
+                "typing",
                 "torch",
                 "PIL",
+                "onnxruntime",
                 "cv2",
-                "typing",
-                "numpy",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.face_liveness_xc_pipeline"
         },
         "('PIPELINES', 'face-quality-assessment', 'manual-face-quality-assessment-fqa')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/face_quality_assessment_pipeline.py",
             "imports": [
-                "onnxruntime",
+                "os",
+                "typing",
                 "torch",
                 "PIL",
+                "onnxruntime",
                 "cv2",
-                "typing",
-                "numpy",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.face_quality_assessment_pipeline"
         },
         "('PIPELINES', 'face-recognition', 'ir-face-recognition-rts')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/face_recognition_ood_pipeline.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
                 "torch",
                 "PIL",
                 "cv2",
-                "typing",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.face_recognition_ood_pipeline"
         },
         "('PIPELINES', 'face-recognition', 'ir101-face-recognition-cfglint')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/face_recognition_pipeline.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
                 "torch",
                 "PIL",
                 "cv2",
-                "typing",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.face_recognition_pipeline"
         },
         "('PIPELINES', 'face-recognition', 'ir50-face-recognition-arcface')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/arc_face_recognition_pipeline.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
                 "torch",
                 "PIL",
                 "cv2",
-                "typing",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.arc_face_recognition_pipeline"
         },
         "('PIPELINES', 'face-recognition', 'manual-face-recognition-frfm')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/face_recognition_onnx_fm_pipeline.py",
             "imports": [
-                "onnxruntime",
+                "os",
+                "typing",
                 "torch",
                 "PIL",
+                "onnxruntime",
                 "cv2",
-                "typing",
-                "numpy",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.face_recognition_onnx_fm_pipeline"
         },
         "('PIPELINES', 'face-recognition', 'manual-face-recognition-frir')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/face_recognition_onnx_ir_pipeline.py",
             "imports": [
-                "onnxruntime",
+                "os",
+                "typing",
                 "torch",
                 "PIL",
+                "onnxruntime",
                 "cv2",
-                "typing",
-                "numpy",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.face_recognition_onnx_ir_pipeline"
         },
         "('PIPELINES', 'face-recognition', 'resnet-face-recognition-facemask')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/mask_face_recognition_pipeline.py",
             "imports": [
-                "torch",
                 "collections",
+                "os",
+                "typing",
+                "torch",
                 "PIL",
                 "cv2",
-                "typing",
-                "numpy",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.mask_face_recognition_pipeline"
         },
         "('PIPELINES', 'face-reconstruction', 'resnet50-face-reconstruction')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/face_reconstruction_pipeline.py",
             "imports": [
+                "os",
+                "scipy",
+                "typing",
                 "face_alignment",
-                "numpy",
                 "tensorflow",
                 "torch",
+                "io",
                 "PIL",
-                "scipy",
-                "cv2",
-                "typing",
                 "shutil",
-                "os",
-                "io"
+                "cv2",
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.face_reconstruction_pipeline"
         },
         "('PIPELINES', 'facial-expression-recognition', 'vgg19-facial-expression-recognition-fer')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/facial_expression_recognition_pipeline.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
                 "torch",
                 "PIL",
                 "cv2",
-                "typing",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.facial_expression_recognition_pipeline"
         },
         "('PIPELINES', 'faq-question-answering', 'faq-question-answering')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/faq_question_answering_pipeline.py",
             "imports": [
                 "typing"
             ],
             "module": "modelscope.pipelines.nlp.faq_question_answering_pipeline"
         },
         "('PIPELINES', 'feature-extraction', 'feature-extraction')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/feature_extraction_pipeline.py",
             "imports": [
+                "os",
                 "torch",
-                "typing",
-                "os"
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.feature_extraction_pipeline"
         },
         "('PIPELINES', 'fid-dialogue', 'fid-dialogue')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/fid_dialogue_pipeline.py",
             "imports": [
                 "re",
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.fid_dialogue_pipeline"
         },
         "('PIPELINES', 'fill-mask', 'fill-mask')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/fill_mask_pipeline.py",
             "imports": [
-                "typing",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.fill_mask_pipeline"
         },
         "('PIPELINES', 'fill-mask', 'fill-mask-ponet')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/fill_mask_pipeline.py",
             "imports": [
-                "typing",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.fill_mask_pipeline"
         },
         "('PIPELINES', 'general-recognition', 'resnet101-general-recognition')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/general_recognition_pipeline.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
                 "torch",
                 "PIL",
-                "torchvision",
                 "cv2",
-                "typing",
-                "os"
+                "torchvision",
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.general_recognition_pipeline"
         },
         "('PIPELINES', 'generative-multi-modal-embedding', 'generative-multi-modal-embedding')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/generative_multi_modal_embedding_pipeline.py",
             "imports": [
                 "typing"
             ],
             "module": "modelscope.pipelines.multi_modal.generative_multi_modal_embedding_pipeline"
         },
         "('PIPELINES', 'hand-static', 'hand-static')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/hand_static_pipeline.py",
             "imports": [
-                "typing",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.hand_static_pipeline"
         },
         "('PIPELINES', 'human-detection', 'resnet18-human-detection')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_detection_pipeline.py",
             "imports": [
-                "typing",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.image_detection_pipeline"
         },
         "('PIPELINES', 'human-reconstruction', 'human-reconstruction')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/human_reconstruction_pipeline.py",
             "imports": [
                 "trimesh",
-                "numpy",
-                "shutil",
-                "torch",
+                "os",
                 "typing",
-                "os"
+                "torch",
+                "shutil",
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.human_reconstruction_pipeline"
         },
         "('PIPELINES', 'image-body-reshaping', 'flow-based-body-reshaping')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_body_reshaping_pipeline.py",
             "imports": [
                 "typing"
             ],
             "module": "modelscope.pipelines.cv.image_body_reshaping_pipeline"
         },
         "('PIPELINES', 'image-captioning', 'image-captioning')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/image_captioning_pipeline.py",
             "imports": [
-                "typing",
                 "torch",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.multi_modal.image_captioning_pipeline"
         },
         "('PIPELINES', 'image-classification', 'bnext-small_image-classification_ImageNet-labels')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_classification_pipeline.py",
             "imports": [
-                "typing",
                 "torch",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.image_classification_pipeline"
         },
         "('PIPELINES', 'image-classification', 'common-image-classification')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_classification_pipeline.py",
             "imports": [
-                "typing",
                 "torch",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.image_classification_pipeline"
         },
         "('PIPELINES', 'image-classification', 'convnext-base_image-classification_garbage')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_classification_pipeline.py",
             "imports": [
-                "typing",
                 "torch",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.image_classification_pipeline"
         },
         "('PIPELINES', 'image-classification', 'easyrobust-classification')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_classification_pipeline.py",
             "imports": [
-                "typing",
                 "torch",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.image_classification_pipeline"
         },
         "('PIPELINES', 'image-classification', 'image-classification')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_classification_pipeline.py",
             "imports": [
-                "typing",
                 "torch",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.image_classification_pipeline"
         },
         "('PIPELINES', 'image-classification', 'image-structured-model-probing')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_structured_model_probing_pipeline.py",
             "imports": [
+                "mmcv",
+                "os",
                 "math",
-                "numpy",
+                "typing",
                 "torch",
                 "torchvision",
-                "mmcv",
-                "typing",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.image_structured_model_probing_pipeline"
         },
         "('PIPELINES', 'image-classification', 'nextvit-small_image-classification_Dailylife-labels')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_classification_pipeline.py",
             "imports": [
-                "typing",
                 "torch",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.image_classification_pipeline"
         },
         "('PIPELINES', 'image-classification', 'resnet50-image-classification-cc')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/content_check_pipeline.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
                 "torch",
                 "PIL",
-                "torchvision",
                 "cv2",
-                "typing",
-                "os"
+                "torchvision",
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.content_check_pipeline"
         },
         "('PIPELINES', 'image-classification', 'tinynas-classification')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/tinynas_classification_pipeline.py",
             "imports": [
+                "os",
                 "math",
-                "torch",
-                "torchvision",
                 "typing",
-                "os"
+                "torch",
+                "torchvision"
             ],
             "module": "modelscope.pipelines.cv.tinynas_classification_pipeline"
         },
         "('PIPELINES', 'image-classification', 'vit-base_image-classification_Dailylife-labels')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_classification_pipeline.py",
             "imports": [
-                "typing",
                 "torch",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.image_classification_pipeline"
         },
         "('PIPELINES', 'image-classification', 'vit-base_image-classification_ImageNet-labels')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_classification_pipeline.py",
             "imports": [
-                "typing",
                 "torch",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.image_classification_pipeline"
         },
         "('PIPELINES', 'image-color-enhancement', 'adaint-image-color-enhance')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_color_enhance_pipeline.py",
             "imports": [
+                "torch",
                 "torchvision",
-                "typing",
-                "torch"
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.image_color_enhance_pipeline"
         },
         "('PIPELINES', 'image-color-enhancement', 'csrnet-image-color-enhance')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_color_enhance_pipeline.py",
             "imports": [
+                "torch",
                 "torchvision",
-                "typing",
-                "torch"
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.image_color_enhance_pipeline"
         },
         "('PIPELINES', 'image-color-enhancement', 'deeplpf-image-color-enhance')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_color_enhance_pipeline.py",
             "imports": [
+                "torch",
                 "torchvision",
-                "typing",
-                "torch"
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.image_color_enhance_pipeline"
         },
         "('PIPELINES', 'image-colorization', 'ddcolor-image-colorization')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/ddcolor_image_colorization_pipeline.py",
             "imports": [
-                "numpy",
+                "typing",
                 "torch",
-                "torchvision",
                 "cv2",
-                "typing"
+                "torchvision",
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.ddcolor_image_colorization_pipeline"
         },
         "('PIPELINES', 'image-colorization', 'unet-image-colorization')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_colorization_pipeline.py",
             "imports": [
+                "typing",
                 "torch",
                 "PIL",
-                "torchvision",
                 "cv2",
-                "typing",
+                "torchvision",
                 "numpy"
             ],
             "module": "modelscope.pipelines.cv.image_colorization_pipeline"
         },
         "('PIPELINES', 'image-debanding', 'rrdb-image-debanding')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_debanding_pipeline.py",
             "imports": [
+                "torch",
                 "torchvision",
-                "typing",
-                "torch"
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.image_debanding_pipeline"
         },
         "('PIPELINES', 'image-deblurring', 'nafnet-image-deblur')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_deblur_pipeline.py",
             "imports": [
+                "torch",
                 "torchvision",
-                "typing",
-                "torch"
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.image_deblur_pipeline"
         },
         "('PIPELINES', 'image-demoireing', 'uhdm-image-demoireing')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_restoration_pipeline.py",
             "imports": [
                 "typing"
             ],
             "module": "modelscope.pipelines.cv.image_restoration_pipeline"
         },
         "('PIPELINES', 'image-denoising', 'nafnet-image-denoise')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_denoise_pipeline.py",
             "imports": [
+                "torch",
                 "torchvision",
-                "typing",
-                "torch"
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.image_denoise_pipeline"
         },
         "('PIPELINES', 'image-depth-estimation', 'image-bts-depth-estimation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_bts_depth_estimation_pipeline.py",
             "imports": [
-                "numpy",
-                "torch",
                 "albumentations",
+                "typing",
+                "torch",
                 "cv2",
-                "typing"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.image_bts_depth_estimation_pipeline"
         },
         "('PIPELINES', 'image-depth-estimation', 'image-depth-estimation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_depth_estimation_pipeline.py",
             "imports": [
-                "numpy",
+                "typing",
                 "torch",
                 "PIL",
                 "cv2",
-                "typing"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.image_depth_estimation_pipeline"
         },
         "('PIPELINES', 'image-driving-perception', 'yolopv2_image-driving-percetion_bdd100k')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_driving_perception_pipeline.py",
             "imports": [
+                "os",
                 "cv2",
-                "typing",
                 "numpy",
-                "os"
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.image_driving_perception_pipeline"
         },
         "('PIPELINES', 'image-face-fusion', 'image-face-fusion')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_face_fusion_pipeline.py",
             "imports": [
-                "typing",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.image_face_fusion_pipeline"
         },
         "('PIPELINES', 'image-fewshot-detection', 'image-fewshot-detection')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_defrcn_fewshot_pipeline.py",
             "imports": [
-                "typing",
-                "numpy",
+                "os",
                 "torch",
-                "os"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.image_defrcn_fewshot_pipeline"
         },
         "('PIPELINES', 'image-inpainting', 'fft-inpainting')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_inpainting_pipeline.py",
             "imports": [
-                "numpy",
+                "typing",
                 "torch",
                 "PIL",
                 "cv2",
-                "typing"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.image_inpainting_pipeline"
         },
         "('PIPELINES', 'image-inpainting', 'image-inpainting-sdv2')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_inpainting_sdv2_pipeline.py",
             "imports": [
-                "diffusers",
-                "math",
                 "numpy",
-                "sys",
+                "os",
+                "math",
+                "typing",
                 "torch",
+                "diffusers",
                 "tempfile",
                 "cv2",
-                "typing",
-                "os"
+                "sys"
             ],
             "module": "modelscope.pipelines.cv.image_inpainting_sdv2_pipeline"
         },
         "('PIPELINES', 'image-matching', 'image-matching')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_matching_pipeline.py",
             "imports": [
-                "numpy",
+                "typing",
                 "torch",
                 "PIL",
                 "cv2",
-                "typing"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.image_matching_pipeline"
         },
         "('PIPELINES', 'image-multi-view-depth-estimation', 'image-multi-view-depth-estimation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_mvs_depth_estimation_pipeline.py",
             "imports": [
-                "tempfile",
-                "typing",
                 "os",
-                "shutil"
+                "tempfile",
+                "shutil",
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.image_mvs_depth_estimation_pipeline"
         },
         "('PIPELINES', 'image-object-detection', 'abnormal-object-detection')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_detection_pipeline.py",
             "imports": [
-                "typing",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.image_detection_pipeline"
         },
         "('PIPELINES', 'image-object-detection', 'tbs-detection')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/tbs_detection_pipeline.py",
             "imports": [
-                "numpy",
-                "colorsys",
+                "os",
+                "typing",
                 "torch",
+                "colorsys",
                 "PIL",
                 "cv2",
-                "typing",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.tbs_detection_pipeline"
         },
         "('PIPELINES', 'image-object-detection', 'tinynas-detection')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/tinynas_detection_pipeline.py",
             "imports": [
                 "typing"
             ],
             "module": "modelscope.pipelines.cv.tinynas_detection_pipeline"
         },
         "('PIPELINES', 'image-object-detection', 'vidt')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/vidt_pipeline.py",
             "imports": [
+                "torch",
                 "torchvision",
-                "typing",
-                "torch"
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.vidt_pipeline"
         },
         "('PIPELINES', 'image-object-detection', 'vit-object-detection')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_detection_pipeline.py",
             "imports": [
-                "typing",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.image_detection_pipeline"
         },
         "('PIPELINES', 'image-paintbyexample', 'stablediffusion-paintbyexample')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_paintbyexample_pipeline.py",
             "imports": [
+                "einops",
+                "typing",
                 "torch",
-                "torchvision",
                 "PIL",
                 "cv2",
-                "typing",
-                "einops",
+                "torchvision",
                 "numpy"
             ],
             "module": "modelscope.pipelines.cv.image_paintbyexample_pipeline"
         },
         "('PIPELINES', 'image-portrait-enhancement', 'gpen-image-portrait-enhancement')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_portrait_enhancement_pipeline.py",
             "imports": [
+                "scipy",
                 "math",
+                "typing",
                 "torch",
                 "PIL",
-                "scipy",
                 "cv2",
-                "typing",
                 "numpy"
             ],
             "module": "modelscope.pipelines.cv.image_portrait_enhancement_pipeline"
         },
         "('PIPELINES', 'image-portrait-stylization', 'unet-person-image-cartoon')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_cartoon_pipeline.py",
             "imports": [
-                "numpy",
                 "os",
-                "cv2",
                 "typing",
-                "tensorflow"
+                "tensorflow",
+                "cv2",
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.image_cartoon_pipeline"
         },
         "('PIPELINES', 'image-quality-assessment-degradation', 'image-quality-assessment-degradation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_quality_assessment_degradation_pipeline.py",
             "imports": [
                 "math",
+                "typing",
                 "torch",
-                "torchvision",
                 "tempfile",
                 "cv2",
-                "typing",
+                "torchvision",
                 "numpy"
             ],
             "module": "modelscope.pipelines.cv.image_quality_assessment_degradation_pipeline"
         },
         "('PIPELINES', 'image-quality-assessment-mos', 'image-quality-assessment-man')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_quality_assessment_man_pipeline.py",
             "imports": [
                 "math",
+                "typing",
                 "torch",
-                "torchvision",
                 "tempfile",
                 "cv2",
-                "typing",
+                "torchvision",
                 "numpy"
             ],
             "module": "modelscope.pipelines.cv.image_quality_assessment_man_pipeline"
         },
         "('PIPELINES', 'image-quality-assessment-mos', 'image-quality-assessment-mos')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_quality_assessment_mos_pipeline.py",
             "imports": [
                 "math",
+                "typing",
                 "torch",
-                "torchvision",
                 "tempfile",
                 "cv2",
-                "typing",
+                "torchvision",
                 "numpy"
             ],
             "module": "modelscope.pipelines.cv.image_quality_assessment_mos_pipeline"
         },
         "('PIPELINES', 'image-reid-person', 'passvitb-image-reid-person')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_reid_person_pipeline.py",
             "imports": [
+                "os",
                 "math",
+                "typing",
                 "torch",
                 "PIL",
-                "torchvision",
-                "typing",
-                "os"
+                "torchvision"
             ],
             "module": "modelscope.pipelines.cv.image_reid_person_pipeline"
         },
         "('PIPELINES', 'image-segmentation', 'cascade-mask-rcnn-swin-image-instance-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_instance_segmentation_pipeline.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
                 "torch",
                 "PIL",
                 "cv2",
-                "typing",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.image_instance_segmentation_pipeline"
         },
         "('PIPELINES', 'image-segmentation', 'fast-instance-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/fast_instance_segmentation_pipeline.py",
             "imports": [
+                "torch",
                 "torchvision",
-                "typing",
                 "numpy",
-                "torch"
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.fast_instance_segmentation_pipeline"
         },
         "('PIPELINES', 'image-segmentation', 'image-panoptic-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_panoptic_segmentation_pipeline.py",
             "imports": [
-                "numpy",
+                "typing",
                 "torch",
                 "PIL",
                 "cv2",
-                "typing"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.image_panoptic_segmentation_pipeline"
         },
         "('PIPELINES', 'image-segmentation', 'image-semantic-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_semantic_segmentation_pipeline.py",
             "imports": [
-                "numpy",
+                "typing",
                 "torch",
                 "PIL",
                 "cv2",
-                "typing"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.image_semantic_segmentation_pipeline"
         },
         "('PIPELINES', 'image-segmentation', 'm2fp-image-human-parsing')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_human_parsing_pipeline.py",
             "imports": [
+                "torch",
                 "torchvision",
-                "typing",
                 "numpy",
-                "torch"
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.image_human_parsing_pipeline"
         },
         "('PIPELINES', 'image-segmentation', 'maskdino-swin-image-instance-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/maskdino_instance_segmentation_pipeline.py",
             "imports": [
+                "torch",
                 "torchvision",
-                "typing",
-                "torch"
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.maskdino_instance_segmentation_pipeline"
         },
         "('PIPELINES', 'image-segmentation', 'vision-middleware-multi-task')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/vision_middleware_pipeline.py",
             "imports": [
+                "mmcv",
+                "os",
                 "math",
-                "numpy",
+                "typing",
                 "torch",
                 "torchvision",
-                "mmcv",
-                "typing",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.vision_middleware_pipeline"
         },
         "('PIPELINES', 'image-skychange', 'image-skychange')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_skychange_pipeline.py",
             "imports": [
-                "numpy",
-                "PIL",
+                "pdb",
+                "typing",
                 "time",
+                "PIL",
                 "cv2",
-                "pdb",
-                "typing"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.image_skychange_pipeline"
         },
         "('PIPELINES', 'image-style-transfer', 'AAMS-style-transfer')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_style_transfer_pipeline.py",
             "imports": [
+                "os",
                 "cv2",
-                "typing",
                 "numpy",
-                "os"
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.image_style_transfer_pipeline"
         },
         "('PIPELINES', 'image-super-resolution', 'mobile-image-super-resolution')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/mobile_image_super_resolution_pipeline.py",
             "imports": [
-                "numpy",
+                "typing",
                 "torch",
+                "skimage",
                 "torchvision",
-                "typing",
-                "skimage"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.mobile_image_super_resolution_pipeline"
         },
         "('PIPELINES', 'image-super-resolution', 'rrdb-image-super-resolution')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_super_resolution_pipeline.py",
             "imports": [
-                "numpy",
+                "typing",
                 "torch",
                 "PIL",
                 "cv2",
-                "typing"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.image_super_resolution_pipeline"
         },
         "('PIPELINES', 'image-text-retrieval', 'image-text-retrieval')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/image_text_retrieval_pipeline.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.pipelines.multi_modal.image_text_retrieval_pipeline"
         },
         "('PIPELINES', 'image-text-retrieval', 'multi-modal-embedding')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/multi_modal_embedding_pipeline.py",
             "imports": [
                 "typing"
             ],
             "module": "modelscope.pipelines.multi_modal.multi_modal_embedding_pipeline"
         },
         "('PIPELINES', 'image-to-image-generation', 'image-to-image-generation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_to_image_generate_pipeline.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
                 "torch",
                 "PIL",
-                "torchvision",
                 "cv2",
-                "typing",
-                "os"
+                "torchvision",
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.image_to_image_generate_pipeline"
         },
         "('PIPELINES', 'image-to-image-translation', 'image-to-image-translation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_to_image_translation_pipeline.py",
             "imports": [
                 "numpy",
-                "sys",
+                "os",
+                "typing",
                 "torch",
-                "torchvision",
+                "io",
                 "PIL",
                 "cv2",
-                "typing",
-                "os",
-                "io"
+                "torchvision",
+                "sys"
             ],
             "module": "modelscope.pipelines.cv.image_to_image_translation_pipeline"
         },
         "('PIPELINES', 'image-try-on', 'image-try-on')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_try_on_pipeline.py",
             "imports": [
-                "typing",
                 "torch",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.image_try_on_pipeline"
         },
         "('PIPELINES', 'indoor-layout-estimation', 'indoor-layout-estimation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/indoor_layout_estimation_pipeline.py",
             "imports": [
                 "cv2",
-                "typing",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.indoor_layout_estimation_pipeline"
         },
         "('PIPELINES', 'information-extraction', 'relation-extraction')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/information_extraction_pipeline.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.information_extraction_pipeline"
         },
         "('PIPELINES', 'inverse-text-processing', 'itn-inference')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/inverse_text_processing_pipeline.py",
             "imports": [
+                "os",
                 "yaml",
                 "shutil",
-                "typing",
-                "os"
+                "typing"
             ],
             "module": "modelscope.pipelines.audio.inverse_text_processing_pipeline"
         },
         "('PIPELINES', 'keyword-spotting', 'kws-kwsbp')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/kws_kwsbp_pipeline.py",
             "imports": [
-                "typing",
                 "os",
-                "json"
+                "json",
+                "typing"
             ],
             "module": "modelscope.pipelines.audio.kws_kwsbp_pipeline"
         },
         "('PIPELINES', 'keyword-spotting', 'speech_dfsmn_kws_char_farfield')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/kws_farfield_pipeline.py",
             "imports": [
                 "wave",
-                "numpy",
                 "soundfile",
                 "typing",
-                "io"
+                "io",
+                "numpy"
             ],
             "module": "modelscope.pipelines.audio.kws_farfield_pipeline"
         },
         "('PIPELINES', 'language-guided-video-summarization', 'clip-it-video-summarization')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/language_guided_video_summarization_pipeline.py",
             "imports": [
-                "numpy",
+                "os",
                 "clip",
-                "shutil",
-                "torch",
-                "PIL",
                 "tempfile",
                 "cv2",
                 "typing",
+                "torch",
+                "PIL",
+                "shutil",
                 "random",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.language_guided_video_summarization_pipeline"
         },
         "('PIPELINES', 'language-score-prediction', 'language-score-prediction')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/lm_infer_pipeline.py",
             "imports": [
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.pipelines.audio.lm_infer_pipeline"
         },
         "('PIPELINES', 'license-plate-detection', 'resnet18-license-plate-detection')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/license_plate_detection_pipeline.py",
             "imports": [
+                "os",
                 "math",
-                "numpy",
+                "typing",
                 "torch",
                 "PIL",
                 "cv2",
-                "typing",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.license_plate_detection_pipeline"
         },
         "('PIPELINES', 'lineless-table-recognition', 'lore-lineless-table-recognition')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/lineless_table_recognition_pipeline.py",
             "imports": [
+                "os",
                 "math",
-                "numpy",
+                "typing",
                 "torch",
                 "PIL",
                 "cv2",
-                "typing",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.lineless_table_recognition_pipeline"
         },
         "('PIPELINES', 'live-category', 'live-category')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/live_category_pipeline.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
                 "decord",
                 "torch",
-                "torchvision",
                 "PIL",
-                "typing",
-                "os"
+                "torchvision",
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.live_category_pipeline"
         },
         "('PIPELINES', 'motion-generation', 'mdm-motion-generation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/motion_generation_pipeline.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
                 "torch",
                 "tempfile",
-                "typing",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.motion_generation_pipeline"
         },
         "('PIPELINES', 'movie-scene-segmentation', 'resnet50-bert-movie-scene-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/movie_scene_segmentation_pipeline.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.movie_scene_segmentation_pipeline"
         },
         "('PIPELINES', 'multi-modal-embedding', 'gridvlp-multi-modal-embedding')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/gridvlp_pipeline.py",
             "imports": [
-                "traceback",
-                "numpy",
+                "os",
+                "transformers",
+                "typing",
+                "time",
                 "torch",
+                "traceback",
                 "PIL",
-                "time",
-                "typing",
-                "transformers",
-                "os",
+                "numpy",
                 "json"
             ],
             "module": "modelscope.pipelines.multi_modal.gridvlp_pipeline"
         },
         "('PIPELINES', 'multi-modal-embedding', 'multi-modal-embedding')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/multi_modal_embedding_pipeline.py",
             "imports": [
@@ -7610,16 +7560,16 @@
                 "typing"
             ],
             "module": "modelscope.pipelines.multi_modal.team_multi_modal_similarity_pipeline"
         },
         "('PIPELINES', 'multimodal-dialogue', 'multimodal-dialogue')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/multimodal_dialogue_pipeline.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.pipelines.multi_modal.multimodal_dialogue_pipeline"
         },
         "('PIPELINES', 'named-entity-recognition', 'named-entity-recognition')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/named_entity_recognition_pipeline.py",
             "imports": [
                 "typing"
@@ -7660,211 +7610,211 @@
                 "typing"
             ],
             "module": "modelscope.pipelines.cv.nerf_recon_vq_compression_pipeline"
         },
         "('PIPELINES', 'nli', 'nli')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/text_classification_pipeline.py",
             "imports": [
-                "typing",
                 "torch",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.text_classification_pipeline"
         },
         "('PIPELINES', 'object-detection-3d', 'object-detection-3d-depe')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/object_detection_3d_pipeline.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
                 "torch",
                 "PIL",
                 "tempfile",
                 "cv2",
-                "typing",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.object_detection_3d_pipeline"
         },
         "('PIPELINES', 'ocr-detection', 'resnet18-ocr-detection')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/ocr_detection_pipeline.py",
             "imports": [
+                "os",
+                "typing",
                 "math",
                 "tensorflow",
                 "torch",
                 "tf_slim",
                 "cv2",
-                "typing",
-                "numpy",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.ocr_detection_pipeline"
         },
         "('PIPELINES', 'ocr-recognition', 'convnextTiny-ocr-recognition')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/ocr_recognition_pipeline.py",
             "imports": [],
             "module": "modelscope.pipelines.cv.ocr_recognition_pipeline"
         },
         "('PIPELINES', 'ocr-recognition', 'ofa-ocr-recognition')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/ocr_recognition_pipeline.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.pipelines.multi_modal.ocr_recognition_pipeline"
         },
         "('PIPELINES', 'open-vocabulary-detection', 'open-vocabulary-detection-vild')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_open_vocabulary_detection_pipeline.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
                 "torch",
                 "PIL",
                 "cv2",
-                "typing",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.image_open_vocabulary_detection_pipeline"
         },
         "('PIPELINES', 'panorama-depth-estimation', 'panorama-depth-estimation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/panorama_depth_estimation_pipeline.py",
             "imports": [
-                "numpy",
+                "typing",
                 "torch",
                 "PIL",
                 "cv2",
-                "typing"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.panorama_depth_estimation_pipeline"
         },
         "('PIPELINES', 'panorama-depth-estimation', 'panorama-depth-estimation-s2net')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/panorama_depth_estimation_s2net_pipeline.py",
             "imports": [
-                "numpy",
+                "typing",
                 "torch",
                 "PIL",
                 "cv2",
-                "typing"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.panorama_depth_estimation_s2net_pipeline"
         },
         "('PIPELINES', 'part-of-speech', 'part-of-speech')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/token_classification_pipeline.py",
             "imports": [
-                "typing",
                 "torch",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.token_classification_pipeline"
         },
         "('PIPELINES', 'pedestrian-attribute-recognition', 'resnet50_pedestrian-attribute-recognition_image')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/pedestrian_attribute_recognition_pipeline.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
                 "torch",
                 "PIL",
-                "torchvision",
                 "cv2",
-                "typing",
-                "os",
+                "torchvision",
+                "numpy",
                 "json"
             ],
             "module": "modelscope.pipelines.cv.pedestrian_attribute_recognition_pipeline"
         },
         "('PIPELINES', 'pointcloud-sceneflow-estimation', 'pointcloud-sceneflow-estimation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/pointcloud_sceneflow_estimation_pipeline.py",
             "imports": [
-                "typing",
                 "plyfile",
                 "numpy",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.pointcloud_sceneflow_estimation_pipeline"
         },
         "('PIPELINES', 'portrait-matting', 'unet-image-matting')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_matting_pipeline.py",
             "imports": [
-                "numpy",
                 "os",
-                "cv2",
                 "typing",
-                "tensorflow"
+                "tensorflow",
+                "cv2",
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.image_matting_pipeline"
         },
         "('PIPELINES', 'product-retrieval-embedding', 'resnet50-product-retrieval-embedding')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/product_retrieval_embedding_pipeline.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
                 "torch",
                 "PIL",
-                "torchvision",
                 "cv2",
-                "typing",
-                "os"
+                "torchvision",
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.product_retrieval_embedding_pipeline"
         },
         "('PIPELINES', 'product-segmentation', 'product-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/product_segmentation_pipeline.py",
             "imports": [
-                "typing",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.product_segmentation_pipeline"
         },
         "('PIPELINES', 'protein-structure', 'unifold-protein-structure')": {
             "filepath": "TEMPLATE_PATH/pipelines/science/protein_structure_pipeline.py",
             "imports": [
-                "numpy",
-                "unicore",
-                "torch",
-                "time",
-                "typing",
                 "os",
+                "typing",
+                "time",
+                "torch",
+                "unicore",
+                "numpy",
                 "json"
             ],
             "module": "modelscope.pipelines.science.protein_structure_pipeline"
         },
         "('PIPELINES', 'punctuation', 'punc-inference')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/punctuation_processing_pipeline.py",
             "imports": [
+                "os",
                 "yaml",
                 "shutil",
-                "typing",
-                "os"
+                "typing"
             ],
             "module": "modelscope.pipelines.audio.punctuation_processing_pipeline"
         },
         "('PIPELINES', 'referring-video-object-segmentation', 'referring-video-object-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/referring_video_object_segmentation_pipeline.py",
             "imports": [
-                "numpy",
-                "torch",
+                "einops",
+                "tqdm",
                 "moviepy",
-                "torchvision",
+                "typing",
+                "torch",
                 "PIL",
                 "tempfile",
-                "einops",
-                "typing",
-                "tqdm"
+                "torchvision",
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.referring_video_object_segmentation_pipeline"
         },
         "('PIPELINES', 'relation-extraction', 'relation-extraction')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/information_extraction_pipeline.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.information_extraction_pipeline"
         },
         "('PIPELINES', 'semantic-segmentation', 'ddpm-image-semantic-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/ddpm_semantic_segmentation_pipeline.py",
             "imports": [
+                "torch",
                 "torchvision",
-                "typing",
-                "torch"
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.ddpm_semantic_segmentation_pipeline"
         },
         "('PIPELINES', 'semantic-segmentation', 'res2net-camouflaged-detection')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_salient_detection_pipeline.py",
             "imports": [
                 "typing"
@@ -7884,223 +7834,223 @@
                 "typing"
             ],
             "module": "modelscope.pipelines.cv.image_salient_detection_pipeline"
         },
         "('PIPELINES', 'sentence-embedding', 'sentence-embedding')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/sentence_embedding_pipeline.py",
             "imports": [
-                "typing",
                 "torch",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.sentence_embedding_pipeline"
         },
         "('PIPELINES', 'sentence-similarity', 'sentence-similarity')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/text_classification_pipeline.py",
             "imports": [
-                "typing",
                 "torch",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.text_classification_pipeline"
         },
         "('PIPELINES', 'sentence-similarity', 'translation-quality-estimation')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/translation_quality_estimation_pipeline.py",
             "imports": [
-                "torch",
+                "os",
                 "transformers",
                 "typing",
-                "os",
+                "torch",
                 "io"
             ],
             "module": "modelscope.pipelines.nlp.translation_quality_estimation_pipeline"
         },
         "('PIPELINES', 'sentiment-classification', 'sentiment-classification')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/text_classification_pipeline.py",
             "imports": [
-                "typing",
                 "torch",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.text_classification_pipeline"
         },
         "('PIPELINES', 'shop-segmentation', 'shop-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/shop_segmentation_pipleline.py",
             "imports": [
                 "typing"
             ],
             "module": "modelscope.pipelines.cv.shop_segmentation_pipleline"
         },
         "('PIPELINES', 'siamese-uie', 'siamese-uie')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/siamese_uie_pipeline.py",
             "imports": [
-                "typing",
-                "math",
-                "torch",
-                "pathlib",
-                "scipy",
-                "copy",
-                "time",
                 "logging",
                 "os",
-                "json",
-                "tqdm"
+                "tqdm",
+                "copy",
+                "scipy",
+                "math",
+                "time",
+                "typing",
+                "pathlib",
+                "torch",
+                "json"
             ],
             "module": "modelscope.pipelines.nlp.siamese_uie_pipeline"
         },
         "('PIPELINES', 'skin-retouching', 'unet-skin-retouching')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/skin_retouching_pipeline.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
+                "tensorflow",
                 "torch",
                 "PIL",
-                "torchvision",
                 "cv2",
-                "typing",
-                "os",
-                "tensorflow"
+                "torchvision",
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.skin_retouching_pipeline"
         },
         "('PIPELINES', 'speaker-diarization', 'segmentation-clustering')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/segmentation_clustering_pipeline.py",
             "imports": [
-                "torchaudio",
-                "torch",
                 "soundfile",
                 "typing",
-                "numpy",
-                "io"
+                "torch",
+                "io",
+                "torchaudio",
+                "numpy"
             ],
             "module": "modelscope.pipelines.audio.segmentation_clustering_pipeline"
         },
         "('PIPELINES', 'speaker-diarization', 'speaker-change-locating')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/speaker_change_locating_pipeline.py",
             "imports": [
-                "torchaudio",
-                "torch",
                 "soundfile",
                 "typing",
-                "numpy",
-                "io"
+                "torch",
+                "io",
+                "torchaudio",
+                "numpy"
             ],
             "module": "modelscope.pipelines.audio.speaker_change_locating_pipeline"
         },
         "('PIPELINES', 'speaker-diarization', 'speaker-diarization-inference')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/speaker_diarization_pipeline.py",
             "imports": [
-                "yaml",
-                "numpy",
-                "shutil",
                 "os",
                 "typing",
+                "yaml",
+                "shutil",
+                "numpy",
                 "json"
             ],
             "module": "modelscope.pipelines.audio.speaker_diarization_pipeline"
         },
         "('PIPELINES', 'speaker-diarization-dialogue-detection', 'speaker-diarization-dialogue-detection')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/speaker_diarization_dialogue_detection_pipeline.py",
             "imports": [
-                "typing",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.audio.speaker_diarization_dialogue_detection_pipeline"
         },
         "('PIPELINES', 'speaker-diarization-semantic-speaker-turn-detection', 'speaker-diarization-semantic-speaker-turn-detection')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/speaker_diarization_semantic_speaker_turn_detection_pipeline.py",
             "imports": [
-                "typing",
                 "torch",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.audio.speaker_diarization_semantic_speaker_turn_detection_pipeline"
         },
         "('PIPELINES', 'speaker-verification', 'speaker-verification')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/speaker_verification_light_pipeline.py",
             "imports": [
-                "numpy",
-                "torchaudio",
-                "torch",
                 "soundfile",
-                "typing",
                 "os",
-                "io"
+                "typing",
+                "torch",
+                "io",
+                "torchaudio",
+                "numpy"
             ],
             "module": "modelscope.pipelines.audio.speaker_verification_light_pipeline"
         },
         "('PIPELINES', 'speaker-verification', 'speaker-verification-eres2net')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/speaker_verification_eres2net_pipeline.py",
             "imports": [
-                "typing",
-                "torch",
+                "soundfile",
                 "io",
-                "soundfile"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.pipelines.audio.speaker_verification_eres2net_pipeline"
         },
         "('PIPELINES', 'speaker-verification', 'speaker-verification-rdino')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/speaker_verification_rdino_pipeline.py",
             "imports": [
-                "typing",
-                "torch",
+                "soundfile",
                 "io",
-                "soundfile"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.pipelines.audio.speaker_verification_rdino_pipeline"
         },
         "('PIPELINES', 'speaker-verification', 'sv-inference')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/speaker_verification_pipeline.py",
             "imports": [
+                "os",
                 "yaml",
                 "shutil",
-                "typing",
-                "os"
+                "typing"
             ],
             "module": "modelscope.pipelines.audio.speaker_verification_pipeline"
         },
         "('PIPELINES', 'speech-language-recognition', 'speech-language-recognition')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/language_recognition_pipeline.py",
             "imports": [
-                "numpy",
-                "torchaudio",
-                "torch",
                 "soundfile",
-                "typing",
                 "os",
-                "io"
+                "typing",
+                "torch",
+                "io",
+                "torchaudio",
+                "numpy"
             ],
             "module": "modelscope.pipelines.audio.language_recognition_pipeline"
         },
         "('PIPELINES', 'speech-separation', 'speech-separation')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/separation_pipeline.py",
             "imports": [
-                "numpy",
-                "torch",
                 "soundfile",
                 "typing",
-                "io"
+                "torch",
+                "io",
+                "numpy"
             ],
             "module": "modelscope.pipelines.audio.separation_pipeline"
         },
         "('PIPELINES', 'speech-timestamp', 'speech-timestamp-inference')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/timestamp_pipeline.py",
             "imports": [
-                "yaml",
-                "funasr",
                 "os",
+                "funasr",
                 "typing",
+                "yaml",
                 "json"
             ],
             "module": "modelscope.pipelines.audio.timestamp_pipeline"
         },
         "('PIPELINES', 'sudoku', 'ofa-sudoku')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/sudoku_pipeline.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.pipelines.multi_modal.sudoku_pipeline"
         },
         "('PIPELINES', 'table-question-answering', 'conversational-text-to-sql')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/conversational_text_to_sql_pipeline.py",
             "imports": [
                 "typing",
@@ -8108,32 +8058,32 @@
                 "text2sql_lgesql"
             ],
             "module": "modelscope.pipelines.nlp.conversational_text_to_sql_pipeline"
         },
         "('PIPELINES', 'table-question-answering', 'table-question-answering-pipeline')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/table_question_answering_pipeline.py",
             "imports": [
-                "torch",
                 "os",
                 "transformers",
                 "typing",
+                "torch",
                 "json"
             ],
             "module": "modelscope.pipelines.nlp.table_question_answering_pipeline"
         },
         "('PIPELINES', 'table-recognition', 'dla34-table-recognition')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/table_recognition_pipeline.py",
             "imports": [
+                "os",
                 "math",
-                "numpy",
+                "typing",
                 "torch",
                 "PIL",
                 "cv2",
-                "typing",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.table_recognition_pipeline"
         },
         "('PIPELINES', 'task-oriented-conversation', 'dialog-intent-prediction')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/dialog_intent_prediction_pipeline.py",
             "imports": [
                 "typing"
@@ -8153,1751 +8103,1736 @@
                 "typing"
             ],
             "module": "modelscope.pipelines.nlp.dialog_state_tracking_pipeline"
         },
         "('PIPELINES', 'task-template', 'pipeline-template')": {
             "filepath": "TEMPLATE_PATH/pipelines/pipeline_template.py",
             "imports": [
-                "typing",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.pipeline_template"
         },
         "('PIPELINES', 'text-classification', 'domain-classification')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/fasttext_text_classification_pipeline.py",
             "imports": [
-                "numpy",
                 "fasttext",
+                "os",
                 "sentencepiece",
                 "typing",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.pipelines.nlp.fasttext_text_classification_pipeline"
         },
         "('PIPELINES', 'text-classification', 'language_identification')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/language_identification_pipline.py",
             "imports": [
-                "re",
-                "numpy",
                 "os",
                 "typing",
-                "tensorflow"
+                "tensorflow",
+                "re",
+                "numpy"
             ],
             "module": "modelscope.pipelines.nlp.language_identification_pipline"
         },
         "('PIPELINES', 'text-classification', 'sentence-similarity')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/text_classification_pipeline.py",
             "imports": [
-                "typing",
                 "torch",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.text_classification_pipeline"
         },
         "('PIPELINES', 'text-classification', 'sentiment-analysis')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/text_classification_pipeline.py",
             "imports": [
-                "typing",
                 "torch",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.text_classification_pipeline"
         },
         "('PIPELINES', 'text-classification', 'sentiment-classification')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/text_classification_pipeline.py",
             "imports": [
-                "typing",
                 "torch",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.text_classification_pipeline"
         },
         "('PIPELINES', 'text-classification', 'text-classification')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/text_classification_pipeline.py",
             "imports": [
-                "typing",
                 "torch",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.text_classification_pipeline"
         },
         "('PIPELINES', 'text-classification', 'user-satisfaction-estimation')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/user_satisfaction_estimation_pipeline.py",
             "imports": [
-                "typing",
                 "torch",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.user_satisfaction_estimation_pipeline"
         },
         "('PIPELINES', 'text-driven-segmentation', 'text-driven-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/text_driven_segmentation_pipleline.py",
             "imports": [
                 "typing"
             ],
             "module": "modelscope.pipelines.cv.text_driven_segmentation_pipleline"
         },
         "('PIPELINES', 'text-error-correction', 'text-error-correction')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/text_error_correction_pipeline.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.text_error_correction_pipeline"
         },
         "('PIPELINES', 'text-generation', 'glm130b-text-generation')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/glm130b_text_generation_pipeline.py",
             "imports": [
                 "typing"
             ],
             "module": "modelscope.pipelines.nlp.glm130b_text_generation_pipeline"
         },
         "('PIPELINES', 'text-generation', 'gpt-moe-generation')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/distributed_gpt_moe_pipeline.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.distributed_gpt_moe_pipeline"
         },
         "('PIPELINES', 'text-generation', 'gpt3-generation')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/distributed_gpt3_pipeline.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.distributed_gpt3_pipeline"
         },
         "('PIPELINES', 'text-generation', 'llama2-text-generation-pipeline')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/llama2_text_generation_pipeline.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.llama2_text_generation_pipeline"
         },
         "('PIPELINES', 'text-generation', 'plug-generation')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/distributed_plug_pipeline.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.distributed_plug_pipeline"
         },
         "('PIPELINES', 'text-generation', 'polylm-text-generation')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/polylm_text_generation_pipeline.py",
             "imports": [
+                "os",
                 "torch",
-                "typing",
-                "os"
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.polylm_text_generation_pipeline"
         },
-        "('PIPELINES', 'text-generation', 'qwen-text-generation')": {
-            "filepath": "TEMPLATE_PATH/pipelines/nlp/text_generation_pipeline.py",
-            "imports": [
-                "transformers",
-                "typing",
-                "torch",
-                "os"
-            ],
-            "module": "modelscope.pipelines.nlp.text_generation_pipeline"
-        },
         "('PIPELINES', 'text-generation', 'text-generation')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/text_generation_pipeline.py",
             "imports": [
-                "transformers",
-                "typing",
+                "os",
                 "torch",
-                "os"
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.text_generation_pipeline"
         },
         "('PIPELINES', 'text-ranking', 'mgeo-ranking')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/mgeo_ranking_pipeline.py",
             "imports": [
-                "typing",
                 "torch",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.multi_modal.mgeo_ranking_pipeline"
         },
         "('PIPELINES', 'text-ranking', 'text-ranking')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/text_ranking_pipeline.py",
             "imports": [
-                "typing",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.text_ranking_pipeline"
         },
         "('PIPELINES', 'text-summarization', 'mglm-text-summarization')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/mglm_text_summarization_pipeline.py",
             "imports": [
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.mglm_text_summarization_pipeline"
         },
         "('PIPELINES', 'text-summarization', 'text-generation')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/summarization_pipeline.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.summarization_pipeline"
         },
         "('PIPELINES', 'text-to-360panorama-image', 'text-to-360panorama-image')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/text_to_360panorama_image_pipeline.py",
             "imports": [
-                "realesrgan",
-                "diffusers",
-                "numpy",
                 "basicsr",
+                "realesrgan",
+                "typing",
                 "torch",
+                "diffusers",
                 "PIL",
                 "random",
-                "typing"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.text_to_360panorama_image_pipeline"
         },
         "('PIPELINES', 'text-to-image-synthesis', 'chinese-stable-diffusion')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/chinese_stable_diffusion_pipeline.py",
             "imports": [
-                "diffusers",
-                "numpy",
+                "transformers",
+                "typing",
                 "torch",
+                "diffusers",
                 "PIL",
                 "cv2",
-                "transformers",
-                "typing"
+                "numpy"
             ],
             "module": "modelscope.pipelines.multi_modal.diffusers_wrapped.stable_diffusion.chinese_stable_diffusion_pipeline"
         },
         "('PIPELINES', 'text-to-image-synthesis', 'diffusers-stable-diffusion')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/stable_diffusion_pipeline.py",
             "imports": [
-                "diffusers",
-                "numpy",
+                "os",
+                "typing",
                 "torch",
-                "torchvision",
+                "diffusers",
                 "PIL",
                 "cv2",
-                "typing",
-                "os"
+                "torchvision",
+                "numpy"
             ],
             "module": "modelscope.pipelines.multi_modal.diffusers_wrapped.stable_diffusion.stable_diffusion_pipeline"
         },
         "('PIPELINES', 'text-to-image-synthesis', 'disco_guided_diffusion')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/disco_guided_diffusion_pipeline/disco_guided_diffusion.py",
             "imports": [
-                "math",
-                "numpy",
+                "os",
                 "clip",
+                "math",
                 "importlib",
                 "torch",
                 "gc",
-                "torchvision",
                 "PIL",
                 "cv2",
-                "os",
+                "torchvision",
+                "numpy",
                 "json"
             ],
             "module": "modelscope.pipelines.multi_modal.disco_guided_diffusion_pipeline.disco_guided_diffusion"
         },
         "('PIPELINES', 'text-to-image-synthesis', 'text-to-image-synthesis')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/text_to_image_synthesis_pipeline.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.pipelines.multi_modal.text_to_image_synthesis_pipeline"
         },
         "('PIPELINES', 'text-to-speech', 'sambert-hifigan-tts')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/text_to_speech_pipeline.py",
             "imports": [
-                "typing",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.audio.text_to_speech_pipeline"
         },
         "('PIPELINES', 'text-to-video-synthesis', 'latent-text-to-video-synthesis')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/text_to_video_synthesis_pipeline.py",
             "imports": [
-                "torch",
-                "tempfile",
-                "cv2",
                 "einops",
+                "os",
                 "typing",
-                "os"
+                "torch",
+                "tempfile",
+                "cv2"
             ],
             "module": "modelscope.pipelines.multi_modal.text_to_video_synthesis_pipeline"
         },
         "('PIPELINES', 'text2sql', 'ofa-text2sql')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/text2sql_pipeline.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.pipelines.multi_modal.text2sql_pipeline"
         },
         "('PIPELINES', 'text2text-generation', 'text2text-generation')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/text_generation_pipeline.py",
             "imports": [
-                "transformers",
-                "typing",
+                "os",
                 "torch",
-                "os"
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.text_generation_pipeline"
         },
         "('PIPELINES', 'text2text-generation', 'translation_en_to_de')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/text_generation_pipeline.py",
             "imports": [
-                "transformers",
-                "typing",
+                "os",
                 "torch",
-                "os"
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.text_generation_pipeline"
         },
         "('PIPELINES', 'text2text-generation', 'translation_en_to_fr')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/text_generation_pipeline.py",
             "imports": [
-                "transformers",
-                "typing",
+                "os",
                 "torch",
-                "os"
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.text_generation_pipeline"
         },
         "('PIPELINES', 'text2text-generation', 'translation_en_to_ro')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/text_generation_pipeline.py",
             "imports": [
-                "transformers",
-                "typing",
+                "os",
                 "torch",
-                "os"
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.text_generation_pipeline"
         },
         "('PIPELINES', 'token-classification', 'named-entity-recognition')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/token_classification_pipeline.py",
             "imports": [
-                "typing",
                 "torch",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.token_classification_pipeline"
         },
         "('PIPELINES', 'token-classification', 'part-of-speech')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/token_classification_pipeline.py",
             "imports": [
-                "typing",
                 "torch",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.token_classification_pipeline"
         },
         "('PIPELINES', 'token-classification', 'token-classification')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/token_classification_pipeline.py",
             "imports": [
-                "typing",
                 "torch",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.token_classification_pipeline"
         },
         "('PIPELINES', 'token-classification', 'word-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/token_classification_pipeline.py",
             "imports": [
-                "typing",
                 "torch",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.token_classification_pipeline"
         },
         "('PIPELINES', 'translation', 'automatic-post-editing')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/automatic_post_editing_pipeline.py",
             "imports": [
                 "sacremoses",
-                "sentencepiece",
-                "tensorflow",
-                "html",
                 "jieba",
+                "os",
+                "sentencepiece",
                 "typing",
+                "tensorflow",
                 "numpy",
-                "os"
+                "html"
             ],
             "module": "modelscope.pipelines.nlp.automatic_post_editing_pipeline"
         },
         "('PIPELINES', 'translation', 'csanmt-translation')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/translation_pipeline.py",
             "imports": [
                 "sacremoses",
-                "tensorflow",
-                "subword_nmt",
                 "jieba",
+                "os",
                 "typing",
-                "numpy",
-                "os"
+                "subword_nmt",
+                "tensorflow",
+                "numpy"
             ],
             "module": "modelscope.pipelines.nlp.translation_pipeline"
         },
         "('PIPELINES', 'translation', 'interactive-translation')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/interactive_translation_pipeline.py",
             "imports": [
                 "sacremoses",
-                "tensorflow",
-                "subword_nmt",
                 "jieba",
+                "os",
                 "typing",
-                "numpy",
-                "os"
+                "subword_nmt",
+                "tensorflow",
+                "numpy"
             ],
             "module": "modelscope.pipelines.nlp.interactive_translation_pipeline"
         },
         "('PIPELINES', 'translation-evaluation', 'translation-evaluation')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/translation_evaluation_pipeline.py",
             "imports": [
-                "numpy",
-                "torch",
+                "enum",
                 "os",
                 "typing",
-                "enum"
+                "torch",
+                "numpy"
             ],
             "module": "modelscope.pipelines.nlp.translation_evaluation_pipeline"
         },
         "('PIPELINES', 'universal-matting', 'unet-universal-matting')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_matting_pipeline.py",
             "imports": [
-                "numpy",
                 "os",
-                "cv2",
                 "typing",
-                "tensorflow"
+                "tensorflow",
+                "cv2",
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.image_matting_pipeline"
         },
         "('PIPELINES', 'video-captioning', 'video-captioning')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/video_captioning_pipeline.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.pipelines.multi_modal.video_captioning_pipeline"
         },
         "('PIPELINES', 'video-category', 'video-category')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_category_pipeline.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
                 "decord",
                 "torch",
-                "torchvision",
                 "PIL",
-                "typing",
-                "os",
+                "torchvision",
+                "numpy",
                 "json"
             ],
             "module": "modelscope.pipelines.cv.video_category_pipeline"
         },
         "('PIPELINES', 'video-colorization', 'video-colorization')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_colorization_pipeline.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
                 "torch",
                 "PIL",
-                "torchvision",
                 "tempfile",
                 "cv2",
-                "typing",
-                "os",
-                "subprocess"
+                "subprocess",
+                "numpy",
+                "torchvision"
             ],
             "module": "modelscope.pipelines.cv.video_colorization_pipeline"
         },
         "('PIPELINES', 'video-deinterlace', 'video-deinterlace')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_deinterlace_pipeline.py",
             "imports": [
+                "os",
                 "math",
-                "numpy",
+                "typing",
                 "torch",
-                "torchvision",
                 "tempfile",
                 "cv2",
-                "typing",
-                "os",
-                "subprocess"
+                "subprocess",
+                "numpy",
+                "torchvision"
             ],
             "module": "modelscope.pipelines.cv.video_deinterlace_pipeline"
         },
         "('PIPELINES', 'video-depth-estimation', 'video-depth-estimation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_depth_estimation_pipeline.py",
             "imports": [
                 "typing"
             ],
             "module": "modelscope.pipelines.cv.video_depth_estimation_pipeline"
         },
         "('PIPELINES', 'video-embedding', 'cmdssl-r2p1d_video_embedding')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/cmdssl_video_embedding_pipeline.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
                 "decord",
                 "torch",
-                "torchvision",
                 "PIL",
-                "typing",
-                "os"
+                "torchvision",
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.cmdssl_video_embedding_pipeline"
         },
         "('PIPELINES', 'video-embedding', 'hicossl-s3dg-video_embedding')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/hicossl_video_embedding_pipeline.py",
             "imports": [
+                "os",
                 "torch",
-                "typing",
                 "math",
-                "os"
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.hicossl_video_embedding_pipeline"
         },
         "('PIPELINES', 'video-frame-interpolation', 'video-frame-interpolation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_frame_interpolation_pipeline.py",
             "imports": [
-                "glob",
+                "os",
+                "tempfile",
                 "math",
-                "numpy",
+                "typing",
                 "torch",
-                "torchvision",
-                "tempfile",
+                "glob",
                 "cv2",
-                "typing",
-                "os",
-                "subprocess"
+                "subprocess",
+                "numpy",
+                "torchvision"
             ],
             "module": "modelscope.pipelines.cv.video_frame_interpolation_pipeline"
         },
         "('PIPELINES', 'video-human-matting', 'video-human-matting')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_human_matting_pipeline.py",
             "imports": [
-                "numpy",
+                "os",
                 "moviepy",
+                "typing",
                 "torch",
                 "cv2",
-                "typing",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.video_human_matting_pipeline"
         },
         "('PIPELINES', 'video-inpainting', 'video-inpainting')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_inpainting_pipeline.py",
             "imports": [
                 "typing"
             ],
             "module": "modelscope.pipelines.cv.video_inpainting_pipeline"
         },
         "('PIPELINES', 'video-instance-segmentation', 'video-instance-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_instance_segmentation_pipeline.py",
             "imports": [
+                "mmcv",
+                "os",
                 "tqdm",
+                "typing",
                 "torch",
                 "cv2",
-                "typing",
-                "numpy",
-                "os",
-                "mmcv"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.video_instance_segmentation_pipeline"
         },
         "('PIPELINES', 'video-multi-modal-embedding', 'video-multi-modal-embedding')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/video_multi_modal_embedding_pipeline.py",
             "imports": [
                 "typing"
             ],
             "module": "modelscope.pipelines.multi_modal.video_multi_modal_embedding_pipeline"
         },
         "('PIPELINES', 'video-multi-object-tracking', 'video-multi-object-tracking')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_multi_object_tracking_pipeline.py",
             "imports": [
+                "os",
                 "torch",
-                "typing",
-                "os"
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.video_multi_object_tracking_pipeline"
         },
         "('PIPELINES', 'video-object-detection', 'cspnet_realtime-video-object-detection_streamyolo')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/realtime_video_object_detection_pipeline.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
                 "torch",
                 "PIL",
-                "torchvision",
                 "cv2",
-                "typing",
-                "os",
+                "torchvision",
+                "numpy",
                 "json"
             ],
             "module": "modelscope.pipelines.cv.realtime_video_object_detection_pipeline"
         },
         "('PIPELINES', 'video-object-segmentation', 'video-object-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_object_segmentation_pipeline.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
                 "torch",
                 "PIL",
                 "torchvision",
-                "typing",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.video_object_segmentation_pipeline"
         },
         "('PIPELINES', 'video-panoptic-segmentation', 'video-panoptic-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_panoptic_segmentation_pipeline.py",
             "imports": [
+                "mmcv",
+                "os",
                 "tqdm",
+                "typing",
                 "torch",
                 "cv2",
-                "typing",
-                "numpy",
-                "os",
-                "mmcv"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.video_panoptic_segmentation_pipeline"
         },
         "('PIPELINES', 'video-question-answering', 'video-question-answering')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/video_question_answering_pipeline.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.pipelines.multi_modal.video_question_answering_pipeline"
         },
         "('PIPELINES', 'video-single-object-tracking', 'ostrack-vitb-video-single-object-tracking')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_single_object_tracking_pipeline.py",
             "imports": [
+                "os",
                 "cv2",
-                "typing",
-                "os"
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.video_single_object_tracking_pipeline"
         },
         "('PIPELINES', 'video-single-object-tracking', 'procontext-vitb-video-single-object-tracking')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_single_object_tracking_pipeline.py",
             "imports": [
+                "os",
                 "cv2",
-                "typing",
-                "os"
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.video_single_object_tracking_pipeline"
         },
         "('PIPELINES', 'video-stabilization', 'video-stabilization')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_stabilization_pipeline.py",
             "imports": [
-                "glob",
+                "os",
+                "tempfile",
                 "math",
-                "numpy",
+                "typing",
                 "torch",
-                "tempfile",
+                "glob",
                 "cv2",
-                "typing",
-                "os",
-                "subprocess"
+                "subprocess",
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.video_stabilization_pipeline"
         },
         "('PIPELINES', 'video-summarization', 'googlenet_pgl_video_summarization')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_summarization_pipeline.py",
             "imports": [
-                "numpy",
+                "os",
+                "tqdm",
+                "typing",
                 "torch",
                 "cv2",
-                "typing",
-                "os",
-                "tqdm"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.video_summarization_pipeline"
         },
         "('PIPELINES', 'video-super-resolution', 'realbasicvsr-video-super-resolution')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_super_resolution_pipeline.py",
             "imports": [
+                "os",
                 "math",
-                "numpy",
+                "typing",
                 "torch",
-                "torchvision",
                 "tempfile",
                 "cv2",
-                "typing",
-                "os",
-                "subprocess"
+                "subprocess",
+                "numpy",
+                "torchvision"
             ],
             "module": "modelscope.pipelines.cv.video_super_resolution_pipeline"
         },
         "('PIPELINES', 'video-temporal-grounding', 'soonet-video-temporal-grounding')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/soonet_video_temporal_grounding_pipeline.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
                 "torch",
                 "torchvision",
-                "typing",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.pipelines.multi_modal.soonet_video_temporal_grounding_pipeline"
         },
         "('PIPELINES', 'video-text-retrieval', 'vop-video-text-retrieval')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/vop_retrieval_pipeline.py",
             "imports": [
-                "gzip",
-                "math",
-                "numpy",
-                "torch",
                 "collections",
+                "os",
+                "tqdm",
+                "math",
                 "typing",
-                "random",
                 "pickle",
-                "os",
-                "tqdm"
+                "torch",
+                "gzip",
+                "random",
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.vop_retrieval_pipeline"
         },
         "('PIPELINES', 'video-text-retrieval', 'vop-video-text-retrieval-se')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/vop_retrieval_se_pipeline.py",
             "imports": [
-                "gzip",
-                "numpy",
-                "torch",
+                "os",
                 "typing",
-                "os"
+                "torch",
+                "gzip",
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.vop_retrieval_se_pipeline"
         },
         "('PIPELINES', 'virtual-try-on', 'virtual-try-on')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/virtual_try_on_pipeline.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
                 "torch",
                 "PIL",
                 "cv2",
-                "typing",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.virtual_try_on_pipeline"
         },
         "('PIPELINES', 'vision-efficient-tuning', 'vision-efficient-tuning')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/vision_efficient_tuning_pipeline.py",
             "imports": [
+                "torch",
                 "torchvision",
-                "typing",
                 "numpy",
-                "torch"
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.vision_efficient_tuning_pipeline"
         },
         "('PIPELINES', 'visual-entailment', 'visual-entailment')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/visual_entailment_pipeline.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.pipelines.multi_modal.visual_entailment_pipeline"
         },
         "('PIPELINES', 'visual-grounding', 'visual-grounding')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/visual_grounding_pipeline.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.pipelines.multi_modal.visual_grounding_pipeline"
         },
         "('PIPELINES', 'visual-question-answering', 'gridvlp-multi-modal-classification')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/gridvlp_pipeline.py",
             "imports": [
-                "traceback",
-                "numpy",
+                "os",
+                "transformers",
+                "typing",
+                "time",
                 "torch",
+                "traceback",
                 "PIL",
-                "time",
-                "typing",
-                "transformers",
-                "os",
+                "numpy",
                 "json"
             ],
             "module": "modelscope.pipelines.multi_modal.gridvlp_pipeline"
         },
         "('PIPELINES', 'visual-question-answering', 'visual-question-answering')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/visual_question_answering_pipeline.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.pipelines.multi_modal.visual_question_answering_pipeline"
         },
         "('PIPELINES', 'voice-activity-detection', 'vad-inference')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/voice_activity_detection_pipeline.py",
             "imports": [
-                "yaml",
-                "funasr",
                 "os",
+                "funasr",
                 "typing",
+                "yaml",
                 "json"
             ],
             "module": "modelscope.pipelines.audio.voice_activity_detection_pipeline"
         },
         "('PIPELINES', 'word-alignment', 'word-alignment')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/word_alignment_pipeline.py",
             "imports": [
-                "typing",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.word_alignment_pipeline"
         },
         "('PIPELINES', 'word-segmentation', 'multilingual-word-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/word_segmentation_pipeline.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.word_segmentation_pipeline"
         },
         "('PIPELINES', 'word-segmentation', 'word-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/word_segmentation_pipeline.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.word_segmentation_pipeline"
         },
         "('PIPELINES', 'word-segmentation', 'word-segmentation-thai')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/word_segmentation_pipeline.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.word_segmentation_pipeline"
         },
         "('PIPELINES', 'zero-shot-classification', 'zero-shot-classification')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/zero_shot_classification_pipeline.py",
             "imports": [
+                "torch",
                 "scipy",
-                "typing",
-                "torch"
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.zero_shot_classification_pipeline"
         },
         "('POSITIONAL_ENCODING', 'default', 'SinePositionalEncoding3D')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/positional_encoding.py",
             "imports": [
+                "torch",
                 "mmcv",
-                "math",
-                "torch"
+                "math"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.utils.positional_encoding"
         },
         "('PREPROCESSORS', 'audio', 'LinearAECAndFbank')": {
             "filepath": "TEMPLATE_PATH/preprocessors/audio.py",
             "imports": [
-                "numpy",
-                "torch",
+                "os",
                 "scipy",
                 "typing",
-                "os",
-                "io"
+                "torch",
+                "io",
+                "numpy"
             ],
             "module": "modelscope.preprocessors.audio"
         },
         "('PREPROCESSORS', 'audio', 'sen-cls-tokenizer')": {
             "filepath": "TEMPLATE_PATH/preprocessors/speaker.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.preprocessors.speaker"
         },
         "('PREPROCESSORS', 'audio', 'token-cls-tokenizer')": {
             "filepath": "TEMPLATE_PATH/preprocessors/speaker.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.preprocessors.speaker"
         },
         "('PREPROCESSORS', 'audio', 'wav-to-lists')": {
             "filepath": "TEMPLATE_PATH/preprocessors/kws.py",
             "imports": [
+                "os",
                 "yaml",
-                "typing",
-                "os"
+                "typing"
             ],
             "module": "modelscope.preprocessors.kws"
         },
         "('PREPROCESSORS', 'audio', 'wav-to-scp')": {
             "filepath": "TEMPLATE_PATH/preprocessors/asr.py",
             "imports": [
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.preprocessors.asr"
         },
         "('PREPROCESSORS', 'cv', 'CenterCrop')": {
             "filepath": "TEMPLATE_PATH/preprocessors/cv/image_classification_preprocessor.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
                 "torch",
-                "torchvision",
                 "PIL",
                 "cv2",
-                "typing",
-                "os"
+                "torchvision",
+                "numpy"
             ],
             "module": "modelscope.preprocessors.cv.image_classification_preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'ImageToTensor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/cv/image_classification_preprocessor.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
                 "torch",
-                "torchvision",
                 "PIL",
                 "cv2",
-                "typing",
-                "os"
+                "torchvision",
+                "numpy"
             ],
             "module": "modelscope.preprocessors.cv.image_classification_preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'Normalize')": {
             "filepath": "TEMPLATE_PATH/preprocessors/cv/image_classification_preprocessor.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
                 "torch",
-                "torchvision",
                 "PIL",
                 "cv2",
-                "typing",
-                "os"
+                "torchvision",
+                "numpy"
             ],
             "module": "modelscope.preprocessors.cv.image_classification_preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'RandomCrop')": {
             "filepath": "TEMPLATE_PATH/preprocessors/cv/image_classification_preprocessor.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
                 "torch",
-                "torchvision",
                 "PIL",
                 "cv2",
-                "typing",
-                "os"
+                "torchvision",
+                "numpy"
             ],
             "module": "modelscope.preprocessors.cv.image_classification_preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'RandomHorizontalFlip')": {
             "filepath": "TEMPLATE_PATH/preprocessors/cv/image_classification_preprocessor.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
                 "torch",
-                "torchvision",
                 "PIL",
                 "cv2",
-                "typing",
-                "os"
+                "torchvision",
+                "numpy"
             ],
             "module": "modelscope.preprocessors.cv.image_classification_preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'RandomResizedCrop')": {
             "filepath": "TEMPLATE_PATH/preprocessors/cv/image_classification_preprocessor.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
                 "torch",
-                "torchvision",
                 "PIL",
                 "cv2",
-                "typing",
-                "os"
+                "torchvision",
+                "numpy"
             ],
             "module": "modelscope.preprocessors.cv.image_classification_preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'Resize')": {
             "filepath": "TEMPLATE_PATH/preprocessors/cv/image_classification_preprocessor.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
                 "torch",
-                "torchvision",
                 "PIL",
                 "cv2",
-                "typing",
-                "os"
+                "torchvision",
+                "numpy"
             ],
             "module": "modelscope.preprocessors.cv.image_classification_preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'bad-image-detecting-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/cv/bad_image_detecting_preprocessor.py",
             "imports": [
                 "math",
+                "typing",
                 "torch",
-                "torchvision",
                 "PIL",
-                "typing",
+                "torchvision",
                 "numpy"
             ],
             "module": "modelscope.preprocessors.cv.bad_image_detecting_preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'controllable-image-generation-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/cv/controllable_image_generation.py",
             "imports": [
+                "os",
                 "math",
-                "numpy",
+                "typing",
                 "torch",
                 "PIL",
-                "torchvision",
                 "cv2",
-                "typing",
-                "os"
+                "torchvision",
+                "numpy"
             ],
             "module": "modelscope.preprocessors.cv.controllable_image_generation"
         },
         "('PREPROCESSORS', 'cv', 'image-classification-bypass-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/image.py",
             "imports": [
-                "numpy",
+                "typing",
+                "io",
                 "PIL",
                 "cv2",
-                "typing",
-                "io"
+                "numpy"
             ],
             "module": "modelscope.preprocessors.image"
         },
         "('PREPROCESSORS', 'cv', 'image-classification-mmcv-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/cv/mmcls_preprocessor.py",
             "imports": [
-                "typing",
                 "os",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.preprocessors.cv.mmcls_preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'image-classification-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/cv/image_classification_preprocessor.py",
             "imports": [
-                "numpy",
+                "os",
+                "typing",
                 "torch",
-                "torchvision",
                 "PIL",
                 "cv2",
-                "typing",
-                "os"
+                "torchvision",
+                "numpy"
             ],
             "module": "modelscope.preprocessors.cv.image_classification_preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'image-color-enhance-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/image.py",
             "imports": [
-                "numpy",
+                "typing",
+                "io",
                 "PIL",
                 "cv2",
-                "typing",
-                "io"
+                "numpy"
             ],
             "module": "modelscope.preprocessors.image"
         },
         "('PREPROCESSORS', 'cv', 'image-deblur-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/image.py",
             "imports": [
-                "numpy",
+                "typing",
+                "io",
                 "PIL",
                 "cv2",
-                "typing",
-                "io"
+                "numpy"
             ],
             "module": "modelscope.preprocessors.image"
         },
         "('PREPROCESSORS', 'cv', 'image-demoire-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/cv/image_restoration_preprocessor.py",
             "imports": [
                 "math",
+                "typing",
                 "torch",
-                "torchvision",
                 "PIL",
-                "typing",
+                "torchvision",
                 "numpy"
             ],
             "module": "modelscope.preprocessors.cv.image_restoration_preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'image-denoise-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/image.py",
             "imports": [
-                "numpy",
+                "typing",
+                "io",
                 "PIL",
                 "cv2",
-                "typing",
-                "io"
+                "numpy"
             ],
             "module": "modelscope.preprocessors.image"
         },
         "('PREPROCESSORS', 'cv', 'image-driving-perception-preprocessor')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_driving_perception/preprocessor.py",
             "imports": [
-                "cv2",
-                "typing",
+                "torch",
                 "numpy",
-                "torch"
+                "cv2",
+                "typing"
             ],
             "module": "modelscope.models.cv.image_driving_perception.preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'image-instance-segmentation-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/image.py",
             "imports": [
-                "numpy",
+                "typing",
+                "io",
                 "PIL",
                 "cv2",
-                "typing",
-                "io"
+                "numpy"
             ],
             "module": "modelscope.preprocessors.image"
         },
         "('PREPROCESSORS', 'cv', 'image-portrait-enhancement-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/image.py",
             "imports": [
-                "numpy",
+                "typing",
+                "io",
                 "PIL",
                 "cv2",
-                "typing",
-                "io"
+                "numpy"
             ],
             "module": "modelscope.preprocessors.image"
         },
         "('PREPROCESSORS', 'cv', 'image-quality_assessment-man-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/cv/image_quality_assessment_man.py",
             "imports": [
                 "math",
+                "typing",
                 "torch",
-                "torchvision",
                 "PIL",
-                "typing",
+                "torchvision",
                 "numpy"
             ],
             "module": "modelscope.preprocessors.cv.image_quality_assessment_man"
         },
         "('PREPROCESSORS', 'cv', 'image-quality_assessment-mos-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/cv/image_quality_assessment_mos.py",
             "imports": [
                 "math",
-                "numpy",
-                "torchvision",
+                "typing",
                 "cv2",
-                "typing"
+                "torchvision",
+                "numpy"
             ],
             "module": "modelscope.preprocessors.cv.image_quality_assessment_mos"
         },
         "('PREPROCESSORS', 'cv', 'image-sky-change-preprocessor')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_skychange/preprocessor.py",
             "imports": [
-                "numbers",
+                "pdb",
+                "typing",
                 "torch",
-                "torchvision",
                 "cv2",
-                "typing",
-                "pdb",
+                "torchvision",
                 "numpy",
+                "numbers",
                 "json"
             ],
             "module": "modelscope.models.cv.image_skychange.preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'load-image')": {
             "filepath": "TEMPLATE_PATH/preprocessors/image.py",
             "imports": [
-                "numpy",
+                "typing",
+                "io",
                 "PIL",
                 "cv2",
-                "typing",
-                "io"
+                "numpy"
             ],
             "module": "modelscope.preprocessors.image"
         },
         "('PREPROCESSORS', 'cv', 'movie-scene-segmentation-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/video.py",
             "imports": [
-                "uuid",
+                "urllib",
+                "os",
                 "math",
-                "numpy",
                 "decord",
-                "urllib",
                 "torch",
-                "torchvision",
+                "uuid",
                 "tempfile",
                 "random",
-                "os"
+                "torchvision",
+                "numpy"
             ],
             "module": "modelscope.preprocessors.video"
         },
         "('PREPROCESSORS', 'cv', 'nerf-recon-acc-preprocessor')": {
-            "filepath": "TEMPLATE_PATH/models/cv/nerf_recon_4k/nerf_preprocess.py",
+            "filepath": "TEMPLATE_PATH/models/cv/nerf_recon_acc/nerf_preprocess.py",
             "imports": [
-                "glob",
-                "numpy",
-                "cv2",
-                "typing",
                 "os",
+                "typing",
                 "tensorflow",
-                "subprocess"
+                "glob",
+                "cv2",
+                "subprocess",
+                "numpy"
             ],
-            "module": "modelscope.models.cv.nerf_recon_4k.nerf_preprocess"
+            "module": "modelscope.models.cv.nerf_recon_acc.nerf_preprocess"
         },
         "('PREPROCESSORS', 'cv', 'object-detection-scrfd')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/preprocessor.py",
             "imports": [
+                "numpy",
                 "PIL",
-                "typing",
-                "numpy"
+                "typing"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'object-detection-tinynas-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/image.py",
             "imports": [
-                "numpy",
+                "typing",
+                "io",
                 "PIL",
                 "cv2",
-                "typing",
-                "io"
+                "numpy"
             ],
             "module": "modelscope.preprocessors.image"
         },
         "('PREPROCESSORS', 'cv', 'ocr-detection')": {
             "filepath": "TEMPLATE_PATH/models/cv/ocr_detection/preprocessor.py",
             "imports": [
+                "os",
                 "math",
-                "numpy",
+                "typing",
                 "torch",
                 "PIL",
                 "cv2",
-                "typing",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.models.cv.ocr_detection.preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'ocr-recognition')": {
             "filepath": "TEMPLATE_PATH/models/cv/ocr_recognition/preprocessor.py",
             "imports": [
+                "os",
                 "torch",
                 "PIL",
                 "cv2",
-                "numpy",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.models.cv.ocr_recognition.preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'video-summarization-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/image.py",
             "imports": [
-                "numpy",
+                "typing",
+                "io",
                 "PIL",
                 "cv2",
-                "typing",
-                "io"
+                "numpy"
             ],
             "module": "modelscope.preprocessors.image"
         },
         "('PREPROCESSORS', 'default', 'Compose')": {
             "filepath": "TEMPLATE_PATH/preprocessors/common.py",
             "imports": [
-                "numpy",
-                "torch",
                 "collections",
+                "typing",
                 "time",
-                "typing"
+                "torch",
+                "numpy"
             ],
             "module": "modelscope.preprocessors.common"
         },
         "('PREPROCESSORS', 'default', 'Filter')": {
             "filepath": "TEMPLATE_PATH/preprocessors/common.py",
             "imports": [
-                "numpy",
-                "torch",
                 "collections",
+                "typing",
                 "time",
-                "typing"
+                "torch",
+                "numpy"
             ],
             "module": "modelscope.preprocessors.common"
         },
         "('PREPROCESSORS', 'default', 'Identity')": {
             "filepath": "TEMPLATE_PATH/preprocessors/common.py",
             "imports": [
-                "numpy",
-                "torch",
                 "collections",
+                "typing",
                 "time",
-                "typing"
+                "torch",
+                "numpy"
             ],
             "module": "modelscope.preprocessors.common"
         },
         "('PREPROCESSORS', 'default', 'Rename')": {
             "filepath": "TEMPLATE_PATH/preprocessors/common.py",
             "imports": [
-                "numpy",
-                "torch",
                 "collections",
+                "typing",
                 "time",
-                "typing"
+                "torch",
+                "numpy"
             ],
             "module": "modelscope.preprocessors.common"
         },
         "('PREPROCESSORS', 'default', 'ToNumpy')": {
             "filepath": "TEMPLATE_PATH/preprocessors/common.py",
             "imports": [
-                "numpy",
-                "torch",
                 "collections",
+                "typing",
                 "time",
-                "typing"
+                "torch",
+                "numpy"
             ],
             "module": "modelscope.preprocessors.common"
         },
         "('PREPROCESSORS', 'default', 'ToTensor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/common.py",
             "imports": [
-                "numpy",
-                "torch",
                 "collections",
+                "typing",
                 "time",
-                "typing"
+                "torch",
+                "numpy"
             ],
             "module": "modelscope.preprocessors.common"
         },
         "('PREPROCESSORS', 'multi-modal', 'clip-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/multi_modal.py",
             "imports": [
-                "re",
-                "numpy",
+                "os",
+                "timm",
+                "typing",
                 "decord",
                 "torch",
-                "timm",
+                "io",
                 "PIL",
+                "re",
                 "torchvision",
-                "typing",
-                "os",
-                "json",
-                "io"
+                "numpy",
+                "json"
             ],
             "module": "modelscope.preprocessors.multi_modal"
         },
         "('PREPROCESSORS', 'multi-modal', 'diffusion-image-generation-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/multi_modal.py",
             "imports": [
-                "re",
-                "numpy",
+                "os",
+                "timm",
+                "typing",
                 "decord",
                 "torch",
-                "timm",
+                "io",
                 "PIL",
+                "re",
                 "torchvision",
-                "typing",
-                "os",
-                "json",
-                "io"
+                "numpy",
+                "json"
             ],
             "module": "modelscope.preprocessors.multi_modal"
         },
         "('PREPROCESSORS', 'multi-modal', 'hitea-tasks-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/multi_modal.py",
             "imports": [
-                "re",
-                "numpy",
+                "os",
+                "timm",
+                "typing",
                 "decord",
                 "torch",
-                "timm",
+                "io",
                 "PIL",
+                "re",
                 "torchvision",
-                "typing",
-                "os",
-                "json",
-                "io"
+                "numpy",
+                "json"
             ],
             "module": "modelscope.preprocessors.multi_modal"
         },
         "('PREPROCESSORS', 'multi-modal', 'image-captioning-clip-interrogator-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/multi_modal.py",
             "imports": [
-                "re",
-                "numpy",
+                "os",
+                "timm",
+                "typing",
                 "decord",
                 "torch",
-                "timm",
+                "io",
                 "PIL",
+                "re",
                 "torchvision",
-                "typing",
-                "os",
-                "json",
-                "io"
+                "numpy",
+                "json"
             ],
             "module": "modelscope.preprocessors.multi_modal"
         },
         "('PREPROCESSORS', 'multi-modal', 'mplug-owl-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/multi_modal.py",
             "imports": [
-                "re",
-                "numpy",
+                "os",
+                "timm",
+                "typing",
                 "decord",
                 "torch",
-                "timm",
+                "io",
                 "PIL",
+                "re",
                 "torchvision",
-                "typing",
-                "os",
-                "json",
-                "io"
+                "numpy",
+                "json"
             ],
             "module": "modelscope.preprocessors.multi_modal"
         },
         "('PREPROCESSORS', 'multi-modal', 'mplug-tasks-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/multi_modal.py",
             "imports": [
-                "re",
-                "numpy",
+                "os",
+                "timm",
+                "typing",
                 "decord",
                 "torch",
-                "timm",
+                "io",
                 "PIL",
+                "re",
                 "torchvision",
-                "typing",
-                "os",
-                "json",
-                "io"
+                "numpy",
+                "json"
             ],
             "module": "modelscope.preprocessors.multi_modal"
         },
         "('PREPROCESSORS', 'multi-modal', 'ofa-tasks-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/multi_modal.py",
             "imports": [
-                "re",
-                "numpy",
+                "os",
+                "timm",
+                "typing",
                 "decord",
                 "torch",
-                "timm",
+                "io",
                 "PIL",
+                "re",
                 "torchvision",
-                "typing",
-                "os",
-                "json",
-                "io"
+                "numpy",
+                "json"
             ],
             "module": "modelscope.preprocessors.multi_modal"
         },
         "('PREPROCESSORS', 'multi-modal', 'vldoc-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/multi_modal.py",
             "imports": [
-                "re",
-                "numpy",
+                "os",
+                "timm",
+                "typing",
                 "decord",
                 "torch",
-                "timm",
+                "io",
                 "PIL",
+                "re",
                 "torchvision",
-                "typing",
-                "os",
-                "json",
-                "io"
+                "numpy",
+                "json"
             ],
             "module": "modelscope.preprocessors.multi_modal"
         },
         "('PREPROCESSORS', 'nlp', 'Tokenize')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/bert_seq_cls_tokenizer.py",
             "imports": [
                 "transformers",
                 "typing"
             ],
             "module": "modelscope.preprocessors.nlp.bert_seq_cls_tokenizer"
         },
         "('PREPROCESSORS', 'nlp', 'bert-seq-cls-tokenizer')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/text_classification_preprocessor.py",
             "imports": [
-                "typing",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.preprocessors.nlp.text_classification_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'canmt-translation')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/canmt_translation.py",
             "imports": [
                 "sacremoses",
-                "torch",
-                "subword_nmt",
                 "jieba",
+                "os",
                 "typing",
-                "os"
+                "subword_nmt",
+                "torch"
             ],
             "module": "modelscope.preprocessors.nlp.canmt_translation"
         },
         "('PREPROCESSORS', 'nlp', 'conversational-text-to-sql')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/space_T_en/conversational_text_to_sql_preprocessor.py",
             "imports": [
-                "torch",
-                "text2sql_lgesql",
                 "json",
+                "os",
                 "typing",
-                "os"
+                "torch",
+                "text2sql_lgesql"
             ],
             "module": "modelscope.preprocessors.nlp.space_T_en.conversational_text_to_sql_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'dialog-intent-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/space/dialog_intent_prediction_preprocessor.py",
             "imports": [
-                "typing",
                 "os",
-                "json"
+                "json",
+                "typing"
             ],
             "module": "modelscope.preprocessors.nlp.space.dialog_intent_prediction_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'dialog-modeling-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/space/dialog_modeling_preprocessor.py",
             "imports": [
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.preprocessors.nlp.space.dialog_modeling_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'dialog-state-tracking-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/space/dialog_state_tracking_preprocessor.py",
             "imports": [
                 "typing"
             ],
             "module": "modelscope.preprocessors.nlp.space.dialog_state_tracking_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'dialog-use-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/dialog_classification_use_preprocessor.py",
             "imports": [
                 "transformers",
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.preprocessors.nlp.dialog_classification_use_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'document-grounded-dialog-generate')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/document_grounded_dialog_generate_preprocessor.py",
             "imports": [
-                "transformers",
-                "typing",
+                "os",
                 "torch",
-                "os"
+                "transformers",
+                "typing"
             ],
             "module": "modelscope.preprocessors.nlp.document_grounded_dialog_generate_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'document-grounded-dialog-rerank')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/document_grounded_dialog_rerank_preprocessor.py",
             "imports": [
-                "torch",
-                "copy",
+                "os",
                 "transformers",
+                "copy",
                 "typing",
-                "os"
+                "torch"
             ],
             "module": "modelscope.preprocessors.nlp.document_grounded_dialog_rerank_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'document-grounded-dialog-retrieval')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/document_grounded_dialog_retrieval_preprocessor.py",
             "imports": [
-                "transformers",
-                "typing",
+                "os",
                 "torch",
-                "os"
+                "transformers",
+                "typing"
             ],
             "module": "modelscope.preprocessors.nlp.document_grounded_dialog_retrieval_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'document-segmentation')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/document_segmentation_preprocessor.py",
             "imports": [
                 "typing"
             ],
             "module": "modelscope.preprocessors.nlp.document_segmentation_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'faq-question-answering-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/faq_question_answering_preprocessor.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.preprocessors.nlp.faq_question_answering_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'feature-extraction')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/feature_extraction_preprocessor.py",
             "imports": [
-                "typing",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.preprocessors.nlp.feature_extraction_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'fill-mask')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/fill_mask_preprocessor.py",
             "imports": [
+                "os",
                 "abc",
-                "re",
-                "numpy",
-                "torch",
                 "typing",
-                "os"
+                "torch",
+                "re",
+                "numpy"
             ],
             "module": "modelscope.preprocessors.nlp.fill_mask_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'fill-mask-ponet')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/fill_mask_preprocessor.py",
             "imports": [
+                "os",
                 "abc",
-                "re",
-                "numpy",
-                "torch",
                 "typing",
-                "os"
+                "torch",
+                "re",
+                "numpy"
             ],
             "module": "modelscope.preprocessors.nlp.fill_mask_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'mgeo-ranking')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/mgeo_ranking_preprocessor.py",
             "imports": [
                 "transformers",
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.preprocessors.nlp.mgeo_ranking_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'mglm-summarization')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/mglm_summarization_preprocessor.py",
             "imports": [
+                "os",
                 "re",
-                "typing",
-                "os"
+                "typing"
             ],
             "module": "modelscope.preprocessors.nlp.mglm_summarization_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'ner-tokenizer')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/token_classification_preprocessor.py",
             "imports": [
-                "typing",
+                "torch",
                 "numpy",
-                "torch"
+                "typing"
             ],
             "module": "modelscope.preprocessors.nlp.token_classification_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'nli-tokenizer')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/text_classification_preprocessor.py",
             "imports": [
-                "typing",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.preprocessors.nlp.text_classification_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 're-tokenizer')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/relation_extraction_preprocessor.py",
             "imports": [
                 "transformers",
                 "typing"
             ],
             "module": "modelscope.preprocessors.nlp.relation_extraction_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'sen-cls-tokenizer')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/text_classification_preprocessor.py",
             "imports": [
-                "typing",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.preprocessors.nlp.text_classification_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'sen-sim-tokenizer')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/text_classification_preprocessor.py",
             "imports": [
-                "typing",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.preprocessors.nlp.text_classification_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'sentence-embedding')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/sentence_embedding_preprocessor.py",
             "imports": [
                 "typing"
             ],
             "module": "modelscope.preprocessors.nlp.sentence_embedding_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'sentence-piece')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/text_generation_preprocessor.py",
             "imports": [
-                "typing",
-                "numpy",
+                "os",
                 "torch",
-                "os"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.preprocessors.nlp.text_generation_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'sequence-labeling-tokenizer')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/token_classification_preprocessor.py",
             "imports": [
-                "typing",
+                "torch",
                 "numpy",
-                "torch"
+                "typing"
             ],
             "module": "modelscope.preprocessors.nlp.token_classification_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'siamese-uie-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/siamese_uie_preprocessor.py",
             "imports": [
                 "transformers",
                 "typing"
             ],
             "module": "modelscope.preprocessors.nlp.siamese_uie_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'table-question-answering-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/space_T_cn/table_question_answering_preprocessor.py",
             "imports": [
-                "transformers",
-                "typing",
+                "os",
                 "torch",
-                "os"
+                "transformers",
+                "typing"
             ],
             "module": "modelscope.preprocessors.nlp.space_T_cn.table_question_answering_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'text-error-correction')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/text_error_correction.py",
             "imports": [
-                "transformers",
-                "typing",
+                "os",
                 "torch",
-                "os"
+                "transformers",
+                "typing"
             ],
             "module": "modelscope.preprocessors.nlp.text_error_correction"
         },
         "('PREPROCESSORS', 'nlp', 'text-gen-jieba-tokenizer')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/text_generation_preprocessor.py",
             "imports": [
-                "typing",
-                "numpy",
+                "os",
                 "torch",
-                "os"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.preprocessors.nlp.text_generation_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'text-gen-tokenizer')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/text_generation_preprocessor.py",
             "imports": [
-                "typing",
-                "numpy",
+                "os",
                 "torch",
-                "os"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.preprocessors.nlp.text_generation_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'text-ranking')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/text_ranking_preprocessor.py",
             "imports": [
                 "transformers",
                 "typing"
             ],
             "module": "modelscope.preprocessors.nlp.text_ranking_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'text2text-gen-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/text_generation_preprocessor.py",
             "imports": [
-                "typing",
-                "numpy",
+                "os",
                 "torch",
-                "os"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.preprocessors.nlp.text_generation_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'thai-ner-tokenizer')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/token_classification_thai_preprocessor.py",
             "imports": [
                 "typing"
@@ -9910,530 +9845,530 @@
                 "typing"
             ],
             "module": "modelscope.preprocessors.nlp.token_classification_thai_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'token-cls-tokenizer')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/token_classification_preprocessor.py",
             "imports": [
-                "typing",
+                "torch",
                 "numpy",
-                "torch"
+                "typing"
             ],
             "module": "modelscope.preprocessors.nlp.token_classification_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'translation-evaluation-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/translation_evaluation_preprocessor.py",
             "imports": [
                 "transformers",
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.preprocessors.nlp.translation_evaluation_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'viet-ner-tokenizer')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/token_classification_viet_preprocessor.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.preprocessors.nlp.token_classification_viet_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'word-alignment')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/word_alignment_preprocessor.py",
             "imports": [
-                "numpy",
-                "itertools",
-                "torch",
+                "os",
                 "typing",
-                "os"
+                "torch",
+                "itertools",
+                "numpy"
             ],
             "module": "modelscope.preprocessors.nlp.word_alignment_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'word-segment-text-to-label-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/token_classification_preprocessor.py",
             "imports": [
-                "typing",
+                "torch",
                 "numpy",
-                "torch"
+                "typing"
             ],
             "module": "modelscope.preprocessors.nlp.token_classification_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'zero-shot-cls-tokenizer')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/zero_shot_classification_preprocessor.py",
             "imports": [
                 "typing"
             ],
             "module": "modelscope.preprocessors.nlp.zero_shot_classification_preprocessor"
         },
         "('PREPROCESSORS', 'science', 'unifold-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/science/uni_fold.py",
             "imports": [
-                "re",
                 "tarfile",
-                "pathlib",
-                "requests",
-                "torch",
-                "pickle",
-                "typing",
-                "os",
-                "numpy",
+                "hashlib",
+                "logging",
                 "tqdm",
-                "gzip",
                 "time",
-                "hashlib",
+                "pickle",
+                "torch",
+                "gzip",
+                "re",
+                "numpy",
                 "ipdb",
-                "logging",
+                "os",
+                "requests",
+                "typing",
+                "pathlib",
                 "random",
                 "unittest",
                 "json"
             ],
             "module": "modelscope.preprocessors.science.uni_fold"
         },
         "('PREPROCESSORS', 'text-to-speech', 'kantts-data-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/tts.py",
             "imports": [
+                "os",
                 "kantts",
-                "typing",
-                "os"
+                "typing"
             ],
             "module": "modelscope.preprocessors.tts"
         },
         "('ROI_EXTRACTORS', 'default', 'SingleRoINExtractor')": {
             "filepath": "TEMPLATE_PATH/models/cv/abnormal_object_detection/mmdet_ms/roi_head/roi_extractors/single_level_roi_extractor.py",
             "imports": [
-                "mmcv",
+                "mmdet",
                 "torch",
-                "mmdet"
+                "mmcv"
             ],
             "module": "modelscope.models.cv.abnormal_object_detection.mmdet_ms.roi_head.roi_extractors.single_level_roi_extractor"
         },
         "('TRACKERS', 'default', 'QuasiDenseEmbedTracker')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/track/quasi_dense_embed_tracker.py",
             "imports": [
-                "mmcv",
+                "mmdet",
                 "torch",
-                "mmdet"
+                "mmcv"
             ],
             "module": "modelscope.models.cv.video_panoptic_segmentation.track.quasi_dense_embed_tracker"
         },
         "('TRAINERS', 'default', 'action-detection')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/action_detection_trainer.py",
             "imports": [
-                "fvcore",
+                "os",
                 "detectron2",
-                "torch",
+                "fvcore",
                 "typing",
-                "os"
+                "torch"
             ],
             "module": "modelscope.trainers.cv.action_detection_trainer"
         },
         "('TRAINERS', 'default', 'bert-sentiment-analysis')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/sequence_classification_trainer.py",
             "imports": [
-                "typing",
                 "time",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.trainers.nlp.sequence_classification_trainer"
         },
         "('TRAINERS', 'default', 'card-detection-scrfd')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/card_detection_scrfd_trainer.py",
             "imports": [],
             "module": "modelscope.trainers.cv.card_detection_scrfd_trainer"
         },
         "('TRAINERS', 'default', 'cartoon-translation')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/cartoon_translation_trainer.py",
             "imports": [
-                "numpy",
                 "os",
+                "tqdm",
                 "typing",
                 "tensorflow",
                 "packaging",
-                "tqdm"
+                "numpy"
             ],
             "module": "modelscope.trainers.cv.cartoon_translation_trainer"
         },
         "('TRAINERS', 'default', 'clip-multi-modal-embedding')": {
             "filepath": "TEMPLATE_PATH/trainers/multi_modal/clip/clip_trainer.py",
             "imports": [
+                "os",
                 "torch",
-                "typing",
                 "math",
-                "os"
+                "typing"
             ],
             "module": "modelscope.trainers.multi_modal.clip.clip_trainer"
         },
         "('TRAINERS', 'default', 'csanmt-translation')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/csanmt_translation_trainer.py",
             "imports": [
-                "typing",
-                "tensorflow",
+                "os",
                 "time",
-                "os"
+                "tensorflow",
+                "typing"
             ],
             "module": "modelscope.trainers.nlp.csanmt_translation_trainer"
         },
         "('TRAINERS', 'default', 'custom-diffusion')": {
             "filepath": "TEMPLATE_PATH/trainers/multi_modal/custom_diffusion/custom_diffusion_trainer.py",
             "imports": [
-                "diffusers",
-                "numpy",
-                "itertools",
-                "torch",
-                "pathlib",
-                "PIL",
-                "torchvision",
-                "warnings",
                 "hashlib",
+                "os",
+                "warnings",
+                "tqdm",
                 "typing",
+                "pathlib",
+                "torch",
+                "diffusers",
+                "PIL",
+                "itertools",
                 "random",
-                "os",
-                "json",
-                "tqdm"
+                "torchvision",
+                "numpy",
+                "json"
             ],
             "module": "modelscope.trainers.multi_modal.custom_diffusion.custom_diffusion_trainer"
         },
         "('TRAINERS', 'default', 'dialog-intent-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/space/dialog_intent_trainer.py",
             "imports": [
-                "typing",
                 "os",
-                "numpy"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.trainers.nlp.space.dialog_intent_trainer"
         },
         "('TRAINERS', 'default', 'dialog-modeling-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/space/dialog_modeling_trainer.py",
             "imports": [
-                "typing",
-                "numpy",
                 "os",
-                "time"
+                "time",
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.trainers.nlp.space.dialog_modeling_trainer"
         },
         "('TRAINERS', 'default', 'document-grounded-dialog-generate-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/document_grounded_dialog_generate_trainer.py",
             "imports": [
-                "re",
-                "torch",
                 "collections",
                 "sacrebleu",
-                "rouge",
                 "string",
-                "transformers",
                 "os",
-                "json",
-                "tqdm"
+                "tqdm",
+                "transformers",
+                "rouge",
+                "torch",
+                "re",
+                "json"
             ],
             "module": "modelscope.trainers.nlp.document_grounded_dialog_generate_trainer"
         },
         "('TRAINERS', 'default', 'document-grounded-dialog-rerank-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/document_grounded_dialog_rerank_trainer.py",
             "imports": [
-                "numpy",
-                "torch",
-                "time",
+                "os",
+                "transformers",
                 "typing",
+                "time",
+                "torch",
                 "random",
-                "transformers",
-                "os"
+                "numpy"
             ],
             "module": "modelscope.trainers.nlp.document_grounded_dialog_rerank_trainer"
         },
         "('TRAINERS', 'default', 'document-grounded-dialog-retrieval-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/document_grounded_dialog_retrieval_trainer.py",
             "imports": [
+                "os",
+                "tqdm",
+                "transformers",
+                "torch",
                 "faiss",
                 "numpy",
-                "torch",
-                "transformers",
-                "os",
-                "json",
-                "tqdm"
+                "json"
             ],
             "module": "modelscope.trainers.nlp.document_grounded_dialog_retrieval_trainer"
         },
         "('TRAINERS', 'default', 'dreambooth-diffusion')": {
             "filepath": "TEMPLATE_PATH/trainers/multi_modal/dreambooth_diffusion/dreambooth_diffusion_trainer.py",
             "imports": [
-                "diffusers",
-                "itertools",
-                "torch",
-                "pathlib",
                 "collections",
-                "PIL",
-                "torchvision",
-                "warnings",
                 "hashlib",
+                "tqdm",
+                "warnings",
                 "typing",
+                "pathlib",
+                "torch",
+                "diffusers",
+                "PIL",
+                "itertools",
                 "shutil",
-                "tqdm"
+                "torchvision"
             ],
             "module": "modelscope.trainers.multi_modal.dreambooth_diffusion.dreambooth_diffusion_trainer"
         },
         "('TRAINERS', 'default', 'dummy')": {
             "filepath": "TEMPLATE_PATH/trainers/base.py",
             "imports": [
-                "abc",
-                "typing",
                 "os",
-                "time"
+                "time",
+                "abc",
+                "typing"
             ],
             "module": "modelscope.trainers.base"
         },
         "('TRAINERS', 'default', 'efficient-diffusion-tuning')": {
             "filepath": "TEMPLATE_PATH/trainers/multi_modal/efficient_diffusion_tuning/efficient_diffusion_tuning_trainer.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.trainers.multi_modal.efficient_diffusion_tuning.efficient_diffusion_tuning_trainer"
         },
         "('TRAINERS', 'default', 'face-detection-scrfd')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/face_detection_scrfd_trainer.py",
             "imports": [
-                "copy",
-                "typing",
                 "os",
-                "time"
+                "time",
+                "copy",
+                "typing"
             ],
             "module": "modelscope.trainers.cv.face_detection_scrfd_trainer"
         },
         "('TRAINERS', 'default', 'faq-question-answering-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/faq_question_answering_trainer.py",
             "imports": [
-                "distutils",
-                "dataclasses",
-                "torch",
                 "collections",
+                "functools",
+                "distutils",
                 "contextlib",
                 "typing",
-                "functools",
+                "torch",
+                "dataclasses",
                 "numpy"
             ],
             "module": "modelscope.trainers.nlp.faq_question_answering_trainer"
         },
         "('TRAINERS', 'default', 'image-classification')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/image_classifition_trainer.py",
             "imports": [
-                "numpy",
-                "torch",
+                "os",
                 "copy",
-                "time",
                 "typing",
-                "os"
+                "time",
+                "torch",
+                "numpy"
             ],
             "module": "modelscope.trainers.cv.image_classifition_trainer"
         },
         "('TRAINERS', 'default', 'image-classification-team')": {
             "filepath": "TEMPLATE_PATH/trainers/multi_modal/team/team_trainer.py",
             "imports": [
-                "numpy",
-                "torch",
                 "collections",
-                "typing",
                 "sklearn",
-                "os"
+                "os",
+                "typing",
+                "torch",
+                "numpy"
             ],
             "module": "modelscope.trainers.multi_modal.team.team_trainer"
         },
         "('TRAINERS', 'default', 'image-fewshot-detection')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/image_defrcn_fewshot_detection_trainer.py",
             "imports": [
-                "detectron2",
-                "torch",
                 "collections",
+                "os",
+                "detectron2",
                 "typing",
-                "os"
+                "torch"
             ],
             "module": "modelscope.trainers.cv.image_defrcn_fewshot_detection_trainer"
         },
         "('TRAINERS', 'default', 'image-inpainting')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/image_inpainting_trainer.py",
             "imports": [
-                "torch",
                 "time",
+                "torch",
                 "collections"
             ],
             "module": "modelscope.trainers.cv.image_inpainting_trainer"
         },
         "('TRAINERS', 'default', 'image-instance-segmentation')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/image_instance_segmentation_trainer.py",
             "imports": [],
             "module": "modelscope.trainers.cv.image_instance_segmentation_trainer"
         },
         "('TRAINERS', 'default', 'image-portrait-enhancement')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/image_portrait_enhancement_trainer.py",
             "imports": [
-                "torch",
-                "collections"
+                "collections",
+                "torch"
             ],
             "module": "modelscope.trainers.cv.image_portrait_enhancement_trainer"
         },
         "('TRAINERS', 'default', 'lora-diffusion')": {
             "filepath": "TEMPLATE_PATH/trainers/multi_modal/lora_diffusion/lora_diffusion_trainer.py",
             "imports": [
-                "typing",
                 "torch",
-                "diffusers"
+                "diffusers",
+                "typing"
             ],
             "module": "modelscope.trainers.multi_modal.lora_diffusion.lora_diffusion_trainer"
         },
         "('TRAINERS', 'default', 'mgeo-ranking-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/multi_modal/mgeo_ranking_trainer.py",
             "imports": [
                 "dataclasses",
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.trainers.multi_modal.mgeo_ranking_trainer"
         },
         "('TRAINERS', 'default', 'movie-scene-segmentation')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/movie_scene_segmentation_trainer.py",
             "imports": [],
             "module": "modelscope.trainers.cv.movie_scene_segmentation_trainer"
         },
         "('TRAINERS', 'default', 'mplug')": {
             "filepath": "TEMPLATE_PATH/trainers/multi_modal/mplug/mplug_trainer.py",
             "imports": [
-                "typing",
+                "collections",
                 "torch",
-                "collections"
+                "typing"
             ],
             "module": "modelscope.trainers.multi_modal.mplug.mplug_trainer"
         },
         "('TRAINERS', 'default', 'nerf-recon-acc')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/nerf_recon_acc_trainer.py",
             "imports": [
-                "glob",
-                "numpy",
-                "torch",
                 "datetime",
-                "time",
+                "os",
+                "tqdm",
                 "cv2",
                 "typing",
+                "time",
+                "torch",
+                "glob",
                 "random",
-                "os",
-                "tqdm"
+                "numpy"
             ],
             "module": "modelscope.trainers.cv.nerf_recon_acc_trainer"
         },
         "('TRAINERS', 'default', 'nlp-base-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp_trainer.py",
             "imports": [
-                "typing",
-                "numpy",
+                "os",
                 "torch",
-                "os"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.trainers.nlp_trainer"
         },
         "('TRAINERS', 'default', 'nlp-gpt-moe-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/gpt_moe_trainer.py",
             "imports": [
                 "megatron_util",
-                "torch",
                 "collections",
+                "os",
                 "typing",
-                "os"
+                "torch"
             ],
             "module": "modelscope.trainers.nlp.gpt_moe_trainer"
         },
         "('TRAINERS', 'default', 'nlp-gpt3-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/gpt3_trainer.py",
             "imports": [
-                "typing",
-                "torch",
+                "os",
                 "copy",
-                "os"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.trainers.nlp.gpt3_trainer"
         },
         "('TRAINERS', 'default', 'nlp-plug-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/plug_trainer.py",
             "imports": [
                 "megatron_util",
                 "deepspeed",
-                "torch",
+                "os",
                 "typing",
-                "os"
+                "torch"
             ],
             "module": "modelscope.trainers.nlp.plug_trainer"
         },
         "('TRAINERS', 'default', 'nlp-sentence-embedding-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/sentence_embedding_trainer.py",
             "imports": [
-                "numpy",
-                "dataclasses",
-                "torch",
-                "time",
+                "tqdm",
                 "transformers",
                 "typing",
-                "tqdm"
-            ],
-            "module": "modelscope.trainers.nlp.sentence_embedding_trainer"
+                "time",
+                "torch",
+                "dataclasses",
+                "numpy"
+            ],
+            "module": "modelscope.trainers.nlp.sentence_embedding_trainer"
         },
         "('TRAINERS', 'default', 'nlp-text-ranking-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/text_ranking_trainer.py",
             "imports": [
-                "numpy",
-                "dataclasses",
-                "torch",
-                "time",
+                "tqdm",
                 "typing",
-                "tqdm"
+                "time",
+                "torch",
+                "dataclasses",
+                "numpy"
             ],
             "module": "modelscope.trainers.nlp.text_ranking_trainer"
         },
         "('TRAINERS', 'default', 'nlp-veco-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp_trainer.py",
             "imports": [
-                "typing",
-                "numpy",
+                "os",
                 "torch",
-                "os"
+                "numpy",
+                "typing"
             ],
             "module": "modelscope.trainers.nlp_trainer"
         },
         "('TRAINERS', 'default', 'ocr-detection-db')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/ocr_detection_db_trainer.py",
             "imports": [
+                "datetime",
+                "os",
+                "tqdm",
+                "copy",
                 "math",
-                "numpy",
-                "torch",
-                "easydict",
                 "time",
-                "copy",
                 "typing",
-                "datetime",
-                "os",
-                "tqdm"
+                "torch",
+                "easydict",
+                "numpy"
             ],
             "module": "modelscope.trainers.cv.ocr_detection_db_trainer"
         },
         "('TRAINERS', 'default', 'ocr-recognition')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/ocr_recognition_trainer.py",
             "imports": [
-                "torch",
                 "time",
+                "torch",
                 "collections"
             ],
             "module": "modelscope.trainers.cv.ocr_recognition_trainer"
         },
         "('TRAINERS', 'default', 'ofa')": {
             "filepath": "TEMPLATE_PATH/trainers/multi_modal/ofa/ofa_trainer.py",
             "imports": [
-                "math",
+                "functools",
+                "os",
                 "shutil",
+                "typing",
+                "math",
                 "torch",
                 "tempfile",
-                "os",
-                "functools",
-                "typing",
                 "json"
             ],
             "module": "modelscope.trainers.multi_modal.ofa.ofa_trainer"
         },
         "('TRAINERS', 'default', 'referring-video-object-segmentation')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/referring_video_object_segmentation_trainer.py",
             "imports": [
@@ -10441,987 +10376,987 @@
                 "torch"
             ],
             "module": "modelscope.trainers.cv.referring_video_object_segmentation_trainer"
         },
         "('TRAINERS', 'default', 'siamese-uie-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/siamese_uie_trainer.py",
             "imports": [
-                "math",
-                "torch",
                 "collections",
-                "json",
-                "time",
+                "os",
                 "typing",
+                "time",
+                "math",
+                "torch",
                 "random",
                 "numpy",
-                "os"
+                "json"
             ],
             "module": "modelscope.trainers.nlp.siamese_uie_trainer"
         },
         "('TRAINERS', 'default', 'speech-asr-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/audio/asr_trainer.py",
             "imports": [
+                "os",
                 "funasr",
                 "shutil",
-                "tempfile",
-                "os",
                 "typing",
+                "tempfile",
                 "json"
             ],
             "module": "modelscope.trainers.audio.asr_trainer"
         },
         "('TRAINERS', 'default', 'speech-kantts-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/audio/tts_trainer.py",
             "imports": [
-                "shutil",
-                "tempfile",
                 "zipfile",
                 "os",
+                "shutil",
                 "typing",
+                "tempfile",
                 "json"
             ],
             "module": "modelscope.trainers.audio.tts_trainer"
         },
         "('TRAINERS', 'default', 'speech-separation')": {
             "filepath": "TEMPLATE_PATH/trainers/audio/separation_trainer.py",
             "imports": [
-                "speechbrain",
                 "csv",
-                "numpy",
-                "torchaudio",
-                "torch",
-                "typing",
                 "os",
-                "tqdm"
+                "tqdm",
+                "typing",
+                "speechbrain",
+                "torch",
+                "torchaudio",
+                "numpy"
             ],
             "module": "modelscope.trainers.audio.separation_trainer"
         },
         "('TRAINERS', 'default', 'speech_dfsmn_kws_char_farfield')": {
             "filepath": "TEMPLATE_PATH/trainers/audio/kws_farfield_trainer.py",
             "imports": [
-                "glob",
+                "datetime",
+                "os",
                 "math",
-                "numpy",
-                "torch",
                 "typing",
                 "pickle",
-                "datetime",
-                "os"
+                "torch",
+                "glob",
+                "numpy"
             ],
             "module": "modelscope.trainers.audio.kws_farfield_trainer"
         },
         "('TRAINERS', 'default', 'speech_frcrn_ans_cirm_16k')": {
             "filepath": "TEMPLATE_PATH/trainers/audio/ans_trainer.py",
             "imports": [],
             "module": "modelscope.trainers.audio.ans_trainer"
         },
         "('TRAINERS', 'default', 'speech_kws_fsmn_char_ctc_nearfield')": {
             "filepath": "TEMPLATE_PATH/trainers/audio/kws_nearfield_trainer.py",
             "imports": [
-                "yaml",
-                "re",
-                "torch",
-                "tensorboardX",
-                "copy",
                 "datetime",
+                "os",
+                "copy",
                 "typing",
-                "os"
+                "yaml",
+                "torch",
+                "re",
+                "tensorboardX"
             ],
             "module": "modelscope.trainers.audio.kws_nearfield_trainer"
         },
         "('TRAINERS', 'default', 'stable-diffusion')": {
             "filepath": "TEMPLATE_PATH/trainers/multi_modal/stable_diffusion/stable_diffusion_trainer.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.trainers.multi_modal.stable_diffusion.stable_diffusion_trainer"
         },
         "('TRAINERS', 'default', 'table-question-answering-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/table_question_answering_trainer.py",
             "imports": [
-                "numpy",
-                "torch",
-                "time",
-                "typing",
                 "os",
-                "json",
-                "tqdm"
+                "tqdm",
+                "typing",
+                "time",
+                "torch",
+                "numpy",
+                "json"
             ],
             "module": "modelscope.trainers.nlp.table_question_answering_trainer"
         },
         "('TRAINERS', 'default', 'text-generation-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/text_generation_trainer.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.trainers.nlp.text_generation_trainer"
         },
         "('TRAINERS', 'default', 'tinynas-damoyolo')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/image_detection_damoyolo_trainer.py",
             "imports": [
+                "datetime",
+                "os",
                 "math",
-                "torch",
-                "easydict",
                 "time",
                 "typing",
-                "datetime",
-                "os"
+                "torch",
+                "easydict"
             ],
             "module": "modelscope.trainers.cv.image_detection_damoyolo_trainer"
         },
         "('TRAINERS', 'default', 'trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/trainer.py",
             "imports": [
-                "distutils",
-                "torch",
                 "collections",
-                "inspect",
+                "functools",
+                "distutils",
                 "os",
                 "copy",
-                "functools",
                 "typing",
+                "torch",
+                "inspect",
                 "json"
             ],
             "module": "modelscope.trainers.trainer"
         },
         "('TRAINERS', 'default', 'translation-evaluation-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/translation_evaluation_trainer.py",
             "imports": [
+                "os",
+                "tqdm",
+                "transformers",
+                "typing",
                 "math",
-                "pandas",
                 "torch",
                 "random",
-                "transformers",
-                "typing",
-                "os",
-                "tqdm"
+                "pandas"
             ],
             "module": "modelscope.trainers.nlp.translation_evaluation_trainer"
         },
         "('TRAINERS', 'default', 'vision-efficient-tuning')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/vision_efficient_tuning_trainer.py",
             "imports": [
-                "typing",
-                "torch"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.trainers.cv.vision_efficient_tuning_trainer"
         },
         "('TRANSFORMER', 'default', 'PETRDNTransformer')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/petr_transformer.py",
             "imports": [
-                "math",
+                "torch",
+                "mmcv",
                 "warnings",
+                "mmdet",
                 "copy",
-                "mmcv",
-                "typing",
-                "torch",
-                "mmdet"
+                "math",
+                "typing"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.utils.petr_transformer"
         },
         "('TRANSFORMER_LAYER', 'default', 'KernelUpdator')": {
-            "filepath": "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/kernel_updator.py",
+            "filepath": "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_updator.py",
             "imports": [
-                "mmcv",
-                "torch"
+                "torch",
+                "mmcv"
             ],
-            "module": "modelscope.models.cv.video_panoptic_segmentation.head.kernel_updator"
+            "module": "modelscope.models.cv.video_instance_segmentation.head.kernel_updator"
         },
         "('TRANSFORMER_LAYER', 'default', 'PETRTransformerDecoderLayer')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/petr_transformer.py",
             "imports": [
-                "math",
+                "torch",
+                "mmcv",
                 "warnings",
+                "mmdet",
                 "copy",
-                "mmcv",
-                "typing",
-                "torch",
-                "mmdet"
+                "math",
+                "typing"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.utils.petr_transformer"
         },
         "('TRANSFORMER_LAYER_SEQUENCE', 'default', 'PETRTransformerDecoder')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/petr_transformer.py",
             "imports": [
-                "math",
+                "torch",
+                "mmcv",
                 "warnings",
+                "mmdet",
                 "copy",
-                "mmcv",
-                "typing",
-                "torch",
-                "mmdet"
+                "math",
+                "typing"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.utils.petr_transformer"
         },
         "('TRANSFORMER_LAYER_SEQUENCE', 'default', 'PETRTransformerEncoder')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/petr_transformer.py",
             "imports": [
-                "math",
+                "torch",
+                "mmcv",
                 "warnings",
+                "mmdet",
                 "copy",
-                "mmcv",
-                "typing",
-                "torch",
-                "mmdet"
+                "math",
+                "typing"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.utils.petr_transformer"
         }
     },
-    "md5": "81939cc6fb1ae5d0373a885bf843d4a0",
+    "md5": "4b40ac8d80a58f5de056693197bb6d18",
     "modelscope_path": "TEMPLATE_PATH",
     "requirements": {
         "modelscope.exporters.audio.ans_dfsmn_exporter": [
             "os",
             "torch"
         ],
         "modelscope.exporters.base": [
+            "os",
             "abc",
-            "typing",
-            "os"
+            "typing"
         ],
         "modelscope.exporters.builder": [],
         "modelscope.exporters.cv.cartoon_translation_exporter": [
-            "typing",
-            "tensorflow",
+            "os",
             "packaging",
-            "os"
+            "tensorflow",
+            "typing"
         ],
         "modelscope.exporters.cv.face_detection_scrfd_exporter": [
-            "numpy",
-            "onnx",
-            "torch",
             "functools",
+            "os",
             "typing",
-            "os"
+            "torch",
+            "onnx",
+            "numpy"
         ],
         "modelscope.exporters.cv.object_detection_damoyolo_exporter": [
-            "numpy",
-            "onnx",
-            "torch",
             "functools",
+            "os",
             "typing",
-            "os"
+            "torch",
+            "onnx",
+            "numpy"
         ],
         "modelscope.exporters.multi_modal.stable_diffusion_exporter": [
-            "diffusers",
-            "onnx",
-            "torch",
-            "pathlib",
             "collections",
-            "argparse",
-            "typing",
-            "shutil",
             "os",
-            "packaging"
+            "typing",
+            "argparse",
+            "pathlib",
+            "torch",
+            "onnx",
+            "diffusers",
+            "packaging",
+            "shutil"
         ],
         "modelscope.exporters.nlp.csanmt_for_translation_exporter": [
-            "typing",
             "os",
-            "tensorflow"
+            "tensorflow",
+            "typing"
         ],
         "modelscope.exporters.nlp.model_for_token_classification_exporter": [
-            "typing",
+            "collections",
             "torch",
-            "collections"
+            "typing"
         ],
         "modelscope.exporters.nlp.sbert_for_sequence_classification_exporter": [
-            "typing",
+            "collections",
             "torch",
-            "collections"
+            "typing"
         ],
         "modelscope.exporters.nlp.sbert_for_zero_shot_classification_exporter": [
-            "typing",
-            "collections"
+            "collections",
+            "typing"
         ],
         "modelscope.exporters.tf_model_exporter": [
-            "typing",
+            "os",
             "tensorflow",
-            "os"
+            "typing"
         ],
         "modelscope.exporters.torch_model_exporter": [
-            "itertools",
-            "torch",
             "contextlib",
+            "os",
             "typing",
-            "os"
+            "torch",
+            "itertools"
         ],
         "modelscope.metrics.accuracy_metric": [
-            "typing",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.metrics.action_detection_evaluator": [
-            "detectron2",
-            "pandas",
-            "numpy",
             "collections",
-            "scipy",
-            "copy",
             "logging",
-            "os"
+            "os",
+            "detectron2",
+            "copy",
+            "scipy",
+            "pandas",
+            "numpy"
         ],
         "modelscope.metrics.audio_noise_metric": [
             "typing"
         ],
         "modelscope.metrics.base": [
             "abc",
             "typing"
         ],
         "modelscope.metrics.bleu_metric": [
-            "sacrebleu",
             "itertools",
+            "sacrebleu",
             "typing"
         ],
         "modelscope.metrics.builder": [
             "typing"
         ],
         "modelscope.metrics.ciderD.ciderD": [
             "__future__"
         ],
         "modelscope.metrics.ciderD.ciderD_scorer": [
-            "__future__",
-            "math",
-            "numpy",
             "collections",
-            "copy",
             "pdb",
             "os",
-            "six"
+            "__future__",
+            "copy",
+            "math",
+            "six",
+            "numpy"
         ],
         "modelscope.metrics.image_color_enhance_metric": [
             "cv2",
-            "typing",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.metrics.image_colorization_metric": [
-            "torch",
-            "torchvision",
             "scipy",
-            "cv2",
             "typing",
+            "torch",
+            "cv2",
+            "torchvision",
             "numpy"
         ],
         "modelscope.metrics.image_denoise_metric": [
-            "cv2",
-            "typing",
+            "torch",
             "numpy",
-            "torch"
+            "cv2",
+            "typing"
         ],
         "modelscope.metrics.image_inpainting_metric": [
+            "torch",
             "scipy",
-            "typing",
             "numpy",
-            "torch"
+            "typing"
         ],
         "modelscope.metrics.image_instance_segmentation_metric": [
-            "numpy",
             "collections",
-            "tempfile",
-            "pycocotools",
+            "os",
             "typing",
-            "os"
+            "pycocotools",
+            "tempfile",
+            "numpy"
         ],
         "modelscope.metrics.image_portrait_enhancement_metric": [
             "cv2",
-            "typing",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.metrics.image_quality_assessment_degradation_metric": [
-            "numpy",
-            "sys",
-            "torch",
             "collections",
+            "sys",
+            "os",
+            "tqdm",
             "scipy",
+            "typing",
+            "torch",
             "tempfile",
             "cv2",
-            "typing",
-            "os",
-            "tqdm"
+            "numpy"
         ],
         "modelscope.metrics.image_quality_assessment_mos_metric": [
-            "torch",
-            "sys",
+            "numpy",
+            "os",
+            "tqdm",
             "scipy",
+            "typing",
+            "torch",
             "tempfile",
             "cv2",
-            "typing",
-            "numpy",
-            "os",
-            "tqdm"
+            "sys"
         ],
         "modelscope.metrics.inbatch_recall_metric": [
-            "typing",
+            "torch",
             "numpy",
-            "torch"
+            "typing"
         ],
         "modelscope.metrics.loss_metric": [
             "sklearn",
-            "typing",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.metrics.map_metric": [
-            "typing",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.metrics.movie_scene_segmentation_metric": [
-            "typing",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.metrics.ned_metric": [
-            "typing",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.metrics.ocr_recognition_metric": [
-            "typing",
-            "edit_distance",
+            "torch",
             "numpy",
-            "torch"
+            "edit_distance",
+            "typing"
         ],
         "modelscope.metrics.ppl_metric": [
             "torch",
-            "typing",
             "numpy",
-            "math"
+            "math",
+            "typing"
         ],
         "modelscope.metrics.prediction_saving_wrapper": [
             "sklearn",
-            "typing",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.metrics.referring_video_object_segmentation_metric": [
-            "numpy",
+            "tqdm",
+            "typing",
             "torch",
             "pycocotools",
-            "typing",
-            "tqdm"
+            "numpy"
         ],
         "modelscope.metrics.sequence_classification_metric": [
             "sklearn",
-            "typing",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.metrics.text_generation_metric": [
-            "nltk",
-            "sys",
             "contextlib",
+            "nltk",
             "rouge",
-            "typing"
+            "typing",
+            "sys"
         ],
         "modelscope.metrics.text_ranking_metric": [
-            "typing",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.metrics.token_classification_metric": [
             "importlib",
-            "typing",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.metrics.translation_evaluation_metric": [
             "importlib",
-            "typing",
-            "pandas"
+            "pandas",
+            "typing"
         ],
         "modelscope.metrics.video_frame_interpolation_metric": [
+            "lpips",
             "math",
-            "numpy",
+            "typing",
             "torch",
-            "lpips",
-            "typing"
+            "numpy"
         ],
         "modelscope.metrics.video_stabilization_metric": [
             "numpy",
-            "sys",
+            "os",
+            "tqdm",
+            "typing",
             "tempfile",
             "cv2",
-            "typing",
-            "os",
-            "tqdm"
+            "sys"
         ],
         "modelscope.metrics.video_summarization_metric": [
-            "typing",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.metrics.video_super_resolution_metric.matlab_functions": [
-            "math",
+            "torch",
             "numpy",
-            "torch"
+            "math"
         ],
         "modelscope.metrics.video_super_resolution_metric.metric_util": [
             "numpy"
         ],
         "modelscope.metrics.video_super_resolution_metric.niqe": [
             "cv2",
             "scipy",
             "numpy",
             "math"
         ],
         "modelscope.metrics.video_super_resolution_metric.video_super_resolution_metric": [
-            "typing",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.models.audio.aec.layers.activations": [
             "torch"
         ],
         "modelscope.models.audio.aec.layers.affine_transform": [
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.audio.aec.layers.deep_fsmn": [
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.audio.aec.layers.layer_base": [
-            "abc",
-            "re",
+            "torch",
             "numpy",
-            "torch"
+            "abc",
+            "re"
         ],
         "modelscope.models.audio.aec.layers.uni_deep_fsmn": [
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.audio.aec.network.loss": [
             "torch"
         ],
         "modelscope.models.audio.aec.network.modulation_loss": [
-            "math",
+            "torchaudio",
             "torch",
-            "torchaudio"
+            "math"
         ],
         "modelscope.models.audio.aec.network.se_net": [
             "torch"
         ],
         "modelscope.models.audio.ans.complex_nn": [
             "torch"
         ],
         "modelscope.models.audio.ans.conv_stft": [
+            "torch",
             "scipy",
-            "numpy",
-            "torch"
+            "numpy"
         ],
         "modelscope.models.audio.ans.denoise_net": [
             "torch"
         ],
         "modelscope.models.audio.ans.frcrn": [
-            "typing",
+            "os",
             "torch",
-            "os"
+            "typing"
         ],
         "modelscope.models.audio.ans.layers.activations": [
             "torch"
         ],
         "modelscope.models.audio.ans.layers.affine_transform": [
             "torch"
         ],
         "modelscope.models.audio.ans.layers.layer_base": [
-            "torch",
-            "abc",
+            "six",
             "numpy",
-            "six"
+            "abc",
+            "torch"
         ],
         "modelscope.models.audio.ans.layers.uni_deep_fsmn": [
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.audio.ans.se_module_complex": [
             "torch"
         ],
         "modelscope.models.audio.ans.unet": [
             "torch"
         ],
         "modelscope.models.audio.asr.generic_automatic_speech_recognition": [
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.audio.asr.wenet_automatic_speech_recognition": [
             "os",
-            "typing",
+            "wenetruntime",
             "json",
-            "wenetruntime"
+            "typing"
         ],
         "modelscope.models.audio.itn.generic_inverse_text_processing": [
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.audio.kws.farfield.fsmn": [
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.audio.kws.farfield.fsmn_sele_v2": [
             "torch"
         ],
         "modelscope.models.audio.kws.farfield.fsmn_sele_v3": [
             "torch"
         ],
         "modelscope.models.audio.kws.farfield.model": [
+            "os",
             "tempfile",
-            "typing",
-            "os"
+            "typing"
         ],
         "modelscope.models.audio.kws.farfield.model_def": [
-            "math",
             "enum",
-            "struct"
+            "struct",
+            "math"
         ],
         "modelscope.models.audio.kws.generic_key_word_spotting": [
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.audio.kws.nearfield.cmvn": [
-            "re",
+            "torch",
             "numpy",
-            "torch"
+            "re"
         ],
         "modelscope.models.audio.kws.nearfield.fsmn": [
-            "typing",
             "torch",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.models.audio.kws.nearfield.model": [
-            "sys",
+            "os",
+            "typing",
             "torch",
             "tempfile",
-            "typing",
-            "os"
+            "sys"
         ],
         "modelscope.models.audio.punc.generic_punctuation": [
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.audio.separation.layer_norm": [
             "__future__",
             "torch"
         ],
         "modelscope.models.audio.separation.mossformer": [
-            "typing",
-            "torch",
+            "os",
             "copy",
-            "os"
+            "torch",
+            "typing"
         ],
         "modelscope.models.audio.separation.mossformer_block": [
             "torch"
         ],
         "modelscope.models.audio.separation.mossformer_conv_module": [
             "torch"
         ],
         "modelscope.models.audio.sv.DTDNN": [
-            "numpy",
-            "torchaudio",
-            "torch",
             "collections",
+            "os",
             "typing",
-            "os"
+            "torch",
+            "torchaudio",
+            "numpy"
         ],
         "modelscope.models.audio.sv.DTDNN_layers": [
             "torch"
         ],
         "modelscope.models.audio.sv.ERes2Net": [
+            "os",
+            "typing",
             "math",
-            "torchaudio",
             "torch",
-            "typing",
-            "os"
+            "torchaudio"
         ],
         "modelscope.models.audio.sv.ERes2Net_aug": [
+            "os",
+            "typing",
             "math",
-            "torchaudio",
             "torch",
-            "typing",
-            "os"
+            "torchaudio"
         ],
         "modelscope.models.audio.sv.cluster_backend": [
-            "umap",
-            "numpy",
-            "sklearn",
             "hdbscan",
+            "sklearn",
             "scipy",
-            "typing"
+            "umap",
+            "typing",
+            "numpy"
         ],
         "modelscope.models.audio.sv.ecapa_tdnn": [
+            "os",
+            "typing",
             "math",
-            "numpy",
-            "torchaudio",
             "torch",
-            "typing",
-            "os"
+            "torchaudio",
+            "numpy"
         ],
         "modelscope.models.audio.sv.fusion": [
             "torch"
         ],
         "modelscope.models.audio.sv.generic_speaker_verification": [
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.audio.sv.lanuage_recognition_model": [
-            "numpy",
-            "torchaudio",
-            "torch",
+            "os",
             "typing",
-            "os"
+            "torch",
+            "torchaudio",
+            "numpy"
         ],
         "modelscope.models.audio.sv.pooling_layers": [
             "torch"
         ],
         "modelscope.models.audio.sv.rdino": [
+            "os",
             "math",
-            "torchaudio",
-            "torch",
             "typing",
-            "os"
+            "torch",
+            "torchaudio"
         ],
         "modelscope.models.audio.sv.speaker_change_locator": [
-            "numpy",
-            "torchaudio",
-            "torch",
             "collections",
+            "os",
             "typing",
-            "os"
+            "torch",
+            "torchaudio",
+            "numpy"
         ],
         "modelscope.models.audio.sv.speaker_diarization_dialogue_detection": [
             "torch"
         ],
         "modelscope.models.audio.sv.speaker_diarization_semantic_speaker_turn_detection": [
             "torch"
         ],
         "modelscope.models.audio.tts.sambert_hifi": [
-            "yaml",
-            "__future__",
             "wave",
-            "numpy",
             "zipfile",
-            "json",
-            "matplotlib",
             "datetime",
+            "os",
+            "__future__",
+            "matplotlib",
+            "yaml",
             "shutil",
-            "os"
+            "numpy",
+            "json"
         ],
         "modelscope.models.audio.tts.voice": [
-            "yaml",
-            "numpy",
-            "threading",
-            "torch",
             "collections",
-            "kantts",
+            "os",
             "time",
+            "yaml",
+            "kantts",
             "pickle",
-            "os",
+            "torch",
+            "threading",
+            "numpy",
             "json"
         ],
         "modelscope.models.base.base_head": [
             "abc",
             "typing"
         ],
         "modelscope.models.base.base_model": [
+            "os",
             "abc",
-            "typing",
-            "os"
+            "typing"
         ],
         "modelscope.models.base.base_torch_head": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.models.base.base_torch_model": [
-            "torch",
-            "copy",
             "functools",
-            "typing",
             "os",
+            "copy",
+            "typing",
+            "torch",
             "packaging"
         ],
         "modelscope.models.builder": [],
         "modelscope.models.cv.abnormal_object_detection.mmdet_model": [
-            "numpy",
+            "os",
             "torch",
-            "os"
+            "numpy"
         ],
         "modelscope.models.cv.abnormal_object_detection.mmdet_ms.roi_head.mask_scoring_roi_head": [
-            "torch",
-            "mmdet"
+            "mmdet",
+            "torch"
         ],
         "modelscope.models.cv.abnormal_object_detection.mmdet_ms.roi_head.roi_extractors.single_level_roi_extractor": [
-            "mmcv",
+            "mmdet",
             "torch",
-            "mmdet"
+            "mmcv"
         ],
         "modelscope.models.cv.action_detection.action_detection_onnx": [
-            "uuid",
-            "numpy",
-            "onnxruntime",
             "urllib",
+            "os",
             "tempfile",
-            "cv2",
+            "uuid",
+            "onnxruntime",
             "shutil",
-            "os",
-            "subprocess"
+            "cv2",
+            "subprocess",
+            "numpy"
         ],
         "modelscope.models.cv.action_detection.modules.action_detection_pytorch": [
-            "fvcore",
-            "detectron2",
-            "torch",
             "logging",
-            "typing"
+            "detectron2",
+            "fvcore",
+            "typing",
+            "torch"
         ],
         "modelscope.models.cv.action_detection.modules.resnet": [
             "detectron2",
             "torch"
         ],
         "modelscope.models.cv.action_recognition.models": [
             "torch"
         ],
         "modelscope.models.cv.action_recognition.s3dg": [
             "torch"
         ],
         "modelscope.models.cv.action_recognition.tada_convnext": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.cv.action_recognition.temporal_patch_shift_transformer": [
-            "abc",
+            "functools",
             "operator",
             "einops",
-            "torch",
             "timm",
+            "abc",
+            "torch",
             "torchvision",
-            "functools",
             "numpy"
         ],
         "modelscope.models.cv.animal_recognition.resnet": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.cv.animal_recognition.splat": [
             "torch"
         ],
         "modelscope.models.cv.bad_image_detecting.bad_image_detecting": [
-            "numpy",
+            "os",
+            "typing",
             "torch",
             "torchvision",
-            "typing",
-            "os"
+            "numpy"
         ],
         "modelscope.models.cv.body_2d_keypoints.hrnet_basic_modules": [
             "torch"
         ],
         "modelscope.models.cv.body_2d_keypoints.hrnet_v2": [
             "os",
             "torch",
             "numpy"
         ],
         "modelscope.models.cv.body_2d_keypoints.w48": [],
         "modelscope.models.cv.body_3d_keypoints.cannonical_pose.body_3d_pose": [
-            "numpy",
-            "torch",
             "logging",
+            "os",
             "typing",
-            "os"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.body_3d_keypoints.cannonical_pose.canonical_pose_modules": [
             "torch"
         ],
         "modelscope.models.cv.body_3d_keypoints.hdformer.backbone": [
             "torch"
         ],
         "modelscope.models.cv.body_3d_keypoints.hdformer.block": [
-            "einops",
+            "torch",
             "math",
-            "torch"
+            "einops"
         ],
         "modelscope.models.cv.body_3d_keypoints.hdformer.directed_graph": [
-            "typing",
             "sys",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.models.cv.body_3d_keypoints.hdformer.hdformer": [
             "torch"
         ],
         "modelscope.models.cv.body_3d_keypoints.hdformer.hdformer_detector": [
-            "typing",
-            "numpy",
+            "os",
             "torch",
-            "os"
+            "numpy",
+            "typing"
         ],
         "modelscope.models.cv.body_3d_keypoints.hdformer.skeleton": [
             "numpy"
         ],
         "modelscope.models.cv.cartoon.facelib.LK.lk": [
             "numpy"
         ],
         "modelscope.models.cv.cartoon.facelib.config": [
-            "easydict",
             "os",
+            "easydict",
             "numpy"
         ],
         "modelscope.models.cv.cartoon.facelib.face_detector": [
-            "cv2",
-            "numpy",
+            "time",
             "tensorflow",
-            "time"
+            "numpy",
+            "cv2"
         ],
         "modelscope.models.cv.cartoon.facelib.face_landmark": [
-            "cv2",
+            "tensorflow",
             "numpy",
-            "tensorflow"
+            "cv2"
         ],
         "modelscope.models.cv.cartoon.facelib.facer": [
+            "time",
             "cv2",
-            "numpy",
-            "time"
+            "numpy"
         ],
         "modelscope.models.cv.cartoon.loss": [
+            "os",
+            "scipy",
             "joblib",
             "tensorflow",
-            "scipy",
-            "os",
-            "numpy",
-            "skimage"
+            "skimage",
+            "numpy"
         ],
         "modelscope.models.cv.cartoon.model_tf": [
-            "typing",
-            "tensorflow"
+            "tensorflow",
+            "typing"
         ],
         "modelscope.models.cv.cartoon.mtcnn_pytorch.src.align_trans": [
             "cv2",
             "numpy"
         ],
         "modelscope.models.cv.cartoon.mtcnn_pytorch.src.matlab_cp2tform": [
             "numpy"
         ],
         "modelscope.models.cv.cartoon.network": [
             "tensorflow"
         ],
         "modelscope.models.cv.cartoon.utils": [
             "tensorflow",
-            "cv2",
             "random",
-            "numpy",
-            "os"
+            "os",
+            "cv2",
+            "numpy"
         ],
         "modelscope.models.cv.cmdssl_video_embedding.c3d": [
             "torch"
         ],
         "modelscope.models.cv.cmdssl_video_embedding.resnet2p1d": [
             "torch"
         ],
         "modelscope.models.cv.cmdssl_video_embedding.resnet3d": [
             "torch"
         ],
         "modelscope.models.cv.controllable_image_generation.annotator.annotator": [
-            "torch",
-            "mmseg",
-            "cv2",
             "einops",
-            "numpy",
+            "mmcv",
             "os",
-            "mmcv"
+            "mmseg",
+            "torch",
+            "cv2",
+            "numpy"
         ],
         "modelscope.models.cv.controllable_image_generation.annotator.midas.api": [
-            "torchvision",
             "os",
             "torch",
+            "torchvision",
             "cv2"
         ],
         "modelscope.models.cv.controllable_image_generation.annotator.midas.midas.base_model": [
             "torch"
         ],
         "modelscope.models.cv.controllable_image_generation.annotator.midas.midas.blocks": [
             "torch"
@@ -11437,291 +11372,291 @@
         ],
         "modelscope.models.cv.controllable_image_generation.annotator.midas.midas.transforms": [
             "cv2",
             "numpy",
             "math"
         ],
         "modelscope.models.cv.controllable_image_generation.annotator.midas.midas.vit": [
-            "torch",
-            "types",
             "timm",
+            "types",
+            "torch",
             "math"
         ],
         "modelscope.models.cv.controllable_image_generation.annotator.midas.utils": [
+            "numpy",
             "torch",
             "re",
             "cv2",
-            "numpy",
             "sys"
         ],
         "modelscope.models.cv.controllable_image_generation.annotator.mlsd.mbv2_mlsd_large": [
-            "sys",
             "os",
-            "torch"
+            "torch",
+            "sys"
         ],
         "modelscope.models.cv.controllable_image_generation.annotator.mlsd.utils": [
-            "cv2",
             "os",
-            "torch",
-            "numpy"
+            "cv2",
+            "numpy",
+            "torch"
         ],
         "modelscope.models.cv.controllable_image_generation.annotator.openpose.body": [
-            "math",
-            "torch",
-            "torchvision",
             "scipy",
             "matplotlib",
+            "math",
             "time",
+            "torch",
             "cv2",
+            "torchvision",
             "numpy"
         ],
         "modelscope.models.cv.controllable_image_generation.annotator.openpose.hand": [
-            "math",
-            "skimage",
-            "torch",
             "scipy",
             "matplotlib",
+            "math",
             "time",
+            "skimage",
+            "torch",
             "cv2",
             "numpy",
             "json"
         ],
         "modelscope.models.cv.controllable_image_generation.annotator.openpose.model": [
-            "torch",
-            "collections"
+            "collections",
+            "torch"
         ],
         "modelscope.models.cv.controllable_image_generation.annotator.openpose.util": [
             "cv2",
-            "matplotlib",
             "numpy",
+            "matplotlib",
             "math"
         ],
         "modelscope.models.cv.controllable_image_generation.controlnet": [
+            "numpy",
             "einops",
+            "os",
+            "cv2",
             "math",
-            "numpy",
-            "sys",
+            "typing",
             "torch",
             "PIL",
-            "tempfile",
             "control_ldm",
-            "cv2",
-            "typing",
+            "tempfile",
             "random",
-            "os"
+            "sys"
         ],
         "modelscope.models.cv.crowd_counting.cc_model": [
+            "os",
             "torch",
-            "typing",
-            "os"
+            "typing"
         ],
         "modelscope.models.cv.crowd_counting.hrnet_aspp_relu": [
-            "torch",
-            "logging",
             "functools",
-            "numpy",
-            "os"
+            "logging",
+            "os",
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.face_attribute_recognition.fair_face.face_attribute_recognition": [
+            "os",
             "torch",
-            "torchvision",
             "PIL",
             "cv2",
-            "numpy",
-            "os"
+            "torchvision",
+            "numpy"
         ],
         "modelscope.models.cv.face_detection.mogface.models.detectors": [
-            "cv2",
-            "numpy",
+            "os",
             "torch",
-            "os"
+            "numpy",
+            "cv2"
         ],
         "modelscope.models.cv.face_detection.mogface.models.mogface": [
             "torch"
         ],
         "modelscope.models.cv.face_detection.mogface.models.mogprednet": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.cv.face_detection.mogface.models.resnet": [
             "torch"
         ],
         "modelscope.models.cv.face_detection.mogface.models.utils": [
-            "torch",
             "itertools",
+            "torch",
             "numpy",
             "math"
         ],
         "modelscope.models.cv.face_detection.mtcnn.models.box_utils": [
-            "PIL",
-            "numpy"
+            "numpy",
+            "PIL"
         ],
         "modelscope.models.cv.face_detection.mtcnn.models.detector": [
-            "PIL",
-            "numpy",
+            "os",
             "torch",
-            "os"
+            "numpy",
+            "PIL"
         ],
         "modelscope.models.cv.face_detection.mtcnn.models.first_stage": [
             "torch",
-            "PIL",
             "numpy",
+            "PIL",
             "math"
         ],
         "modelscope.models.cv.face_detection.mtcnn.models.get_nets": [
-            "numpy",
+            "collections",
             "torch",
-            "collections"
+            "numpy"
         ],
         "modelscope.models.cv.face_detection.peppa_pig_face.LK.lk": [
             "numpy"
         ],
         "modelscope.models.cv.face_detection.peppa_pig_face.face_detector": [
-            "cv2",
+            "tensorflow",
             "numpy",
-            "tensorflow"
+            "cv2"
         ],
         "modelscope.models.cv.face_detection.peppa_pig_face.face_landmark": [
-            "cv2",
+            "tensorflow",
             "numpy",
-            "tensorflow"
+            "cv2"
         ],
         "modelscope.models.cv.face_detection.peppa_pig_face.facer": [
             "cv2",
             "numpy"
         ],
         "modelscope.models.cv.face_detection.retinaface.detection": [
-            "cv2",
+            "torch",
             "numpy",
-            "torch"
+            "cv2"
         ],
         "modelscope.models.cv.face_detection.retinaface.models.net": [
-            "torchvision",
+            "time",
             "torch",
-            "time"
+            "torchvision"
         ],
         "modelscope.models.cv.face_detection.retinaface.models.retinaface": [
-            "torchvision",
+            "collections",
             "torch",
-            "collections"
+            "torchvision"
         ],
         "modelscope.models.cv.face_detection.retinaface.utils": [
             "itertools",
-            "math",
+            "torch",
             "numpy",
-            "torch"
+            "math"
         ],
         "modelscope.models.cv.face_detection.scrfd.damofd_detect": [
-            "typing",
-            "torch",
+            "os",
             "copy",
-            "os"
+            "torch",
+            "typing"
         ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.core.bbox.transforms": [
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.core.post_processing.bbox_nms": [
             "torch"
         ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.datasets.pipelines.auto_augment": [
-            "copy",
-            "cv2",
             "mmcv",
+            "mmdet",
+            "copy",
             "numpy",
-            "mmdet"
+            "cv2"
         ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.datasets.pipelines.formating": [
-            "mmcv",
-            "numpy",
+            "mmdet",
             "torch",
-            "mmdet"
+            "mmcv",
+            "numpy"
         ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.datasets.pipelines.loading": [
             "os",
             "pycocotools",
             "mmdet",
             "numpy"
         ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.datasets.pipelines.transforms": [
+            "mmdet",
             "mmcv",
-            "numpy",
-            "mmdet"
+            "numpy"
         ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.datasets.retinaface": [
-            "numpy",
-            "mmdet"
+            "mmdet",
+            "numpy"
         ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.backbones.master_net": [
-            "torch",
-            "mmdet"
+            "mmdet",
+            "torch"
         ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.backbones.mobilenet": [
-            "mmcv",
+            "mmdet",
             "torch",
-            "mmdet"
+            "mmcv"
         ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.backbones.resnet": [
-            "mmcv",
+            "mmdet",
             "torch",
-            "mmdet"
+            "mmcv"
         ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.dense_heads.scrfd_head": [
-            "mmcv",
-            "numpy",
+            "mmdet",
             "torch",
-            "mmdet"
+            "mmcv",
+            "numpy"
         ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.detectors.base": [
-            "abc",
-            "mmdet",
+            "collections",
+            "torch",
             "mmcv",
+            "mmdet",
             "numpy",
-            "torch",
-            "collections"
+            "abc"
         ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.detectors.scrfd": [
-            "torch",
-            "mmdet"
+            "mmdet",
+            "torch"
         ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.detectors.single_stage": [
-            "torch",
-            "mmdet"
+            "mmdet",
+            "torch"
         ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.detectors.tinymog": [
-            "torch",
-            "mmdet"
+            "mmdet",
+            "torch"
         ],
         "modelscope.models.cv.face_detection.scrfd.preprocessor": [
+            "numpy",
             "PIL",
-            "typing",
-            "numpy"
+            "typing"
         ],
         "modelscope.models.cv.face_detection.scrfd.scrfd_detect": [
-            "numpy",
-            "torch",
+            "os",
             "copy",
             "typing",
-            "os"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.face_detection.scrfd.tinymog_detect": [
-            "typing",
-            "torch",
+            "os",
             "copy",
-            "os"
+            "torch",
+            "typing"
         ],
         "modelscope.models.cv.face_detection.ulfd_slim.detection": [
-            "cv2",
-            "numpy",
+            "os",
             "torch",
-            "os"
+            "numpy",
+            "cv2"
         ],
         "modelscope.models.cv.face_detection.ulfd_slim.vision.box_utils": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.cv.face_detection.ulfd_slim.vision.mb_tiny": [
             "torch"
         ],
         "modelscope.models.cv.face_detection.ulfd_slim.vision.ssd.data_preprocessing": [],
         "modelscope.models.cv.face_detection.ulfd_slim.vision.ssd.fd_config": [
             "numpy"
@@ -11729,602 +11664,602 @@
         "modelscope.models.cv.face_detection.ulfd_slim.vision.ssd.mb_tiny_fd": [
             "torch"
         ],
         "modelscope.models.cv.face_detection.ulfd_slim.vision.ssd.predictor": [
             "torch"
         ],
         "modelscope.models.cv.face_detection.ulfd_slim.vision.ssd.ssd": [
-            "typing",
-            "numpy",
+            "collections",
             "torch",
-            "collections"
+            "numpy",
+            "typing"
         ],
         "modelscope.models.cv.face_detection.ulfd_slim.vision.transforms": [
             "cv2",
             "types",
             "numpy",
             "torch"
         ],
         "modelscope.models.cv.face_emotion.efficient.model": [
             "torch"
         ],
         "modelscope.models.cv.face_emotion.efficient.utils": [
-            "re",
-            "math",
+            "collections",
             "functools",
             "torch",
-            "collections"
+            "re",
+            "math"
         ],
         "modelscope.models.cv.face_emotion.emotion_infer": [
-            "PIL",
+            "torch",
             "torchvision",
-            "torch"
+            "PIL"
         ],
         "modelscope.models.cv.face_emotion.emotion_model": [
-            "sys",
             "os",
-            "torch"
+            "torch",
+            "sys"
         ],
         "modelscope.models.cv.face_emotion.face_alignment.face": [
-            "cv2",
             "os",
             "tensorflow",
+            "cv2",
             "numpy"
         ],
         "modelscope.models.cv.face_emotion.face_alignment.face_align": [
             "sys",
+            "os",
             "PIL",
             "cv2",
-            "numpy",
-            "os"
+            "numpy"
         ],
         "modelscope.models.cv.face_generation.op.conv2d_gradfix": [
-            "contextlib",
             "warnings",
-            "torch"
+            "torch",
+            "contextlib"
         ],
         "modelscope.models.cv.face_generation.op.fused_act": [
             "os",
             "torch"
         ],
         "modelscope.models.cv.face_generation.op.upfirdn2d": [
             "os",
             "torch",
             "collections"
         ],
         "modelscope.models.cv.face_generation.stylegan2": [
+            "functools",
             "operator",
-            "random",
             "math",
             "torch",
-            "functools"
+            "random"
         ],
         "modelscope.models.cv.face_human_hand_detection.det_infer": [
-            "cv2",
+            "torch",
             "numpy",
-            "torch"
+            "cv2"
         ],
         "modelscope.models.cv.face_human_hand_detection.ghost_pan": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.cv.face_human_hand_detection.nanodet_plus_head": [
             "math",
             "torch",
-            "torchvision",
             "cv2",
+            "torchvision",
             "numpy"
         ],
         "modelscope.models.cv.face_human_hand_detection.one_stage_detector": [
             "torch"
         ],
         "modelscope.models.cv.face_human_hand_detection.shufflenetv2": [
             "torch"
         ],
         "modelscope.models.cv.face_human_hand_detection.utils": [
             "torch"
         ],
         "modelscope.models.cv.face_recognition.align_face": [
-            "cv2",
+            "skimage",
             "numpy",
-            "skimage"
+            "cv2"
         ],
         "modelscope.models.cv.face_recognition.torchkit.backbone.arcface_backbone": [
             "torch"
         ],
         "modelscope.models.cv.face_recognition.torchkit.backbone.common": [
             "torch"
         ],
         "modelscope.models.cv.face_recognition.torchkit.backbone.facemask_backbone": [
-            "torch",
-            "collections"
+            "collections",
+            "torch"
         ],
         "modelscope.models.cv.face_recognition.torchkit.backbone.model_irse": [
-            "torch",
-            "collections"
+            "collections",
+            "torch"
         ],
         "modelscope.models.cv.face_recognition.torchkit.backbone.model_resnet": [
             "torch"
         ],
         "modelscope.models.cv.face_recognition.torchkit.rts_backbone": [
+            "collections",
             "torch",
             "os",
-            "math",
-            "collections"
+            "math"
         ],
         "modelscope.models.cv.face_reconstruction.models.bfm": [
-            "scipy",
-            "numpy",
+            "os",
             "torch",
-            "os"
+            "scipy",
+            "numpy"
         ],
         "modelscope.models.cv.face_reconstruction.models.de_retouching_module": [
             "torch"
         ],
         "modelscope.models.cv.face_reconstruction.models.facelandmark.large_base_lmks_infer": [
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.face_reconstruction.models.facelandmark.nets.large_base_lmks_net": [
             "torch"
         ],
         "modelscope.models.cv.face_reconstruction.models.facelandmark.nets.large_eyeball_net": [
             "torch"
         ],
         "modelscope.models.cv.face_reconstruction.models.facerecon_model": [
-            "numpy",
-            "torch",
             "collections",
+            "os",
+            "torch",
             "cv2",
-            "os"
+            "numpy"
         ],
         "modelscope.models.cv.face_reconstruction.models.losses": [
-            "kornia",
             "numpy",
-            "torch"
+            "torch",
+            "kornia"
         ],
         "modelscope.models.cv.face_reconstruction.models.networks": [
-            "typing",
-            "kornia",
             "os",
-            "torch"
+            "torch",
+            "kornia",
+            "typing"
         ],
         "modelscope.models.cv.face_reconstruction.models.nv_diffrast": [
-            "nvdiffrast",
-            "numpy",
-            "torch",
             "warnings",
-            "typing"
+            "typing",
+            "torch",
+            "nvdiffrast",
+            "numpy"
         ],
         "modelscope.models.cv.face_reconstruction.models.opt": [],
         "modelscope.models.cv.face_reconstruction.models.pix2pix.networks": [
             "functools",
             "torch"
         ],
         "modelscope.models.cv.face_reconstruction.models.pix2pix.pix2pix_model": [
             "torch"
         ],
         "modelscope.models.cv.face_reconstruction.models.pix2pix.pix2pix_options": [],
         "modelscope.models.cv.face_reconstruction.models.renderer": [
-            "skimage",
-            "numpy",
+            "imageio",
             "torch",
-            "imageio"
+            "numpy",
+            "skimage"
         ],
         "modelscope.models.cv.face_reconstruction.models.unet": [
             "warnings",
             "torch"
         ],
         "modelscope.models.cv.face_reconstruction.utils": [
+            "os",
+            "scipy",
+            "array",
             "math",
-            "numpy",
+            "argparse",
             "torch",
-            "array",
-            "PIL",
-            "scipy",
             "numba",
-            "argparse",
+            "PIL",
             "cv2",
-            "os"
+            "numpy"
         ],
         "modelscope.models.cv.facial_expression_recognition.fer.facial_expression_recognition": [
+            "os",
             "torch",
             "PIL",
             "cv2",
-            "numpy",
-            "os"
+            "numpy"
         ],
         "modelscope.models.cv.facial_expression_recognition.fer.transforms": [
-            "PIL",
-            "numbers",
+            "torch",
             "types",
+            "PIL",
             "numpy",
-            "torch"
+            "numbers"
         ],
         "modelscope.models.cv.facial_expression_recognition.fer.vgg": [
             "torch"
         ],
         "modelscope.models.cv.facial_landmark_confidence.flc.facial_landmark_confidence": [
+            "os",
             "torch",
             "PIL",
             "cv2",
-            "numpy",
-            "os"
+            "numpy"
         ],
         "modelscope.models.cv.facial_landmark_confidence.flc.manual_landmark_net": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.cv.hand_static.hand_model": [
+            "numpy",
+            "os",
             "torch",
-            "sys",
-            "torchvision",
             "PIL",
             "cv2",
-            "numpy",
-            "os"
+            "torchvision",
+            "sys"
         ],
         "modelscope.models.cv.hand_static.networks": [
-            "torchvision",
             "os",
-            "torch"
+            "torch",
+            "torchvision"
         ],
         "modelscope.models.cv.human_reconstruction.Reconstruction": [
-            "numpy",
+            "os",
+            "typing",
+            "skimage",
             "torch",
             "PIL",
-            "torchvision",
             "cv2",
-            "typing",
-            "os",
-            "skimage"
+            "torchvision",
+            "numpy"
         ],
         "modelscope.models.cv.human_reconstruction.models.Embedding": [
             "torch"
         ],
         "modelscope.models.cv.human_reconstruction.models.PixToMesh": [
             "torch"
         ],
         "modelscope.models.cv.human_reconstruction.models.Res_backbone": [
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.human_reconstruction.models.Surface_head": [
             "torch"
         ],
         "modelscope.models.cv.human_reconstruction.models.detectors": [
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.human_reconstruction.models.geometry": [
             "torch"
         ],
         "modelscope.models.cv.human_reconstruction.models.human_segmenter": [
-            "cv2",
+            "tensorflow",
             "numpy",
-            "tensorflow"
+            "cv2"
         ],
         "modelscope.models.cv.human_reconstruction.models.networks": [
             "functools",
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.human_reconstruction.utils": [
-            "numpy",
             "os",
+            "mcubes",
             "torch",
-            "mcubes"
+            "numpy"
         ],
         "modelscope.models.cv.image_binary_quant_classification.binary_quant_model": [
-            "os",
+            "collections",
             "torch",
-            "collections"
+            "os"
         ],
         "modelscope.models.cv.image_binary_quant_classification.bnext": [
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.image_body_reshaping.image_body_reshaping": [
-            "numpy",
+            "os",
+            "typing",
             "torch",
             "cv2",
-            "typing",
-            "os"
+            "numpy"
         ],
         "modelscope.models.cv.image_body_reshaping.model": [
             "torch"
         ],
         "modelscope.models.cv.image_body_reshaping.person_info": [
-            "cv2",
             "numpy",
-            "torch",
-            "copy"
+            "copy",
+            "cv2",
+            "torch"
         ],
         "modelscope.models.cv.image_body_reshaping.pose_estimator.body": [
+            "scipy",
             "math",
             "torch",
-            "scipy",
             "cv2",
             "numpy"
         ],
         "modelscope.models.cv.image_body_reshaping.pose_estimator.model": [
-            "torch",
-            "collections"
+            "collections",
+            "torch"
         ],
         "modelscope.models.cv.image_body_reshaping.pose_estimator.util": [
             "numpy"
         ],
         "modelscope.models.cv.image_body_reshaping.slim_utils": [
-            "math",
+            "torch",
             "numba",
+            "random",
             "os",
             "cv2",
-            "random",
             "numpy",
-            "torch"
+            "math"
         ],
         "modelscope.models.cv.image_classification.backbones.beit_v2": [
+            "collections",
+            "functools",
             "einops",
-            "math",
-            "itertools",
-            "torch",
+            "mmcv",
             "mmcls",
-            "collections",
+            "os",
             "warnings",
-            "mmcv",
+            "math",
             "typing",
-            "functools",
-            "os"
+            "torch",
+            "itertools"
         ],
         "modelscope.models.cv.image_classification.backbones.nextvit": [
+            "collections",
+            "functools",
             "einops",
-            "math",
-            "itertools",
-            "torch",
+            "mmcv",
             "mmcls",
-            "collections",
+            "os",
             "warnings",
-            "mmcv",
+            "math",
             "typing",
-            "functools",
-            "os"
+            "torch",
+            "itertools"
         ],
         "modelscope.models.cv.image_classification.mmcls_model": [
             "os"
         ],
         "modelscope.models.cv.image_classification.resnet50_cc": [
+            "collections",
+            "os",
             "math",
             "torch",
-            "collections",
-            "torchvision",
-            "os"
+            "torchvision"
         ],
         "modelscope.models.cv.image_classification.utils": [
-            "math",
+            "collections",
+            "torch",
+            "mmcls",
             "os",
             "itertools",
             "numpy",
-            "torch",
-            "mmcls",
-            "collections"
+            "math"
         ],
         "modelscope.models.cv.image_color_enhance.adaint.adaint": [
-            "numbers",
+            "os",
+            "typing",
             "torch",
             "torchvision",
-            "typing",
-            "os"
+            "numbers"
         ],
         "modelscope.models.cv.image_color_enhance.csrnet": [
             "functools",
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.cv.image_color_enhance.deeplpf.deeplpf_image_color_enhance": [
-            "typing",
+            "os",
             "torch",
-            "os"
+            "typing"
         ],
         "modelscope.models.cv.image_color_enhance.deeplpf.deeplpfnet": [
-            "matplotlib",
             "torch",
+            "matplotlib",
             "math"
         ],
         "modelscope.models.cv.image_color_enhance.image_color_enhance": [
-            "typing",
+            "os",
             "torch",
-            "os"
+            "typing"
         ],
         "modelscope.models.cv.image_colorization.ddcolor.ddcolor": [
             "torch"
         ],
         "modelscope.models.cv.image_colorization.ddcolor.ddcolor_for_image_colorization": [
-            "numpy",
-            "torch",
+            "os",
             "copy",
             "typing",
-            "os"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.image_colorization.ddcolor.loss": [
             "torch"
         ],
         "modelscope.models.cv.image_colorization.ddcolor.utils.convnext": [
             "torch",
             "timm"
         ],
         "modelscope.models.cv.image_colorization.ddcolor.utils.position_encoding": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.cv.image_colorization.ddcolor.utils.transformer_utils": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.models.cv.image_colorization.ddcolor.utils.unet": [
+            "collections",
             "torch",
-            "enum",
-            "collections"
+            "enum"
         ],
         "modelscope.models.cv.image_colorization.ddcolor.utils.vgg": [
-            "torchvision",
             "os",
             "torch",
+            "torchvision",
             "collections"
         ],
         "modelscope.models.cv.image_colorization.unet.unet": [
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.image_colorization.unet.utils": [
-            "torch",
             "functools",
+            "torch",
             "enum"
         ],
         "modelscope.models.cv.image_debanding.rrdb.rrdb_image_debanding": [
+            "os",
             "torch",
-            "typing",
-            "os"
+            "typing"
         ],
         "modelscope.models.cv.image_deblur.nafnet_for_image_deblur": [
+            "os",
             "torch",
-            "typing",
-            "os"
+            "typing"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.defrcn_for_fewshot": [
-            "typing",
+            "os",
             "torch",
-            "os"
+            "typing"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.evaluation.coco_evaluation": [
-            "fvcore",
-            "detectron2",
-            "tabulate",
-            "numpy",
-            "itertools",
-            "torch",
             "collections",
-            "contextlib",
-            "copy",
+            "tabulate",
             "pycocotools",
+            "contextlib",
             "logging",
             "os",
-            "json",
-            "io"
+            "detectron2",
+            "copy",
+            "fvcore",
+            "torch",
+            "io",
+            "itertools",
+            "numpy",
+            "json"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.evaluation.evaluator": [
+            "datetime",
+            "logging",
             "detectron2",
-            "torch",
             "time",
-            "logging",
-            "datetime"
+            "torch"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.evaluation.pascal_voc_evaluation": [
-            "tempfile",
+            "collections",
             "detectron2",
-            "numpy",
             "os",
-            "collections"
+            "tempfile",
+            "numpy"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.models.calibration_layer": [
-            "cv2",
-            "sklearn",
             "detectron2",
-            "torch"
+            "sklearn",
+            "torch",
+            "cv2"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.models.defrcn": [
+            "os",
             "typing",
-            "detectron2",
             "torch",
-            "os"
+            "detectron2"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.models.fast_rcnn": [
             "detectron2",
-            "fvcore",
+            "torch",
             "numpy",
-            "torch"
+            "fvcore"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.models.gdl": [
             "torch"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.models.resnet": [
-            "torchvision",
-            "torch"
+            "torch",
+            "torchvision"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.models.roi_heads": [
             "detectron2",
             "torch"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.utils.coco_register": [
+            "io",
             "contextlib",
-            "fvcore",
-            "detectron2",
-            "pycocotools",
             "os",
-            "io"
+            "pycocotools",
+            "detectron2",
+            "fvcore"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.utils.configuration_mapper": [
             "detectron2"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.utils.model_surgery_op": [
-            "os",
+            "argparse",
             "torch",
-            "argparse"
+            "os"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.utils.register_data": [],
         "modelscope.models.cv.image_defrcn_fewshot.utils.requirements_check": [
-            "importlib",
-            "sys",
             "collections",
             "importlib_metadata",
-            "packaging"
+            "importlib",
+            "packaging",
+            "sys"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.utils.voc_register": [
             "xml",
-            "fvcore",
             "detectron2",
-            "numpy",
-            "os"
+            "os",
+            "fvcore",
+            "numpy"
         ],
         "modelscope.models.cv.image_denoise.nafnet.NAFNet_arch": [
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.image_denoise.nafnet.arch_util": [
             "torch"
         ],
         "modelscope.models.cv.image_denoise.nafnet_for_image_denoise": [
-            "typing",
+            "os",
             "torch",
-            "os"
+            "typing"
         ],
         "modelscope.models.cv.image_depth_estimation.networks.newcrf_depth": [
             "torch"
         ],
         "modelscope.models.cv.image_depth_estimation.networks.newcrf_layers": [
             "torch",
             "numpy",
             "timm"
         ],
         "modelscope.models.cv.image_depth_estimation.networks.newcrf_utils": [
-            "torchvision",
-            "warnings",
-            "os",
-            "pkgutil",
+            "collections",
             "importlib",
             "torch",
-            "collections"
+            "pkgutil",
+            "os",
+            "warnings",
+            "torchvision"
         ],
         "modelscope.models.cv.image_depth_estimation.networks.swin_transformer": [
             "torch",
             "numpy",
             "timm"
         ],
         "modelscope.models.cv.image_depth_estimation.networks.uper_crf_head": [
-            "mmcv",
-            "torch"
+            "torch",
+            "mmcv"
         ],
         "modelscope.models.cv.image_depth_estimation.newcrfs_model": [
             "os",
             "torch",
             "numpy"
         ],
         "modelscope.models.cv.image_depth_estimation_bts.depth_estimation_bts_model": [
@@ -12334,188 +12269,188 @@
         "modelscope.models.cv.image_depth_estimation_bts.networks.bts_model": [
             "torch"
         ],
         "modelscope.models.cv.image_depth_estimation_bts.networks.decoder": [
             "torch"
         ],
         "modelscope.models.cv.image_depth_estimation_bts.networks.encoder": [
-            "torchvision",
-            "torch"
+            "torch",
+            "torchvision"
         ],
         "modelscope.models.cv.image_depth_estimation_bts.networks.utils": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.cv.image_driving_perception.image_driving_percetion_model": [
-            "numpy",
+            "os",
+            "typing",
             "torch",
             "cv2",
-            "typing",
-            "os"
+            "numpy"
         ],
         "modelscope.models.cv.image_driving_perception.preprocessor": [
-            "cv2",
-            "typing",
+            "torch",
             "numpy",
-            "torch"
+            "cv2",
+            "typing"
         ],
         "modelscope.models.cv.image_driving_perception.utils": [
-            "torchvision",
-            "numpy",
+            "time",
             "torch",
-            "time"
+            "torchvision",
+            "numpy"
         ],
         "modelscope.models.cv.image_face_fusion.facegan.gan_wrap": [
+            "os",
             "torch",
             "PIL",
-            "torchvision",
             "cv2",
-            "numpy",
-            "os"
+            "torchvision",
+            "numpy"
         ],
         "modelscope.models.cv.image_face_fusion.facegan.model": [
             "random",
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.cv.image_face_fusion.facegan.op.conv2d_gradfix": [
-            "contextlib",
             "warnings",
-            "torch"
+            "torch",
+            "contextlib"
         ],
         "modelscope.models.cv.image_face_fusion.facegan.op.fused_act": [
             "torch"
         ],
         "modelscope.models.cv.image_face_fusion.facegan.op.upfirdn2d": [
-            "torch",
-            "collections"
+            "collections",
+            "torch"
         ],
         "modelscope.models.cv.image_face_fusion.facelib.align_trans": [
             "cv2",
             "numpy"
         ],
         "modelscope.models.cv.image_face_fusion.facelib.matlab_cp2tform": [
             "numpy"
         ],
         "modelscope.models.cv.image_face_fusion.image_face_fusion": [
-            "torch",
             "collections",
+            "os",
+            "typing",
+            "torch",
             "PIL",
-            "torchvision",
             "cv2",
-            "typing",
-            "numpy",
-            "os"
+            "torchvision",
+            "numpy"
         ],
         "modelscope.models.cv.image_face_fusion.network.aad_layer": [
             "torch"
         ],
         "modelscope.models.cv.image_face_fusion.network.aei_flow_net": [
             "torch"
         ],
         "modelscope.models.cv.image_face_fusion.network.bfm": [
-            "scipy",
             "os",
             "torch",
+            "scipy",
             "numpy"
         ],
         "modelscope.models.cv.image_face_fusion.network.dense_motion": [
             "torch"
         ],
         "modelscope.models.cv.image_face_fusion.network.facerecon_model": [
+            "os",
             "torch",
-            "typing",
-            "os"
+            "typing"
         ],
         "modelscope.models.cv.image_face_fusion.network.model_irse": [
-            "torch",
-            "collections"
+            "collections",
+            "torch"
         ],
         "modelscope.models.cv.image_face_fusion.network.ops": [
             "torch"
         ],
         "modelscope.models.cv.image_human_parsing.backbone.deeplab_resnet": [
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.image_human_parsing.m2fp.m2fp_decoder": [
             "torch"
         ],
         "modelscope.models.cv.image_human_parsing.m2fp.m2fp_encoder": [
-            "typing",
             "torch",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.models.cv.image_human_parsing.m2fp_net": [
-            "typing",
+            "os",
             "torch",
-            "os"
+            "typing"
         ],
         "modelscope.models.cv.image_human_parsing.parsing_utils": [
             "PIL",
+            "copy",
             "numpy",
-            "torch",
-            "copy"
+            "torch"
         ],
         "modelscope.models.cv.image_inpainting.base": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.models.cv.image_inpainting.default": [
-            "bisect",
-            "torch"
+            "torch",
+            "bisect"
         ],
         "modelscope.models.cv.image_inpainting.model": [
+            "os",
             "torch",
-            "typing",
-            "os"
+            "typing"
         ],
         "modelscope.models.cv.image_inpainting.modules.ade20k.base": [
             "os",
             "torch"
         ],
         "modelscope.models.cv.image_inpainting.modules.ade20k.resnet": [
-            "math",
             "os",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.cv.image_inpainting.modules.adversarial": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.models.cv.image_inpainting.modules.feature_matching": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.models.cv.image_inpainting.modules.ffc": [
-            "kornia",
             "numpy",
-            "torch"
+            "torch",
+            "kornia"
         ],
         "modelscope.models.cv.image_inpainting.modules.inception": [
-            "torchvision",
-            "torch"
+            "torch",
+            "torchvision"
         ],
         "modelscope.models.cv.image_inpainting.modules.perceptual": [
-            "torchvision",
-            "torch"
+            "torch",
+            "torchvision"
         ],
         "modelscope.models.cv.image_inpainting.modules.pix2pixhd": [
-            "logging",
+            "collections",
             "functools",
-            "numpy",
             "torch",
-            "collections"
+            "logging",
+            "numpy"
         ],
         "modelscope.models.cv.image_inpainting.refinement": [
-            "kornia",
+            "numpy",
+            "tqdm",
             "torch",
             "cv2",
-            "numpy",
-            "tqdm"
+            "kornia"
         ],
         "modelscope.models.cv.image_instance_segmentation.backbones.resnet": [
             "torch"
         ],
         "modelscope.models.cv.image_instance_segmentation.backbones.swin_transformer": [
             "torch",
             "numpy",
@@ -12527,1139 +12462,1139 @@
             "collections"
         ],
         "modelscope.models.cv.image_instance_segmentation.datasets.transforms": [
             "os",
             "numpy"
         ],
         "modelscope.models.cv.image_instance_segmentation.fastinst.fastinst_decoder": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.cv.image_instance_segmentation.fastinst.fastinst_encoder": [
             "logging",
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.models.cv.image_instance_segmentation.fastinst_model": [
-            "typing",
+            "os",
             "torch",
-            "os"
+            "typing"
         ],
         "modelscope.models.cv.image_instance_segmentation.maskdino.dino_decoder": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.models.cv.image_instance_segmentation.maskdino.maskdino_decoder": [
             "torch"
         ],
         "modelscope.models.cv.image_instance_segmentation.maskdino.maskdino_encoder": [
-            "typing",
             "torch",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.models.cv.image_instance_segmentation.maskdino.ms_deform_attn": [
+            "torch",
+            "mmcv",
             "__future__",
-            "math",
             "warnings",
-            "mmcv",
-            "torch"
+            "math"
         ],
         "modelscope.models.cv.image_instance_segmentation.maskdino.position_encoding": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.cv.image_instance_segmentation.maskdino.utils": [
-            "math",
+            "copy",
             "torch",
-            "copy"
+            "math"
         ],
         "modelscope.models.cv.image_instance_segmentation.maskdino_model": [
+            "os",
             "torch",
-            "typing",
-            "os"
+            "typing"
         ],
         "modelscope.models.cv.image_instance_segmentation.maskdino_swin": [
             "os",
             "torch"
         ],
         "modelscope.models.cv.image_instance_segmentation.model": [
+            "os",
             "torch",
-            "typing",
-            "os"
+            "typing"
         ],
         "modelscope.models.cv.image_instance_segmentation.postprocess_utils": [
-            "itertools",
-            "torch",
             "pycocotools",
+            "torch",
+            "itertools",
             "cv2",
             "numpy"
         ],
         "modelscope.models.cv.image_matching.config.default": [
             "yacs"
         ],
         "modelscope.models.cv.image_matching.loftr_quadtree.backbone.resnet_fpn": [
             "torch"
         ],
         "modelscope.models.cv.image_matching.loftr_quadtree.loftr": [
-            "einops",
-            "torch"
+            "torch",
+            "einops"
         ],
         "modelscope.models.cv.image_matching.loftr_quadtree.loftr_module.fine_preprocess": [
-            "einops",
-            "torch"
+            "torch",
+            "einops"
         ],
         "modelscope.models.cv.image_matching.loftr_quadtree.loftr_module.linear_attention": [
             "torch"
         ],
         "modelscope.models.cv.image_matching.loftr_quadtree.loftr_module.quadtree_attention": [
             "torch",
             "timm"
         ],
         "modelscope.models.cv.image_matching.loftr_quadtree.loftr_module.transformer": [
-            "math",
-            "torch",
-            "timm",
+            "einops",
             "copy",
-            "einops"
+            "timm",
+            "math",
+            "torch"
         ],
         "modelscope.models.cv.image_matching.loftr_quadtree.utils.coarse_matching": [
-            "einops",
-            "torch"
+            "torch",
+            "einops"
         ],
         "modelscope.models.cv.image_matching.loftr_quadtree.utils.fine_matching": [
+            "torch",
             "kornia",
-            "math",
-            "torch"
+            "math"
         ],
         "modelscope.models.cv.image_matching.loftr_quadtree.utils.position_encoding": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.cv.image_matching.quadtree_attention_model": [
-            "torch",
+            "os",
             "pathlib",
+            "torch",
             "cv2",
-            "numpy",
-            "os"
+            "numpy"
         ],
         "modelscope.models.cv.image_matching.utils.misc": [
             "yacs"
         ],
         "modelscope.models.cv.image_mvs_depth_estimation.cas_mvsnet": [
             "torch"
         ],
         "modelscope.models.cv.image_mvs_depth_estimation.casmvs_model": [
-            "torch",
+            "os",
             "easydict",
+            "torch",
             "cv2",
-            "numpy",
-            "os"
+            "numpy"
         ],
         "modelscope.models.cv.image_mvs_depth_estimation.colmap2mvsnet": [
-            "multiprocessing",
-            "__future__",
-            "numpy",
-            "shutil",
             "collections",
-            "cv2",
             "functools",
             "os",
-            "struct"
+            "__future__",
+            "struct",
+            "shutil",
+            "cv2",
+            "numpy",
+            "multiprocessing"
         ],
         "modelscope.models.cv.image_mvs_depth_estimation.depth_filter": [
+            "os",
             "plyfile",
             "PIL",
             "cv2",
-            "numpy",
-            "os"
+            "numpy"
         ],
         "modelscope.models.cv.image_mvs_depth_estimation.general_eval_dataset": [
+            "numpy",
             "torch",
             "PIL",
             "re",
             "os",
             "cv2",
-            "numpy",
             "sys"
         ],
         "modelscope.models.cv.image_mvs_depth_estimation.module": [
             "torch"
         ],
         "modelscope.models.cv.image_mvs_depth_estimation.utils": [
-            "torchvision",
             "random",
+            "torchvision",
             "numpy",
             "torch"
         ],
         "modelscope.models.cv.image_paintbyexample.model": [
-            "paint_ldm",
-            "torch",
             "omegaconf",
+            "os",
             "typing",
-            "os"
+            "torch",
+            "paint_ldm"
         ],
         "modelscope.models.cv.image_panoptic_segmentation.panseg_model": [
             "os",
             "torch"
         ],
         "modelscope.models.cv.image_portrait_enhancement.align_faces": [
-            "cv2",
+            "skimage",
             "numpy",
-            "skimage"
+            "cv2"
         ],
         "modelscope.models.cv.image_portrait_enhancement.eqface.fqa": [
-            "cv2",
-            "numpy",
+            "os",
             "torch",
-            "os"
+            "numpy",
+            "cv2"
         ],
         "modelscope.models.cv.image_portrait_enhancement.eqface.model_resnet": [
             "torch"
         ],
         "modelscope.models.cv.image_portrait_enhancement.gpen": [
+            "functools",
             "operator",
             "math",
-            "itertools",
             "torch",
-            "functools",
+            "itertools",
             "random"
         ],
         "modelscope.models.cv.image_portrait_enhancement.image_portrait_enhancement": [
+            "os",
             "torch",
-            "typing",
             "math",
-            "os"
+            "typing"
         ],
         "modelscope.models.cv.image_portrait_enhancement.losses.helpers": [
-            "torch",
-            "collections"
+            "collections",
+            "torch"
         ],
         "modelscope.models.cv.image_portrait_enhancement.losses.losses": [
             "torch"
         ],
         "modelscope.models.cv.image_portrait_enhancement.losses.model_irse": [
             "torch"
         ],
         "modelscope.models.cv.image_portrait_enhancement.retinaface.detection": [
-            "cv2",
-            "numpy",
+            "os",
             "torch",
-            "os"
+            "numpy",
+            "cv2"
         ],
         "modelscope.models.cv.image_portrait_enhancement.retinaface.models.net": [
-            "torchvision",
+            "time",
             "torch",
-            "time"
+            "torchvision"
         ],
         "modelscope.models.cv.image_portrait_enhancement.retinaface.models.retinaface": [
-            "torchvision",
+            "collections",
             "torch",
-            "collections"
+            "torchvision"
         ],
         "modelscope.models.cv.image_portrait_enhancement.retinaface.utils": [
             "itertools",
-            "math",
+            "torch",
             "numpy",
-            "torch"
+            "math"
         ],
         "modelscope.models.cv.image_probing_model.backbone": [
+            "collections",
+            "sys",
+            "functools",
             "operator",
             "math",
             "torch",
-            "sys",
-            "collections",
             "PIL",
             "torchvision",
-            "functools",
             "numpy"
         ],
         "modelscope.models.cv.image_probing_model.model": [
-            "typing",
-            "json",
+            "os",
             "torch",
-            "os"
+            "json",
+            "typing"
         ],
         "modelscope.models.cv.image_probing_model.utils": [
-            "re",
-            "torch"
+            "torch",
+            "re"
         ],
         "modelscope.models.cv.image_quality_assessment_degradation.degradation_model": [
-            "torchvision",
-            "json",
+            "collections",
             "time",
+            "torch",
             "cv2",
+            "torchvision",
             "numpy",
-            "torch",
-            "collections"
+            "json"
         ],
         "modelscope.models.cv.image_quality_assessment_degradation.image_quality_assessment_degradation": [
+            "os",
             "torch",
-            "typing",
-            "os"
+            "typing"
         ],
         "modelscope.models.cv.image_quality_assessment_man.image_quality_assessment_man": [
+            "os",
             "torch",
-            "typing",
-            "os"
+            "typing"
         ],
         "modelscope.models.cv.image_quality_assessment_man.maniqa": [
-            "einops",
             "timm",
-            "torch"
+            "torch",
+            "einops"
         ],
         "modelscope.models.cv.image_quality_assessment_man.swin": [
-            "einops",
-            "math",
-            "warnings",
+            "collections",
             "itertools",
+            "einops",
             "torch",
-            "collections"
+            "warnings",
+            "math"
         ],
         "modelscope.models.cv.image_quality_assessment_mos.backbones.resnet": [
             "os",
             "torch"
         ],
         "modelscope.models.cv.image_quality_assessment_mos.censeo_ivqa_model": [
             "torch"
         ],
         "modelscope.models.cv.image_quality_assessment_mos.heads.simple_head": [
             "torch"
         ],
         "modelscope.models.cv.image_quality_assessment_mos.image_quality_assessment_mos": [
+            "os",
             "torch",
-            "typing",
-            "os"
+            "typing"
         ],
         "modelscope.models.cv.image_reid_person.pass_model": [
-            "enum",
             "os",
+            "enum",
             "torch"
         ],
         "modelscope.models.cv.image_reid_person.transreid_model": [
+            "collections",
             "functools",
-            "itertools",
             "torch",
-            "collections"
+            "itertools"
         ],
         "modelscope.models.cv.image_restoration.demoire_models.nets": [
             "torch"
         ],
         "modelscope.models.cv.image_restoration.image_restoration_model": [
-            "cv2",
-            "numpy",
+            "os",
             "torch",
-            "os"
+            "numpy",
+            "cv2"
         ],
         "modelscope.models.cv.image_semantic_segmentation.ddpm_seg.data_util": [],
         "modelscope.models.cv.image_semantic_segmentation.ddpm_seg.feature_extractors": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.models.cv.image_semantic_segmentation.ddpm_seg.pixel_classifier": [
-            "numpy",
-            "torch",
             "collections",
+            "os",
+            "torch",
             "PIL",
-            "os"
+            "numpy"
         ],
         "modelscope.models.cv.image_semantic_segmentation.ddpm_seg.utils": [
             "PIL",
             "random",
             "numpy",
             "torch"
         ],
         "modelscope.models.cv.image_semantic_segmentation.ddpm_segmentation_model": [
-            "ddpm_guided_diffusion",
-            "typing",
+            "os",
             "torch",
-            "os"
+            "ddpm_guided_diffusion",
+            "typing"
         ],
         "modelscope.models.cv.image_semantic_segmentation.pan_merge.base_panoptic_fusion_head": [
+            "mmdet",
             "mmcv",
-            "abc",
-            "mmdet"
+            "abc"
         ],
         "modelscope.models.cv.image_semantic_segmentation.pan_merge.maskformer_semantic_head": [
-            "torch",
-            "mmdet"
+            "mmdet",
+            "torch"
         ],
         "modelscope.models.cv.image_semantic_segmentation.semantic_seg_model": [
             "os",
             "torch",
             "numpy"
         ],
         "modelscope.models.cv.image_semantic_segmentation.vit_adapter.models.backbone.adapter_modules": [
+            "functools",
             "torch",
             "logging",
-            "functools",
-            "timm",
-            "mmdet"
+            "mmdet",
+            "timm"
         ],
         "modelscope.models.cv.image_semantic_segmentation.vit_adapter.models.backbone.base.beit": [
-            "math",
-            "mmcv",
             "functools",
-            "timm",
             "torch",
-            "mmdet"
+            "mmcv",
+            "mmdet",
+            "timm",
+            "math"
         ],
         "modelscope.models.cv.image_semantic_segmentation.vit_adapter.models.backbone.beit_adapter": [
+            "logging",
+            "timm",
             "math",
             "torch",
-            "timm",
-            "mmdet",
-            "logging"
+            "mmdet"
         ],
         "modelscope.models.cv.image_semantic_segmentation.vit_adapter.models.decode_heads.base_decode_head": [
-            "mmcv",
-            "abc",
+            "mmdet",
             "torch",
-            "mmdet"
+            "mmcv",
+            "abc"
         ],
         "modelscope.models.cv.image_semantic_segmentation.vit_adapter.models.decode_heads.mask2former_head_from_mmseg": [
-            "mmcv",
-            "torch",
+            "mmdet",
             "copy",
-            "mmdet"
+            "mmcv",
+            "torch"
         ],
         "modelscope.models.cv.image_semantic_segmentation.vit_adapter.models.segmentors.base_segmentor": [
-            "abc",
-            "warnings",
+            "collections",
+            "torch",
             "mmcv",
+            "warnings",
             "numpy",
-            "torch",
-            "collections"
+            "abc"
         ],
         "modelscope.models.cv.image_semantic_segmentation.vit_adapter.models.segmentors.encoder_decoder_mask2former": [
-            "torch",
-            "mmdet"
+            "mmdet",
+            "torch"
         ],
         "modelscope.models.cv.image_semantic_segmentation.vit_adapter.utils.builder": [
             "mmcv"
         ],
         "modelscope.models.cv.image_semantic_segmentation.vit_adapter.utils.data_process_func": [
-            "mmcv",
-            "mmdet"
+            "mmdet",
+            "mmcv"
         ],
         "modelscope.models.cv.image_semantic_segmentation.vit_adapter.utils.seg_func": [
             "warnings",
             "torch"
         ],
         "modelscope.models.cv.image_skychange.preprocessor": [
-            "numbers",
+            "pdb",
+            "typing",
             "torch",
-            "torchvision",
             "cv2",
-            "typing",
-            "pdb",
+            "torchvision",
             "numpy",
+            "numbers",
             "json"
         ],
         "modelscope.models.cv.image_skychange.ptsemseg.BlockModules": [
             "torch"
         ],
         "modelscope.models.cv.image_skychange.ptsemseg.hrnet_backnone": [
             "logging",
-            "os",
             "torch",
-            "numpy"
+            "numpy",
+            "os"
         ],
         "modelscope.models.cv.image_skychange.ptsemseg.hrnet_super_and_ocr": [
             "__future__",
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.image_skychange.ptsemseg.unet": [
             "torch"
         ],
         "modelscope.models.cv.image_skychange.skychange": [
-            "numbers",
-            "numpy",
-            "torch",
             "collections",
-            "PIL",
-            "torchvision",
-            "cv2",
             "pdb",
             "os",
+            "torch",
+            "PIL",
+            "cv2",
+            "torchvision",
+            "numpy",
+            "numbers",
             "json"
         ],
         "modelscope.models.cv.image_skychange.skychange_model": [
-            "math",
-            "torch",
             "collections",
-            "json",
-            "time",
-            "cv2",
             "pdb",
+            "os",
+            "math",
+            "time",
             "typing",
-            "os"
+            "torch",
+            "cv2",
+            "json"
         ],
         "modelscope.models.cv.image_to_image_generation.data.transforms": [
-            "torchvision",
             "random",
-            "math",
-            "PIL"
+            "torchvision",
+            "PIL",
+            "math"
         ],
         "modelscope.models.cv.image_to_image_generation.model": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.cv.image_to_image_generation.models.autoencoder": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.cv.image_to_image_generation.models.clip": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.cv.image_to_image_generation.ops.diffusion": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.cv.image_to_image_generation.ops.losses": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.cv.image_to_image_translation.data.transforms": [
-            "torchvision",
             "random",
-            "math",
-            "PIL"
+            "torchvision",
+            "PIL",
+            "math"
         ],
         "modelscope.models.cv.image_to_image_translation.model_translation": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.cv.image_to_image_translation.models.autoencoder": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.cv.image_to_image_translation.models.clip": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.cv.image_to_image_translation.ops.apps": [
+            "os",
+            "artist",
             "torch",
-            "torchvision",
             "PIL",
-            "artist",
-            "numpy",
-            "os"
+            "torchvision",
+            "numpy"
         ],
         "modelscope.models.cv.image_to_image_translation.ops.degradation": [
-            "scipy",
-            "math",
+            "torch",
+            "random",
             "os",
             "cv2",
-            "random",
+            "scipy",
             "numpy",
-            "torch"
+            "math"
         ],
         "modelscope.models.cv.image_to_image_translation.ops.diffusion": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.cv.image_to_image_translation.ops.losses": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.cv.image_to_image_translation.ops.metrics": [
+            "torch",
             "scipy",
-            "numpy",
-            "torch"
+            "numpy"
         ],
         "modelscope.models.cv.image_to_image_translation.ops.random_color": [
-            "colorsys",
-            "random"
+            "random",
+            "colorsys"
         ],
         "modelscope.models.cv.image_to_image_translation.ops.random_mask": [
             "cv2",
             "numpy"
         ],
         "modelscope.models.cv.image_to_image_translation.ops.svd": [
             "torch"
         ],
         "modelscope.models.cv.image_to_image_translation.ops.utils": [
-            "io",
-            "multiprocessing",
+            "hashlib",
+            "zipfile",
+            "os",
+            "cv2",
             "math",
-            "numpy",
+            "base64",
             "torch",
+            "io",
             "PIL",
-            "zipfile",
-            "json",
-            "hashlib",
-            "cv2",
-            "os",
             "binascii",
-            "base64"
+            "numpy",
+            "multiprocessing",
+            "json"
         ],
         "modelscope.models.cv.image_try_on.generator": [
-            "torchvision",
-            "functools",
             "os",
-            "torch"
+            "functools",
+            "torch",
+            "torchvision"
         ],
         "modelscope.models.cv.image_try_on.landmark": [
             "logging",
-            "os",
-            "torch"
+            "torch",
+            "os"
         ],
         "modelscope.models.cv.image_try_on.try_on_infer": [
+            "os",
             "yaml",
+            "argparse",
             "torch",
-            "torchvision",
             "PIL",
-            "argparse",
             "cv2",
-            "numpy",
-            "os"
+            "torchvision",
+            "numpy"
         ],
         "modelscope.models.cv.image_try_on.warping": [
-            "math",
+            "collections",
+            "torch",
             "cv2",
             "numpy",
-            "torch",
-            "collections"
+            "math"
         ],
         "modelscope.models.cv.indoor_layout_estimation.networks.backbone.resnet_DA": [
-            "torchvision",
-            "torch"
+            "torch",
+            "torchvision"
         ],
         "modelscope.models.cv.indoor_layout_estimation.networks.backbone.vit_horizon_pry_image": [
             "timm",
             "numpy",
             "torch"
         ],
         "modelscope.models.cv.indoor_layout_estimation.networks.misc.fourier": [
-            "PIL",
             "scipy",
-            "numpy"
+            "numpy",
+            "PIL"
         ],
         "modelscope.models.cv.indoor_layout_estimation.networks.misc.panostretch": [
-            "scipy",
             "functools",
+            "scipy",
             "numpy"
         ],
         "modelscope.models.cv.indoor_layout_estimation.networks.misc.post_proc": [
-            "scipy",
             "sklearn",
+            "scipy",
             "numpy"
         ],
         "modelscope.models.cv.indoor_layout_estimation.networks.modality.layout": [
-            "math",
+            "scipy",
             "shapely",
+            "math",
             "torch",
-            "scipy",
             "numpy"
         ],
         "modelscope.models.cv.indoor_layout_estimation.networks.panovit": [
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.indoor_layout_estimation.networks.utils": [
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.indoor_layout_estimation.panovit": [
             "os",
             "numpy",
             "torch",
             "yacs"
         ],
         "modelscope.models.cv.language_guided_video_summarization.summarizer": [
-            "torch",
-            "bmt_clipit",
-            "videofeatures_clipit",
-            "argparse",
+            "os",
             "typing",
+            "argparse",
+            "videofeatures_clipit",
+            "torch",
             "numpy",
-            "os"
+            "bmt_clipit"
         ],
         "modelscope.models.cv.language_guided_video_summarization.transformer.layers": [
             "torch"
         ],
         "modelscope.models.cv.language_guided_video_summarization.transformer.models": [
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.language_guided_video_summarization.transformer.modules": [
             "torch"
         ],
         "modelscope.models.cv.language_guided_video_summarization.transformer.sub_layers": [
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.motion_generation.model": [],
         "modelscope.models.cv.motion_generation.modules.cfg_sampler": [
-            "torch",
-            "copy"
+            "copy",
+            "torch"
         ],
         "modelscope.models.cv.motion_generation.modules.gaussian_diffusion": [
             "enum",
-            "math",
+            "torch",
             "copy",
             "numpy",
-            "torch"
+            "math"
         ],
         "modelscope.models.cv.motion_generation.modules.mdm": [
             "clip",
             "numpy",
             "torch"
         ],
         "modelscope.models.cv.motion_generation.modules.respace": [
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.motion_generation.modules.rotation2xyz": [
             "torch"
         ],
         "modelscope.models.cv.motion_generation.modules.smpl": [
+            "torch",
             "contextlib",
             "os",
             "smplx",
-            "numpy",
-            "torch"
+            "numpy"
         ],
         "modelscope.models.cv.movie_scene_segmentation.get_model": [],
         "modelscope.models.cv.movie_scene_segmentation.model": [
+            "einops",
+            "os",
+            "tqdm",
             "math",
-            "numpy",
-            "torch",
+            "typing",
             "shotdetect_scenedetect_lgss",
-            "torchvision",
+            "torch",
             "PIL",
-            "typing",
-            "einops",
-            "os",
-            "tqdm"
+            "torchvision",
+            "numpy"
         ],
         "modelscope.models.cv.movie_scene_segmentation.utils.head": [
             "torch"
         ],
         "modelscope.models.cv.movie_scene_segmentation.utils.save_op": [
-            "cv2",
-            "numpy",
             "os",
+            "cv2",
             "subprocess",
+            "numpy",
             "tqdm"
         ],
         "modelscope.models.cv.movie_scene_segmentation.utils.shot_encoder": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.models.cv.movie_scene_segmentation.utils.trn": [
             "transformers",
             "torch"
         ],
         "modelscope.models.cv.nerf_recon_4k.dataloader.load_blender": [
-            "json",
+            "torch",
             "os",
-            "imageio",
             "cv2",
             "numpy",
-            "torch"
+            "imageio",
+            "json"
         ],
         "modelscope.models.cv.nerf_recon_4k.dataloader.load_data": [
             "numpy"
         ],
         "modelscope.models.cv.nerf_recon_4k.dataloader.load_llff": [
-            "scipy",
+            "torch",
             "os",
-            "imageio",
+            "scipy",
             "numpy",
-            "torch"
+            "imageio"
         ],
         "modelscope.models.cv.nerf_recon_4k.dataloader.load_tankstemple": [
-            "glob",
             "os",
-            "imageio",
-            "numpy"
+            "glob",
+            "numpy",
+            "imageio"
         ],
         "modelscope.models.cv.nerf_recon_4k.dataloader.read_write_model": [
+            "collections",
             "argparse",
-            "numpy",
-            "os",
             "struct",
-            "collections"
+            "os",
+            "numpy"
         ],
         "modelscope.models.cv.nerf_recon_4k.nerf_preprocess": [
-            "glob",
-            "numpy",
-            "cv2",
-            "typing",
             "os",
+            "typing",
             "tensorflow",
-            "subprocess"
+            "glob",
+            "cv2",
+            "subprocess",
+            "numpy"
         ],
         "modelscope.models.cv.nerf_recon_4k.nerf_recon_4k": [
-            "torch",
-            "argparse",
+            "mmcv",
+            "os",
+            "tqdm",
             "time",
+            "argparse",
+            "torch",
             "imageio",
-            "mmcv",
             "random",
-            "numpy",
-            "os",
-            "tqdm"
+            "numpy"
         ],
         "modelscope.models.cv.nerf_recon_4k.network.dvgo": [
+            "functools",
+            "os",
+            "copy",
             "math",
-            "numpy",
+            "time",
             "torch",
             "torch_scatter",
-            "time",
-            "copy",
-            "functools",
-            "os"
+            "numpy"
         ],
         "modelscope.models.cv.nerf_recon_4k.network.utils": [
-            "collections",
             "mcubes",
-            "tinycudann",
-            "numpy",
+            "collections",
             "torch",
-            "gc"
+            "gc",
+            "tinycudann",
+            "numpy"
         ],
         "modelscope.models.cv.nerf_recon_acc.dataloader.nerf_dataset": [
+            "os",
             "math",
-            "numpy",
             "torch",
-            "torchvision",
             "PIL",
-            "os",
+            "torchvision",
+            "numpy",
             "json"
         ],
         "modelscope.models.cv.nerf_recon_acc.dataloader.read_write_model": [
+            "collections",
             "argparse",
-            "numpy",
-            "os",
             "struct",
-            "collections"
+            "os",
+            "numpy"
         ],
         "modelscope.models.cv.nerf_recon_acc.nerf_preprocess": [
-            "glob",
-            "numpy",
-            "cv2",
-            "typing",
             "os",
+            "typing",
             "tensorflow",
-            "subprocess"
+            "glob",
+            "cv2",
+            "subprocess",
+            "numpy"
         ],
         "modelscope.models.cv.nerf_recon_acc.nerf_recon_acc": [
-            "glob",
-            "torch",
+            "os",
+            "tqdm",
             "time",
+            "torch",
+            "glob",
             "cv2",
-            "numpy",
-            "os",
-            "tqdm"
+            "numpy"
         ],
         "modelscope.models.cv.nerf_recon_acc.network.nerf": [
+            "nerfacc",
             "tinycudann",
-            "numpy",
             "torch",
-            "nerfacc"
+            "numpy"
         ],
         "modelscope.models.cv.nerf_recon_acc.network.segmenter": [
-            "numpy",
-            "tensorflow"
+            "tensorflow",
+            "numpy"
         ],
         "modelscope.models.cv.nerf_recon_acc.network.utils": [
-            "collections",
             "mcubes",
-            "tinycudann",
-            "numpy",
+            "collections",
             "torch",
-            "gc"
+            "gc",
+            "tinycudann",
+            "numpy"
         ],
         "modelscope.models.cv.nerf_recon_vq_compression.dataloader.blender": [
-            "numpy",
+            "os",
+            "tqdm",
             "torch",
             "PIL",
-            "torchvision",
             "cv2",
-            "os",
-            "json",
-            "tqdm"
+            "torchvision",
+            "numpy",
+            "json"
         ],
         "modelscope.models.cv.nerf_recon_vq_compression.dataloader.llff": [
-            "glob",
+            "os",
             "torch",
             "PIL",
+            "glob",
             "torchvision",
-            "numpy",
-            "os"
+            "numpy"
         ],
         "modelscope.models.cv.nerf_recon_vq_compression.dataloader.nsvf": [
+            "os",
+            "tqdm",
             "torch",
             "PIL",
-            "torchvision",
-            "os",
-            "tqdm"
+            "torchvision"
         ],
         "modelscope.models.cv.nerf_recon_vq_compression.dataloader.ray_utils": [
             "kornia",
-            "re",
+            "torch",
             "numpy",
-            "torch"
+            "re"
         ],
         "modelscope.models.cv.nerf_recon_vq_compression.dataloader.tankstemple": [
+            "os",
+            "tqdm",
             "torch",
             "PIL",
-            "torchvision",
-            "os",
-            "tqdm"
+            "torchvision"
         ],
         "modelscope.models.cv.nerf_recon_vq_compression.nerf_recon_vq_compression": [
-            "glob",
-            "torch",
-            "time",
-            "cv2",
             "functools",
-            "numpy",
             "os",
-            "tqdm"
+            "tqdm",
+            "time",
+            "torch",
+            "glob",
+            "cv2",
+            "numpy"
         ],
         "modelscope.models.cv.nerf_recon_vq_compression.network.tensoRF": [],
         "modelscope.models.cv.nerf_recon_vq_compression.network.tensoRF_VQ": [
-            "torch",
-            "random",
-            "typing",
             "os",
-            "tqdm"
+            "tqdm",
+            "typing",
+            "torch",
+            "random"
         ],
         "modelscope.models.cv.nerf_recon_vq_compression.network.tensorBase": [
-            "numpy",
+            "time",
             "torch",
-            "time"
+            "numpy"
         ],
         "modelscope.models.cv.nerf_recon_vq_compression.network.weighted_vq": [
             "contextlib",
             "torch",
             "einops"
         ],
         "modelscope.models.cv.nerf_recon_vq_compression.renderer": [
-            "sys",
-            "torch",
-            "imageio",
             "numpy",
             "os",
-            "tqdm"
+            "tqdm",
+            "torch",
+            "imageio",
+            "sys"
         ],
         "modelscope.models.cv.nerf_recon_vq_compression.utils": [
+            "scipy",
+            "skimage",
             "plyfile",
             "torch",
-            "torchvision",
-            "scipy",
             "PIL",
             "cv2",
-            "numpy",
-            "skimage"
+            "torchvision",
+            "numpy"
         ],
         "modelscope.models.cv.object_detection.mmdet_model": [
-            "numpy",
+            "os",
             "torch",
-            "os"
+            "numpy"
         ],
         "modelscope.models.cv.object_detection.mmdet_ms.backbones.vit": [
+            "functools",
+            "timm",
             "math",
             "torch",
-            "timm",
-            "mmdet",
-            "functools"
+            "mmdet"
         ],
         "modelscope.models.cv.object_detection.mmdet_ms.dense_heads.anchor_head": [
             "mmdet"
         ],
         "modelscope.models.cv.object_detection.mmdet_ms.dense_heads.rpn_head": [
-            "mmcv",
-            "torch",
+            "mmdet",
             "copy",
-            "mmdet"
+            "mmcv",
+            "torch"
         ],
         "modelscope.models.cv.object_detection.mmdet_ms.necks.fpn": [
-            "mmcv",
+            "mmdet",
             "torch",
-            "mmdet"
+            "mmcv"
         ],
         "modelscope.models.cv.object_detection.mmdet_ms.roi_heads.bbox_heads.convfc_bbox_head": [
-            "torch",
-            "mmdet"
+            "mmdet",
+            "torch"
         ],
         "modelscope.models.cv.object_detection.mmdet_ms.roi_heads.mask_heads.fcn_mask_head": [
+            "mmcv",
+            "warnings",
             "torch",
             "mmdet",
-            "warnings",
-            "mmcv",
             "numpy"
         ],
         "modelscope.models.cv.object_detection.mmdet_ms.utils.checkpoint": [
+            "collections",
+            "mmcv",
             "pkgutil",
+            "os",
+            "warnings",
+            "time",
             "importlib",
             "torch",
-            "collections",
-            "torchvision",
+            "io",
             "tempfile",
-            "warnings",
-            "time",
-            "mmcv",
-            "os",
-            "io"
+            "torchvision"
         ],
         "modelscope.models.cv.object_detection.mmdet_ms.utils.convModule_norm": [
             "mmcv"
         ],
         "modelscope.models.cv.object_detection_3d.depe.depe_detect": [
-            "typing",
-            "numpy",
+            "os",
             "torch",
-            "os"
+            "numpy",
+            "typing"
         ],
         "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.core.bbox.assigners.hungarian_assigner_3d": [
-            "scipy",
+            "mmdet",
             "torch",
-            "mmdet"
+            "scipy"
         ],
         "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.core.bbox.coders.nms_free_coder": [
-            "torch",
-            "mmdet"
+            "mmdet",
+            "torch"
         ],
         "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.core.bbox.match_costs.match_cost": [
-            "torch",
-            "mmdet"
+            "mmdet",
+            "torch"
         ],
         "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.core.bbox.util": [
+            "torch",
             "mmdet3d",
-            "numpy",
-            "torch"
+            "numpy"
         ],
         "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.datasets.nuscenes_dataset": [
+            "mmdet",
             "mmdet3d",
-            "numpy",
-            "mmdet"
+            "numpy"
         ],
         "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.datasets.pipelines.loading": [
+            "mmdet",
             "mmcv",
-            "numpy",
-            "mmdet"
+            "numpy"
         ],
         "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.datasets.pipelines.transform_3d": [
-            "mmdet3d",
+            "torch",
+            "mmcv",
             "PIL",
+            "mmdet",
             "copy",
-            "mmcv",
-            "numpy",
-            "torch",
-            "mmdet"
+            "mmdet3d",
+            "numpy"
         ],
         "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.backbones.vovnet": [
-            "mmcv",
-            "torch",
+            "collections",
             "mmdet",
-            "collections"
+            "torch",
+            "mmcv"
         ],
         "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.dense_heads.depth_net": [
-            "mmcv",
+            "mmdet",
             "torch",
-            "mmdet"
+            "mmcv"
         ],
         "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.dense_heads.petrv2_dednhead": [
-            "mmdet3d",
+            "mmcv",
+            "copy",
             "math",
             "torch",
             "mmdet",
-            "copy",
-            "mmcv",
+            "mmdet3d",
             "numpy"
         ],
         "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.detectors.petr3d": [
-            "mmdet3d",
+            "mmcv",
             "torch",
             "mmdet",
-            "mmcv",
+            "mmdet3d",
             "numpy"
         ],
         "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.necks.cp_fpn": [
-            "mmcv",
+            "mmdet",
             "torch",
-            "mmdet"
+            "mmcv"
         ],
         "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.utils.petr_transformer": [
-            "math",
+            "torch",
+            "mmcv",
             "warnings",
+            "mmdet",
             "copy",
-            "mmcv",
-            "typing",
-            "torch",
-            "mmdet"
+            "math",
+            "typing"
         ],
         "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.utils.positional_encoding": [
+            "torch",
             "mmcv",
-            "math",
-            "torch"
+            "math"
         ],
         "modelscope.models.cv.object_detection_3d.depe.result_vis": [
-            "mmdet3d",
-            "pyquaternion",
-            "numpy",
+            "os",
             "argparse",
-            "cv2",
             "pickle",
-            "os",
+            "pyquaternion",
+            "cv2",
+            "mmdet3d",
+            "numpy",
             "json"
         ],
         "modelscope.models.cv.ocr_detection.model": [
-            "typing",
-            "numpy",
+            "os",
             "torch",
-            "os"
+            "numpy",
+            "typing"
         ],
         "modelscope.models.cv.ocr_detection.modules.dbnet": [
+            "collections",
+            "os",
             "math",
             "torch",
-            "sys",
-            "collections",
-            "os"
+            "sys"
         ],
         "modelscope.models.cv.ocr_detection.modules.layers": [
-            "numpy",
+            "collections",
             "torch",
-            "collections"
+            "numpy"
         ],
         "modelscope.models.cv.ocr_detection.modules.mix_ops": [
-            "math",
+            "torch",
             "numpy",
-            "torch"
+            "math"
         ],
         "modelscope.models.cv.ocr_detection.modules.proxyless": [
-            "re",
-            "sys",
+            "numpy",
             "torch",
-            "numpy"
+            "sys",
+            "re"
         ],
         "modelscope.models.cv.ocr_detection.modules.seg_detector_loss": [
-            "sys",
-            "torch"
+            "torch",
+            "sys"
         ],
         "modelscope.models.cv.ocr_detection.preprocessor": [
+            "os",
             "math",
-            "numpy",
+            "typing",
             "torch",
             "PIL",
             "cv2",
-            "typing",
-            "os"
+            "numpy"
         ],
         "modelscope.models.cv.ocr_detection.utils": [
-            "cv2",
-            "pyclipper",
             "shapely",
-            "numpy"
+            "cv2",
+            "numpy",
+            "pyclipper"
         ],
         "modelscope.models.cv.ocr_recognition.model": [
             "os",
             "torch"
         ],
         "modelscope.models.cv.ocr_recognition.modules.CRNN.main_model": [
             "torch"
@@ -13667,393 +13602,393 @@
         "modelscope.models.cv.ocr_recognition.modules.ConvNextViT.convnext": [
             "torch"
         ],
         "modelscope.models.cv.ocr_recognition.modules.ConvNextViT.main_model": [
             "torch"
         ],
         "modelscope.models.cv.ocr_recognition.modules.ConvNextViT.timm_tinyc": [
+            "collections",
             "functools",
-            "math",
-            "copy",
+            "torch",
             "logging",
             "itertools",
-            "torch",
-            "collections"
+            "copy",
+            "math"
         ],
         "modelscope.models.cv.ocr_recognition.modules.ConvNextViT.vitstr": [
+            "functools",
+            "logging",
             "__future__",
-            "torch",
             "copy",
-            "logging",
-            "functools"
+            "torch"
         ],
         "modelscope.models.cv.ocr_recognition.modules.LightweightEdge.main_model": [
-            "torch",
-            "collections"
+            "collections",
+            "torch"
         ],
         "modelscope.models.cv.ocr_recognition.modules.LightweightEdge.nas_block.layers": [
-            "numpy",
+            "collections",
             "torch",
-            "collections"
+            "numpy"
         ],
         "modelscope.models.cv.ocr_recognition.modules.LightweightEdge.nas_block.mix_ops": [
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.ocr_recognition.modules.LightweightEdge.nas_block.proxyless": [
-            "re",
             "sys",
             "torch",
-            "numpy",
-            "queue"
+            "queue",
+            "re",
+            "numpy"
         ],
         "modelscope.models.cv.ocr_recognition.preprocessor": [
+            "os",
             "torch",
             "PIL",
             "cv2",
-            "numpy",
-            "os"
+            "numpy"
         ],
         "modelscope.models.cv.open_vocabulary_detection_vild.vild": [
-            "numpy",
+            "os",
             "clip",
-            "torch",
             "scipy",
             "typing",
-            "os",
-            "tensorflow"
+            "tensorflow",
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.panorama_depth_estimation.networks.equi": [
+            "collections",
             "__future__",
-            "numpy",
             "torch",
-            "collections"
+            "numpy"
         ],
         "modelscope.models.cv.panorama_depth_estimation.networks.layers": [
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.panorama_depth_estimation.networks.mobilenet": [
             "torch"
         ],
         "modelscope.models.cv.panorama_depth_estimation.networks.resnet": [
             "torch"
         ],
         "modelscope.models.cv.panorama_depth_estimation.networks.unifuse": [
+            "collections",
             "__future__",
-            "numpy",
             "torch",
-            "collections"
+            "numpy"
         ],
         "modelscope.models.cv.panorama_depth_estimation.networks.util": [
             "cv2",
             "scipy",
             "numpy"
         ],
         "modelscope.models.cv.panorama_depth_estimation.unifuse_model": [
-            "torchvision",
-            "numpy",
+            "os",
             "torch",
-            "os"
+            "torchvision",
+            "numpy"
         ],
         "modelscope.models.cv.pedestrian_attribute_recognition.model": [
-            "torchvision",
-            "numpy",
+            "os",
             "torch",
-            "os"
+            "torchvision",
+            "numpy"
         ],
         "modelscope.models.cv.pointcloud_sceneflow_estimation.common": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.cv.pointcloud_sceneflow_estimation.pointnet2_utils": [
+            "torch",
             "pointnet2_cuda",
-            "typing",
-            "torch"
+            "typing"
         ],
         "modelscope.models.cv.pointcloud_sceneflow_estimation.rcp_model": [
-            "numpy",
+            "os",
             "torch",
-            "os"
+            "numpy"
         ],
         "modelscope.models.cv.pointcloud_sceneflow_estimation.sf_rcp": [
             "torch"
         ],
         "modelscope.models.cv.product_retrieval_embedding.item_detection": [
             "cv2",
             "numpy"
         ],
         "modelscope.models.cv.product_retrieval_embedding.item_embedding": [
             "cv2",
             "numpy",
             "torch"
         ],
         "modelscope.models.cv.product_retrieval_embedding.item_model": [
-            "typing",
-            "numpy",
+            "os",
             "torch",
-            "os"
+            "numpy",
+            "typing"
         ],
         "modelscope.models.cv.product_segmentation.net": [
             "torch"
         ],
         "modelscope.models.cv.product_segmentation.seg_infer": [
-            "PIL",
             "numpy",
             "torch",
-            "cv2"
+            "cv2",
+            "PIL"
         ],
         "modelscope.models.cv.referring_video_object_segmentation.model": [
-            "typing",
+            "os",
             "torch",
-            "os"
+            "typing"
         ],
         "modelscope.models.cv.referring_video_object_segmentation.utils.backbone": [
+            "torch",
             "torchvision",
-            "einops",
-            "torch"
+            "einops"
         ],
         "modelscope.models.cv.referring_video_object_segmentation.utils.criterion": [
             "torch"
         ],
         "modelscope.models.cv.referring_video_object_segmentation.utils.matcher": [
-            "scipy",
-            "torch"
+            "torch",
+            "scipy"
         ],
         "modelscope.models.cv.referring_video_object_segmentation.utils.misc": [
-            "torchvision",
             "pickle",
-            "typing",
-            "torch"
+            "torchvision",
+            "torch",
+            "typing"
         ],
         "modelscope.models.cv.referring_video_object_segmentation.utils.mttr": [
-            "einops",
-            "torch"
+            "torch",
+            "einops"
         ],
         "modelscope.models.cv.referring_video_object_segmentation.utils.multimodal_transformer": [
-            "torch",
-            "copy",
             "einops",
+            "os",
             "transformers",
+            "copy",
             "typing",
-            "os"
+            "torch"
         ],
         "modelscope.models.cv.referring_video_object_segmentation.utils.position_encoding_2d": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.cv.referring_video_object_segmentation.utils.postprocessing": [
-            "einops",
-            "numpy",
+            "pycocotools",
             "torch",
-            "pycocotools"
+            "numpy",
+            "einops"
         ],
         "modelscope.models.cv.referring_video_object_segmentation.utils.segmentation": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.models.cv.referring_video_object_segmentation.utils.swin_transformer": [
-            "operator",
             "functools",
+            "operator",
             "einops",
+            "torch",
             "timm",
-            "numpy",
-            "torch"
+            "numpy"
         ],
         "modelscope.models.cv.robust_image_classification.easyrobust_model": [
             "os",
             "torch"
         ],
         "modelscope.models.cv.s2net_panorama_depth_estimation.networks.config": [
-            "yaml",
             "os",
+            "yaml",
             "yacs"
         ],
         "modelscope.models.cv.s2net_panorama_depth_estimation.networks.decoder": [
-            "einops",
+            "torch",
             "numpy",
-            "torch"
+            "einops"
         ],
         "modelscope.models.cv.s2net_panorama_depth_estimation.networks.model": [
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.s2net_panorama_depth_estimation.networks.resnet": [
             "torch"
         ],
         "modelscope.models.cv.s2net_panorama_depth_estimation.networks.swin_transformer": [
             "torch",
             "numpy",
             "timm"
         ],
         "modelscope.models.cv.s2net_panorama_depth_estimation.networks.util_helper": [
-            "numpy",
             "pkgutil",
+            "os",
+            "warnings",
             "importlib",
             "torch",
-            "torchvision",
-            "apex",
             "healpy",
-            "warnings",
-            "os"
+            "apex",
+            "torchvision",
+            "numpy"
         ],
         "modelscope.models.cv.s2net_panorama_depth_estimation.s2net_model": [
-            "torchvision",
-            "numpy",
+            "os",
             "torch",
-            "os"
+            "torchvision",
+            "numpy"
         ],
         "modelscope.models.cv.salient_detection.models.backbone.Res2Net_v1b": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.cv.salient_detection.models.modules": [
             "torch"
         ],
         "modelscope.models.cv.salient_detection.models.senet": [
             "torch"
         ],
         "modelscope.models.cv.salient_detection.models.u2net": [
             "torch"
         ],
         "modelscope.models.cv.salient_detection.models.utils": [
             "torch"
         ],
         "modelscope.models.cv.salient_detection.salient_model": [
+            "os",
             "torch",
-            "torchvision",
             "PIL",
             "cv2",
-            "os"
+            "torchvision"
         ],
         "modelscope.models.cv.shop_segmentation.common": [
             "warnings",
             "torch"
         ],
         "modelscope.models.cv.shop_segmentation.head_fpn": [
-            "torch",
-            "numpy",
             "timm",
-            "mmcv"
+            "mmcv",
+            "numpy",
+            "torch"
         ],
         "modelscope.models.cv.shop_segmentation.models": [
+            "collections",
             "torch",
             "timm",
-            "math",
-            "collections"
+            "math"
         ],
         "modelscope.models.cv.shop_segmentation.neck_fpn": [
+            "torch",
             "mmcv",
-            "timm",
-            "torch"
+            "timm"
         ],
         "modelscope.models.cv.shop_segmentation.shop_seg_base": [
             "torch"
         ],
         "modelscope.models.cv.shop_segmentation.shop_seg_model": [
-            "numpy",
-            "torch",
-            "PIL",
             "os",
             "typing",
+            "torch",
+            "PIL",
+            "numpy",
             "json"
         ],
         "modelscope.models.cv.shop_segmentation.utils": [
-            "gzip",
-            "ftfy",
-            "torch",
-            "html",
-            "typing",
             "functools",
+            "ftfy",
             "os",
-            "regex"
+            "regex",
+            "typing",
+            "torch",
+            "gzip",
+            "html"
         ],
         "modelscope.models.cv.skin_retouching.detection_model.detection_module": [
             "torch"
         ],
         "modelscope.models.cv.skin_retouching.detection_model.detection_unet_in": [
             "torch"
         ],
         "modelscope.models.cv.skin_retouching.inpainting_model.gconv": [
             "torch"
         ],
         "modelscope.models.cv.skin_retouching.inpainting_model.inpainting_unet": [
             "torch"
         ],
         "modelscope.models.cv.skin_retouching.retinaface.box_utils": [
-            "typing",
             "torch",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.models.cv.skin_retouching.retinaface.net": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.models.cv.skin_retouching.retinaface.network": [
+            "torch",
             "torchvision",
-            "typing",
-            "torch"
+            "typing"
         ],
         "modelscope.models.cv.skin_retouching.retinaface.predict_single": [
-            "numpy",
-            "torch",
             "albumentations",
+            "typing",
+            "torch",
             "torchvision",
-            "typing"
+            "numpy"
         ],
         "modelscope.models.cv.skin_retouching.retinaface.prior_box": [
-            "torch",
             "itertools",
+            "torch",
             "math"
         ],
         "modelscope.models.cv.skin_retouching.retinaface.utils": [
+            "pathlib",
+            "torch",
             "re",
-            "numpy",
             "cv2",
-            "typing",
-            "torch",
-            "pathlib"
+            "numpy",
+            "typing"
         ],
         "modelscope.models.cv.skin_retouching.unet_deploy": [
             "warnings",
             "torch"
         ],
         "modelscope.models.cv.skin_retouching.utils": [
             "time",
-            "cv2",
-            "typing",
             "einops",
+            "torch",
+            "cv2",
             "numpy",
-            "torch"
+            "typing"
         ],
         "modelscope.models.cv.skin_retouching.weights_init": [
             "torch"
         ],
         "modelscope.models.cv.stream_yolo.data.data_augment": [
+            "numpy",
             "cv2",
             "random",
-            "numpy",
             "math"
         ],
         "modelscope.models.cv.stream_yolo.exp.base_exp": [
-            "abc",
-            "torch"
+            "torch",
+            "abc"
         ],
         "modelscope.models.cv.stream_yolo.exp.build": [
             "os",
             "sys"
         ],
         "modelscope.models.cv.stream_yolo.exp.default.streamyolo": [
-            "sys",
             "os",
-            "torch"
+            "torch",
+            "sys"
         ],
         "modelscope.models.cv.stream_yolo.exp.yolox_base": [
-            "random",
             "os",
+            "random",
             "torch"
         ],
         "modelscope.models.cv.stream_yolo.models.darknet": [
             "torch"
         ],
         "modelscope.models.cv.stream_yolo.models.dfp_pafpn": [
             "torch"
@@ -14064,179 +13999,179 @@
         "modelscope.models.cv.stream_yolo.models.streamyolo": [
             "torch"
         ],
         "modelscope.models.cv.stream_yolo.models.tal_head": [
             "torch"
         ],
         "modelscope.models.cv.stream_yolo.realtime_video_detector": [
-            "torch",
-            "json",
+            "logging",
+            "os",
+            "tqdm",
             "time",
             "argparse",
+            "torch",
             "cv2",
-            "logging",
             "numpy",
-            "os",
-            "tqdm"
+            "json"
         ],
         "modelscope.models.cv.stream_yolo.utils.boxes": [
-            "torchvision",
-            "torch"
+            "torch",
+            "torchvision"
         ],
         "modelscope.models.cv.stream_yolo.utils.format": [
             "math"
         ],
         "modelscope.models.cv.super_resolution.arch_util": [
-            "torchvision",
-            "math",
-            "warnings",
+            "collections",
             "itertools",
             "torch",
-            "collections"
+            "warnings",
+            "torchvision",
+            "math"
         ],
         "modelscope.models.cv.super_resolution.ecb": [
             "torch"
         ],
         "modelscope.models.cv.super_resolution.ecbsr_model": [
-            "typing",
+            "os",
             "torch",
-            "os"
+            "typing"
         ],
         "modelscope.models.cv.super_resolution.rrdbnet_arch": [
             "torch"
         ],
         "modelscope.models.cv.table_recognition.lineless_table_process": [
             "cv2",
-            "shapely",
             "numpy",
+            "shapely",
             "torch"
         ],
         "modelscope.models.cv.table_recognition.model_lore": [
-            "math",
-            "numpy",
-            "torch",
+            "os",
             "copy",
             "typing",
-            "os"
+            "math",
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.table_recognition.modules.lore_detector": [
-            "math",
-            "copy",
+            "torch",
             "os",
+            "copy",
             "numpy",
-            "torch"
+            "math"
         ],
         "modelscope.models.cv.table_recognition.modules.lore_processor": [
-            "math",
-            "copy",
+            "torch",
             "os",
+            "copy",
             "numpy",
-            "torch"
+            "math"
         ],
         "modelscope.models.cv.text_driven_segmentation.clip": [
             "urllib",
-            "torch",
-            "PIL",
-            "torchvision",
-            "warnings",
             "hashlib",
+            "os",
+            "warnings",
+            "tqdm",
             "typing",
             "pkg_resources",
-            "os",
-            "tqdm"
+            "torch",
+            "PIL",
+            "torchvision"
         ],
         "modelscope.models.cv.text_driven_segmentation.lseg_base": [
             "torch"
         ],
         "modelscope.models.cv.text_driven_segmentation.lseg_blocks": [
             "torch"
         ],
         "modelscope.models.cv.text_driven_segmentation.lseg_model": [
-            "numpy",
-            "torch",
-            "PIL",
             "os",
             "typing",
+            "torch",
+            "PIL",
+            "numpy",
             "json"
         ],
         "modelscope.models.cv.text_driven_segmentation.lseg_net": [
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.text_driven_segmentation.lseg_vit": [
-            "torch",
-            "types",
             "timm",
+            "types",
+            "torch",
             "math"
         ],
         "modelscope.models.cv.text_driven_segmentation.model": [
-            "typing",
-            "numpy",
+            "collections",
             "torch",
-            "collections"
+            "numpy",
+            "typing"
         ],
         "modelscope.models.cv.text_driven_segmentation.simple_tokenizer": [
-            "gzip",
-            "ftfy",
-            "regex",
             "functools",
+            "ftfy",
+            "gzip",
             "os",
+            "regex",
             "html"
         ],
         "modelscope.models.cv.text_to_360panorama_image.pipeline_base": [
-            "re",
-            "diffusers",
-            "torch",
-            "warnings",
             "transformers",
+            "warnings",
             "typing",
-            "inspect",
-            "packaging"
+            "torch",
+            "diffusers",
+            "re",
+            "packaging",
+            "inspect"
         ],
         "modelscope.models.cv.text_to_360panorama_image.pipeline_sr": [
-            "re",
-            "diffusers",
             "numpy",
-            "torch",
-            "PIL",
+            "os",
             "warnings",
+            "transformers",
             "copy",
             "typing",
-            "transformers",
-            "os",
+            "torch",
+            "PIL",
+            "re",
+            "diffusers",
             "inspect"
         ],
         "modelscope.models.cv.tinynas_classfication.basic_blocks": [
-            "uuid",
             "torch",
-            "numpy"
+            "numpy",
+            "uuid"
         ],
         "modelscope.models.cv.tinynas_classfication.global_utils": [],
         "modelscope.models.cv.tinynas_classfication.master_net": [
             "torch"
         ],
         "modelscope.models.cv.tinynas_classfication.model_zoo": [],
         "modelscope.models.cv.tinynas_classfication.plain_net_utils": [
             "torch"
         ],
         "modelscope.models.cv.tinynas_classfication.super_blocks": [
-            "uuid",
-            "torch"
+            "torch",
+            "uuid"
         ],
         "modelscope.models.cv.tinynas_classfication.super_res_idwexkx": [
-            "uuid",
-            "torch"
+            "torch",
+            "uuid"
         ],
         "modelscope.models.cv.tinynas_classfication.super_res_k1kxk1": [
-            "uuid",
-            "torch"
+            "torch",
+            "uuid"
         ],
         "modelscope.models.cv.tinynas_classfication.super_res_kxkx": [
-            "uuid",
-            "torch"
+            "torch",
+            "uuid"
         ],
         "modelscope.models.cv.tinynas_detection.damo.apis.detector_evaluater": [
             "os",
             "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.apis.detector_inference": [
             "os",
@@ -14248,90 +14183,90 @@
             "numpy"
         ],
         "modelscope.models.cv.tinynas_detection.damo.augmentations.box_level_augs.color_augs": [
             "random",
             "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.augmentations.box_level_augs.gaussian_maps": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.cv.tinynas_detection.damo.augmentations.box_level_augs.geometric_augs": [
+            "copy",
             "torchvision",
             "random",
-            "torch",
-            "copy"
+            "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.augmentations.scale_aware_aug": [
             "copy"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.backbones.darknet": [
             "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.backbones.tinynas_csp": [
             "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.backbones.tinynas_res": [
             "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.core.base_ops": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.core.neck_ops": [
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.core.ops": [
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.core.ota_assigner": [
             "warnings",
             "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.core.repvgg_block": [
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.core.utils": [
             "functools",
             "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.core.weight_init": [
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.heads.gfocal_v2_tiny": [
             "functools",
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.heads.zero_head": [
             "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.losses.distill_loss": [
             "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.losses.gfocal_loss": [
             "functools",
             "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.necks.giraffe_config": [
-            "networkx",
-            "collections"
+            "collections",
+            "networkx"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.necks.giraffe_fpn": [
-            "math",
-            "numpy",
-            "torch",
-            "timm",
             "collections",
             "functools",
-            "typing"
+            "timm",
+            "typing",
+            "math",
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.necks.giraffe_fpn_btn": [
             "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.detectors.detector": [
             "torch"
         ],
@@ -14342,60 +14277,60 @@
             "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.structures.image_list": [
             "__future__",
             "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.utils.boxes": [
+            "torch",
             "torchvision",
-            "numpy",
-            "torch"
+            "numpy"
         ],
         "modelscope.models.cv.tinynas_detection.damo.utils.model_utils": [
+            "time",
             "thop",
-            "math",
+            "torch",
             "copy",
-            "time",
-            "torch"
+            "math"
         ],
         "modelscope.models.cv.tinynas_detection.damo.utils.scheduler": [
             "math"
         ],
         "modelscope.models.cv.tinynas_detection.detector": [
-            "torchvision",
-            "pickle",
             "os",
+            "pickle",
+            "torchvision",
             "torch"
         ],
         "modelscope.models.cv.tinynas_detection.tinynas_damoyolo": [],
         "modelscope.models.cv.tinynas_detection.tinynas_detector": [],
         "modelscope.models.cv.tinynas_detection.utils": [
-            "tempfile",
+            "sys",
+            "importlib",
             "easydict",
             "os",
             "shutil",
-            "importlib",
-            "sys"
+            "tempfile"
         ],
         "modelscope.models.cv.video_deinterlace.UNet_for_video_deinterlace": [
-            "typing",
-            "torch",
+            "os",
             "copy",
-            "os"
+            "torch",
+            "typing"
         ],
         "modelscope.models.cv.video_deinterlace.deinterlace_arch": [
             "torch"
         ],
         "modelscope.models.cv.video_deinterlace.models.archs": [
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.video_deinterlace.models.deep_fourier_upsampling": [
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.video_deinterlace.models.enh": [
             "torch"
         ],
         "modelscope.models.cv.video_deinterlace.models.fre": [
             "torch"
         ],
@@ -14403,157 +14338,157 @@
             "torch"
         ],
         "modelscope.models.cv.video_depth_estimation.configs.default_config": [
             "os",
             "yacs"
         ],
         "modelscope.models.cv.video_depth_estimation.dro_model": [
-            "glob",
+            "os",
+            "tqdm",
             "torch",
+            "glob",
             "cv2",
-            "numpy",
-            "os",
-            "tqdm"
+            "numpy"
         ],
         "modelscope.models.cv.video_depth_estimation.geometry.camera": [
             "functools",
             "torch"
         ],
         "modelscope.models.cv.video_depth_estimation.geometry.camera_utils": [
             "torch"
         ],
         "modelscope.models.cv.video_depth_estimation.geometry.pose": [
             "torch"
         ],
         "modelscope.models.cv.video_depth_estimation.geometry.pose_utils": [
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.video_depth_estimation.models.model_checkpoint": [
-            "re",
             "os",
             "torch",
-            "numpy"
+            "numpy",
+            "re"
         ],
         "modelscope.models.cv.video_depth_estimation.models.model_utils": [],
         "modelscope.models.cv.video_depth_estimation.models.model_wrapper": [
+            "collections",
             "importlib",
             "torch",
-            "collections",
             "random",
             "numpy"
         ],
         "modelscope.models.cv.video_depth_estimation.models.sfm_model_mf": [
             "random",
             "torch"
         ],
         "modelscope.models.cv.video_depth_estimation.models.sup_model_mf": [],
         "modelscope.models.cv.video_depth_estimation.networks.depth_pose.depth_pose_net": [
             "functools",
             "torch"
         ],
         "modelscope.models.cv.video_depth_estimation.networks.layers.resnet.depth_decoder": [
+            "collections",
             "__future__",
-            "numpy",
             "torch",
-            "collections"
+            "numpy"
         ],
         "modelscope.models.cv.video_depth_estimation.networks.layers.resnet.layers": [
             "__future__",
             "torch"
         ],
         "modelscope.models.cv.video_depth_estimation.networks.layers.resnet.pose_decoder": [
+            "collections",
             "__future__",
-            "torch",
-            "collections"
+            "torch"
         ],
         "modelscope.models.cv.video_depth_estimation.networks.layers.resnet.resnet_encoder": [
-            "torchvision",
             "__future__",
-            "numpy",
-            "torch"
+            "torch",
+            "torchvision",
+            "numpy"
         ],
         "modelscope.models.cv.video_depth_estimation.networks.optim.extractor": [
-            "torchvision",
-            "torch"
+            "torch",
+            "torchvision"
         ],
         "modelscope.models.cv.video_depth_estimation.networks.optim.update": [
             "torch"
         ],
         "modelscope.models.cv.video_depth_estimation.utils.augmentations": [
-            "torchvision",
+            "random",
             "PIL",
             "cv2",
-            "random",
+            "torchvision",
             "numpy"
         ],
         "modelscope.models.cv.video_depth_estimation.utils.config": [
             "os",
-            "datetime",
             "torch",
-            "yacs"
+            "yacs",
+            "datetime"
         ],
         "modelscope.models.cv.video_depth_estimation.utils.depth": [
+            "torch",
             "torchvision",
-            "matplotlib",
             "numpy",
-            "torch"
+            "matplotlib"
         ],
         "modelscope.models.cv.video_depth_estimation.utils.horovod": [
             "horovod"
         ],
         "modelscope.models.cv.video_depth_estimation.utils.image": [
+            "functools",
+            "os",
             "torch",
             "PIL",
             "cv2",
-            "functools",
-            "numpy",
-            "os"
+            "numpy"
         ],
         "modelscope.models.cv.video_depth_estimation.utils.image_gt": [
-            "PIL",
             "functools",
             "torch",
-            "cv2"
+            "cv2",
+            "PIL"
         ],
         "modelscope.models.cv.video_depth_estimation.utils.load": [
-            "importlib",
-            "torch",
             "collections",
-            "warnings",
             "logging",
+            "warnings",
             "os",
+            "importlib",
+            "torch",
             "inspect"
         ],
         "modelscope.models.cv.video_depth_estimation.utils.misc": [
             "termcolor"
         ],
         "modelscope.models.cv.video_depth_estimation.utils.types": [
             "numpy",
             "torch",
             "yacs"
         ],
         "modelscope.models.cv.video_frame_interpolation.VFINet_arch": [
             "torch"
         ],
         "modelscope.models.cv.video_frame_interpolation.VFINet_for_video_frame_interpolation": [
-            "typing",
-            "torch",
+            "os",
             "copy",
-            "os"
+            "torch",
+            "typing"
         ],
         "modelscope.models.cv.video_frame_interpolation.flow_model.corr": [
             "torch"
         ],
         "modelscope.models.cv.video_frame_interpolation.flow_model.extractor": [
             "torch"
         ],
         "modelscope.models.cv.video_frame_interpolation.flow_model.raft": [
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.video_frame_interpolation.flow_model.update": [
             "torch"
         ],
         "modelscope.models.cv.video_frame_interpolation.interp_model.IFNet_swin": [
             "torch",
             "numpy",
@@ -14562,149 +14497,149 @@
         "modelscope.models.cv.video_frame_interpolation.interp_model.UNet": [
             "torch"
         ],
         "modelscope.models.cv.video_frame_interpolation.interp_model.flow_reversal": [
             "torch"
         ],
         "modelscope.models.cv.video_frame_interpolation.interp_model.refinenet_arch": [
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.video_frame_interpolation.interp_model.transformer_layers": [
-            "sys",
-            "math",
             "functools",
+            "torch",
             "timm",
-            "torch"
+            "sys",
+            "math"
         ],
         "modelscope.models.cv.video_frame_interpolation.utils.scene_change_detection": [
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.video_frame_interpolation.utils.utils": [
+            "torch",
             "scipy",
-            "numpy",
-            "torch"
+            "numpy"
         ],
         "modelscope.models.cv.video_human_matting.model": [
-            "numpy",
+            "os",
+            "typing",
             "torch",
             "torchvision",
-            "typing",
-            "os"
+            "numpy"
         ],
         "modelscope.models.cv.video_human_matting.models.decoder": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.models.cv.video_human_matting.models.deep_guided_filter": [
             "torch"
         ],
         "modelscope.models.cv.video_human_matting.models.effv2": [
             "torch"
         ],
         "modelscope.models.cv.video_human_matting.models.lraspp": [
             "torch"
         ],
         "modelscope.models.cv.video_human_matting.models.matting": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.models.cv.video_inpainting.inpainting": [
+            "time",
+            "torch",
             "PIL",
-            "torchvision",
             "os",
-            "time",
             "cv2",
-            "numpy",
-            "torch"
+            "torchvision",
+            "numpy"
         ],
         "modelscope.models.cv.video_inpainting.inpainting_model": [
             "torch",
             "torchvision",
-            "math",
-            "numpy"
+            "numpy",
+            "math"
         ],
         "modelscope.models.cv.video_instance_segmentation.head.kernel_frame_iter_head": [
-            "mmcv",
+            "mmdet",
             "torch",
-            "mmdet"
+            "mmcv"
         ],
         "modelscope.models.cv.video_instance_segmentation.head.kernel_head": [
-            "mmcv",
+            "mmdet",
             "torch",
-            "mmdet"
+            "mmcv"
         ],
         "modelscope.models.cv.video_instance_segmentation.head.kernel_iter_head": [
-            "torch",
-            "mmdet"
+            "mmdet",
+            "torch"
         ],
         "modelscope.models.cv.video_instance_segmentation.head.kernel_update_head": [
-            "mmcv",
-            "numpy",
+            "mmdet",
             "torch",
-            "mmdet"
+            "mmcv",
+            "numpy"
         ],
         "modelscope.models.cv.video_instance_segmentation.head.kernel_updator": [
-            "mmcv",
-            "torch"
+            "torch",
+            "mmcv"
         ],
         "modelscope.models.cv.video_instance_segmentation.neck.msdeformattn_decoder": [
-            "mmcv",
+            "mmdet",
             "torch",
-            "mmdet"
+            "mmcv"
         ],
         "modelscope.models.cv.video_instance_segmentation.track.kernel_update_head": [
-            "mmcv",
-            "numpy",
+            "mmdet",
             "torch",
-            "mmdet"
+            "mmcv",
+            "numpy"
         ],
         "modelscope.models.cv.video_instance_segmentation.track.mask_hungarian_assigner": [
-            "scipy",
-            "numpy",
+            "mmdet",
             "torch",
-            "mmdet"
+            "scipy",
+            "numpy"
         ],
         "modelscope.models.cv.video_instance_segmentation.utils": [
-            "numpy",
+            "mmdet",
             "torch",
-            "mmdet"
+            "numpy"
         ],
         "modelscope.models.cv.video_instance_segmentation.video_knet": [
-            "torch",
-            "mmdet"
+            "mmdet",
+            "torch"
         ],
         "modelscope.models.cv.video_multi_object_tracking.models.common": [
             "torch"
         ],
         "modelscope.models.cv.video_multi_object_tracking.models.decode": [
             "torch"
         ],
         "modelscope.models.cv.video_multi_object_tracking.models.model": [
             "torch"
         ],
         "modelscope.models.cv.video_multi_object_tracking.models.yolo": [
+            "copy",
             "torch",
-            "math",
-            "copy"
+            "math"
         ],
         "modelscope.models.cv.video_multi_object_tracking.tracker.basetrack": [
-            "numpy",
-            "collections"
+            "collections",
+            "numpy"
         ],
         "modelscope.models.cv.video_multi_object_tracking.tracker.matching": [
+            "lap",
             "scipy",
-            "numpy",
-            "lap"
+            "numpy"
         ],
         "modelscope.models.cv.video_multi_object_tracking.tracker.multitracker": [
-            "numpy",
+            "collections",
             "torch",
-            "collections"
+            "numpy"
         ],
         "modelscope.models.cv.video_multi_object_tracking.utils.image": [
             "cv2",
             "numpy"
         ],
         "modelscope.models.cv.video_multi_object_tracking.utils.kalman_filter": [
             "scipy",
@@ -14728,113 +14663,113 @@
         "modelscope.models.cv.video_object_segmentation.eval_network": [
             "torch"
         ],
         "modelscope.models.cv.video_object_segmentation.inference_core": [
             "torch"
         ],
         "modelscope.models.cv.video_object_segmentation.inference_memory_bank": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.cv.video_object_segmentation.mod_resnet": [
-            "math",
+            "collections",
             "torch",
-            "collections"
+            "math"
         ],
         "modelscope.models.cv.video_object_segmentation.model": [
+            "os",
             "torch",
-            "typing",
-            "os"
+            "typing"
         ],
         "modelscope.models.cv.video_object_segmentation.modules": [
-            "torchvision",
-            "torch"
+            "torch",
+            "torchvision"
         ],
         "modelscope.models.cv.video_object_segmentation.network": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.cv.video_panoptic_segmentation.backbone.swin_checkpoint": [
-            "torchvision",
-            "os",
-            "pkgutil",
+            "collections",
             "importlib",
             "torch",
-            "collections"
+            "pkgutil",
+            "os",
+            "torchvision"
         ],
         "modelscope.models.cv.video_panoptic_segmentation.backbone.swin_transformer": [
-            "torch",
-            "numpy",
+            "mmdet",
             "timm",
-            "mmdet"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.cv.video_panoptic_segmentation.head.kernel_head": [
-            "mmcv",
-            "torch"
+            "torch",
+            "mmcv"
         ],
         "modelscope.models.cv.video_panoptic_segmentation.head.kernel_iter_head": [
-            "torch",
-            "mmdet"
+            "mmdet",
+            "torch"
         ],
         "modelscope.models.cv.video_panoptic_segmentation.head.kernel_update_head": [
-            "mmcv",
-            "numpy",
+            "mmdet",
             "torch",
-            "mmdet"
+            "mmcv",
+            "numpy"
         ],
         "modelscope.models.cv.video_panoptic_segmentation.head.kernel_updator": [
-            "mmcv",
-            "torch"
+            "torch",
+            "mmcv"
         ],
         "modelscope.models.cv.video_panoptic_segmentation.head.mask": [
-            "__future__",
+            "torch",
             "pycocotools",
+            "__future__",
             "cv2",
-            "numpy",
-            "torch"
+            "numpy"
         ],
         "modelscope.models.cv.video_panoptic_segmentation.head.semantic_fpn_wrapper": [
-            "mmcv",
+            "mmdet",
             "torch",
-            "mmdet"
+            "mmcv"
         ],
         "modelscope.models.cv.video_panoptic_segmentation.head.track_heads": [
+            "torch",
             "mmcv",
-            "numpy",
-            "torch"
+            "numpy"
         ],
         "modelscope.models.cv.video_panoptic_segmentation.neck.fpn": [
-            "mmcv",
-            "torch"
+            "torch",
+            "mmcv"
         ],
         "modelscope.models.cv.video_panoptic_segmentation.track.quasi_dense_embed_tracker": [
-            "mmcv",
+            "mmdet",
             "torch",
-            "mmdet"
+            "mmcv"
         ],
         "modelscope.models.cv.video_panoptic_segmentation.video_k_net": [
-            "mmcv",
-            "numpy",
+            "mmdet",
             "torch",
-            "mmdet"
+            "mmcv",
+            "numpy"
         ],
         "modelscope.models.cv.video_panoptic_segmentation.visualizer": [
+            "hashlib",
             "cv2",
-            "numpy",
-            "hashlib"
+            "numpy"
         ],
         "modelscope.models.cv.video_single_object_tracking.config.ostrack": [
             "easydict"
         ],
         "modelscope.models.cv.video_single_object_tracking.models.layers.attn": [
             "torch"
         ],
         "modelscope.models.cv.video_single_object_tracking.models.layers.attn_blocks": [
-            "timm",
             "torch",
+            "timm",
             "math"
         ],
         "modelscope.models.cv.video_single_object_tracking.models.layers.head": [
             "torch"
         ],
         "modelscope.models.cv.video_single_object_tracking.models.layers.patch_embed": [
             "torch",
@@ -14847,150 +14782,150 @@
         "modelscope.models.cv.video_single_object_tracking.models.ostrack.ostrack": [
             "torch"
         ],
         "modelscope.models.cv.video_single_object_tracking.models.ostrack.utils": [
             "torch"
         ],
         "modelscope.models.cv.video_single_object_tracking.models.ostrack.vit_ce": [
-            "torch",
             "functools",
-            "timm"
+            "timm",
+            "torch"
         ],
         "modelscope.models.cv.video_single_object_tracking.models.procontext.procontext": [
             "torch"
         ],
         "modelscope.models.cv.video_single_object_tracking.models.procontext.utils": [
             "torch"
         ],
         "modelscope.models.cv.video_single_object_tracking.models.procontext.vit_ce": [
-            "torch",
             "functools",
-            "timm"
+            "timm",
+            "torch"
         ],
         "modelscope.models.cv.video_single_object_tracking.tracker.ostrack": [
             "torch"
         ],
         "modelscope.models.cv.video_single_object_tracking.tracker.procontext": [
-            "torch",
-            "copy"
+            "copy",
+            "torch"
         ],
         "modelscope.models.cv.video_single_object_tracking.utils.utils": [
-            "math",
+            "torch",
             "cv2",
-            "typing",
             "numpy",
-            "torch"
+            "math",
+            "typing"
         ],
         "modelscope.models.cv.video_stabilization.DUT.DUT_raft": [
-            "torch",
             "numpy",
+            "torch",
             "sys",
             "cv2"
         ],
         "modelscope.models.cv.video_stabilization.DUT.MotionPro": [
+            "os",
             "math",
             "torch",
             "cv2",
-            "numpy",
-            "os"
+            "numpy"
         ],
         "modelscope.models.cv.video_stabilization.DUT.RAFT.corr": [
             "alt_cuda_corr",
             "torch"
         ],
         "modelscope.models.cv.video_stabilization.DUT.RAFT.extractor": [
             "torch"
         ],
         "modelscope.models.cv.video_stabilization.DUT.RAFT.raft": [
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.video_stabilization.DUT.RAFT.update": [
             "torch"
         ],
         "modelscope.models.cv.video_stabilization.DUT.Smoother": [
-            "math",
+            "torch",
             "numpy",
-            "torch"
+            "math"
         ],
         "modelscope.models.cv.video_stabilization.DUT.config": [
             "__future__",
             "easydict"
         ],
         "modelscope.models.cv.video_stabilization.DUT.rf_det_module": [
             "torch"
         ],
         "modelscope.models.cv.video_stabilization.DUT.rf_det_so": [
             "torch"
         ],
         "modelscope.models.cv.video_stabilization.DUTRAFTStabilizer": [
-            "math",
             "numpy",
-            "sys",
+            "os",
+            "math",
+            "typing",
             "torch",
             "tempfile",
             "cv2",
-            "typing",
-            "os"
+            "sys"
         ],
         "modelscope.models.cv.video_stabilization.utils.IterativeSmooth": [
-            "torch",
             "os",
-            "math",
-            "numpy"
+            "torch",
+            "numpy",
+            "math"
         ],
         "modelscope.models.cv.video_stabilization.utils.MedianFilter": [
             "torch",
             "numpy",
-            "math",
-            "cv2"
+            "cv2",
+            "math"
         ],
         "modelscope.models.cv.video_stabilization.utils.ProjectionUtils": [
             "torch",
             "numpy",
-            "math",
-            "cv2"
+            "cv2",
+            "math"
         ],
         "modelscope.models.cv.video_stabilization.utils.RAFTUtils": [
+            "torch",
             "scipy",
-            "numpy",
-            "torch"
+            "numpy"
         ],
         "modelscope.models.cv.video_stabilization.utils.WarpUtils": [
-            "numpy",
+            "tqdm",
             "torch",
-            "tqdm"
+            "numpy"
         ],
         "modelscope.models.cv.video_stabilization.utils.image_utils": [
-            "torch",
-            "skimage"
+            "skimage",
+            "torch"
         ],
         "modelscope.models.cv.video_stabilization.utils.math_utils": [
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.video_streaming_perception.longshortnet.exp.longshortnet_base": [],
         "modelscope.models.cv.video_streaming_perception.longshortnet.longshortnet": [
-            "torch",
-            "json",
+            "logging",
+            "os",
+            "tqdm",
             "time",
             "argparse",
+            "torch",
             "cv2",
-            "logging",
             "numpy",
-            "os",
-            "tqdm"
+            "json"
         ],
         "modelscope.models.cv.video_streaming_perception.longshortnet.models.dfp_pafpn_long": [
-            "torch",
-            "collections"
+            "collections",
+            "torch"
         ],
         "modelscope.models.cv.video_streaming_perception.longshortnet.models.dfp_pafpn_short": [
-            "torch",
-            "collections"
+            "collections",
+            "torch"
         ],
         "modelscope.models.cv.video_streaming_perception.longshortnet.models.longshort": [
             "torch"
         ],
         "modelscope.models.cv.video_streaming_perception.longshortnet.models.longshort_backbone_neck": [
             "torch"
         ],
@@ -15002,64 +14937,64 @@
         "modelscope.models.cv.video_summarization.kts.cpd_auto": [
             "numpy"
         ],
         "modelscope.models.cv.video_summarization.kts.cpd_nonlin": [
             "numpy"
         ],
         "modelscope.models.cv.video_summarization.pgl_sum": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.cv.video_summarization.summarizer": [
-            "typing",
-            "numpy",
+            "os",
             "torch",
-            "os"
+            "numpy",
+            "typing"
         ],
         "modelscope.models.cv.video_super_resolution.basicvsr_net": [
             "torch"
         ],
         "modelscope.models.cv.video_super_resolution.common": [
             "torch"
         ],
         "modelscope.models.cv.video_super_resolution.msrresnet_lite_model": [
+            "os",
             "functools",
-            "typing",
             "torch",
-            "os"
+            "typing"
         ],
         "modelscope.models.cv.video_super_resolution.real_basicvsr_for_video_super_resolution": [
+            "os",
             "torch",
-            "typing",
-            "os"
+            "typing"
         ],
         "modelscope.models.cv.video_super_resolution.real_basicvsr_net": [
             "torch"
         ],
         "modelscope.models.cv.vidt.backbone": [
-            "math",
+            "torch",
             "os",
             "timm",
             "numpy",
-            "torch"
+            "math"
         ],
         "modelscope.models.cv.vidt.deformable_transformer": [
-            "math",
+            "torch",
+            "timm",
             "warnings",
             "copy",
-            "timm",
-            "torch"
+            "math"
         ],
         "modelscope.models.cv.vidt.fpn_fusion": [
             "torch"
         ],
         "modelscope.models.cv.vidt.head": [
-            "math",
+            "copy",
             "torch",
-            "copy"
+            "math"
         ],
         "modelscope.models.cv.vidt.model": [
             "os",
             "torch"
         ],
         "modelscope.models.cv.virual_tryon.sdafnet": [
             "random",
@@ -15070,1821 +15005,1784 @@
             "functools",
             "torch"
         ],
         "modelscope.models.cv.vision_efficient_tuning.head": [
             "torch"
         ],
         "modelscope.models.cv.vision_efficient_tuning.model": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.models.cv.vision_efficient_tuning.petl": [
+            "collections",
             "torch",
             "torchvision",
-            "math",
-            "collections"
+            "math"
         ],
         "modelscope.models.cv.vision_efficient_tuning.timm_helpers": [
-            "torch",
             "itertools",
+            "torch",
             "typing",
             "math"
         ],
         "modelscope.models.cv.vision_efficient_tuning.timm_vision_transformer": [
-            "math",
-            "itertools",
-            "torch",
             "collections",
+            "functools",
             "logging",
-            "functools"
+            "math",
+            "torch",
+            "itertools"
         ],
         "modelscope.models.cv.vision_efficient_tuning.timm_weight_init": [
-            "math",
             "warnings",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.cv.vision_efficient_tuning.vision_efficient_tuning": [
             "os",
             "torch",
             "collections"
         ],
         "modelscope.models.cv.vision_middleware.backbone": [
-            "math",
-            "numpy",
-            "torch",
             "collections",
+            "os",
             "typing",
-            "os"
+            "math",
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.vision_middleware.head": [
+            "torch",
             "mmcv",
-            "abc",
             "numpy",
-            "torch"
+            "abc"
         ],
         "modelscope.models.cv.vision_middleware.model": [
-            "typing",
-            "json",
+            "os",
             "torch",
-            "os"
+            "json",
+            "typing"
         ],
         "modelscope.models.cv.vision_middleware.vim": [
-            "einops",
+            "torch",
             "math",
-            "torch"
+            "einops"
         ],
         "modelscope.models.cv.vop_retrieval.backbone": [
-            "numpy",
-            "urllib",
-            "torch",
             "collections",
-            "warnings",
+            "urllib",
             "hashlib",
-            "typing",
             "os",
-            "tqdm"
+            "warnings",
+            "tqdm",
+            "typing",
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.vop_retrieval.basic_utils": [
-            "numpy",
-            "ujson",
-            "shutil",
-            "torch",
             "collections",
-            "PIL",
-            "torchvision",
             "zipfile",
+            "ujson",
+            "os",
             "cv2",
-            "random",
             "pickle",
-            "os"
+            "torch",
+            "PIL",
+            "shutil",
+            "random",
+            "torchvision",
+            "numpy"
         ],
         "modelscope.models.cv.vop_retrieval.model": [
             "os",
             "torch"
         ],
         "modelscope.models.cv.vop_retrieval.model_se": [
             "os",
             "torch"
         ],
         "modelscope.models.cv.vop_retrieval.tokenization_clip": [
-            "html",
-            "gzip",
-            "ftfy",
-            "regex",
             "functools",
+            "ftfy",
+            "torch",
+            "gzip",
             "os",
-            "torch"
+            "regex",
+            "html"
         ],
         "modelscope.models.multi_modal.clip.bert_tokenizer": [
-            "__future__",
-            "re",
+            "collections",
             "unicodedata",
+            "re",
             "os",
-            "six",
-            "collections"
+            "__future__",
+            "six"
         ],
         "modelscope.models.multi_modal.clip.configuration_bert": [
             "logging",
             "__future__"
         ],
         "modelscope.models.multi_modal.clip.model": [
-            "numpy",
-            "torch",
             "collections",
-            "json",
+            "os",
             "typing",
-            "os"
+            "torch",
+            "numpy",
+            "json"
         ],
         "modelscope.models.multi_modal.clip.modeling_bert": [
+            "logging",
+            "os",
             "__future__",
             "math",
-            "sys",
             "torch",
-            "logging",
-            "os",
-            "json",
-            "io"
+            "io",
+            "sys",
+            "json"
         ],
         "modelscope.models.multi_modal.clip_interrogator.model": [
-            "torch",
-            "requests",
-            "numpy",
-            "typing",
-            "os",
+            "hashlib",
+            "safetensors",
+            "open_clip",
             "tqdm",
             "math",
+            "time",
+            "torch",
+            "PIL",
             "dataclasses",
             "torchvision",
-            "PIL",
-            "open_clip",
-            "safetensors",
-            "time",
-            "hashlib",
-            "transformers"
+            "numpy",
+            "os",
+            "requests",
+            "transformers",
+            "typing"
         ],
         "modelscope.models.multi_modal.diffusion.diffusion": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.multi_modal.diffusion.model": [
-            "numpy",
-            "torch",
             "os",
             "typing",
+            "torch",
+            "numpy",
             "json"
         ],
         "modelscope.models.multi_modal.diffusion.structbert": [
-            "__future__",
+            "torch",
+            "six",
             "json",
-            "math",
+            "__future__",
             "copy",
-            "six",
             "numpy",
-            "torch"
+            "math"
         ],
         "modelscope.models.multi_modal.diffusion.tokenizer": [
+            "collections",
             "__future__",
-            "unicodedata",
             "six",
-            "collections"
+            "unicodedata"
         ],
         "modelscope.models.multi_modal.diffusion.unet_generator": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.multi_modal.diffusion.unet_upsampler_1024": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.multi_modal.diffusion.unet_upsampler_256": [
             "functools",
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.multi_modal.dpm_solver_pytorch": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.multi_modal.efficient_diffusion_tuning.efficient_stable_diffusion": [
-            "diffusers",
-            "torch",
             "functools",
+            "os",
             "transformers",
             "typing",
-            "os"
+            "torch",
+            "diffusers"
         ],
         "modelscope.models.multi_modal.gemm.gemm_base": [
-            "numpy",
-            "torch",
             "collections",
-            "json",
+            "os",
             "typing",
-            "os"
+            "torch",
+            "numpy",
+            "json"
         ],
         "modelscope.models.multi_modal.gemm.gemm_model": [
-            "numpy",
+            "os",
+            "typing",
             "torch",
             "PIL",
             "torchvision",
-            "json",
-            "typing",
-            "os"
+            "numpy",
+            "json"
         ],
         "modelscope.models.multi_modal.gemm.tokenizer": [
-            "html",
-            "gzip",
-            "ftfy",
-            "regex",
             "functools",
+            "ftfy",
+            "torch",
+            "gzip",
             "os",
-            "torch"
+            "regex",
+            "html"
         ],
         "modelscope.models.multi_modal.guided_diffusion.gaussian_diffusion": [
-            "torch",
-            "math",
+            "enum",
             "numpy",
-            "enum"
+            "torch",
+            "math"
         ],
         "modelscope.models.multi_modal.guided_diffusion.respace": [
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.multi_modal.guided_diffusion.script": [],
         "modelscope.models.multi_modal.guided_diffusion.unet": [
-            "abc",
-            "math",
+            "torch",
             "transformers",
             "numpy",
-            "torch"
+            "abc",
+            "math"
         ],
         "modelscope.models.multi_modal.mgeo.backbone": [
-            "math",
-            "dataclasses",
-            "torch",
+            "os",
             "warnings",
-            "typing",
-            "random",
             "transformers",
-            "os"
+            "dataclasses",
+            "math",
+            "typing",
+            "torch",
+            "random"
         ],
         "modelscope.models.multi_modal.mgeo.text_classification": [
             "torch"
         ],
         "modelscope.models.multi_modal.mgeo.text_ranking": [
             "torch"
         ],
         "modelscope.models.multi_modal.mgeo.token_classification": [
             "torch"
         ],
         "modelscope.models.multi_modal.mmr.dataloaders.rawvideo_util": [
             "torch",
-            "torchvision",
             "PIL",
             "cv2",
+            "torchvision",
             "numpy"
         ],
         "modelscope.models.multi_modal.mmr.models.clip_for_mm_video_embedding": [
-            "uuid",
-            "numpy",
-            "decord",
             "urllib",
+            "os",
+            "typing",
+            "decord",
             "torch",
+            "uuid",
             "PIL",
             "tempfile",
-            "typing",
             "random",
-            "os",
+            "numpy",
             "json"
         ],
         "modelscope.models.multi_modal.mmr.models.dynamic_inverted_softmax": [
             "numpy"
         ],
         "modelscope.models.multi_modal.mmr.models.modeling": [
-            "torch",
             "collections",
-            "platform",
+            "os",
+            "torch",
             "types",
-            "os"
+            "platform"
         ],
         "modelscope.models.multi_modal.mmr.models.module_clip": [
-            "urllib",
-            "torch",
             "collections",
-            "warnings",
+            "urllib",
             "hashlib",
-            "typing",
             "os",
-            "tqdm"
+            "warnings",
+            "tqdm",
+            "typing",
+            "torch"
         ],
         "modelscope.models.multi_modal.mmr.models.module_cross": [
-            "__future__",
-            "torch",
             "collections",
             "logging",
+            "__future__",
+            "torch",
             "json"
         ],
         "modelscope.models.multi_modal.mmr.models.tokenization_clip": [
-            "gzip",
-            "ftfy",
-            "regex",
             "functools",
+            "ftfy",
+            "gzip",
             "os",
+            "regex",
             "html"
         ],
         "modelscope.models.multi_modal.mmr.models.until_module": [
-            "torch",
             "logging",
+            "torch",
             "numpy",
             "math"
         ],
         "modelscope.models.multi_modal.mplug.clip.clip": [
-            "typing",
+            "collections",
             "torch",
-            "collections"
+            "typing"
         ],
         "modelscope.models.multi_modal.mplug.configuration_mplug": [
+            "os",
             "yaml",
             "transformers",
-            "typing",
-            "os"
+            "typing"
         ],
         "modelscope.models.multi_modal.mplug.modeling_mplug": [
-            "math",
-            "torch",
+            "os",
             "transformers",
+            "math",
             "typing",
-            "os"
+            "torch"
         ],
         "modelscope.models.multi_modal.mplug.mvit": [
-            "torch",
+            "collections",
             "functools",
+            "torch",
             "fairscale",
-            "numpy",
             "timm",
-            "collections"
+            "numpy"
         ],
         "modelscope.models.multi_modal.mplug.predictor": [
             "__future__",
             "torch"
         ],
         "modelscope.models.multi_modal.mplug_for_all_tasks": [
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.multi_modal.mplug_owl.configuration_mplug_owl": [
-            "transformers",
-            "typing",
             "os",
-            "copy"
+            "copy",
+            "transformers",
+            "typing"
         ],
         "modelscope.models.multi_modal.mplug_owl.modeling_mplug_owl": [
-            "typing",
-            "math",
-            "dataclasses",
-            "torch",
-            "copy",
             "logging",
-            "random",
-            "transformers",
             "os",
-            "io"
+            "transformers",
+            "copy",
+            "dataclasses",
+            "math",
+            "typing",
+            "torch",
+            "io",
+            "random"
         ],
         "modelscope.models.multi_modal.multi_stage_diffusion.clip": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.multi_modal.multi_stage_diffusion.decoder": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.multi_modal.multi_stage_diffusion.gaussian_diffusion": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.multi_modal.multi_stage_diffusion.model": [
+            "os",
             "math",
-            "numpy",
+            "typing",
             "torch",
             "PIL",
-            "typing",
-            "os",
+            "numpy",
             "json"
         ],
         "modelscope.models.multi_modal.multi_stage_diffusion.prior": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.multi_modal.multi_stage_diffusion.tokenizer": [
-            "html",
-            "gzip",
-            "ftfy",
-            "regex",
             "functools",
+            "ftfy",
+            "torch",
+            "gzip",
             "transformers",
-            "torch"
+            "regex",
+            "html"
         ],
         "modelscope.models.multi_modal.multi_stage_diffusion.upsampler": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.multi_modal.multi_stage_diffusion.xglm": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.multi_modal.ofa.configuration_mmspeech": [
             "transformers",
             "warnings"
         ],
         "modelscope.models.multi_modal.ofa.configuration_ofa": [
             "transformers",
             "warnings"
         ],
         "modelscope.models.multi_modal.ofa.generate.incremental_decoding_utils": [
+            "torch",
             "uuid",
-            "typing",
-            "torch"
+            "typing"
         ],
         "modelscope.models.multi_modal.ofa.generate.multihead_attention": [
-            "torch",
             "fairseq",
             "typing",
+            "torch",
             "math"
         ],
         "modelscope.models.multi_modal.ofa.generate.ngram_repeat_block": [
             "fairseq",
-            "math",
+            "torch",
             "warnings",
-            "typing",
-            "torch"
+            "math",
+            "typing"
         ],
         "modelscope.models.multi_modal.ofa.generate.search": [
+            "torch",
             "math",
-            "typing",
-            "torch"
+            "typing"
         ],
         "modelscope.models.multi_modal.ofa.generate.sequence_generator": [
             "torch",
             "sys",
-            "typing",
-            "math"
+            "math",
+            "typing"
         ],
         "modelscope.models.multi_modal.ofa.generate.token_generation_constraints": [
-            "typing",
+            "collections",
             "torch",
-            "collections"
+            "typing"
         ],
         "modelscope.models.multi_modal.ofa.generate.utils": [
-            "torch_xla",
+            "collections",
             "amp_C",
-            "itertools",
             "torch",
-            "collections"
+            "torch_xla",
+            "itertools"
         ],
         "modelscope.models.multi_modal.ofa.modeling_mmspeech": [
-            "math",
-            "numpy",
-            "dataclasses",
-            "torch",
-            "apex",
-            "fairseq",
             "transformers",
             "typing",
-            "packaging"
-        ],
-        "modelscope.models.multi_modal.ofa.modeling_ofa": [
             "math",
-            "dataclasses",
+            "fairseq",
             "torch",
             "apex",
+            "packaging",
+            "dataclasses",
+            "numpy"
+        ],
+        "modelscope.models.multi_modal.ofa.modeling_ofa": [
             "random",
             "transformers",
             "typing",
-            "packaging"
+            "math",
+            "torch",
+            "apex",
+            "packaging",
+            "dataclasses"
         ],
         "modelscope.models.multi_modal.ofa.resnet": [
             "torch"
         ],
         "modelscope.models.multi_modal.ofa.tokenization_ofa": [
+            "collections",
             "transformers",
-            "typing",
             "os",
-            "collections"
+            "typing"
         ],
         "modelscope.models.multi_modal.ofa.tokenization_ofa_fast": [
             "tokenizers",
             "transformers",
-            "typing",
-            "json"
+            "json",
+            "typing"
         ],
         "modelscope.models.multi_modal.ofa.utils.constant": [],
         "modelscope.models.multi_modal.ofa.utils.utils": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.models.multi_modal.ofa.vit": [
+            "collections",
             "fairseq",
-            "torch",
-            "collections"
+            "torch"
         ],
         "modelscope.models.multi_modal.ofa_for_all_tasks": [
-            "re",
-            "math",
-            "torch",
-            "json",
-            "string",
             "functools",
+            "string",
+            "os",
             "typing",
-            "os"
+            "math",
+            "torch",
+            "re",
+            "json"
         ],
         "modelscope.models.multi_modal.ofa_for_text_to_image_synthesis_model": [
             "taming",
-            "numpy",
+            "os",
+            "typing",
+            "pkg_resources",
             "torch",
             "PIL",
             "torchvision",
-            "json",
-            "pkg_resources",
-            "typing",
-            "os"
+            "numpy",
+            "json"
         ],
         "modelscope.models.multi_modal.rleg.model": [
             "os",
             "torch",
             "json"
         ],
         "modelscope.models.multi_modal.rleg.rleg": [
+            "torch",
             "torchvision",
-            "typing",
-            "torch"
+            "typing"
         ],
         "modelscope.models.multi_modal.soonet.blocks": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.multi_modal.soonet.clip": [
+            "collections",
+            "torch",
             "warnings",
-            "typing",
             "numpy",
-            "torch",
-            "collections"
+            "typing"
         ],
         "modelscope.models.multi_modal.soonet.model": [
             "os",
             "torch"
         ],
         "modelscope.models.multi_modal.soonet.swin_transformer": [
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.multi_modal.soonet.tokenizer": [
-            "html",
-            "gzip",
+            "functools",
             "ftfy",
+            "torch",
+            "gzip",
             "regex",
-            "functools",
-            "torch"
+            "html"
         ],
         "modelscope.models.multi_modal.soonet.utils": [
+            "tqdm",
             "decord",
-            "numpy",
             "copy",
-            "tqdm"
+            "numpy"
         ],
         "modelscope.models.multi_modal.stable_diffusion.stable_diffusion": [
-            "diffusers",
-            "torch",
             "functools",
+            "os",
             "transformers",
             "typing",
-            "os",
+            "torch",
+            "diffusers",
             "packaging"
         ],
         "modelscope.models.multi_modal.team.team_model": [
             "tokenizers",
+            "typing",
             "torch",
-            "torchvision",
             "PIL",
             "cv2",
-            "typing",
+            "torchvision",
             "numpy"
         ],
         "modelscope.models.multi_modal.team.utils": [
-            "typing",
+            "collections",
+            "torch",
             "transformers",
             "numpy",
-            "torch",
-            "collections"
+            "typing"
         ],
         "modelscope.models.multi_modal.video_synthesis.autoencoder": [
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.multi_modal.video_synthesis.diffusion": [
             "torch"
         ],
         "modelscope.models.multi_modal.video_synthesis.text_to_video_synthesis_model": [
-            "torch",
-            "open_clip",
             "einops",
+            "os",
+            "open_clip",
             "typing",
-            "os"
+            "torch"
         ],
         "modelscope.models.multi_modal.video_synthesis.unet_sd": [
-            "einops",
+            "torch",
             "math",
-            "torch"
+            "einops"
         ],
         "modelscope.models.multi_modal.vldoc.conv_fpn_trans": [
-            "torch",
-            "timm",
             "collections",
+            "timm",
+            "torch",
             "apex",
             "random"
         ],
         "modelscope.models.multi_modal.vldoc.convnext": [
-            "torch",
             "os",
+            "torch",
             "timm"
         ],
         "modelscope.models.multi_modal.vldoc.model": [
-            "re",
+            "logging",
+            "os",
+            "copy",
             "math",
-            "sys",
             "torch",
+            "re",
             "torchvision",
-            "copy",
-            "logging",
-            "os",
+            "sys",
             "json"
         ],
         "modelscope.models.multi_modal.vldoc.modeling_layout_roberta": [
-            "math",
+            "packaging",
+            "torch",
             "transformers",
             "os",
-            "torch",
-            "packaging"
+            "math"
         ],
         "modelscope.models.multi_modal.vldoc.processing": [
-            "torch",
-            "timm",
             "collections",
+            "timm",
+            "typing",
+            "torch",
             "PIL",
-            "torchvision",
             "cv2",
-            "typing",
+            "torchvision",
             "numpy"
         ],
         "modelscope.models.multi_modal.vldoc.tokenization": [
-            "transformers",
-            "os"
+            "os",
+            "transformers"
         ],
         "modelscope.models.multi_modal.vldoc.transformer_local": [
-            "torch",
-            "copy"
+            "copy",
+            "torch"
         ],
         "modelscope.models.nlp.T5.backbone": [
-            "math",
-            "torch",
+            "os",
             "warnings",
-            "copy",
             "transformers",
+            "copy",
+            "math",
             "typing",
-            "os"
+            "torch"
         ],
         "modelscope.models.nlp.T5.configuration": [
             "transformers",
             "typing"
         ],
         "modelscope.models.nlp.T5.text2text_generation": [
-            "torch",
+            "transformers",
             "warnings",
             "copy",
-            "transformers",
-            "typing"
+            "typing",
+            "torch"
         ],
         "modelscope.models.nlp.bart.text_error_correction": [
+            "os",
             "torch",
-            "typing",
-            "os"
+            "typing"
         ],
         "modelscope.models.nlp.bert.backbone": [
-            "torch",
             "transformers",
-            "math",
-            "packaging"
+            "packaging",
+            "torch",
+            "math"
         ],
         "modelscope.models.nlp.bert.configuration": [
+            "collections",
             "transformers",
-            "typing",
-            "collections"
+            "typing"
         ],
         "modelscope.models.nlp.bert.document_segmentation": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.models.nlp.bert.fill_mask": [],
         "modelscope.models.nlp.bert.sentence_embedding": [
             "torch"
         ],
         "modelscope.models.nlp.bert.siamese_uie": [
-            "torch",
-            "copy"
+            "copy",
+            "torch"
         ],
         "modelscope.models.nlp.bert.text_classification": [],
         "modelscope.models.nlp.bert.text_ranking": [],
         "modelscope.models.nlp.bert.token_classification": [],
         "modelscope.models.nlp.bert.word_alignment": [
             "torch"
         ],
         "modelscope.models.nlp.bloom.backbone": [
             "transformers"
         ],
         "modelscope.models.nlp.canmt.canmt_model": [
             "fairseq",
-            "math",
-            "typing",
+            "torch",
             "numpy",
-            "torch"
+            "math",
+            "typing"
         ],
         "modelscope.models.nlp.canmt.canmt_translation": [
+            "os",
             "math",
-            "numpy",
-            "torch",
             "typing",
-            "os"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.nlp.canmt.sequence_generator": [
             "fairseq",
+            "numpy",
+            "torch",
             "sys",
             "math",
-            "typing",
-            "numpy",
-            "torch"
+            "typing"
         ],
         "modelscope.models.nlp.chatglm.configuration": [
             "transformers"
         ],
         "modelscope.models.nlp.chatglm.quantization": [
-            "torch",
-            "cpm_kernels",
-            "ctypes",
             "bz2",
+            "ctypes",
             "typing",
-            "base64"
+            "base64",
+            "torch",
+            "cpm_kernels"
         ],
         "modelscope.models.nlp.chatglm.text_generation": [
-            "re",
-            "math",
-            "torch",
-            "sys",
+            "os",
             "warnings",
-            "copy",
             "transformers",
+            "copy",
+            "math",
             "typing",
-            "os"
+            "torch",
+            "re",
+            "sys"
         ],
         "modelscope.models.nlp.chatglm.tokenization": [
-            "numpy",
-            "sentencepiece",
+            "os",
             "transformers",
+            "sentencepiece",
             "typing",
-            "os"
+            "numpy"
         ],
         "modelscope.models.nlp.chatglm2.configuration": [
             "transformers"
         ],
         "modelscope.models.nlp.chatglm2.quantization": [
-            "torch",
-            "cpm_kernels",
-            "ctypes",
             "bz2",
+            "ctypes",
             "typing",
-            "base64"
+            "base64",
+            "torch",
+            "cpm_kernels"
         ],
         "modelscope.models.nlp.chatglm2.text_generation": [
-            "math",
-            "torch",
-            "sys",
+            "transformers",
             "warnings",
             "copy",
-            "transformers",
-            "typing"
+            "typing",
+            "math",
+            "torch",
+            "sys"
         ],
         "modelscope.models.nlp.chatglm2.tokenization": [
-            "typing",
-            "transformers",
             "os",
-            "sentencepiece"
+            "sentencepiece",
+            "transformers",
+            "typing"
         ],
         "modelscope.models.nlp.codegeex.codegeex": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.nlp.codegeex.codegeex_for_code_generation": [
-            "typing",
+            "copy",
             "torch",
-            "copy"
+            "typing"
         ],
         "modelscope.models.nlp.codegeex.codegeex_for_code_translation": [
-            "typing",
+            "copy",
             "torch",
-            "copy"
+            "typing"
         ],
         "modelscope.models.nlp.codegeex.inference": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.models.nlp.codegeex.tokenizer": [
             "transformers",
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.models.nlp.csanmt.translation": [
+            "collections",
             "tensorflow",
-            "typing",
             "math",
-            "collections"
+            "typing"
         ],
         "modelscope.models.nlp.deberta_v2.backbone": [
-            "transformers",
-            "typing",
+            "collections",
             "torch",
-            "collections"
+            "transformers",
+            "typing"
         ],
         "modelscope.models.nlp.deberta_v2.configuration": [
             "transformers"
         ],
         "modelscope.models.nlp.deberta_v2.fill_mask": [
             "transformers",
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.models.nlp.deberta_v2.tokenization": [
-            "sentencepiece",
             "transformers",
-            "typing",
+            "unicodedata",
             "os",
-            "unicodedata"
+            "sentencepiece",
+            "typing"
         ],
         "modelscope.models.nlp.deberta_v2.tokenization_fast": [
-            "typing",
-            "transformers",
+            "os",
             "shutil",
-            "os"
+            "transformers",
+            "typing"
         ],
         "modelscope.models.nlp.dgds.backbone": [
-            "__future__",
-            "transformers",
             "os",
-            "torch"
+            "__future__",
+            "torch",
+            "transformers"
         ],
         "modelscope.models.nlp.dgds.document_grounded_dialog_generate": [
-            "typing",
+            "os",
             "torch",
-            "os"
+            "typing"
         ],
         "modelscope.models.nlp.dgds.document_grounded_dialog_rerank": [
-            "typing",
+            "os",
             "torch",
-            "os"
+            "typing"
         ],
         "modelscope.models.nlp.dgds.document_grounded_dialog_retrieval": [
-            "typing",
+            "os",
             "torch",
-            "os"
+            "typing"
         ],
         "modelscope.models.nlp.fid_T5.text_generation": [
-            "transformers",
             "os",
             "torch",
+            "transformers",
             "io"
         ],
         "modelscope.models.nlp.fid_plug.backbone": [
-            "math",
-            "dataclasses",
-            "torch",
+            "os",
+            "transformers",
             "copy",
             "typing",
-            "transformers",
-            "numpy",
-            "os"
+            "math",
+            "torch",
+            "dataclasses",
+            "numpy"
         ],
         "modelscope.models.nlp.fid_plug.configuration": [
             "transformers"
         ],
         "modelscope.models.nlp.fid_plug.text_generation": [
-            "transformers",
             "os",
             "torch",
+            "transformers",
             "io"
         ],
         "modelscope.models.nlp.glm_130b.generation.strategies": [
-            "numpy",
+            "SwissArmyTransformer",
             "torch",
-            "SwissArmyTransformer"
+            "numpy"
         ],
         "modelscope.models.nlp.glm_130b.initialize": [
-            "torch",
-            "SwissArmyTransformer",
+            "argparse",
             "time",
-            "argparse"
+            "SwissArmyTransformer",
+            "torch"
         ],
         "modelscope.models.nlp.glm_130b.quantization.functional": [
             "torch"
         ],
         "modelscope.models.nlp.glm_130b.quantization.layers": [
-            "torch",
-            "SwissArmyTransformer"
+            "SwissArmyTransformer",
+            "torch"
         ],
         "modelscope.models.nlp.glm_130b.text_generation": [
-            "re",
-            "sys",
-            "torch",
-            "SwissArmyTransformer",
             "functools",
+            "os",
             "copy",
-            "time",
             "typing",
+            "time",
+            "torch",
+            "stat",
+            "re",
+            "SwissArmyTransformer",
             "random",
-            "os",
-            "stat"
+            "sys"
         ],
         "modelscope.models.nlp.gpt2.backbone": [
             "transformers"
         ],
         "modelscope.models.nlp.gpt3.backbone": [
-            "math",
-            "torch",
             "addict",
+            "os",
             "transformers",
+            "math",
             "typing",
-            "os"
+            "torch"
         ],
         "modelscope.models.nlp.gpt3.configuration": [
             "transformers",
             "torch"
         ],
         "modelscope.models.nlp.gpt3.distributed_gpt3": [
-            "math",
-            "megatron_util",
-            "torch",
             "collections",
+            "megatron_util",
+            "os",
             "transformers",
             "typing",
-            "os"
+            "math",
+            "torch"
         ],
         "modelscope.models.nlp.gpt3.text_generation": [
-            "transformers",
-            "typing",
+            "collections",
             "torch",
-            "collections"
+            "transformers",
+            "typing"
         ],
         "modelscope.models.nlp.gpt3.tokenizer": [
             "tokenizers",
             "typing"
         ],
         "modelscope.models.nlp.gpt_moe.backbone": [
-            "math",
-            "torch",
             "addict",
+            "os",
             "transformers",
+            "math",
             "typing",
-            "os"
+            "torch"
         ],
         "modelscope.models.nlp.gpt_moe.checkpointing": [
             "os",
             "torch",
             "megatron_util"
         ],
         "modelscope.models.nlp.gpt_moe.configuration": [
             "transformers",
             "torch"
         ],
         "modelscope.models.nlp.gpt_moe.distributed_gpt_moe": [
+            "megatron_util",
             "torch",
             "transformers",
-            "math",
-            "megatron_util"
+            "math"
         ],
         "modelscope.models.nlp.gpt_moe.moe.experts": [
-            "torch",
-            "copy"
+            "copy",
+            "torch"
         ],
         "modelscope.models.nlp.gpt_moe.moe.layer": [
-            "typing",
+            "megatron_util",
             "torch",
-            "megatron_util"
+            "typing"
         ],
         "modelscope.models.nlp.gpt_moe.moe.mappings": [
-            "torch",
-            "megatron_util"
+            "megatron_util",
+            "torch"
         ],
         "modelscope.models.nlp.gpt_moe.moe.sharded_moe": [
-            "math",
             "megatron_util",
-            "torch",
-            "tutel",
-            "apex",
             "scipy",
-            "typing"
+            "typing",
+            "math",
+            "tutel",
+            "torch",
+            "apex"
         ],
         "modelscope.models.nlp.gpt_moe.moe.utils": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.models.nlp.gpt_moe.text_generation": [
             "transformers",
             "typing"
         ],
         "modelscope.models.nlp.gpt_moe.tokenizer": [
             "tokenizers"
         ],
         "modelscope.models.nlp.gpt_neo.backbone": [
             "transformers"
         ],
         "modelscope.models.nlp.heads.crf_head": [
             "transformers",
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.models.nlp.heads.fill_mask_head": [
             "transformers",
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.models.nlp.heads.infromation_extraction_head": [
             "torch"
         ],
         "modelscope.models.nlp.heads.text_classification_head": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.models.nlp.heads.text_generation_head": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.models.nlp.heads.text_ranking_head": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.models.nlp.heads.token_classification_head": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.models.nlp.heads.torch_pretrain_head": [
             "transformers",
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.models.nlp.hf_transformers.backbone": [
             "transformers"
         ],
         "modelscope.models.nlp.llama.backbone": [
-            "torch",
             "transformers",
-            "typing",
-            "math"
+            "torch",
+            "math",
+            "typing"
         ],
         "modelscope.models.nlp.llama.configuration": [
             "transformers"
         ],
         "modelscope.models.nlp.llama.convert_llama_weights_to_hf": [
+            "os",
             "math",
+            "argparse",
             "torch",
             "gc",
-            "json",
-            "argparse",
             "shutil",
-            "os"
+            "json"
         ],
         "modelscope.models.nlp.llama.text_generation": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.models.nlp.llama.tokenization": [
+            "os",
+            "transformers",
             "sentencepiece",
             "typing",
-            "transformers",
-            "shutil",
-            "os"
+            "shutil"
         ],
         "modelscope.models.nlp.llama.tokenization_fast": [
-            "typing",
-            "transformers",
+            "os",
             "shutil",
-            "os"
+            "transformers",
+            "typing"
         ],
         "modelscope.models.nlp.llama2.backbone": [
-            "torch",
             "transformers",
-            "typing",
-            "math"
+            "torch",
+            "math",
+            "typing"
         ],
         "modelscope.models.nlp.llama2.configuration": [
             "transformers"
         ],
         "modelscope.models.nlp.llama2.text_generation": [
             "transformers",
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.models.nlp.llama2.tokenization": [
+            "os",
+            "transformers",
             "sentencepiece",
             "typing",
-            "transformers",
-            "shutil",
-            "os"
+            "shutil"
         ],
         "modelscope.models.nlp.llama2.tokenization_fast": [
             "tokenizers",
-            "shutil",
             "transformers",
+            "os",
             "typing",
-            "os"
+            "shutil"
         ],
         "modelscope.models.nlp.lstm.backbone": [
             "torch"
         ],
         "modelscope.models.nlp.lstm.token_classification": [],
         "modelscope.models.nlp.megatron_bert.backbone": [
-            "torch",
             "transformers",
+            "torch",
             "math"
         ],
         "modelscope.models.nlp.megatron_bert.configuration": [
+            "collections",
             "transformers",
-            "typing",
-            "collections"
+            "typing"
         ],
         "modelscope.models.nlp.megatron_bert.fill_mask": [
             "transformers",
             "torch"
         ],
         "modelscope.models.nlp.mglm.arguments": [
             "deepspeed",
-            "torch",
-            "argparse",
             "os",
+            "argparse",
+            "torch",
             "json"
         ],
         "modelscope.models.nlp.mglm.blocklm_utils": [
-            "scipy",
-            "math",
             "megatron_util",
-            "copy",
+            "torch",
             "random",
+            "copy",
+            "scipy",
             "numpy",
-            "torch"
+            "math"
         ],
         "modelscope.models.nlp.mglm.configure_data": [
             "megatron_util",
-            "numpy",
-            "itertools",
-            "torch",
+            "os",
             "copy",
+            "torch",
             "bisect",
+            "itertools",
             "random",
-            "os"
+            "numpy"
         ],
         "modelscope.models.nlp.mglm.data_utils.corpora": [
-            "multiprocessing",
-            "torch",
             "collections",
-            "random",
             "os",
-            "json",
+            "tqdm",
+            "torch",
             "queue",
-            "tqdm"
+            "random",
+            "multiprocessing",
+            "json"
         ],
         "modelscope.models.nlp.mglm.data_utils.datasets": [
-            "itertools",
-            "torch",
-            "numpy",
-            "os",
-            "tqdm",
-            "csv",
             "operator",
-            "math",
+            "tqdm",
             "nltk",
-            "pandas",
+            "math",
             "time",
+            "torch",
             "bisect",
+            "itertools",
+            "pandas",
+            "numpy",
+            "csv",
+            "os",
             "random",
             "json"
         ],
         "modelscope.models.nlp.mglm.data_utils.extraction": [
-            "glob",
             "os",
-            "json",
-            "nltk"
+            "glob",
+            "nltk",
+            "json"
         ],
         "modelscope.models.nlp.mglm.data_utils.file_utils": [
-            "urllib",
-            "pathlib",
-            "requests",
-            "tempfile",
-            "botocore",
             "functools",
-            "os",
-            "boto3",
-            "tqdm",
-            "__future__",
-            "sys",
             "hashlib",
             "logging",
+            "tqdm",
+            "boto3",
+            "io",
+            "botocore",
+            "tempfile",
             "shutil",
-            "json",
-            "io"
+            "urllib",
+            "os",
+            "requests",
+            "__future__",
+            "pathlib",
+            "sys",
+            "json"
         ],
         "modelscope.models.nlp.mglm.data_utils.lazy_loader": [
+            "time",
+            "pickle",
+            "torch",
             "mmap",
             "os",
-            "time",
             "itertools",
-            "pickle",
-            "numpy",
-            "torch"
+            "numpy"
         ],
         "modelscope.models.nlp.mglm.data_utils.samplers": [
-            "sys",
-            "math",
-            "os",
             "numpy",
-            "torch"
+            "torch",
+            "os",
+            "sys",
+            "math"
         ],
         "modelscope.models.nlp.mglm.data_utils.sp_tokenizer": [
             "os"
         ],
         "modelscope.models.nlp.mglm.data_utils.tokenization": [
+            "collections",
             "csv",
-            "nltk",
+            "os",
             "sentencepiece",
-            "itertools",
+            "nltk",
+            "regex",
             "torch",
-            "collections",
-            "random",
-            "os",
-            "regex"
+            "itertools",
+            "random"
         ],
         "modelscope.models.nlp.mglm.data_utils.tokenization_gpt2": [
-            "__future__",
-            "sys",
-            "json",
-            "logging",
             "functools",
+            "logging",
             "os",
+            "__future__",
             "regex",
-            "io"
+            "io",
+            "sys",
+            "json"
         ],
         "modelscope.models.nlp.mglm.data_utils.wordpiece": [
-            "__future__",
             "collections",
             "logging",
             "os",
-            "io",
-            "unicodedata"
+            "__future__",
+            "unicodedata",
+            "io"
         ],
         "modelscope.models.nlp.mglm.generation_utils": [
-            "abc",
-            "typing",
+            "collections",
             "torch",
-            "collections"
+            "abc",
+            "typing"
         ],
         "modelscope.models.nlp.mglm.mglm_for_text_summarization": [
             "megatron_util",
-            "numpy",
+            "os",
+            "typing",
             "torch",
             "random",
-            "typing",
-            "os"
+            "numpy"
         ],
         "modelscope.models.nlp.mglm.model.distributed": [
-            "torch",
-            "megatron_util"
+            "megatron_util",
+            "torch"
         ],
         "modelscope.models.nlp.mglm.model.downstream": [
             "torch"
         ],
         "modelscope.models.nlp.mglm.model.modeling_bert": [
-            "__future__",
-            "math",
             "megatron_util",
-            "data_utils",
             "tarfile",
+            "logging",
+            "os",
+            "__future__",
+            "copy",
+            "tempfile",
+            "math",
             "torch",
             "apex",
-            "tempfile",
-            "json",
-            "copy",
-            "logging",
+            "data_utils",
             "shutil",
-            "os"
+            "json"
         ],
         "modelscope.models.nlp.mglm.model.modeling_glm": [
-            "torch",
-            "megatron_util"
+            "megatron_util",
+            "torch"
         ],
         "modelscope.models.nlp.mglm.model.prompt": [
             "random",
             "torch"
         ],
         "modelscope.models.nlp.mglm.model.transformer": [
-            "apex",
-            "math",
             "megatron_util",
+            "torch",
+            "apex",
             "deepspeed",
-            "torch"
+            "math"
         ],
         "modelscope.models.nlp.mglm.process_grid": [
+            "os",
             "glob",
-            "json",
             "statistics",
-            "os",
-            "sys"
+            "sys",
+            "json"
         ],
         "modelscope.models.nlp.mglm.run_test": [
-            "sys",
-            "test"
+            "test",
+            "sys"
         ],
         "modelscope.models.nlp.mglm.tasks.data_utils": [
-            "re",
             "megatron_util",
-            "torch",
             "copy",
             "typing",
             "pickle",
+            "torch",
+            "re",
             "numpy",
             "json"
         ],
         "modelscope.models.nlp.mglm.tasks.eval_utils": [
+            "collections",
             "megatron_util",
-            "sklearn",
-            "torch",
             "tasks",
-            "collections",
-            "utils",
-            "time",
-            "typing",
-            "random",
             "datetime",
             "os",
+            "sklearn",
+            "typing",
+            "time",
+            "utils",
+            "torch",
+            "random",
             "finetune_glm"
         ],
         "modelscope.models.nlp.mglm.tasks.language_model.dataset": [
-            "math",
-            "itertools",
-            "torch",
             "tasks",
+            "math",
             "utils",
+            "torch",
             "bisect",
+            "itertools",
             "numpy",
             "json"
         ],
         "modelscope.models.nlp.mglm.tasks.language_model.detokenizer": [
             "re"
         ],
         "modelscope.models.nlp.mglm.tasks.language_model.finetune": [
-            "math",
             "megatron_util",
             "functools",
-            "pretrain_glm",
-            "torch",
             "tasks",
-            "finetune_glm"
+            "torch",
+            "pretrain_glm",
+            "finetune_glm",
+            "math"
         ],
         "modelscope.models.nlp.mglm.tasks.seq2seq.dataset": [
-            "numpy",
-            "data_utils",
-            "torch",
             "tasks",
+            "os",
+            "tqdm",
             "utils",
+            "torch",
+            "data_utils",
             "random",
-            "os",
-            "json",
-            "tqdm"
+            "numpy",
+            "json"
         ],
         "modelscope.models.nlp.mglm.tasks.seq2seq.evaluate": [
             "megatron_util",
-            "rouge_score",
+            "torch",
             "string",
-            "random",
+            "rouge_score",
             "datetime",
-            "torch",
+            "random",
             "generation_utils"
         ],
         "modelscope.models.nlp.mglm.tasks.seq2seq.finetune": [
-            "finetune_glm",
-            "megatron_util",
+            "collections",
             "functools",
-            "pretrain_glm",
-            "torch",
+            "megatron_util",
             "tasks",
-            "collections"
+            "torch",
+            "pretrain_glm",
+            "finetune_glm"
         ],
         "modelscope.models.nlp.mglm.tasks.superglue.dataset": [
-            "abc",
-            "re",
-            "torch",
-            "utils",
             "collections",
-            "copy",
+            "tqdm",
+            "utils",
+            "torch",
+            "data_utils",
+            "re",
+            "pandas",
             "numpy",
-            "typing",
+            "csv",
             "os",
-            "tqdm",
+            "copy",
+            "abc",
+            "typing",
             "glob",
-            "csv",
-            "pandas",
-            "data_utils",
             "random",
             "json"
         ],
         "modelscope.models.nlp.mglm.tasks.superglue.evaluate": [
-            "__future__",
-            "re",
-            "string",
+            "collections",
             "functools",
-            "typing",
             "tasks",
-            "collections"
+            "string",
+            "re",
+            "__future__",
+            "typing"
         ],
         "modelscope.models.nlp.mglm.tasks.superglue.finetune": [
             "collections",
             "tasks",
             "finetune_glm"
         ],
         "modelscope.models.nlp.mglm.tasks.superglue.pvp": [
-            "abc",
-            "random",
-            "math",
-            "tasks",
             "collections",
-            "utils",
+            "tasks",
+            "string",
             "copy",
+            "abc",
+            "math",
             "typing",
-            "string",
+            "utils",
+            "random",
             "numpy"
         ],
         "modelscope.models.nlp.mglm.test.test_block": [
-            "blocklm_utils",
+            "argparse",
             "random",
             "numpy",
-            "argparse"
+            "blocklm_utils"
         ],
         "modelscope.models.nlp.mglm.test.test_rel_shift": [
+            "learning_rates",
             "torch",
             "numpy",
-            "matplotlib",
-            "learning_rates"
+            "matplotlib"
         ],
         "modelscope.models.nlp.mglm.train_utils": [
-            "apex",
-            "deepspeed",
+            "megatron_util",
             "torch",
-            "megatron_util"
+            "apex",
+            "deepspeed"
         ],
         "modelscope.models.nlp.mglm.utils": [
             "megatron_util",
-            "numpy",
-            "torch",
+            "os",
             "time",
+            "torch",
             "random",
-            "os",
-            "json",
-            "subprocess"
+            "subprocess",
+            "numpy",
+            "json"
         ],
         "modelscope.models.nlp.palm_v2.configuration": [
             "transformers"
         ],
         "modelscope.models.nlp.palm_v2.dureader_eval": [
-            "re",
-            "math",
-            "sys",
             "collections",
+            "numpy",
             "zipfile",
             "copy",
-            "argparse",
             "rouge",
-            "numpy",
+            "math",
+            "argparse",
+            "re",
+            "sys",
             "json"
         ],
         "modelscope.models.nlp.palm_v2.text_generation": [
-            "math",
-            "numpy",
-            "dataclasses",
-            "torch",
+            "os",
+            "transformers",
             "copy",
+            "math",
             "typing",
-            "transformers",
-            "os",
-            "json",
+            "codecs",
+            "torch",
+            "dataclasses",
             "subprocess",
-            "codecs"
+            "numpy",
+            "json"
         ],
         "modelscope.models.nlp.peer.backbone": [
+            "transformers",
             "math",
-            "dataclasses",
+            "typing",
             "torch",
-            "transformers",
-            "typing"
+            "dataclasses"
         ],
         "modelscope.models.nlp.peer.configuration": [
             "transformers"
         ],
         "modelscope.models.nlp.peer.sas_utils": [
+            "nltk",
             "random",
             "numpy",
-            "torch",
-            "nltk"
+            "torch"
         ],
         "modelscope.models.nlp.peer.text_classification": [
-            "torch",
-            "copy"
+            "copy",
+            "torch"
         ],
         "modelscope.models.nlp.plug.AnnealingLR": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.nlp.plug.backbone": [
+            "megatron_util",
+            "logging",
             "__future__",
             "math",
-            "megatron_util",
-            "torch",
-            "logging"
+            "torch"
         ],
         "modelscope.models.nlp.plug.configuration": [
             "transformers",
-            "json",
-            "copy"
+            "copy",
+            "json"
         ],
         "modelscope.models.nlp.plug.distributed_plug": [
-            "typing",
+            "megatron_util",
             "torch",
-            "megatron_util"
+            "typing"
         ],
         "modelscope.models.nlp.plug.generator": [
             "torch"
         ],
         "modelscope.models.nlp.plug_mental.adv_utils": [
             "torch"
         ],
         "modelscope.models.nlp.plug_mental.backbone": [
-            "math",
-            "dataclasses",
-            "torch",
             "transformers",
+            "math",
             "typing",
-            "packaging"
+            "torch",
+            "packaging",
+            "dataclasses"
         ],
         "modelscope.models.nlp.plug_mental.configuration": [
             "transformers"
         ],
         "modelscope.models.nlp.plug_mental.text_classification": [
             "torch"
         ],
         "modelscope.models.nlp.polylm.text_generation": [
-            "transformers",
-            "typing",
+            "collections",
             "torch",
-            "collections"
+            "transformers",
+            "typing"
         ],
         "modelscope.models.nlp.ponet.backbone": [
             "distutils",
+            "transformers",
             "math",
             "torch",
-            "transformers",
             "packaging"
         ],
         "modelscope.models.nlp.ponet.configuration": [
             "transformers"
         ],
         "modelscope.models.nlp.ponet.document_segmentation": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.models.nlp.ponet.fill_mask": [
             "transformers",
             "torch"
         ],
         "modelscope.models.nlp.ponet.tokenization": [
             "transformers",
             "typing"
         ],
-        "modelscope.models.nlp.qwen.backbone": [
-            "math",
-            "importlib",
-            "torch",
-            "typing",
-            "einops",
-            "transformers",
-            "flash_attn"
-        ],
-        "modelscope.models.nlp.qwen.configuration": [
-            "transformers",
-            "typing"
-        ],
-        "modelscope.models.nlp.qwen.qwen_generation_utils": [
-            "typing",
-            "transformers",
-            "numpy",
-            "torch"
-        ],
-        "modelscope.models.nlp.qwen.text_generation": [
-            "typing",
-            "transformers",
-            "warnings",
-            "torch"
-        ],
-        "modelscope.models.nlp.qwen.tokenization": [
-            "typing",
-            "__future__",
-            "tiktoken",
-            "io",
-            "logging",
-            "transformers",
-            "os",
-            "json",
-            "base64",
-            "unicodedata"
-        ],
         "modelscope.models.nlp.space.configuration": [],
         "modelscope.models.nlp.space.dialog_intent_prediction": [
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.nlp.space.dialog_modeling": [
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.nlp.space.dialog_state_tracking": [
             "transformers",
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.models.nlp.space.model.gen_unified_transformer": [
             "torch"
         ],
         "modelscope.models.nlp.space.model.generator": [
-            "math",
+            "torch",
             "numpy",
-            "torch"
+            "math"
         ],
         "modelscope.models.nlp.space.model.intent_unified_transformer": [
             "torch"
         ],
         "modelscope.models.nlp.space.model.model_base": [
             "os",
             "torch"
         ],
         "modelscope.models.nlp.space.model.tokenization_space": [
             "transformers"
         ],
         "modelscope.models.nlp.space.model.unified_transformer": [
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.nlp.space.modules.embedder": [
             "torch"
         ],
         "modelscope.models.nlp.space.modules.feedforward": [
             "torch"
         ],
         "modelscope.models.nlp.space.modules.functions": [
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.nlp.space.modules.multihead_attention": [
             "torch"
         ],
         "modelscope.models.nlp.space.modules.transformer_block": [
             "torch"
         ],
         "modelscope.models.nlp.space_T_cn.backbone": [
+            "tarfile",
+            "os",
             "__future__",
+            "copy",
+            "tempfile",
             "math",
-            "numpy",
-            "tarfile",
             "torch",
-            "tempfile",
-            "copy",
             "shutil",
-            "os"
+            "numpy"
         ],
         "modelscope.models.nlp.space_T_cn.configuration": [
             "logging",
             "__future__",
-            "json",
-            "copy"
+            "copy",
+            "json"
         ],
         "modelscope.models.nlp.space_T_cn.table_question_answering": [
-            "numpy",
-            "torch",
+            "os",
             "transformers",
             "typing",
-            "os"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.nlp.space_T_en.text_to_sql": [
             "os",
-            "typing",
             "torch",
-            "text2sql_lgesql"
+            "text2sql_lgesql",
+            "typing"
         ],
         "modelscope.models.nlp.structbert.adv_utils": [
             "torch"
         ],
         "modelscope.models.nlp.structbert.backbone": [
-            "math",
-            "dataclasses",
-            "torch",
             "transformers",
+            "math",
             "typing",
-            "packaging"
+            "torch",
+            "packaging",
+            "dataclasses"
         ],
         "modelscope.models.nlp.structbert.configuration": [
             "transformers"
         ],
         "modelscope.models.nlp.structbert.faq_question_answering": [
-            "math",
-            "torch",
             "collections",
+            "os",
+            "math",
             "typing",
-            "os"
+            "torch"
         ],
         "modelscope.models.nlp.structbert.fill_mask": [
             "transformers",
             "torch"
         ],
         "modelscope.models.nlp.structbert.text_classification": [
             "torch"
         ],
         "modelscope.models.nlp.structbert.token_classification": [
             "torch"
         ],
         "modelscope.models.nlp.task_models.feature_extraction": [
-            "typing",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.models.nlp.task_models.fill_mask": [
-            "typing",
             "torch",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.models.nlp.task_models.information_extraction": [
-            "typing",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.models.nlp.task_models.task_model": [
-            "abc",
-            "re",
-            "torch",
             "collections",
+            "os",
+            "abc",
             "typing",
-            "os"
+            "torch",
+            "re"
         ],
         "modelscope.models.nlp.task_models.text_classification": [
-            "typing",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.models.nlp.task_models.text_generation": [
-            "typing",
             "transformers",
+            "torch",
             "numpy",
-            "torch"
+            "typing"
         ],
         "modelscope.models.nlp.task_models.text_ranking": [
-            "typing",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.models.nlp.task_models.token_classification": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.models.nlp.unite.configuration": [
             "enum"
         ],
         "modelscope.models.nlp.unite.translation_evaluation": [
-            "math",
-            "dataclasses",
-            "torch",
+            "transformers",
             "warnings",
+            "math",
             "typing",
-            "transformers",
-            "numpy",
-            "packaging"
+            "torch",
+            "packaging",
+            "dataclasses",
+            "numpy"
         ],
         "modelscope.models.nlp.use.transformer": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.models.nlp.use.user_satisfaction_estimation": [
-            "numpy",
-            "torch",
+            "os",
             "transformers",
             "typing",
-            "os"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.nlp.veco.backbone": [
             "transformers"
         ],
         "modelscope.models.nlp.veco.configuration": [
             "transformers"
         ],
@@ -16895,2007 +16793,2007 @@
             "transformers"
         ],
         "modelscope.models.nlp.veco.token_classification": [
             "transformers",
             "torch"
         ],
         "modelscope.models.nlp.xlm_roberta.backbone": [
-            "torch",
             "transformers",
-            "math",
-            "packaging"
+            "packaging",
+            "torch",
+            "math"
         ],
         "modelscope.models.nlp.xlm_roberta.configuration": [
+            "collections",
             "transformers",
-            "typing",
-            "collections"
+            "typing"
         ],
         "modelscope.models.science.unifold.config": [
             "ml_collections",
-            "typing",
-            "copy"
+            "copy",
+            "typing"
         ],
         "modelscope.models.science.unifold.data.data_ops": [
+            "functools",
             "operator",
-            "unicore",
-            "itertools",
-            "torch",
             "typing",
-            "functools",
+            "torch",
+            "itertools",
+            "unicore",
             "numpy"
         ],
         "modelscope.models.science.unifold.data.msa_pairing": [
-            "numpy",
-            "pandas",
             "collections",
             "scipy",
-            "typing"
+            "typing",
+            "pandas",
+            "numpy"
         ],
         "modelscope.models.science.unifold.data.process": [
-            "typing",
             "torch",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.models.science.unifold.data.process_multimer": [
-            "typing",
+            "collections",
             "numpy",
-            "collections"
+            "typing"
         ],
         "modelscope.models.science.unifold.data.protein": [
-            "numpy",
-            "dataclasses",
-            "Bio",
             "typing",
-            "io"
+            "Bio",
+            "io",
+            "dataclasses",
+            "numpy"
         ],
         "modelscope.models.science.unifold.data.residue_constants": [
-            "numpy",
-            "unicore",
+            "collections",
             "functools",
-            "typing",
             "os",
-            "collections"
+            "unicore",
+            "numpy",
+            "typing"
         ],
         "modelscope.models.science.unifold.data.utils": [
-            "gzip",
-            "scipy",
+            "functools",
             "copy",
+            "scipy",
             "typing",
-            "functools",
             "pickle",
+            "gzip",
             "numpy",
             "json"
         ],
         "modelscope.models.science.unifold.dataset": [
-            "typing",
             "ml_collections",
-            "numpy",
-            "unicore",
-            "torch",
-            "copy",
             "logging",
             "os",
+            "copy",
+            "typing",
+            "torch",
+            "unicore",
+            "numpy",
             "json"
         ],
         "modelscope.models.science.unifold.model": [
             "os",
-            "typing",
             "torch",
-            "argparse"
+            "argparse",
+            "typing"
         ],
         "modelscope.models.science.unifold.modules.alphafold": [
-            "unicore",
-            "torch"
+            "torch",
+            "unicore"
         ],
         "modelscope.models.science.unifold.modules.attentions": [
-            "unicore",
             "functools",
-            "typing",
-            "torch"
+            "torch",
+            "unicore",
+            "typing"
         ],
         "modelscope.models.science.unifold.modules.auxillary_heads": [
+            "torch",
             "unicore",
-            "typing",
-            "torch"
+            "typing"
         ],
         "modelscope.models.science.unifold.modules.common": [
-            "unicore",
             "functools",
-            "typing",
-            "torch"
+            "torch",
+            "unicore",
+            "typing"
         ],
         "modelscope.models.science.unifold.modules.confidence": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.models.science.unifold.modules.embedders": [
+            "torch",
             "unicore",
-            "typing",
-            "torch"
+            "typing"
         ],
         "modelscope.models.science.unifold.modules.evoformer": [
-            "unicore",
             "functools",
-            "typing",
-            "torch"
+            "torch",
+            "unicore",
+            "typing"
         ],
         "modelscope.models.science.unifold.modules.featurization": [
+            "torch",
             "unicore",
-            "typing",
-            "torch"
+            "typing"
         ],
         "modelscope.models.science.unifold.modules.frame": [
-            "typing",
             "__future__",
+            "torch",
             "numpy",
-            "torch"
+            "typing"
         ],
         "modelscope.models.science.unifold.modules.structure_module": [
+            "torch",
             "unicore",
-            "typing",
             "math",
-            "torch"
+            "typing"
         ],
         "modelscope.models.science.unifold.modules.template": [
+            "functools",
             "math",
-            "unicore",
+            "typing",
             "torch",
-            "functools",
-            "typing"
+            "unicore"
         ],
         "modelscope.models.science.unifold.modules.triangle_multiplication": [
-            "unicore",
             "functools",
-            "typing",
-            "torch"
+            "torch",
+            "unicore",
+            "typing"
         ],
         "modelscope.models.science.unifold.msa.mmcif": [
+            "collections",
+            "functools",
             "Bio",
             "absl",
-            "dataclasses",
-            "functools",
-            "typing",
             "io",
-            "collections"
+            "dataclasses",
+            "typing"
         ],
         "modelscope.models.science.unifold.msa.msa_identifiers": [
-            "dataclasses",
             "re",
+            "dataclasses",
             "typing"
         ],
         "modelscope.models.science.unifold.msa.parsers": [
-            "re",
+            "collections",
             "string",
-            "dataclasses",
+            "re",
             "itertools",
-            "typing",
-            "collections"
+            "dataclasses",
+            "typing"
         ],
         "modelscope.models.science.unifold.msa.pipeline": [
-            "typing",
-            "numpy",
+            "os",
             "absl",
-            "os"
+            "numpy",
+            "typing"
         ],
         "modelscope.models.science.unifold.msa.templates": [
+            "functools",
+            "datetime",
+            "os",
             "abc",
-            "glob",
+            "typing",
             "re",
-            "numpy",
+            "glob",
             "dataclasses",
-            "absl",
-            "typing",
-            "functools",
-            "datetime",
-            "os"
+            "numpy",
+            "absl"
         ],
         "modelscope.models.science.unifold.msa.tools.hhblits": [
-            "glob",
-            "absl",
-            "typing",
             "os",
-            "subprocess"
+            "typing",
+            "glob",
+            "subprocess",
+            "absl"
         ],
         "modelscope.models.science.unifold.msa.tools.hhsearch": [
-            "glob",
-            "absl",
-            "typing",
             "os",
-            "subprocess"
+            "typing",
+            "glob",
+            "subprocess",
+            "absl"
         ],
         "modelscope.models.science.unifold.msa.tools.hmmbuild": [
-            "re",
             "os",
             "absl",
-            "subprocess"
+            "subprocess",
+            "re"
         ],
         "modelscope.models.science.unifold.msa.tools.hmmsearch": [
-            "typing",
+            "os",
             "absl",
             "subprocess",
-            "os"
+            "typing"
         ],
         "modelscope.models.science.unifold.msa.tools.jackhmmer": [
-            "glob",
-            "concurrent",
             "urllib",
-            "absl",
-            "typing",
             "os",
-            "subprocess"
+            "concurrent",
+            "typing",
+            "glob",
+            "subprocess",
+            "absl"
         ],
         "modelscope.models.science.unifold.msa.tools.kalign": [
-            "typing",
+            "os",
             "absl",
             "subprocess",
-            "os"
+            "typing"
         ],
         "modelscope.models.science.unifold.msa.tools.utils": [
-            "tempfile",
-            "contextlib",
-            "absl",
             "time",
-            "typing",
-            "shutil"
+            "absl",
+            "contextlib",
+            "tempfile",
+            "shutil",
+            "typing"
         ],
         "modelscope.models.science.unifold.msa.utils": [
+            "os",
             "typing",
-            "json",
             "absl",
-            "os"
+            "json"
         ],
         "modelscope.msdatasets.audio.asr_dataset": [],
         "modelscope.msdatasets.auth.auth_config": [
             "http",
             "typing"
         ],
         "modelscope.msdatasets.context.dataset_context_config": [
             "typing"
         ],
         "modelscope.msdatasets.data_files.data_files_manager": [
+            "os",
             "datasets",
-            "typing",
-            "os"
+            "typing"
         ],
         "modelscope.msdatasets.data_loader.data_loader": [
-            "abc",
+            "os",
             "datasets",
-            "typing",
-            "os"
+            "abc",
+            "typing"
         ],
         "modelscope.msdatasets.data_loader.data_loader_manager": [
-            "abc",
-            "datasets",
             "os",
-            "enum"
+            "enum",
+            "datasets",
+            "abc"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.audio.asr_dataset": [
             "os"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.audio.kws_farfield_dataset": [
+            "os",
             "math",
-            "numpy",
-            "threading",
             "torch",
-            "os",
-            "queue"
+            "threading",
+            "queue",
+            "numpy"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.audio.kws_nearfield_dataset": [
             "random",
             "torch"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.audio.kws_nearfield_processor": [
-            "torchaudio",
+            "random",
             "kaldiio",
             "torch",
-            "random",
+            "torchaudio",
             "numpy",
             "json"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.bad_image_detecting.bad_image_detecting_dataset": [],
         "modelscope.msdatasets.dataset_cls.custom_datasets.builder": [],
         "modelscope.msdatasets.dataset_cls.custom_datasets.damoyolo.build": [
-            "torch",
+            "copy",
             "bisect",
-            "math",
-            "copy"
+            "torch",
+            "math"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.damoyolo.collate_batch": [],
         "modelscope.msdatasets.dataset_cls.custom_datasets.damoyolo.datasets.coco": [
+            "torch",
             "torchvision",
             "numpy",
-            "torch",
             "cv2"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.damoyolo.datasets.mosaic_wrapper": [
+            "random",
             "math",
             "torch",
             "cv2",
-            "random",
             "numpy"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.damoyolo.evaluation.coco.coco_eval": [
+            "collections",
             "tempfile",
-            "os",
             "torch",
-            "collections"
+            "os"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.damoyolo.samplers.distributed": [
-            "math",
-            "torch"
+            "torch",
+            "math"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.damoyolo.samplers.grouped_batch_sampler": [
             "itertools",
             "torch"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.damoyolo.samplers.iteration_based_batch_sampler": [
             "torch"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.damoyolo.transforms.build": [],
         "modelscope.msdatasets.dataset_cls.custom_datasets.damoyolo.transforms.transforms": [
-            "torchvision",
-            "cv2",
+            "torch",
             "random",
-            "numpy",
-            "torch"
+            "cv2",
+            "torchvision",
+            "numpy"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.easycv_base": [
             "os"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.gopro_image_deblurring_dataset": [
             "cv2",
             "numpy"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.image_colorization.image_colorization_dataset": [
             "cv2",
             "numpy",
             "torch"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.image_inpainting.aug": [
-            "albumentations",
-            "imgaug"
+            "imgaug",
+            "albumentations"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.image_inpainting.image_inpainting_dataset": [
-            "glob",
             "enum",
+            "os",
             "albumentations",
+            "glob",
             "cv2",
-            "numpy",
-            "os"
+            "numpy"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.image_instance_segmentation_coco_dataset": [
-            "numpy",
             "os",
-            "pycocotools"
+            "pycocotools",
+            "numpy"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.image_portrait_enhancement.data_utils": [
             "cv2",
             "torch"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.image_portrait_enhancement.image_portrait_enhancement_dataset": [
             "cv2",
             "numpy"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.image_quality_assessment_degradation.image_quality_assessment_degradation_dataset": [
             "torchvision"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.image_quality_assmessment_mos.image_quality_assessment_mos_dataset": [],
         "modelscope.msdatasets.dataset_cls.custom_datasets.language_guided_video_summarization_dataset": [
+            "os",
             "h5py",
             "torch",
-            "json",
             "numpy",
-            "os"
+            "json"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.mgeo_ranking_dataset": [
-            "random",
-            "typing",
             "torch",
-            "json"
+            "random",
+            "json",
+            "typing"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.movie_scene_segmentation.movie_scene_segmentation_dataset": [
-            "torch",
-            "torchvision",
+            "os",
             "copy",
+            "torch",
             "random",
-            "os",
+            "torchvision",
             "json"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.movie_scene_segmentation.sampler": [
             "random",
             "numpy"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_detection.augmenter": [
             "imgaug"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_detection.data_loader": [
+            "imgaug",
             "math",
             "torch",
-            "imgaug",
             "bisect",
             "numpy"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_detection.image_dataset": [
-            "glob",
-            "math",
-            "numpy",
+            "functools",
             "logging",
+            "os",
+            "math",
             "torch",
-            "cv2",
             "bisect",
-            "functools",
-            "os"
+            "glob",
+            "cv2",
+            "numpy"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_detection.measures.iou_evaluator": [
-            "shapely",
+            "collections",
             "numpy",
-            "collections"
+            "shapely"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_detection.measures.quad_measurer": [
             "numpy"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_detection.processes.augment_data": [
-            "cv2",
             "imgaug",
+            "cv2",
             "numpy",
             "math"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_detection.processes.data_process": [],
         "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_detection.processes.make_border_map": [
-            "cv2",
             "pyclipper",
-            "shapely",
-            "numpy"
-        ],
-        "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_detection.processes.make_icdar_data": [
             "cv2",
             "numpy",
+            "shapely"
+        ],
+        "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_detection.processes.make_icdar_data": [
+            "collections",
             "torch",
-            "collections"
+            "numpy",
+            "cv2"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_detection.processes.make_seg_detection_data": [
-            "cv2",
             "pyclipper",
-            "shapely",
-            "numpy"
+            "cv2",
+            "numpy",
+            "shapely"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_detection.processes.normalize_image": [
-            "numpy",
-            "torch"
+            "torch",
+            "numpy"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_detection.processes.random_crop_data": [
             "cv2",
             "numpy"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_recognition_dataset": [
+            "six",
+            "os",
             "lmdb",
-            "numpy",
             "torch",
             "PIL",
             "cv2",
-            "six",
-            "os",
+            "numpy",
             "json"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.reds_image_deblurring_dataset": [
             "cv2",
             "numpy"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.referring_video_object_segmentation.referring_video_object_segmentation_dataset": [
-            "glob",
-            "pandas",
-            "h5py",
-            "torch",
             "torchvision",
             "os",
+            "tqdm",
+            "h5py",
+            "torch",
             "pycocotools",
+            "glob",
+            "pandas",
             "numpy",
-            "json",
-            "tqdm"
+            "json"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.referring_video_object_segmentation.transformers": [
-            "PIL",
-            "random",
+            "torch",
             "torchvision",
-            "torch"
+            "random",
+            "PIL"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.sidd_image_denoising.data_utils": [
             "cv2",
             "torch"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.sidd_image_denoising.sidd_image_denoising_dataset": [
             "cv2",
             "numpy"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.sidd_image_denoising.transforms": [
             "random"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.text_ranking_dataset": [
             "random",
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.torch_custom_dataset": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.veco_dataset": [
+            "numpy",
             "datasets",
-            "typing",
-            "numpy"
+            "typing"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.video_frame_interpolation.data_utils": [
             "cv2",
             "torch"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.video_frame_interpolation.video_frame_interpolation_dataset": [
             "cv2",
             "numpy",
             "torch"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.video_stabilization.video_stabilization_dataset": [],
         "modelscope.msdatasets.dataset_cls.custom_datasets.video_summarization_dataset": [
+            "os",
             "h5py",
             "torch",
-            "json",
             "numpy",
-            "os"
+            "json"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.video_super_resolution.video_super_resolution_dataset": [
-            "cv2",
-            "numpy",
+            "collections",
             "torch",
-            "collections"
+            "numpy",
+            "cv2"
         ],
         "modelscope.msdatasets.dataset_cls.dataset": [
-            "pandas",
-            "datasets",
-            "copy",
             "os",
-            "tqdm"
+            "tqdm",
+            "copy",
+            "pandas",
+            "datasets"
         ],
         "modelscope.msdatasets.download.dataset_builder": [
-            "pandas",
-            "datasets",
             "pyarrow",
+            "os",
             "typing",
-            "os"
+            "pandas",
+            "datasets"
         ],
         "modelscope.msdatasets.download.download_config": [
             "datasets",
             "typing"
         ],
         "modelscope.msdatasets.download.download_manager": [
             "datasets"
         ],
         "modelscope.msdatasets.meta.data_meta_config": [],
         "modelscope.msdatasets.meta.data_meta_manager": [
-            "shutil",
             "collections",
-            "datasets",
             "os",
+            "shutil",
+            "datasets",
             "json"
         ],
         "modelscope.msdatasets.ms_dataset": [
             "numpy",
-            "datasets",
+            "os",
             "warnings",
             "typing",
-            "os"
+            "datasets"
         ],
         "modelscope.msdatasets.task_datasets.gopro_image_deblurring_dataset": [],
         "modelscope.msdatasets.task_datasets.reds_image_deblurring_dataset": [],
         "modelscope.msdatasets.task_datasets.sidd_image_denoising": [],
         "modelscope.msdatasets.task_datasets.torch_base_dataset": [],
         "modelscope.msdatasets.task_datasets.video_summarization_dataset": [],
         "modelscope.msdatasets.utils.dataset_utils": [
             "collections",
-            "typing",
+            "pandas",
             "os",
-            "pandas"
+            "typing"
         ],
         "modelscope.msdatasets.utils.delete_utils": [],
         "modelscope.msdatasets.utils.maxcompute_utils": [
-            "math",
-            "pandas"
+            "pandas",
+            "math"
         ],
         "modelscope.msdatasets.utils.oss_utils": [
-            "oss2",
+            "os",
             "__future__",
-            "multiprocessing",
+            "oss2",
             "datasets",
-            "os"
+            "multiprocessing"
         ],
         "modelscope.msdatasets.utils.upload_utils": [
-            "multiprocessing",
             "os",
+            "multiprocessing",
             "tqdm"
         ],
         "modelscope.pipelines.audio.ans_dfsmn_pipeline": [
-            "numpy",
-            "sys",
-            "torch",
             "collections",
+            "librosa",
+            "numpy",
             "soundfile",
-            "typing",
             "os",
+            "typing",
+            "torch",
             "io",
-            "librosa"
+            "sys"
         ],
         "modelscope.pipelines.audio.ans_pipeline": [
-            "torch",
+            "librosa",
             "soundfile",
             "typing",
-            "numpy",
+            "torch",
             "io",
-            "librosa"
+            "numpy"
         ],
         "modelscope.pipelines.audio.asr_inference_pipeline": [
+            "os",
             "yaml",
-            "typing",
             "json",
-            "os"
+            "typing"
         ],
         "modelscope.pipelines.audio.asr_wenet_inference_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.audio.inverse_text_processing_pipeline": [
+            "os",
             "yaml",
             "shutil",
-            "typing",
-            "os"
+            "typing"
         ],
         "modelscope.pipelines.audio.kws_farfield_pipeline": [
             "wave",
-            "numpy",
             "soundfile",
             "typing",
-            "io"
+            "io",
+            "numpy"
         ],
         "modelscope.pipelines.audio.kws_kwsbp_pipeline": [
-            "typing",
             "os",
-            "json"
+            "json",
+            "typing"
         ],
         "modelscope.pipelines.audio.language_recognition_pipeline": [
-            "numpy",
-            "torchaudio",
-            "torch",
             "soundfile",
-            "typing",
             "os",
-            "io"
+            "typing",
+            "torch",
+            "io",
+            "torchaudio",
+            "numpy"
         ],
         "modelscope.pipelines.audio.linear_aec_pipeline": [
+            "os",
+            "scipy",
+            "typing",
             "yaml",
-            "numpy",
             "importlib",
             "torch",
-            "scipy",
-            "typing",
-            "os"
+            "numpy"
         ],
         "modelscope.pipelines.audio.lm_infer_pipeline": [
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.pipelines.audio.punctuation_processing_pipeline": [
+            "os",
             "yaml",
             "shutil",
-            "typing",
-            "os"
+            "typing"
         ],
         "modelscope.pipelines.audio.segmentation_clustering_pipeline": [
-            "torchaudio",
-            "torch",
             "soundfile",
             "typing",
-            "numpy",
-            "io"
+            "torch",
+            "io",
+            "torchaudio",
+            "numpy"
         ],
         "modelscope.pipelines.audio.separation_pipeline": [
-            "numpy",
-            "torch",
             "soundfile",
             "typing",
-            "io"
+            "torch",
+            "io",
+            "numpy"
         ],
         "modelscope.pipelines.audio.speaker_change_locating_pipeline": [
-            "torchaudio",
-            "torch",
             "soundfile",
             "typing",
-            "numpy",
-            "io"
+            "torch",
+            "io",
+            "torchaudio",
+            "numpy"
         ],
         "modelscope.pipelines.audio.speaker_diarization_dialogue_detection_pipeline": [
-            "typing",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.pipelines.audio.speaker_diarization_pipeline": [
-            "yaml",
-            "numpy",
-            "shutil",
             "os",
             "typing",
+            "yaml",
+            "shutil",
+            "numpy",
             "json"
         ],
         "modelscope.pipelines.audio.speaker_diarization_semantic_speaker_turn_detection_pipeline": [
-            "typing",
             "torch",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.pipelines.audio.speaker_verification_eres2net_pipeline": [
-            "typing",
-            "torch",
+            "soundfile",
             "io",
-            "soundfile"
+            "torch",
+            "typing"
         ],
         "modelscope.pipelines.audio.speaker_verification_light_pipeline": [
-            "numpy",
-            "torchaudio",
-            "torch",
             "soundfile",
-            "typing",
             "os",
-            "io"
+            "typing",
+            "torch",
+            "io",
+            "torchaudio",
+            "numpy"
         ],
         "modelscope.pipelines.audio.speaker_verification_pipeline": [
+            "os",
             "yaml",
             "shutil",
-            "typing",
-            "os"
+            "typing"
         ],
         "modelscope.pipelines.audio.speaker_verification_rdino_pipeline": [
-            "typing",
-            "torch",
+            "soundfile",
             "io",
-            "soundfile"
+            "torch",
+            "typing"
         ],
         "modelscope.pipelines.audio.text_to_speech_pipeline": [
-            "typing",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.pipelines.audio.timestamp_pipeline": [
-            "yaml",
-            "funasr",
             "os",
+            "funasr",
             "typing",
+            "yaml",
             "json"
         ],
         "modelscope.pipelines.audio.voice_activity_detection_pipeline": [
-            "yaml",
-            "funasr",
             "os",
+            "funasr",
             "typing",
+            "yaml",
             "json"
         ],
         "modelscope.pipelines.base": [
-            "abc",
-            "multiprocessing",
-            "numpy",
-            "threading",
-            "torch",
             "functools",
+            "os",
+            "abc",
             "typing",
+            "torch",
+            "threading",
+            "packaging",
             "random",
-            "os",
-            "packaging"
+            "numpy",
+            "multiprocessing"
         ],
         "modelscope.pipelines.builder": [
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.pipelines.cv.action_detection_pipeline": [
+            "os",
             "math",
-            "typing",
-            "os"
+            "typing"
         ],
         "modelscope.pipelines.cv.action_recognition_pipeline": [
+            "os",
             "torch",
-            "typing",
             "math",
-            "os"
+            "typing"
         ],
         "modelscope.pipelines.cv.animal_recognition_pipeline": [
-            "numpy",
+            "os",
+            "typing",
             "torch",
             "PIL",
-            "torchvision",
             "cv2",
-            "typing",
-            "os"
+            "torchvision",
+            "numpy"
         ],
         "modelscope.pipelines.cv.arc_face_recognition_pipeline": [
-            "numpy",
+            "os",
+            "typing",
             "torch",
             "PIL",
             "cv2",
-            "typing",
-            "os"
+            "numpy"
         ],
         "modelscope.pipelines.cv.bad_image_detecting_pipeline": [
-            "typing",
             "torch",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.pipelines.cv.body_2d_keypoints_pipeline": [
-            "numpy",
+            "os",
+            "typing",
             "torch",
             "PIL",
-            "torchvision",
             "cv2",
-            "typing",
-            "os",
+            "torchvision",
+            "numpy",
             "json"
         ],
         "modelscope.pipelines.cv.body_3d_keypoints_pipeline": [
-            "numpy",
+            "datetime",
+            "os",
             "mpl_toolkits",
+            "matplotlib",
+            "typing",
             "torch",
             "tempfile",
-            "matplotlib",
             "cv2",
-            "typing",
-            "datetime",
-            "os"
+            "numpy"
         ],
         "modelscope.pipelines.cv.card_detection_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.cv.cmdssl_video_embedding_pipeline": [
-            "numpy",
+            "os",
+            "typing",
             "decord",
             "torch",
-            "torchvision",
             "PIL",
-            "typing",
-            "os"
+            "torchvision",
+            "numpy"
         ],
         "modelscope.pipelines.cv.content_check_pipeline": [
-            "numpy",
+            "os",
+            "typing",
             "torch",
             "PIL",
-            "torchvision",
             "cv2",
-            "typing",
-            "os"
+            "torchvision",
+            "numpy"
         ],
         "modelscope.pipelines.cv.controllable_image_generation_pipeline": [
-            "glob",
+            "os",
+            "tempfile",
             "math",
-            "numpy",
+            "typing",
             "torch",
-            "tempfile",
+            "glob",
             "cv2",
-            "typing",
-            "os",
-            "subprocess"
+            "subprocess",
+            "numpy"
         ],
         "modelscope.pipelines.cv.crowd_counting_pipeline": [
             "math",
+            "typing",
             "torch",
-            "torchvision",
             "PIL",
-            "typing",
+            "torchvision",
             "numpy"
         ],
         "modelscope.pipelines.cv.ddcolor_image_colorization_pipeline": [
-            "numpy",
+            "typing",
             "torch",
-            "torchvision",
             "cv2",
-            "typing"
+            "torchvision",
+            "numpy"
         ],
         "modelscope.pipelines.cv.ddpm_semantic_segmentation_pipeline": [
+            "torch",
             "torchvision",
-            "typing",
-            "torch"
+            "typing"
         ],
         "modelscope.pipelines.cv.face_attribute_recognition_pipeline": [
-            "numpy",
+            "os",
+            "typing",
             "torch",
             "PIL",
             "cv2",
-            "typing",
-            "os"
+            "numpy"
         ],
         "modelscope.pipelines.cv.face_detection_pipeline": [
-            "numpy",
+            "os",
+            "typing",
             "torch",
             "PIL",
             "cv2",
-            "typing",
-            "os"
+            "numpy"
         ],
         "modelscope.pipelines.cv.face_emotion_pipeline": [
-            "typing",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.pipelines.cv.face_human_hand_detection_pipeline": [
-            "typing",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.pipelines.cv.face_image_generation_pipeline": [
-            "numpy",
+            "os",
+            "typing",
             "torch",
             "PIL",
             "cv2",
-            "typing",
-            "os"
+            "numpy"
         ],
         "modelscope.pipelines.cv.face_liveness_ir_pipeline": [
-            "onnxruntime",
+            "os",
+            "typing",
             "torch",
             "PIL",
+            "onnxruntime",
             "cv2",
-            "typing",
-            "numpy",
-            "os"
+            "numpy"
         ],
         "modelscope.pipelines.cv.face_liveness_xc_pipeline": [
-            "onnxruntime",
+            "os",
+            "typing",
             "torch",
             "PIL",
+            "onnxruntime",
             "cv2",
-            "typing",
-            "numpy",
-            "os"
+            "numpy"
         ],
         "modelscope.pipelines.cv.face_processing_base_pipeline": [
-            "numpy",
+            "os",
+            "typing",
             "torch",
             "PIL",
             "cv2",
-            "typing",
-            "os"
+            "numpy"
         ],
         "modelscope.pipelines.cv.face_quality_assessment_pipeline": [
-            "onnxruntime",
+            "os",
+            "typing",
             "torch",
             "PIL",
+            "onnxruntime",
             "cv2",
-            "typing",
-            "numpy",
-            "os"
+            "numpy"
         ],
         "modelscope.pipelines.cv.face_recognition_onnx_fm_pipeline": [
-            "onnxruntime",
+            "os",
+            "typing",
             "torch",
             "PIL",
+            "onnxruntime",
             "cv2",
-            "typing",
-            "numpy",
-            "os"
+            "numpy"
         ],
         "modelscope.pipelines.cv.face_recognition_onnx_ir_pipeline": [
-            "onnxruntime",
+            "os",
+            "typing",
             "torch",
             "PIL",
+            "onnxruntime",
             "cv2",
-            "typing",
-            "numpy",
-            "os"
+            "numpy"
         ],
         "modelscope.pipelines.cv.face_recognition_ood_pipeline": [
-            "numpy",
+            "os",
+            "typing",
             "torch",
             "PIL",
             "cv2",
-            "typing",
-            "os"
+            "numpy"
         ],
         "modelscope.pipelines.cv.face_recognition_pipeline": [
-            "numpy",
+            "os",
+            "typing",
             "torch",
             "PIL",
             "cv2",
-            "typing",
-            "os"
+            "numpy"
         ],
         "modelscope.pipelines.cv.face_reconstruction_pipeline": [
+            "os",
+            "scipy",
+            "typing",
             "face_alignment",
-            "numpy",
             "tensorflow",
             "torch",
+            "io",
             "PIL",
-            "scipy",
-            "cv2",
-            "typing",
             "shutil",
-            "os",
-            "io"
+            "cv2",
+            "numpy"
         ],
         "modelscope.pipelines.cv.facial_expression_recognition_pipeline": [
-            "numpy",
+            "os",
+            "typing",
             "torch",
             "PIL",
             "cv2",
-            "typing",
-            "os"
+            "numpy"
         ],
         "modelscope.pipelines.cv.facial_landmark_confidence_pipeline": [
-            "numpy",
+            "os",
+            "typing",
             "torch",
             "PIL",
             "cv2",
-            "typing",
-            "os"
+            "numpy"
         ],
         "modelscope.pipelines.cv.fast_instance_segmentation_pipeline": [
+            "torch",
             "torchvision",
-            "typing",
             "numpy",
-            "torch"
+            "typing"
         ],
         "modelscope.pipelines.cv.general_recognition_pipeline": [
-            "numpy",
+            "os",
+            "typing",
             "torch",
             "PIL",
-            "torchvision",
             "cv2",
-            "typing",
-            "os"
+            "torchvision",
+            "numpy"
         ],
         "modelscope.pipelines.cv.hand_static_pipeline": [
-            "typing",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.pipelines.cv.hicossl_video_embedding_pipeline": [
+            "os",
             "torch",
-            "typing",
             "math",
-            "os"
+            "typing"
         ],
         "modelscope.pipelines.cv.human_reconstruction_pipeline": [
             "trimesh",
-            "numpy",
-            "shutil",
-            "torch",
+            "os",
             "typing",
-            "os"
+            "torch",
+            "shutil",
+            "numpy"
         ],
         "modelscope.pipelines.cv.image_body_reshaping_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.cv.image_bts_depth_estimation_pipeline": [
-            "numpy",
-            "torch",
             "albumentations",
+            "typing",
+            "torch",
             "cv2",
-            "typing"
+            "numpy"
         ],
         "modelscope.pipelines.cv.image_cartoon_pipeline": [
-            "numpy",
             "os",
-            "cv2",
             "typing",
-            "tensorflow"
+            "tensorflow",
+            "cv2",
+            "numpy"
         ],
         "modelscope.pipelines.cv.image_classification_pipeline": [
-            "typing",
             "torch",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.pipelines.cv.image_color_enhance_pipeline": [
+            "torch",
             "torchvision",
-            "typing",
-            "torch"
+            "typing"
         ],
         "modelscope.pipelines.cv.image_colorization_pipeline": [
+            "typing",
             "torch",
             "PIL",
-            "torchvision",
             "cv2",
-            "typing",
+            "torchvision",
             "numpy"
         ],
         "modelscope.pipelines.cv.image_debanding_pipeline": [
+            "torch",
             "torchvision",
-            "typing",
-            "torch"
+            "typing"
         ],
         "modelscope.pipelines.cv.image_deblur_pipeline": [
+            "torch",
             "torchvision",
-            "typing",
-            "torch"
+            "typing"
         ],
         "modelscope.pipelines.cv.image_defrcn_fewshot_pipeline": [
-            "typing",
-            "numpy",
+            "os",
             "torch",
-            "os"
+            "numpy",
+            "typing"
         ],
         "modelscope.pipelines.cv.image_denoise_pipeline": [
+            "torch",
             "torchvision",
-            "typing",
-            "torch"
+            "typing"
         ],
         "modelscope.pipelines.cv.image_depth_estimation_pipeline": [
-            "numpy",
+            "typing",
             "torch",
             "PIL",
             "cv2",
-            "typing"
+            "numpy"
         ],
         "modelscope.pipelines.cv.image_detection_pipeline": [
-            "typing",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.pipelines.cv.image_driving_perception_pipeline": [
+            "os",
             "cv2",
-            "typing",
             "numpy",
-            "os"
+            "typing"
         ],
         "modelscope.pipelines.cv.image_face_fusion_pipeline": [
-            "typing",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.pipelines.cv.image_human_parsing_pipeline": [
+            "torch",
             "torchvision",
-            "typing",
             "numpy",
-            "torch"
+            "typing"
         ],
         "modelscope.pipelines.cv.image_inpainting_pipeline": [
-            "numpy",
+            "typing",
             "torch",
             "PIL",
             "cv2",
-            "typing"
+            "numpy"
         ],
         "modelscope.pipelines.cv.image_inpainting_sdv2_pipeline": [
-            "diffusers",
-            "math",
             "numpy",
-            "sys",
+            "os",
+            "math",
+            "typing",
             "torch",
+            "diffusers",
             "tempfile",
             "cv2",
-            "typing",
-            "os"
+            "sys"
         ],
         "modelscope.pipelines.cv.image_instance_segmentation_pipeline": [
-            "numpy",
+            "os",
+            "typing",
             "torch",
             "PIL",
             "cv2",
-            "typing",
-            "os"
+            "numpy"
         ],
         "modelscope.pipelines.cv.image_matching_pipeline": [
-            "numpy",
+            "typing",
             "torch",
             "PIL",
             "cv2",
-            "typing"
+            "numpy"
         ],
         "modelscope.pipelines.cv.image_matting_pipeline": [
-            "numpy",
             "os",
-            "cv2",
             "typing",
-            "tensorflow"
+            "tensorflow",
+            "cv2",
+            "numpy"
         ],
         "modelscope.pipelines.cv.image_mvs_depth_estimation_pipeline": [
-            "tempfile",
-            "typing",
             "os",
-            "shutil"
+            "tempfile",
+            "shutil",
+            "typing"
         ],
         "modelscope.pipelines.cv.image_open_vocabulary_detection_pipeline": [
-            "numpy",
+            "os",
+            "typing",
             "torch",
             "PIL",
             "cv2",
-            "typing",
-            "os"
+            "numpy"
         ],
         "modelscope.pipelines.cv.image_paintbyexample_pipeline": [
+            "einops",
+            "typing",
             "torch",
-            "torchvision",
             "PIL",
             "cv2",
-            "typing",
-            "einops",
+            "torchvision",
             "numpy"
         ],
         "modelscope.pipelines.cv.image_panoptic_segmentation_pipeline": [
-            "numpy",
+            "typing",
             "torch",
             "PIL",
             "cv2",
-            "typing"
+            "numpy"
         ],
         "modelscope.pipelines.cv.image_portrait_enhancement_pipeline": [
+            "scipy",
             "math",
+            "typing",
             "torch",
             "PIL",
-            "scipy",
             "cv2",
-            "typing",
             "numpy"
         ],
         "modelscope.pipelines.cv.image_quality_assessment_degradation_pipeline": [
             "math",
+            "typing",
             "torch",
-            "torchvision",
             "tempfile",
             "cv2",
-            "typing",
+            "torchvision",
             "numpy"
         ],
         "modelscope.pipelines.cv.image_quality_assessment_man_pipeline": [
             "math",
+            "typing",
             "torch",
-            "torchvision",
             "tempfile",
             "cv2",
-            "typing",
+            "torchvision",
             "numpy"
         ],
         "modelscope.pipelines.cv.image_quality_assessment_mos_pipeline": [
             "math",
+            "typing",
             "torch",
-            "torchvision",
             "tempfile",
             "cv2",
-            "typing",
+            "torchvision",
             "numpy"
         ],
         "modelscope.pipelines.cv.image_reid_person_pipeline": [
+            "os",
             "math",
+            "typing",
             "torch",
             "PIL",
-            "torchvision",
-            "typing",
-            "os"
+            "torchvision"
         ],
         "modelscope.pipelines.cv.image_restoration_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.cv.image_salient_detection_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.cv.image_semantic_segmentation_pipeline": [
-            "numpy",
+            "typing",
             "torch",
             "PIL",
             "cv2",
-            "typing"
+            "numpy"
         ],
         "modelscope.pipelines.cv.image_skychange_pipeline": [
-            "numpy",
-            "PIL",
+            "pdb",
+            "typing",
             "time",
+            "PIL",
             "cv2",
-            "pdb",
-            "typing"
+            "numpy"
         ],
         "modelscope.pipelines.cv.image_structured_model_probing_pipeline": [
+            "mmcv",
+            "os",
             "math",
-            "numpy",
+            "typing",
             "torch",
             "torchvision",
-            "mmcv",
-            "typing",
-            "os"
+            "numpy"
         ],
         "modelscope.pipelines.cv.image_style_transfer_pipeline": [
+            "os",
             "cv2",
-            "typing",
             "numpy",
-            "os"
+            "typing"
         ],
         "modelscope.pipelines.cv.image_super_resolution_pipeline": [
-            "numpy",
+            "typing",
             "torch",
             "PIL",
             "cv2",
-            "typing"
+            "numpy"
         ],
         "modelscope.pipelines.cv.image_to_image_generate_pipeline": [
-            "numpy",
+            "os",
+            "typing",
             "torch",
             "PIL",
-            "torchvision",
             "cv2",
-            "typing",
-            "os"
+            "torchvision",
+            "numpy"
         ],
         "modelscope.pipelines.cv.image_to_image_translation_pipeline": [
             "numpy",
-            "sys",
+            "os",
+            "typing",
             "torch",
-            "torchvision",
+            "io",
             "PIL",
             "cv2",
-            "typing",
-            "os",
-            "io"
+            "torchvision",
+            "sys"
         ],
         "modelscope.pipelines.cv.image_try_on_pipeline": [
-            "typing",
             "torch",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.pipelines.cv.indoor_layout_estimation_pipeline": [
             "cv2",
-            "typing",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.pipelines.cv.language_guided_video_summarization_pipeline": [
-            "numpy",
+            "os",
             "clip",
-            "shutil",
-            "torch",
-            "PIL",
             "tempfile",
             "cv2",
             "typing",
+            "torch",
+            "PIL",
+            "shutil",
             "random",
-            "os"
+            "numpy"
         ],
         "modelscope.pipelines.cv.license_plate_detection_pipeline": [
+            "os",
             "math",
-            "numpy",
+            "typing",
             "torch",
             "PIL",
             "cv2",
-            "typing",
-            "os"
+            "numpy"
         ],
         "modelscope.pipelines.cv.lineless_table_recognition_pipeline": [
+            "os",
             "math",
-            "numpy",
+            "typing",
             "torch",
             "PIL",
             "cv2",
-            "typing",
-            "os"
+            "numpy"
         ],
         "modelscope.pipelines.cv.live_category_pipeline": [
-            "numpy",
+            "os",
+            "typing",
             "decord",
             "torch",
-            "torchvision",
             "PIL",
-            "typing",
-            "os"
+            "torchvision",
+            "numpy"
         ],
         "modelscope.pipelines.cv.mask_face_recognition_pipeline": [
-            "torch",
             "collections",
+            "os",
+            "typing",
+            "torch",
             "PIL",
             "cv2",
-            "typing",
-            "numpy",
-            "os"
+            "numpy"
         ],
         "modelscope.pipelines.cv.maskdino_instance_segmentation_pipeline": [
+            "torch",
             "torchvision",
-            "typing",
-            "torch"
+            "typing"
         ],
         "modelscope.pipelines.cv.mobile_image_super_resolution_pipeline": [
-            "numpy",
+            "typing",
             "torch",
+            "skimage",
             "torchvision",
-            "typing",
-            "skimage"
+            "numpy"
         ],
         "modelscope.pipelines.cv.mog_face_detection_pipeline": [
-            "typing",
             "os",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.pipelines.cv.motion_generation_pipeline": [
-            "numpy",
+            "os",
+            "typing",
             "torch",
             "tempfile",
-            "typing",
-            "os"
+            "numpy"
         ],
         "modelscope.pipelines.cv.movie_scene_segmentation_pipeline": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.pipelines.cv.mtcnn_face_detection_pipeline": [
+            "os",
             "torch",
-            "typing",
-            "os"
+            "typing"
         ],
         "modelscope.pipelines.cv.nerf_recon_4k_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.cv.nerf_recon_acc_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.cv.nerf_recon_vq_compression_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.cv.object_detection_3d_pipeline": [
-            "numpy",
+            "os",
+            "typing",
             "torch",
             "PIL",
             "tempfile",
             "cv2",
-            "typing",
-            "os"
+            "numpy"
         ],
         "modelscope.pipelines.cv.ocr_detection_pipeline": [
+            "os",
+            "typing",
             "math",
             "tensorflow",
             "torch",
             "tf_slim",
             "cv2",
-            "typing",
-            "numpy",
-            "os"
+            "numpy"
         ],
         "modelscope.pipelines.cv.ocr_recognition_pipeline": [],
         "modelscope.pipelines.cv.ocr_utils.model_convnext_transformer": [
             "torch"
         ],
         "modelscope.pipelines.cv.ocr_utils.model_dla34": [
+            "os",
             "torch",
             "numpy",
-            "math",
-            "os"
+            "math"
         ],
         "modelscope.pipelines.cv.ocr_utils.model_resnet18_half": [
             "os",
             "torch"
         ],
         "modelscope.pipelines.cv.ocr_utils.model_resnet_mutex_v4_linewithchar": [
             "tensorflow",
             "tf_slim"
         ],
         "modelscope.pipelines.cv.ocr_utils.model_vlpt": [
+            "os",
             "torch",
             "sys",
-            "os",
             "math"
         ],
         "modelscope.pipelines.cv.ocr_utils.ocr_modules.convnext": [
             "torch"
         ],
         "modelscope.pipelines.cv.ocr_utils.ocr_modules.timm_tinyc": [
+            "collections",
             "functools",
-            "math",
-            "copy",
+            "torch",
             "logging",
             "itertools",
-            "torch",
-            "collections"
+            "copy",
+            "math"
         ],
         "modelscope.pipelines.cv.ocr_utils.ocr_modules.vitstr": [
+            "functools",
+            "logging",
             "__future__",
-            "torch",
             "copy",
-            "logging",
-            "functools"
+            "torch"
         ],
         "modelscope.pipelines.cv.ocr_utils.ops": [
-            "uuid",
-            "math",
             "numpy",
+            "os",
+            "math",
             "tensorflow",
-            "sys",
-            "absl",
-            "cv2",
+            "uuid",
             "shutil",
-            "os"
+            "cv2",
+            "sys",
+            "absl"
         ],
         "modelscope.pipelines.cv.ocr_utils.resnet18_v1": [
             "tensorflow",
             "tf_slim"
         ],
         "modelscope.pipelines.cv.ocr_utils.resnet_utils": [
+            "collections",
             "tensorflow",
-            "tf_slim",
-            "collections"
+            "tf_slim"
         ],
         "modelscope.pipelines.cv.ocr_utils.table_process": [
-            "math",
-            "copy",
-            "cv2",
+            "torch",
             "random",
+            "copy",
             "numpy",
-            "torch"
+            "cv2",
+            "math"
         ],
         "modelscope.pipelines.cv.ocr_utils.utils": [
-            "cv2",
-            "pyclipper",
             "shapely",
-            "numpy"
+            "cv2",
+            "numpy",
+            "pyclipper"
         ],
         "modelscope.pipelines.cv.panorama_depth_estimation_pipeline": [
-            "numpy",
+            "typing",
             "torch",
             "PIL",
             "cv2",
-            "typing"
+            "numpy"
         ],
         "modelscope.pipelines.cv.panorama_depth_estimation_s2net_pipeline": [
-            "numpy",
+            "typing",
             "torch",
             "PIL",
             "cv2",
-            "typing"
+            "numpy"
         ],
         "modelscope.pipelines.cv.pedestrian_attribute_recognition_pipeline": [
-            "numpy",
+            "os",
+            "typing",
             "torch",
             "PIL",
-            "torchvision",
             "cv2",
-            "typing",
-            "os",
+            "torchvision",
+            "numpy",
             "json"
         ],
         "modelscope.pipelines.cv.pointcloud_sceneflow_estimation_pipeline": [
-            "typing",
             "plyfile",
             "numpy",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.pipelines.cv.product_retrieval_embedding_pipeline": [
-            "numpy",
+            "os",
+            "typing",
             "torch",
             "PIL",
-            "torchvision",
             "cv2",
-            "typing",
-            "os"
+            "torchvision",
+            "numpy"
         ],
         "modelscope.pipelines.cv.product_segmentation_pipeline": [
-            "typing",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.pipelines.cv.realtime_video_object_detection_pipeline": [
-            "numpy",
+            "os",
+            "typing",
             "torch",
             "PIL",
-            "torchvision",
             "cv2",
-            "typing",
-            "os",
+            "torchvision",
+            "numpy",
             "json"
         ],
         "modelscope.pipelines.cv.referring_video_object_segmentation_pipeline": [
-            "numpy",
-            "torch",
+            "einops",
+            "tqdm",
             "moviepy",
-            "torchvision",
+            "typing",
+            "torch",
             "PIL",
             "tempfile",
-            "einops",
-            "typing",
-            "tqdm"
+            "torchvision",
+            "numpy"
         ],
         "modelscope.pipelines.cv.retina_face_detection_pipeline": [
-            "numpy",
+            "os",
+            "typing",
             "torch",
             "PIL",
             "cv2",
-            "typing",
-            "os"
+            "numpy"
         ],
         "modelscope.pipelines.cv.shop_segmentation_pipleline": [
             "typing"
         ],
         "modelscope.pipelines.cv.skin_retouching_pipeline": [
-            "numpy",
+            "os",
+            "typing",
+            "tensorflow",
             "torch",
             "PIL",
-            "torchvision",
             "cv2",
-            "typing",
-            "os",
-            "tensorflow"
+            "torchvision",
+            "numpy"
         ],
         "modelscope.pipelines.cv.table_recognition_pipeline": [
+            "os",
             "math",
-            "numpy",
+            "typing",
             "torch",
             "PIL",
             "cv2",
-            "typing",
-            "os"
+            "numpy"
         ],
         "modelscope.pipelines.cv.tbs_detection_pipeline": [
-            "numpy",
-            "colorsys",
+            "os",
+            "typing",
             "torch",
+            "colorsys",
             "PIL",
             "cv2",
-            "typing",
-            "os"
+            "numpy"
         ],
         "modelscope.pipelines.cv.tbs_detection_utils.utils": [
+            "os",
             "__future__",
-            "pandas",
-            "numpy",
-            "colorsys",
+            "matplotlib",
             "torch",
+            "colorsys",
             "PIL",
-            "torchvision",
-            "matplotlib",
-            "os"
+            "pandas",
+            "numpy",
+            "torchvision"
         ],
         "modelscope.pipelines.cv.text_driven_segmentation_pipleline": [
             "typing"
         ],
         "modelscope.pipelines.cv.text_to_360panorama_image_pipeline": [
-            "realesrgan",
-            "diffusers",
-            "numpy",
             "basicsr",
+            "realesrgan",
+            "typing",
             "torch",
+            "diffusers",
             "PIL",
             "random",
-            "typing"
+            "numpy"
         ],
         "modelscope.pipelines.cv.tinynas_classification_pipeline": [
+            "os",
             "math",
-            "torch",
-            "torchvision",
             "typing",
-            "os"
+            "torch",
+            "torchvision"
         ],
         "modelscope.pipelines.cv.tinynas_detection_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.cv.ulfd_face_detection_pipeline": [
-            "numpy",
+            "os",
+            "typing",
             "torch",
             "PIL",
             "cv2",
-            "typing",
-            "os"
+            "numpy"
         ],
         "modelscope.pipelines.cv.video_category_pipeline": [
-            "numpy",
+            "os",
+            "typing",
             "decord",
             "torch",
-            "torchvision",
             "PIL",
-            "typing",
-            "os",
+            "torchvision",
+            "numpy",
             "json"
         ],
         "modelscope.pipelines.cv.video_colorization_pipeline": [
-            "numpy",
+            "os",
+            "typing",
             "torch",
             "PIL",
-            "torchvision",
             "tempfile",
             "cv2",
-            "typing",
-            "os",
-            "subprocess"
+            "subprocess",
+            "numpy",
+            "torchvision"
         ],
         "modelscope.pipelines.cv.video_deinterlace_pipeline": [
+            "os",
             "math",
-            "numpy",
+            "typing",
             "torch",
-            "torchvision",
             "tempfile",
             "cv2",
-            "typing",
-            "os",
-            "subprocess"
+            "subprocess",
+            "numpy",
+            "torchvision"
         ],
         "modelscope.pipelines.cv.video_depth_estimation_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.cv.video_frame_interpolation_pipeline": [
-            "glob",
+            "os",
+            "tempfile",
             "math",
-            "numpy",
+            "typing",
             "torch",
-            "torchvision",
-            "tempfile",
+            "glob",
             "cv2",
-            "typing",
-            "os",
-            "subprocess"
+            "subprocess",
+            "numpy",
+            "torchvision"
         ],
         "modelscope.pipelines.cv.video_human_matting_pipeline": [
-            "numpy",
+            "os",
             "moviepy",
+            "typing",
             "torch",
             "cv2",
-            "typing",
-            "os"
+            "numpy"
         ],
         "modelscope.pipelines.cv.video_inpainting_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.cv.video_instance_segmentation_pipeline": [
+            "mmcv",
+            "os",
             "tqdm",
+            "typing",
             "torch",
             "cv2",
-            "typing",
-            "numpy",
-            "os",
-            "mmcv"
+            "numpy"
         ],
         "modelscope.pipelines.cv.video_multi_object_tracking_pipeline": [
+            "os",
             "torch",
-            "typing",
-            "os"
+            "typing"
         ],
         "modelscope.pipelines.cv.video_object_segmentation_pipeline": [
-            "numpy",
+            "os",
+            "typing",
             "torch",
             "PIL",
             "torchvision",
-            "typing",
-            "os"
+            "numpy"
         ],
         "modelscope.pipelines.cv.video_panoptic_segmentation_pipeline": [
+            "mmcv",
+            "os",
             "tqdm",
+            "typing",
             "torch",
             "cv2",
-            "typing",
-            "numpy",
-            "os",
-            "mmcv"
+            "numpy"
         ],
         "modelscope.pipelines.cv.video_single_object_tracking_pipeline": [
+            "os",
             "cv2",
-            "typing",
-            "os"
+            "typing"
         ],
         "modelscope.pipelines.cv.video_stabilization_pipeline": [
-            "glob",
+            "os",
+            "tempfile",
             "math",
-            "numpy",
+            "typing",
             "torch",
-            "tempfile",
+            "glob",
             "cv2",
-            "typing",
-            "os",
-            "subprocess"
+            "subprocess",
+            "numpy"
         ],
         "modelscope.pipelines.cv.video_summarization_pipeline": [
-            "numpy",
+            "os",
+            "tqdm",
+            "typing",
             "torch",
             "cv2",
-            "typing",
-            "os",
-            "tqdm"
+            "numpy"
         ],
         "modelscope.pipelines.cv.video_super_resolution_pipeline": [
+            "os",
             "math",
-            "numpy",
+            "typing",
             "torch",
-            "torchvision",
             "tempfile",
             "cv2",
-            "typing",
-            "os",
-            "subprocess"
+            "subprocess",
+            "numpy",
+            "torchvision"
         ],
         "modelscope.pipelines.cv.vidt_pipeline": [
+            "torch",
             "torchvision",
-            "typing",
-            "torch"
+            "typing"
         ],
         "modelscope.pipelines.cv.virtual_try_on_pipeline": [
-            "numpy",
+            "os",
+            "typing",
             "torch",
             "PIL",
             "cv2",
-            "typing",
-            "os"
+            "numpy"
         ],
         "modelscope.pipelines.cv.vision_efficient_tuning_pipeline": [
+            "torch",
             "torchvision",
-            "typing",
             "numpy",
-            "torch"
+            "typing"
         ],
         "modelscope.pipelines.cv.vision_middleware_pipeline": [
+            "mmcv",
+            "os",
             "math",
-            "numpy",
+            "typing",
             "torch",
             "torchvision",
-            "mmcv",
-            "typing",
-            "os"
+            "numpy"
         ],
         "modelscope.pipelines.cv.vop_retrieval_pipeline": [
-            "gzip",
-            "math",
-            "numpy",
-            "torch",
             "collections",
+            "os",
+            "tqdm",
+            "math",
             "typing",
-            "random",
             "pickle",
-            "os",
-            "tqdm"
+            "torch",
+            "gzip",
+            "random",
+            "numpy"
         ],
         "modelscope.pipelines.cv.vop_retrieval_se_pipeline": [
-            "gzip",
-            "numpy",
-            "torch",
+            "os",
             "typing",
-            "os"
+            "torch",
+            "gzip",
+            "numpy"
         ],
         "modelscope.pipelines.multi_modal.asr_pipeline": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.pipelines.multi_modal.diffusers_wrapped.diffusers_pipeline": [
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.pipelines.multi_modal.diffusers_wrapped.stable_diffusion.chinese_stable_diffusion_pipeline": [
-            "diffusers",
-            "numpy",
+            "transformers",
+            "typing",
             "torch",
+            "diffusers",
             "PIL",
             "cv2",
-            "transformers",
-            "typing"
+            "numpy"
         ],
         "modelscope.pipelines.multi_modal.diffusers_wrapped.stable_diffusion.stable_diffusion_pipeline": [
-            "diffusers",
-            "numpy",
+            "os",
+            "typing",
             "torch",
-            "torchvision",
+            "diffusers",
             "PIL",
             "cv2",
-            "typing",
-            "os"
+            "torchvision",
+            "numpy"
         ],
         "modelscope.pipelines.multi_modal.disco_guided_diffusion_pipeline.disco_guided_diffusion": [
-            "math",
-            "numpy",
+            "os",
             "clip",
+            "math",
             "importlib",
             "torch",
             "gc",
-            "torchvision",
             "PIL",
             "cv2",
-            "os",
+            "torchvision",
+            "numpy",
             "json"
         ],
         "modelscope.pipelines.multi_modal.disco_guided_diffusion_pipeline.utils": [
-            "math",
+            "torch",
             "warnings",
             "numpy",
-            "torch",
-            "fractions"
+            "fractions",
+            "math"
         ],
         "modelscope.pipelines.multi_modal.document_vl_embedding_pipeline": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.pipelines.multi_modal.efficient_diffusion_tuning_pipeline": [
+            "typing",
             "torch",
-            "torchvision",
             "PIL",
             "cv2",
-            "typing",
+            "torchvision",
             "numpy"
         ],
         "modelscope.pipelines.multi_modal.generative_multi_modal_embedding_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.multi_modal.gridvlp_pipeline": [
-            "traceback",
-            "numpy",
+            "os",
+            "transformers",
+            "typing",
+            "time",
             "torch",
+            "traceback",
             "PIL",
-            "time",
-            "typing",
-            "transformers",
-            "os",
+            "numpy",
             "json"
         ],
         "modelscope.pipelines.multi_modal.image_captioning_pipeline": [
-            "typing",
             "torch",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.pipelines.multi_modal.image_text_retrieval_pipeline": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.pipelines.multi_modal.mgeo_ranking_pipeline": [
-            "typing",
             "torch",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.pipelines.multi_modal.multi_modal_embedding_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.multi_modal.multimodal_dialogue_pipeline": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.pipelines.multi_modal.ocr_recognition_pipeline": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.pipelines.multi_modal.soonet_video_temporal_grounding_pipeline": [
-            "numpy",
+            "os",
+            "typing",
             "torch",
             "torchvision",
-            "typing",
-            "os"
+            "numpy"
         ],
         "modelscope.pipelines.multi_modal.sudoku_pipeline": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.pipelines.multi_modal.team_multi_modal_similarity_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.multi_modal.text2sql_pipeline": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.pipelines.multi_modal.text_to_image_synthesis_pipeline": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.pipelines.multi_modal.text_to_video_synthesis_pipeline": [
-            "torch",
-            "tempfile",
-            "cv2",
             "einops",
+            "os",
             "typing",
-            "os"
+            "torch",
+            "tempfile",
+            "cv2"
         ],
         "modelscope.pipelines.multi_modal.video_captioning_pipeline": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.pipelines.multi_modal.video_multi_modal_embedding_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.multi_modal.video_question_answering_pipeline": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.pipelines.multi_modal.visual_entailment_pipeline": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.pipelines.multi_modal.visual_grounding_pipeline": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.pipelines.multi_modal.visual_question_answering_pipeline": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.pipelines.nlp.automatic_post_editing_pipeline": [
             "sacremoses",
-            "sentencepiece",
-            "tensorflow",
-            "html",
             "jieba",
+            "os",
+            "sentencepiece",
             "typing",
+            "tensorflow",
             "numpy",
-            "os"
+            "html"
         ],
         "modelscope.pipelines.nlp.canmt_translation_pipeline": [
             "os",
-            "typing",
+            "sacremoses",
             "torch",
-            "sacremoses"
+            "typing"
         ],
         "modelscope.pipelines.nlp.codegeex_code_generation_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.nlp.codegeex_code_translation_pipeline": [
             "typing"
         ],
@@ -18910,1419 +18808,1418 @@
         "modelscope.pipelines.nlp.dialog_modeling_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.nlp.dialog_state_tracking_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.nlp.distributed_gpt3_pipeline": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.pipelines.nlp.distributed_gpt_moe_pipeline": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.pipelines.nlp.distributed_plug_pipeline": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.pipelines.nlp.document_grounded_dialog_generate_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.nlp.document_grounded_dialog_rerank_pipeline": [
-            "typing",
-            "re",
+            "collections",
             "numpy",
+            "pprint",
             "ujson",
-            "sys",
-            "torch",
-            "collections",
+            "os",
+            "transformers",
+            "typing",
             "time",
-            "pprint",
+            "torch",
+            "re",
             "random",
-            "transformers",
-            "os"
+            "sys"
         ],
         "modelscope.pipelines.nlp.document_grounded_dialog_retrieval_pipeline": [
-            "faiss",
-            "numpy",
             "os",
             "typing",
+            "faiss",
+            "numpy",
             "json"
         ],
         "modelscope.pipelines.nlp.document_segmentation_pipeline": [
-            "re",
             "numpy",
+            "typing",
             "torch",
-            "datasets",
-            "typing"
+            "re",
+            "datasets"
         ],
         "modelscope.pipelines.nlp.extractive_summarization_pipeline": [
-            "re",
             "numpy",
+            "typing",
             "torch",
-            "datasets",
-            "typing"
+            "re",
+            "datasets"
         ],
         "modelscope.pipelines.nlp.faq_question_answering_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.nlp.fasttext_text_classification_pipeline": [
-            "numpy",
             "fasttext",
+            "os",
             "sentencepiece",
             "typing",
-            "os"
+            "numpy"
         ],
         "modelscope.pipelines.nlp.feature_extraction_pipeline": [
+            "os",
             "torch",
-            "typing",
-            "os"
+            "typing"
         ],
         "modelscope.pipelines.nlp.fid_dialogue_pipeline": [
             "re",
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.pipelines.nlp.fill_mask_pipeline": [
-            "typing",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.pipelines.nlp.glm130b_text_generation_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.nlp.information_extraction_pipeline": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.pipelines.nlp.interactive_translation_pipeline": [
             "sacremoses",
-            "tensorflow",
-            "subword_nmt",
             "jieba",
+            "os",
             "typing",
-            "numpy",
-            "os"
+            "subword_nmt",
+            "tensorflow",
+            "numpy"
         ],
         "modelscope.pipelines.nlp.language_identification_pipline": [
-            "re",
-            "numpy",
             "os",
             "typing",
-            "tensorflow"
+            "tensorflow",
+            "re",
+            "numpy"
         ],
         "modelscope.pipelines.nlp.llama2_text_generation_pipeline": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.pipelines.nlp.mglm_text_summarization_pipeline": [
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.pipelines.nlp.named_entity_recognition_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.nlp.polylm_text_generation_pipeline": [
+            "os",
             "torch",
-            "typing",
-            "os"
+            "typing"
         ],
         "modelscope.pipelines.nlp.sentence_embedding_pipeline": [
-            "typing",
             "torch",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.pipelines.nlp.siamese_uie_pipeline": [
-            "typing",
-            "math",
-            "torch",
-            "pathlib",
-            "scipy",
-            "copy",
-            "time",
             "logging",
             "os",
-            "json",
-            "tqdm"
+            "tqdm",
+            "copy",
+            "scipy",
+            "math",
+            "time",
+            "typing",
+            "pathlib",
+            "torch",
+            "json"
         ],
         "modelscope.pipelines.nlp.summarization_pipeline": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.pipelines.nlp.table_question_answering_pipeline": [
-            "torch",
             "os",
             "transformers",
             "typing",
+            "torch",
             "json"
         ],
         "modelscope.pipelines.nlp.text_classification_pipeline": [
-            "typing",
             "torch",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.pipelines.nlp.text_error_correction_pipeline": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.pipelines.nlp.text_generation_pipeline": [
-            "transformers",
-            "typing",
+            "os",
             "torch",
-            "os"
+            "typing"
         ],
         "modelscope.pipelines.nlp.text_ranking_pipeline": [
-            "typing",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.pipelines.nlp.token_classification_pipeline": [
-            "typing",
             "torch",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.pipelines.nlp.translation_evaluation_pipeline": [
-            "numpy",
-            "torch",
+            "enum",
             "os",
             "typing",
-            "enum"
+            "torch",
+            "numpy"
         ],
         "modelscope.pipelines.nlp.translation_pipeline": [
             "sacremoses",
-            "tensorflow",
-            "subword_nmt",
             "jieba",
+            "os",
             "typing",
-            "numpy",
-            "os"
+            "subword_nmt",
+            "tensorflow",
+            "numpy"
         ],
         "modelscope.pipelines.nlp.translation_quality_estimation_pipeline": [
-            "torch",
+            "os",
             "transformers",
             "typing",
-            "os",
+            "torch",
             "io"
         ],
         "modelscope.pipelines.nlp.user_satisfaction_estimation_pipeline": [
-            "typing",
             "torch",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.pipelines.nlp.word_alignment_pipeline": [
-            "typing",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.pipelines.nlp.word_segmentation_pipeline": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.pipelines.nlp.zero_shot_classification_pipeline": [
+            "torch",
             "scipy",
-            "typing",
-            "torch"
+            "typing"
         ],
         "modelscope.pipelines.pipeline_template": [
-            "typing",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.pipelines.science.protein_structure_pipeline": [
-            "numpy",
-            "unicore",
-            "torch",
-            "time",
-            "typing",
             "os",
+            "typing",
+            "time",
+            "torch",
+            "unicore",
+            "numpy",
             "json"
         ],
         "modelscope.pipelines.util": [
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.preprocessors.asr": [
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.preprocessors.audio": [
-            "numpy",
-            "torch",
+            "os",
             "scipy",
             "typing",
-            "os",
-            "io"
+            "torch",
+            "io",
+            "numpy"
         ],
         "modelscope.preprocessors.base": [
+            "os",
             "abc",
-            "typing",
-            "os"
+            "typing"
         ],
         "modelscope.preprocessors.builder": [],
         "modelscope.preprocessors.common": [
-            "numpy",
-            "torch",
             "collections",
+            "typing",
             "time",
-            "typing"
+            "torch",
+            "numpy"
         ],
         "modelscope.preprocessors.cv.action_detection_mapper": [
-            "scipy",
+            "decord",
+            "torch",
             "random",
             "detectron2",
             "copy",
-            "decord",
-            "numpy",
-            "torch"
+            "scipy",
+            "numpy"
         ],
         "modelscope.preprocessors.cv.bad_image_detecting_preprocessor": [
             "math",
+            "typing",
             "torch",
-            "torchvision",
             "PIL",
-            "typing",
+            "torchvision",
             "numpy"
         ],
         "modelscope.preprocessors.cv.controllable_image_generation": [
+            "os",
             "math",
-            "numpy",
+            "typing",
             "torch",
             "PIL",
-            "torchvision",
             "cv2",
-            "typing",
-            "os"
+            "torchvision",
+            "numpy"
         ],
         "modelscope.preprocessors.cv.cv2_transforms": [
-            "numbers",
-            "math",
-            "cv2",
+            "collections",
+            "torch",
             "random",
+            "cv2",
             "numpy",
-            "torch",
-            "collections"
+            "numbers",
+            "math"
         ],
         "modelscope.preprocessors.cv.image_classification_preprocessor": [
-            "numpy",
+            "os",
+            "typing",
             "torch",
-            "torchvision",
             "PIL",
             "cv2",
-            "typing",
-            "os"
+            "torchvision",
+            "numpy"
         ],
         "modelscope.preprocessors.cv.image_quality_assessment_man": [
             "math",
+            "typing",
             "torch",
-            "torchvision",
             "PIL",
-            "typing",
+            "torchvision",
             "numpy"
         ],
         "modelscope.preprocessors.cv.image_quality_assessment_mos": [
             "math",
-            "numpy",
-            "torchvision",
+            "typing",
             "cv2",
-            "typing"
+            "torchvision",
+            "numpy"
         ],
         "modelscope.preprocessors.cv.image_restoration_preprocessor": [
             "math",
+            "typing",
             "torch",
-            "torchvision",
             "PIL",
-            "typing",
+            "torchvision",
             "numpy"
         ],
         "modelscope.preprocessors.cv.mmcls_preprocessor": [
-            "typing",
             "os",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.preprocessors.cv.timer": [
             "time"
         ],
         "modelscope.preprocessors.cv.util": [
-            "os",
+            "collections",
             "shutil",
             "sys",
-            "collections"
+            "os"
         ],
         "modelscope.preprocessors.cv.video_stabilization": [
             "cv2",
             "numpy",
             "torch"
         ],
         "modelscope.preprocessors.cv.video_super_resolution": [
-            "cv2",
             "os",
+            "cv2",
             "collections"
         ],
         "modelscope.preprocessors.image": [
-            "numpy",
+            "typing",
+            "io",
             "PIL",
             "cv2",
-            "typing",
-            "io"
+            "numpy"
         ],
         "modelscope.preprocessors.kws": [
+            "os",
             "yaml",
-            "typing",
-            "os"
+            "typing"
         ],
         "modelscope.preprocessors.movie_scene_segmentation.transforms": [
-            "numbers",
-            "numpy",
+            "os",
+            "typing",
             "torch",
-            "torchvision",
             "PIL",
-            "typing",
             "random",
-            "os"
+            "torchvision",
+            "numpy",
+            "numbers"
         ],
         "modelscope.preprocessors.multi_modal": [
-            "re",
-            "numpy",
+            "os",
+            "timm",
+            "typing",
             "decord",
             "torch",
-            "timm",
+            "io",
             "PIL",
+            "re",
             "torchvision",
-            "typing",
-            "os",
-            "json",
-            "io"
+            "numpy",
+            "json"
         ],
         "modelscope.preprocessors.nlp.bert_seq_cls_tokenizer": [
             "transformers",
             "typing"
         ],
         "modelscope.preprocessors.nlp.canmt_translation": [
             "sacremoses",
-            "torch",
-            "subword_nmt",
             "jieba",
+            "os",
             "typing",
-            "os"
+            "subword_nmt",
+            "torch"
         ],
         "modelscope.preprocessors.nlp.dialog_classification_use_preprocessor": [
             "transformers",
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.preprocessors.nlp.document_grounded_dialog_generate_preprocessor": [
-            "transformers",
-            "typing",
+            "os",
             "torch",
-            "os"
+            "transformers",
+            "typing"
         ],
         "modelscope.preprocessors.nlp.document_grounded_dialog_rerank_preprocessor": [
-            "torch",
-            "copy",
+            "os",
             "transformers",
+            "copy",
             "typing",
-            "os"
+            "torch"
         ],
         "modelscope.preprocessors.nlp.document_grounded_dialog_retrieval_preprocessor": [
-            "transformers",
-            "typing",
+            "os",
             "torch",
-            "os"
+            "transformers",
+            "typing"
         ],
         "modelscope.preprocessors.nlp.document_segmentation_preprocessor": [
             "typing"
         ],
         "modelscope.preprocessors.nlp.faq_question_answering_preprocessor": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.preprocessors.nlp.feature_extraction_preprocessor": [
-            "typing",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.preprocessors.nlp.fill_mask_preprocessor": [
+            "os",
             "abc",
-            "re",
-            "numpy",
-            "torch",
             "typing",
-            "os"
+            "torch",
+            "re",
+            "numpy"
         ],
         "modelscope.preprocessors.nlp.mgeo_ranking_preprocessor": [
             "transformers",
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.preprocessors.nlp.mglm_summarization_preprocessor": [
+            "os",
             "re",
-            "typing",
-            "os"
+            "typing"
         ],
         "modelscope.preprocessors.nlp.relation_extraction_preprocessor": [
             "transformers",
             "typing"
         ],
         "modelscope.preprocessors.nlp.sentence_embedding_preprocessor": [
             "typing"
         ],
         "modelscope.preprocessors.nlp.siamese_uie_preprocessor": [
             "transformers",
             "typing"
         ],
         "modelscope.preprocessors.nlp.space.args": [
-            "json",
-            "argparse"
+            "argparse",
+            "json"
         ],
         "modelscope.preprocessors.nlp.space.batch": [],
         "modelscope.preprocessors.nlp.space.data_loader": [
             "os",
-            "math",
-            "numpy"
+            "numpy",
+            "math"
         ],
         "modelscope.preprocessors.nlp.space.dialog_intent_prediction_preprocessor": [
-            "typing",
             "os",
-            "json"
+            "json",
+            "typing"
         ],
         "modelscope.preprocessors.nlp.space.dialog_modeling_preprocessor": [
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.preprocessors.nlp.space.dialog_state_tracking_preprocessor": [
             "typing"
         ],
         "modelscope.preprocessors.nlp.space.dst_processors": [
+            "json",
             "re",
             "logging",
             "six",
             "numpy",
-            "json",
             "tqdm"
         ],
         "modelscope.preprocessors.nlp.space.fields.gen_field": [
+            "collections",
             "random",
-            "numpy",
+            "os",
             "itertools",
-            "collections",
             "asyncio",
-            "os",
+            "numpy",
             "json"
         ],
         "modelscope.preprocessors.nlp.space.fields.intent_field": [
-            "glob",
-            "multiprocessing",
-            "re",
-            "numpy",
-            "itertools",
             "collections",
+            "os",
+            "tqdm",
             "time",
+            "re",
+            "itertools",
+            "glob",
             "random",
-            "os",
-            "json",
-            "tqdm"
+            "numpy",
+            "multiprocessing",
+            "json"
         ],
         "modelscope.preprocessors.nlp.space.lazy_dataset": [
             "json"
         ],
         "modelscope.preprocessors.nlp.space.preprocess": [
-            "glob",
-            "os"
+            "os",
+            "glob"
         ],
         "modelscope.preprocessors.nlp.space.sampler": [
             "numpy"
         ],
         "modelscope.preprocessors.nlp.space.tensorlistdataset": [
             "torch"
         ],
         "modelscope.preprocessors.nlp.space.tokenizer": [
-            "__future__",
-            "regex",
-            "sys",
             "collections",
-            "logging",
             "functools",
+            "logging",
             "os",
-            "json",
-            "unicodedata"
+            "__future__",
+            "regex",
+            "unicodedata",
+            "sys",
+            "json"
         ],
         "modelscope.preprocessors.nlp.space_T_cn.fields.database": [
+            "tqdm",
             "sqlite3",
-            "json",
-            "tqdm"
+            "json"
         ],
         "modelscope.preprocessors.nlp.space_T_cn.fields.schema_link": [
             "re"
         ],
         "modelscope.preprocessors.nlp.space_T_cn.fields.struct": [],
         "modelscope.preprocessors.nlp.space_T_cn.table_question_answering_preprocessor": [
-            "transformers",
-            "typing",
+            "os",
             "torch",
-            "os"
+            "transformers",
+            "typing"
         ],
         "modelscope.preprocessors.nlp.space_T_en.conversational_text_to_sql_preprocessor": [
-            "torch",
-            "text2sql_lgesql",
             "json",
+            "os",
             "typing",
-            "os"
+            "torch",
+            "text2sql_lgesql"
         ],
         "modelscope.preprocessors.nlp.space_T_en.fields.common_utils": [
+            "os",
             "nltk",
-            "itertools",
             "sqlite3",
-            "text2sql_lgesql",
+            "itertools",
             "numpy",
-            "os"
+            "text2sql_lgesql"
         ],
         "modelscope.preprocessors.nlp.space_T_en.fields.parse": [],
         "modelscope.preprocessors.nlp.space_T_en.fields.preprocess_dataset": [
             "text2sql_lgesql"
         ],
         "modelscope.preprocessors.nlp.space_T_en.fields.process_dataset": [
-            "pickle",
             "os",
+            "pickle",
             "sys",
             "text2sql_lgesql"
         ],
         "modelscope.preprocessors.nlp.text_classification_preprocessor": [
-            "typing",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.preprocessors.nlp.text_clean": [
-            "re",
+            "codecs",
             "sys",
-            "codecs"
+            "re"
         ],
         "modelscope.preprocessors.nlp.text_error_correction": [
-            "transformers",
-            "typing",
+            "os",
             "torch",
-            "os"
+            "transformers",
+            "typing"
         ],
         "modelscope.preprocessors.nlp.text_generation_preprocessor": [
-            "typing",
-            "numpy",
+            "os",
             "torch",
-            "os"
+            "numpy",
+            "typing"
         ],
         "modelscope.preprocessors.nlp.text_ranking_preprocessor": [
             "transformers",
             "typing"
         ],
         "modelscope.preprocessors.nlp.token_classification_preprocessor": [
-            "typing",
+            "torch",
             "numpy",
-            "torch"
+            "typing"
         ],
         "modelscope.preprocessors.nlp.token_classification_thai_preprocessor": [
             "typing"
         ],
         "modelscope.preprocessors.nlp.token_classification_viet_preprocessor": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.preprocessors.nlp.transformers_tokenizer": [
+            "collections",
             "transformers",
             "os",
-            "json",
-            "collections"
+            "json"
         ],
         "modelscope.preprocessors.nlp.translation_evaluation_preprocessor": [
             "transformers",
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.preprocessors.nlp.utils": [
-            "numpy",
             "collections",
-            "os",
             "transformers",
+            "os",
             "typing",
+            "numpy",
             "json"
         ],
         "modelscope.preprocessors.nlp.word_alignment_preprocessor": [
-            "numpy",
-            "itertools",
-            "torch",
+            "os",
             "typing",
-            "os"
+            "torch",
+            "itertools",
+            "numpy"
         ],
         "modelscope.preprocessors.nlp.zero_shot_classification_preprocessor": [
             "typing"
         ],
         "modelscope.preprocessors.ofa.asr": [
-            "torch",
-            "pathlib",
+            "librosa",
             "soundfile",
-            "fairseq",
-            "random",
-            "typing",
             "os",
-            "librosa"
+            "typing",
+            "fairseq",
+            "pathlib",
+            "torch",
+            "random"
         ],
         "modelscope.preprocessors.ofa.base": [
-            "re",
-            "torchaudio",
+            "string",
+            "os",
             "torch",
+            "io",
             "PIL",
-            "json",
-            "string",
+            "re",
+            "torchaudio",
             "numpy",
-            "os",
-            "io"
+            "json"
         ],
         "modelscope.preprocessors.ofa.image_captioning": [
+            "torch",
             "torchvision",
-            "typing",
-            "torch"
+            "typing"
         ],
         "modelscope.preprocessors.ofa.image_classification": [
-            "torch",
+            "functools",
             "timm",
+            "typing",
+            "torch",
             "PIL",
-            "torchvision",
-            "functools",
-            "typing"
+            "torchvision"
         ],
         "modelscope.preprocessors.ofa.ocr_recognition": [
             "zhconv",
+            "typing",
             "torch",
-            "torchvision",
             "unicodedata2",
-            "typing"
+            "torchvision"
         ],
         "modelscope.preprocessors.ofa.sudoku": [
-            "typing",
+            "torch",
             "numpy",
-            "torch"
+            "typing"
         ],
         "modelscope.preprocessors.ofa.summarization": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.preprocessors.ofa.text2sql": [
-            "re",
-            "torch",
-            "random",
+            "os",
             "typing",
-            "os"
+            "torch",
+            "re",
+            "random"
         ],
         "modelscope.preprocessors.ofa.text_classification": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.preprocessors.ofa.text_to_image_synthesis": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.preprocessors.ofa.utils.audio_helper": [
-            "typing",
             "torch",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.preprocessors.ofa.utils.bridge_content_encoder": [
-            "typing",
+            "difflib",
             "functools",
+            "sqlite3",
             "rapidfuzz",
-            "difflib",
-            "sqlite3"
+            "typing"
         ],
         "modelscope.preprocessors.ofa.utils.collate": [
-            "typing",
             "torch",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.preprocessors.ofa.utils.constant": [],
         "modelscope.preprocessors.ofa.utils.get_tables": [
-            "traceback",
             "sqlite3",
-            "sys"
+            "sys",
+            "traceback"
         ],
         "modelscope.preprocessors.ofa.utils.random_help": [
             "torch_xla",
             "torch"
         ],
         "modelscope.preprocessors.ofa.utils.text2phone": [],
         "modelscope.preprocessors.ofa.utils.transforms": [
-            "torchvision",
+            "torch",
             "PIL",
             "random",
-            "numpy",
-            "torch"
+            "torchvision",
+            "numpy"
         ],
         "modelscope.preprocessors.ofa.utils.vision_helper": [
             "cv2",
             "numpy"
         ],
         "modelscope.preprocessors.ofa.visual_entailment": [
-            "PIL",
-            "typing",
+            "torch",
             "torchvision",
-            "torch"
+            "PIL",
+            "typing"
         ],
         "modelscope.preprocessors.ofa.visual_grounding": [
-            "numpy",
+            "typing",
             "torch",
             "PIL",
             "torchvision",
-            "typing"
+            "numpy"
         ],
         "modelscope.preprocessors.ofa.visual_question_answering": [
-            "PIL",
-            "typing",
+            "torch",
             "torchvision",
-            "torch"
+            "PIL",
+            "typing"
         ],
         "modelscope.preprocessors.science.uni_fold": [
-            "re",
             "tarfile",
-            "pathlib",
-            "requests",
-            "torch",
-            "pickle",
-            "typing",
-            "os",
-            "numpy",
+            "hashlib",
+            "logging",
             "tqdm",
-            "gzip",
             "time",
-            "hashlib",
+            "pickle",
+            "torch",
+            "gzip",
+            "re",
+            "numpy",
             "ipdb",
-            "logging",
+            "os",
+            "requests",
+            "typing",
+            "pathlib",
             "random",
             "unittest",
             "json"
         ],
         "modelscope.preprocessors.speaker": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.preprocessors.tts": [
+            "os",
             "kantts",
-            "typing",
-            "os"
+            "typing"
         ],
         "modelscope.preprocessors.video": [
-            "uuid",
+            "urllib",
+            "os",
             "math",
-            "numpy",
             "decord",
-            "urllib",
             "torch",
-            "torchvision",
+            "uuid",
             "tempfile",
             "random",
-            "os"
+            "torchvision",
+            "numpy"
         ],
         "modelscope.trainers.audio.ans_trainer": [],
         "modelscope.trainers.audio.asr_trainer": [
+            "os",
             "funasr",
             "shutil",
-            "tempfile",
-            "os",
             "typing",
+            "tempfile",
             "json"
         ],
         "modelscope.trainers.audio.kws_farfield_trainer": [
-            "glob",
+            "datetime",
+            "os",
             "math",
-            "numpy",
-            "torch",
             "typing",
             "pickle",
-            "datetime",
-            "os"
+            "torch",
+            "glob",
+            "numpy"
         ],
         "modelscope.trainers.audio.kws_nearfield_trainer": [
-            "yaml",
-            "re",
-            "torch",
-            "tensorboardX",
-            "copy",
             "datetime",
+            "os",
+            "copy",
             "typing",
-            "os"
+            "yaml",
+            "torch",
+            "re",
+            "tensorboardX"
         ],
         "modelscope.trainers.audio.kws_utils.batch_utils": [
-            "math",
-            "sys",
-            "torch",
             "collections",
-            "typing",
+            "sys",
             "datetime",
-            "numpy",
-            "os"
+            "os",
+            "math",
+            "typing",
+            "torch",
+            "numpy"
         ],
         "modelscope.trainers.audio.kws_utils.det_utils": [
-            "glob",
+            "os",
             "kaldiio",
-            "threading",
-            "torch",
-            "json",
             "matplotlib",
+            "torch",
+            "threading",
+            "glob",
             "numpy",
-            "os"
+            "json"
         ],
         "modelscope.trainers.audio.kws_utils.file_utils": [
             "re"
         ],
         "modelscope.trainers.audio.kws_utils.model_utils": [
-            "glob",
-            "yaml",
-            "re",
-            "numpy",
+            "os",
             "shutil",
+            "yaml",
             "torch",
-            "os"
+            "re",
+            "glob",
+            "numpy"
         ],
         "modelscope.trainers.audio.kws_utils.runtime_utils": [
-            "re",
-            "shutil",
-            "sys",
             "collections",
             "os",
-            "json",
+            "codecs",
             "stat",
-            "codecs"
+            "re",
+            "shutil",
+            "sys",
+            "json"
         ],
         "modelscope.trainers.audio.separation_trainer": [
-            "speechbrain",
             "csv",
-            "numpy",
-            "torchaudio",
-            "torch",
-            "typing",
             "os",
-            "tqdm"
+            "tqdm",
+            "typing",
+            "speechbrain",
+            "torch",
+            "torchaudio",
+            "numpy"
         ],
         "modelscope.trainers.audio.tts_trainer": [
-            "shutil",
-            "tempfile",
             "zipfile",
             "os",
+            "shutil",
             "typing",
+            "tempfile",
             "json"
         ],
         "modelscope.trainers.base": [
-            "abc",
-            "typing",
             "os",
-            "time"
+            "time",
+            "abc",
+            "typing"
         ],
         "modelscope.trainers.builder": [],
         "modelscope.trainers.cli_argument_parser": [
+            "argparse",
             "dataclasses",
-            "typing",
-            "argparse"
+            "typing"
         ],
         "modelscope.trainers.cv.action_detection_trainer": [
-            "fvcore",
+            "os",
             "detectron2",
-            "torch",
+            "fvcore",
             "typing",
-            "os"
+            "torch"
         ],
         "modelscope.trainers.cv.card_detection_scrfd_trainer": [],
         "modelscope.trainers.cv.cartoon_translation_trainer": [
-            "numpy",
             "os",
+            "tqdm",
             "typing",
             "tensorflow",
             "packaging",
-            "tqdm"
+            "numpy"
         ],
         "modelscope.trainers.cv.face_detection_scrfd_trainer": [
-            "copy",
-            "typing",
             "os",
-            "time"
+            "time",
+            "copy",
+            "typing"
         ],
         "modelscope.trainers.cv.image_classifition_trainer": [
-            "numpy",
-            "torch",
+            "os",
             "copy",
-            "time",
             "typing",
-            "os"
+            "time",
+            "torch",
+            "numpy"
         ],
         "modelscope.trainers.cv.image_defrcn_fewshot_detection_trainer": [
-            "detectron2",
-            "torch",
             "collections",
+            "os",
+            "detectron2",
             "typing",
-            "os"
+            "torch"
         ],
         "modelscope.trainers.cv.image_detection_damoyolo_trainer": [
+            "datetime",
+            "os",
             "math",
-            "torch",
-            "easydict",
             "time",
             "typing",
-            "datetime",
-            "os"
+            "torch",
+            "easydict"
         ],
         "modelscope.trainers.cv.image_inpainting_trainer": [
-            "torch",
             "time",
+            "torch",
             "collections"
         ],
         "modelscope.trainers.cv.image_instance_segmentation_trainer": [],
         "modelscope.trainers.cv.image_portrait_enhancement_trainer": [
-            "torch",
-            "collections"
+            "collections",
+            "torch"
         ],
         "modelscope.trainers.cv.movie_scene_segmentation_trainer": [],
         "modelscope.trainers.cv.nerf_recon_acc_trainer": [
-            "glob",
-            "numpy",
-            "torch",
             "datetime",
-            "time",
+            "os",
+            "tqdm",
             "cv2",
             "typing",
+            "time",
+            "torch",
+            "glob",
             "random",
-            "os",
-            "tqdm"
+            "numpy"
         ],
         "modelscope.trainers.cv.ocr_detection_db_trainer": [
+            "datetime",
+            "os",
+            "tqdm",
+            "copy",
             "math",
-            "numpy",
-            "torch",
-            "easydict",
             "time",
-            "copy",
             "typing",
-            "datetime",
-            "os",
-            "tqdm"
+            "torch",
+            "easydict",
+            "numpy"
         ],
         "modelscope.trainers.cv.ocr_recognition_trainer": [
-            "torch",
             "time",
+            "torch",
             "collections"
         ],
         "modelscope.trainers.cv.referring_video_object_segmentation_trainer": [
             "os",
             "torch"
         ],
         "modelscope.trainers.cv.vision_efficient_tuning_trainer": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.trainers.default_config": [
             "typing"
         ],
         "modelscope.trainers.hooks.builder": [],
         "modelscope.trainers.hooks.checkpoint.checkpoint_hook": [
-            "shutil",
-            "torch",
-            "json",
+            "os",
             "typing",
+            "torch",
+            "shutil",
             "random",
             "numpy",
-            "os"
+            "json"
         ],
         "modelscope.trainers.hooks.checkpoint.checkpoint_processor": [
-            "re",
+            "os",
             "shutil",
-            "os"
+            "re"
         ],
         "modelscope.trainers.hooks.checkpoint.load_checkpoint_hook": [
-            "numpy",
+            "typing",
             "torch",
+            "packaging",
             "random",
-            "typing",
-            "packaging"
+            "numpy"
         ],
         "modelscope.trainers.hooks.clip_clamp_logit_scale_hook": [
             "torch"
         ],
         "modelscope.trainers.hooks.compression.sparsity_hook": [
             "os"
         ],
         "modelscope.trainers.hooks.compression.utils": [
             "torch"
         ],
         "modelscope.trainers.hooks.distributed.ddp_hook": [],
         "modelscope.trainers.hooks.distributed.deepspeed_hook": [
-            "math",
             "megatron_util",
-            "deepspeed",
-            "torch",
             "functools",
+            "deepspeed",
+            "os",
             "transformers",
-            "shutil",
-            "os"
+            "math",
+            "torch",
+            "shutil"
         ],
         "modelscope.trainers.hooks.distributed.megatron_hook": [
+            "megatron_util",
             "shutil",
             "torch",
-            "megatron_util",
             "os"
         ],
         "modelscope.trainers.hooks.early_stop_hook": [
             "numpy"
         ],
         "modelscope.trainers.hooks.evaluation_hook": [
-            "typing",
-            "collections"
+            "collections",
+            "typing"
         ],
         "modelscope.trainers.hooks.hook": [
             "functools"
         ],
         "modelscope.trainers.hooks.iter_timer_hook": [
             "time"
         ],
         "modelscope.trainers.hooks.logger.base": [
-            "numbers",
-            "numpy",
             "torch",
-            "abc"
+            "numpy",
+            "abc",
+            "numbers"
         ],
         "modelscope.trainers.hooks.logger.tensorboard_hook": [
-            "numpy",
+            "os",
             "torch",
-            "os"
+            "numpy"
         ],
         "modelscope.trainers.hooks.logger.text_logger_hook": [
-            "torch",
             "collections",
-            "json",
             "datetime",
-            "os"
+            "os",
+            "torch",
+            "json"
         ],
         "modelscope.trainers.hooks.lr_scheduler_hook": [],
         "modelscope.trainers.hooks.optimizer.apex_optimizer_hook": [
             "logging",
-            "torch",
-            "packaging"
+            "packaging",
+            "torch"
         ],
         "modelscope.trainers.hooks.optimizer.base": [
             "logging",
             "torch"
         ],
         "modelscope.trainers.hooks.optimizer.torch_optimizer_hook": [
             "logging"
         ],
         "modelscope.trainers.hooks.priority": [
-            "typing",
-            "enum"
+            "enum",
+            "typing"
         ],
         "modelscope.trainers.lrscheduler.builder": [
-            "inspect",
+            "packaging",
             "torch",
-            "packaging"
+            "inspect"
         ],
         "modelscope.trainers.lrscheduler.warmup.base": [
             "torch"
         ],
         "modelscope.trainers.lrscheduler.warmup.warmup": [],
         "modelscope.trainers.multi_modal.clip.clip_trainer": [
+            "os",
             "torch",
-            "typing",
             "math",
-            "os"
+            "typing"
         ],
         "modelscope.trainers.multi_modal.clip.clip_trainer_utils": [
-            "math",
-            "torch",
             "functools",
             "os",
+            "math",
+            "torch",
             "inspect"
         ],
         "modelscope.trainers.multi_modal.custom_diffusion.custom_diffusion_trainer": [
-            "diffusers",
-            "numpy",
-            "itertools",
-            "torch",
-            "pathlib",
-            "PIL",
-            "torchvision",
-            "warnings",
             "hashlib",
+            "os",
+            "warnings",
+            "tqdm",
             "typing",
+            "pathlib",
+            "torch",
+            "diffusers",
+            "PIL",
+            "itertools",
             "random",
-            "os",
-            "json",
-            "tqdm"
+            "torchvision",
+            "numpy",
+            "json"
         ],
         "modelscope.trainers.multi_modal.dreambooth_diffusion.dreambooth_diffusion_trainer": [
-            "diffusers",
-            "itertools",
-            "torch",
-            "pathlib",
             "collections",
-            "PIL",
-            "torchvision",
-            "warnings",
             "hashlib",
+            "tqdm",
+            "warnings",
             "typing",
+            "pathlib",
+            "torch",
+            "diffusers",
+            "PIL",
+            "itertools",
             "shutil",
-            "tqdm"
+            "torchvision"
         ],
         "modelscope.trainers.multi_modal.efficient_diffusion_tuning.efficient_diffusion_tuning_trainer": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.trainers.multi_modal.lora_diffusion.lora_diffusion_trainer": [
-            "typing",
             "torch",
-            "diffusers"
+            "diffusers",
+            "typing"
         ],
         "modelscope.trainers.multi_modal.mgeo_ranking_trainer": [
             "dataclasses",
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.trainers.multi_modal.mplug.mplug_trainer": [
-            "typing",
+            "collections",
             "torch",
-            "collections"
+            "typing"
         ],
         "modelscope.trainers.multi_modal.ofa.ofa_trainer": [
-            "math",
+            "functools",
+            "os",
             "shutil",
+            "typing",
+            "math",
             "torch",
             "tempfile",
-            "os",
-            "functools",
-            "typing",
             "json"
         ],
         "modelscope.trainers.multi_modal.ofa.ofa_trainer_utils": [
-            "math",
+            "torch",
+            "transformers",
             "os",
             "shutil",
-            "transformers",
             "numpy",
-            "torch"
+            "math"
         ],
         "modelscope.trainers.multi_modal.stable_diffusion.stable_diffusion_trainer": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.trainers.multi_modal.team.team_trainer": [
-            "numpy",
-            "torch",
             "collections",
-            "typing",
             "sklearn",
-            "os"
+            "os",
+            "typing",
+            "torch",
+            "numpy"
         ],
         "modelscope.trainers.multi_modal.team.team_trainer_utils": [
+            "torch",
             "torchvision",
-            "PIL",
-            "torch"
+            "PIL"
         ],
         "modelscope.trainers.nlp.csanmt_translation_trainer": [
-            "typing",
-            "tensorflow",
+            "os",
             "time",
-            "os"
+            "tensorflow",
+            "typing"
         ],
         "modelscope.trainers.nlp.document_grounded_dialog_generate_trainer": [
-            "re",
-            "torch",
             "collections",
             "sacrebleu",
-            "rouge",
             "string",
-            "transformers",
             "os",
-            "json",
-            "tqdm"
+            "tqdm",
+            "transformers",
+            "rouge",
+            "torch",
+            "re",
+            "json"
         ],
         "modelscope.trainers.nlp.document_grounded_dialog_rerank_trainer": [
-            "numpy",
-            "torch",
-            "time",
+            "os",
+            "transformers",
             "typing",
+            "time",
+            "torch",
             "random",
-            "transformers",
-            "os"
+            "numpy"
         ],
         "modelscope.trainers.nlp.document_grounded_dialog_retrieval_trainer": [
+            "os",
+            "tqdm",
+            "transformers",
+            "torch",
             "faiss",
             "numpy",
-            "torch",
-            "transformers",
-            "os",
-            "json",
-            "tqdm"
+            "json"
         ],
         "modelscope.trainers.nlp.faq_question_answering_trainer": [
-            "distutils",
-            "dataclasses",
-            "torch",
             "collections",
+            "functools",
+            "distutils",
             "contextlib",
             "typing",
-            "functools",
+            "torch",
+            "dataclasses",
             "numpy"
         ],
         "modelscope.trainers.nlp.gpt3_trainer": [
-            "typing",
-            "torch",
+            "os",
             "copy",
-            "os"
+            "torch",
+            "typing"
         ],
         "modelscope.trainers.nlp.gpt_moe_trainer": [
             "megatron_util",
-            "torch",
             "collections",
+            "os",
             "typing",
-            "os"
+            "torch"
         ],
         "modelscope.trainers.nlp.plug_trainer": [
             "megatron_util",
             "deepspeed",
-            "torch",
+            "os",
             "typing",
-            "os"
+            "torch"
         ],
         "modelscope.trainers.nlp.sentence_embedding_trainer": [
-            "numpy",
-            "dataclasses",
-            "torch",
-            "time",
+            "tqdm",
             "transformers",
             "typing",
-            "tqdm"
+            "time",
+            "torch",
+            "dataclasses",
+            "numpy"
         ],
         "modelscope.trainers.nlp.sequence_classification_trainer": [
-            "typing",
             "time",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.trainers.nlp.siamese_uie_trainer": [
-            "math",
-            "torch",
             "collections",
-            "json",
-            "time",
+            "os",
             "typing",
+            "time",
+            "math",
+            "torch",
             "random",
             "numpy",
-            "os"
+            "json"
         ],
         "modelscope.trainers.nlp.space.dialog_intent_trainer": [
-            "typing",
             "os",
-            "numpy"
+            "numpy",
+            "typing"
         ],
         "modelscope.trainers.nlp.space.dialog_modeling_trainer": [
-            "typing",
-            "numpy",
             "os",
-            "time"
+            "time",
+            "numpy",
+            "typing"
         ],
         "modelscope.trainers.nlp.space.eval": [
-            "math",
-            "nltk",
-            "sklearn",
             "collections",
+            "sklearn",
+            "nltk",
+            "math",
             "numpy",
             "json"
         ],
         "modelscope.trainers.nlp.space.metrics.metrics_tracker": [
-            "math",
-            "collections"
+            "collections",
+            "math"
         ],
         "modelscope.trainers.nlp.space.trainer.gen_trainer": [
-            "numpy",
-            "torch",
             "collections",
-            "time",
-            "transformers",
             "os",
-            "json",
-            "tqdm"
+            "tqdm",
+            "transformers",
+            "time",
+            "torch",
+            "numpy",
+            "json"
         ],
         "modelscope.trainers.nlp.space.trainer.intent_trainer": [
-            "numpy",
-            "torch",
             "collections",
-            "time",
-            "transformers",
             "os",
-            "json",
-            "tqdm"
+            "tqdm",
+            "transformers",
+            "time",
+            "torch",
+            "numpy",
+            "json"
         ],
         "modelscope.trainers.nlp.table_question_answering_trainer": [
-            "numpy",
-            "torch",
-            "time",
-            "typing",
             "os",
-            "json",
-            "tqdm"
+            "tqdm",
+            "typing",
+            "time",
+            "torch",
+            "numpy",
+            "json"
         ],
         "modelscope.trainers.nlp.text_generation_trainer": [
-            "typing",
-            "torch"
+            "torch",
+            "typing"
         ],
         "modelscope.trainers.nlp.text_ranking_trainer": [
-            "numpy",
-            "dataclasses",
-            "torch",
-            "time",
+            "tqdm",
             "typing",
-            "tqdm"
+            "time",
+            "torch",
+            "dataclasses",
+            "numpy"
         ],
         "modelscope.trainers.nlp.translation_evaluation_trainer": [
+            "os",
+            "tqdm",
+            "transformers",
+            "typing",
             "math",
-            "pandas",
             "torch",
             "random",
-            "transformers",
-            "typing",
-            "os",
-            "tqdm"
+            "pandas"
         ],
         "modelscope.trainers.nlp_trainer": [
-            "typing",
-            "numpy",
+            "os",
             "torch",
-            "os"
+            "numpy",
+            "typing"
         ],
         "modelscope.trainers.optimizer.builder": [
+            "torch",
             "inspect",
-            "typing",
-            "torch"
+            "typing"
         ],
         "modelscope.trainers.parallel.builder": [
             "torch"
         ],
         "modelscope.trainers.parallel.utils": [],
         "modelscope.trainers.trainer": [
-            "distutils",
-            "torch",
             "collections",
-            "inspect",
+            "functools",
+            "distutils",
             "os",
             "copy",
-            "functools",
             "typing",
+            "torch",
+            "inspect",
             "json"
         ],
         "modelscope.trainers.training_args": [
-            "re",
-            "dataclasses",
             "addict",
             "copy",
             "typing",
+            "re",
+            "dataclasses",
             "json"
         ],
         "modelscope.trainers.utils.inference": [
-            "torch",
             "collections",
             "logging",
-            "pickle",
-            "shutil",
             "os",
-            "tqdm"
+            "tqdm",
+            "pickle",
+            "torch",
+            "shutil"
         ],
         "modelscope.trainers.utils.log_buffer": [
-            "numpy",
-            "collections"
+            "collections",
+            "numpy"
         ]
     },
-    "version": "1.8.0"
+    "version": "1.8.0rc0"
 }
```

### Comparing `modelscope-1.8.0/modelscope/utils/ast_utils.py` & `modelscope-1.8.0rc0/modelscope/utils/ast_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/audio/audio_utils.py` & `modelscope-1.8.0rc0/modelscope/utils/audio/audio_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/audio/tts_exceptions.py` & `modelscope-1.8.0rc0/modelscope/utils/audio/tts_exceptions.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/checkpoint.py` & `modelscope-1.8.0rc0/modelscope/utils/checkpoint.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/chinese_utils.py` & `modelscope-1.8.0rc0/modelscope/utils/chinese_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/config.py` & `modelscope-1.8.0rc0/modelscope/utils/config.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/config_ds.py` & `modelscope-1.8.0rc0/modelscope/utils/config_ds.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/constant.py` & `modelscope-1.8.0rc0/modelscope/utils/constant.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/cv/image_utils.py` & `modelscope-1.8.0rc0/modelscope/utils/cv/image_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/cv/motion_utils/motion_process.py` & `modelscope-1.8.0rc0/modelscope/utils/cv/motion_utils/motion_process.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/cv/motion_utils/plot_script.py` & `modelscope-1.8.0rc0/modelscope/utils/cv/motion_utils/plot_script.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/cv/motion_utils/rotation_conversions.py` & `modelscope-1.8.0rc0/modelscope/utils/cv/motion_utils/rotation_conversions.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/data_collators.py` & `modelscope-1.8.0rc0/modelscope/utils/data_collators.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/data_utils.py` & `modelscope-1.8.0rc0/modelscope/utils/data_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/device.py` & `modelscope-1.8.0rc0/modelscope/utils/device.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/error.py` & `modelscope-1.8.0rc0/modelscope/utils/error.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/file_utils.py` & `modelscope-1.8.0rc0/modelscope/utils/file_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/hf_util.py` & `modelscope-1.8.0rc0/modelscope/utils/hf_util.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/hub.py` & `modelscope-1.8.0rc0/modelscope/utils/hub.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/import_utils.py` & `modelscope-1.8.0rc0/modelscope/utils/import_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/input_output.py` & `modelscope-1.8.0rc0/modelscope/utils/input_output.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/logger.py` & `modelscope-1.8.0rc0/modelscope/utils/logger.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/megatron_utils.py` & `modelscope-1.8.0rc0/modelscope/utils/megatron_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/metric.py` & `modelscope-1.8.0rc0/modelscope/utils/metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/model_tag.py` & `modelscope-1.8.0rc0/modelscope/utils/model_tag.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/nlp/distributed.py` & `modelscope-1.8.0rc0/modelscope/utils/nlp/distributed.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/nlp/load_checkpoint.py` & `modelscope-1.8.0rc0/modelscope/utils/nlp/load_checkpoint.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/nlp/space/args.py` & `modelscope-1.8.0rc0/modelscope/utils/nlp/space/args.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/nlp/space/clean_dataset.py` & `modelscope-1.8.0rc0/modelscope/utils/nlp/space/clean_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/nlp/space/criterions.py` & `modelscope-1.8.0rc0/modelscope/utils/nlp/space/criterions.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/nlp/space/db_ops.py` & `modelscope-1.8.0rc0/modelscope/utils/nlp/space/db_ops.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/nlp/space/ontology.py` & `modelscope-1.8.0rc0/modelscope/utils/nlp/space/ontology.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/nlp/space/utils.py` & `modelscope-1.8.0rc0/modelscope/utils/nlp/space/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/nlp/space/utils_dst.py` & `modelscope-1.8.0rc0/modelscope/utils/nlp/space/utils_dst.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/nlp/space_T_en/utils.py` & `modelscope-1.8.0rc0/modelscope/utils/nlp/space_T_en/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/nlp/utils.py` & `modelscope-1.8.0rc0/modelscope/utils/nlp/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/plugins.py` & `modelscope-1.8.0rc0/modelscope/utils/plugins.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/pre_compile.py` & `modelscope-1.8.0rc0/modelscope/utils/pre_compile.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/registry.py` & `modelscope-1.8.0rc0/modelscope/utils/registry.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/regress_test_utils.py` & `modelscope-1.8.0rc0/modelscope/utils/regress_test_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/service_utils.py` & `modelscope-1.8.0rc0/modelscope/utils/service_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/streaming_output.py` & `modelscope-1.8.0rc0/modelscope/utils/streaming_output.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/task_utils.py` & `modelscope-1.8.0rc0/modelscope/utils/task_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/tensor_utils.py` & `modelscope-1.8.0rc0/modelscope/utils/tensor_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/timer.py` & `modelscope-1.8.0rc0/modelscope/utils/timer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/torch_utils.py` & `modelscope-1.8.0rc0/modelscope/utils/torch_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/trie.py` & `modelscope-1.8.0rc0/modelscope/utils/trie.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/type_assert.py` & `modelscope-1.8.0rc0/modelscope/utils/type_assert.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope/utils/url_utils.py` & `modelscope-1.8.0rc0/modelscope/utils/url_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.8.0/modelscope.egg-info/PKG-INFO` & `modelscope-1.8.0rc0/modelscope.egg-info/PKG-INFO`

 * *Files 19% similar despite different names*

```diff
@@ -1,16 +1,318 @@
 Metadata-Version: 2.1
 Name: modelscope
-Version: 1.8.0
+Version: 1.8.0rc0
 Summary: ModelScope: bring the notion of Model-as-a-Service to life.
 Home-page: https://github.com/modelscope/modelscope
 Author: ModelScope team
 Author-email: contact@modelscope.cn
 License: Apache License 2.0
+Description: 
+        <p align="center">
+            <br>
+            <img src="https://modelscope.oss-cn-beijing.aliyuncs.com/modelscope.gif" width="400"/>
+            <br>
+        <p>
+        
+        <div align="center">
+        
+        [![PyPI](https://img.shields.io/pypi/v/modelscope)](https://pypi.org/project/modelscope/)
+        <!-- [![Documentation Status](https://readthedocs.org/projects/easy-cv/badge/?version=latest)](https://easy-cv.readthedocs.io/en/latest/) -->
+        [![license](https://img.shields.io/github/license/modelscope/modelscope.svg)](https://github.com/modelscope/modelscope/blob/master/LICENSE)
+        [![open issues](https://isitmaintained.com/badge/open/modelscope/modelscope.svg)](https://github.com/modelscope/modelscope/issues)
+        [![GitHub pull-requests](https://img.shields.io/github/issues-pr/modelscope/modelscope.svg)](https://GitHub.com/modelscope/modelscope/pull/)
+        [![GitHub latest commit](https://badgen.net/github/last-commit/modelscope/modelscope)](https://GitHub.com/modelscope/modelscope/commit/)
+        [![Leaderboard](https://img.shields.io/badge/ModelScope-Check%20Your%20Contribution-orange)](https://opensource.alibaba.com/contribution_leaderboard/details?projectValue=modelscope)
+        
+        <!-- [![GitHub contributors](https://img.shields.io/github/contributors/modelscope/modelscope.svg)](https://GitHub.com/modelscope/modelscope/graphs/contributors/) -->
+        <!-- [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com) -->
+        
+        <h4 align="center">
+            <p>
+                <b>English</b> |
+                <a href="https://github.com/modelscope/modelscope/blob/master/README_zh.md">中文</a> |
+                <a href="https://github.com/modelscope/modelscope/blob/master/README_ja.md">日本語</a>
+            <p>
+        </h4>
+        
+        
+        </div>
+        
+        # Introduction
+        
+        [ModelScope]( https://www.modelscope.cn) is built upon the notion of “Model-as-a-Service” (MaaS). It seeks to bring together most advanced machine learning models from the AI community, and streamlines the process of leveraging AI models in real-world applications. The core ModelScope library open-sourced in this repository provides the interfaces and implementations that allow developers to perform  model inference, training and evaluation.
+        
+        
+        In particular, with rich layers of API-abstraction, the ModelScope library offers unified experience to explore state-of-the-art models spanning across domains such as CV, NLP, Speech, Multi-Modality, and Scientific-computation. Model contributors of different areas can integrate models into the ModelScope ecosystem through the layered-APIs, allowing easy and unified access to their models. Once integrated, model inference, fine-tuning, and evaluations can be done with only a few lines of codes. In the meantime, flexibilities are also provided so that different components in the model applications can be customized wherever necessary.
+        
+        Apart from harboring implementations of a wide range of different models, ModelScope library also enables the necessary interactions with ModelScope backend services, particularly with the Model-Hub and Dataset-Hub. Such interactions facilitate management of  various entities (models and datasets) to be performed seamlessly under-the-hood, including entity lookup, version control, cache management, and many others.
+        
+        # Models and Online Accessibility
+        
+        Hundreds of models are made publicly available on [ModelScope]( https://www.modelscope.cn)  (700+ and counting), covering the latest development in areas such as NLP, CV, Audio, Multi-modality, and AI for Science, etc. Many of these models represent the SOTA in their specific fields, and made their open-sourced debut on ModelScope. Users can visit ModelScope([modelscope.cn](http://www.modelscope.cn)) and experience first-hand how these models perform via online experience, with just a few clicks. Immediate developer-experience is also possible through the ModelScope Notebook, which is backed by ready-to-use CPU/GPU development environment in the cloud - only one click away on [ModelScope](https://www.modelscope.cn).
+        
+        
+        <p align="center">
+            <br>
+            <img src="data/resource/inference.gif" width="1024"/>
+            <br>
+        <p>
+        
+        Some representative examples include:
+        
+        NLP:
+        
+        * [nlp_gpt3_text-generation_2.7B](https://modelscope.cn/models/damo/nlp_gpt3_text-generation_2.7B)
+        
+        * [ChatYuan-large](https://modelscope.cn/models/ClueAI/ChatYuan-large)
+        
+        * [mengzi-t5-base](https://modelscope.cn/models/langboat/mengzi-t5-base)
+        
+        * [nlp_csanmt_translation_en2zh](https://modelscope.cn/models/damo/nlp_csanmt_translation_en2zh)
+        
+        * [nlp_raner_named-entity-recognition_chinese-base-news](https://modelscope.cn/models/damo/nlp_raner_named-entity-recognition_chinese-base-news)
+        
+        * [nlp_structbert_word-segmentation_chinese-base](https://modelscope.cn/models/damo/nlp_structbert_word-segmentation_chinese-base)
+        
+        * [Erlangshen-RoBERTa-330M-Sentiment](https://modelscope.cn/models/fengshenbang/Erlangshen-RoBERTa-330M-Sentiment)
+        
+        * [nlp_convai_text2sql_pretrain_cn](https://modelscope.cn/models/damo/nlp_convai_text2sql_pretrain_cn)
+        
+        Multi-Modal:
+        
+        * [multi-modal_clip-vit-base-patch16_zh](https://modelscope.cn/models/damo/multi-modal_clip-vit-base-patch16_zh)
+        
+        * [ofa_pretrain_base_zh](https://modelscope.cn/models/damo/ofa_pretrain_base_zh)
+        
+        * [Taiyi-Stable-Diffusion-1B-Chinese-v0.1](https://modelscope.cn/models/fengshenbang/Taiyi-Stable-Diffusion-1B-Chinese-v0.1)
+        
+        * [mplug_visual-question-answering_coco_large_en](https://modelscope.cn/models/damo/mplug_visual-question-answering_coco_large_en)
+        
+        CV:
+        
+        * [cv_controlnet_controllable-image-generation_nine-annotators](https://modelscope.cn/models/dienstag/cv_controlnet_controllable-image-generation_nine-annotators/summary)
+        
+        * [cv_tinynas_object-detection_damoyolo](https://modelscope.cn/models/damo/cv_tinynas_object-detection_damoyolo)
+        
+        * [cv_unet_person-image-cartoon_compound-models](https://modelscope.cn/models/damo/cv_unet_person-image-cartoon_compound-models)
+        
+        * [cv_convnextTiny_ocr-recognition-general_damo](https://modelscope.cn/models/damo/cv_convnextTiny_ocr-recognition-general_damo)
+        
+        * [cv_resnet18_human-detection](https://modelscope.cn/models/damo/cv_resnet18_human-detection)
+        
+        * [cv_resnet50_face-detection_retinaface](https://modelscope.cn/models/damo/cv_resnet50_face-detection_retinaface)
+        
+        * [cv_unet_image-matting](https://modelscope.cn/models/damo/cv_unet_image-matting)
+        
+        * [cv_F3Net_product-segmentation](https://modelscope.cn/models/damo/cv_F3Net_product-segmentation)
+        
+        * [cv_resnest101_general_recognition](https://modelscope.cn/models/damo/cv_resnest101_general_recognition)
+        
+        
+        Audio:
+        
+        * [speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch](https://modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch)
+        
+        * [speech_sambert-hifigan_tts_zh-cn_16k](https://modelscope.cn/models/damo/speech_sambert-hifigan_tts_zh-cn_16k)
+        
+        * [speech_charctc_kws_phone-xiaoyun](https://modelscope.cn/models/damo/speech_charctc_kws_phone-xiaoyun)
+        
+        * [u2pp_conformer-asr-cn-16k-online](https://modelscope.cn/models/wenet/u2pp_conformer-asr-cn-16k-online)
+        
+        * [speech_fsmn_vad_zh-cn-16k-common-pytorch](https://modelscope.cn/models/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch/summary)
+        
+        * [punc_ct-transformer_zh-cn-common-vocab272727-pytorch](https://modelscope.cn/models/damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch/summary)
+        
+        * [speech_frcrn_ans_cirm_16k](https://modelscope.cn/models/damo/speech_frcrn_ans_cirm_16k)
+        
+        * [speech_dfsmn_aec_psm_16k](https://modelscope.cn/models/damo/speech_dfsmn_aec_psm_16k)
+        
+        
+        
+        AI for Science:
+        
+        * [uni-fold-monomer](https://modelscope.cn/models/DPTech/uni-fold-monomer/summary)
+        
+        * [uni-fold-multimer](https://modelscope.cn/models/DPTech/uni-fold-multimer/summary)
+        
+        **Note:** Most models on ModelScope are public and can be downloaded without account registration on modelscope website([www.modelscope.cn](www.modelscope.cn)), please refer to instructions for [model download](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8B%E8%BD%BD), for dowloading models with api provided by modelscope library or git.
+        
+        # QuickTour
+        
+        We provide unified interface for inference using `pipeline`, fine-tuning and evaluation using `Trainer` for different tasks.
+        
+        For any given task with any type of input (image, text, audio, video...), inference pipeline can be implemented with only a few lines of code, which will automatically load the underlying model to get inference result, as is exemplified below:
+        
+        ```python
+        >>> from modelscope.pipelines import pipeline
+        >>> word_segmentation = pipeline('word-segmentation',model='damo/nlp_structbert_word-segmentation_chinese-base')
+        >>> word_segmentation('今天天气不错，适合出去游玩')
+        {'output': '今天 天气 不错 ， 适合 出去 游玩'}
+        ```
+        
+        Given an image, portrait matting (aka. background-removal) can be accomplished with the following code snippet:
+        
+        ![image](data/resource/portrait_input.png)
+        
+        ```python
+        >>> import cv2
+        >>> from modelscope.pipelines import pipeline
+        
+        >>> portrait_matting = pipeline('portrait-matting')
+        >>> result = portrait_matting('https://modelscope.oss-cn-beijing.aliyuncs.com/test/images/image_matting.png')
+        >>> cv2.imwrite('result.png', result['output_img'])
+        ```
+        
+        The output image with the background removed is:
+        ![image](data/resource/portrait_output.png)
+        
+        
+        Fine-tuning and evaluation can also be done with a few more lines of code to set up training dataset and trainer, with the heavy-lifting work of training and evaluation a model encapsulated in the implementation of  `traner.train()` and
+        `trainer.evaluate()`  interfaces.
+        
+        For example, the gpt3 base model (1.3B) can be fine-tuned with the chinese-poetry dataset, resulting in a model that can be used for chinese-poetry generation.
+        
+        ```python
+        >>> from modelscope.metainfo import Trainers
+        >>> from modelscope.msdatasets import MsDataset
+        >>> from modelscope.trainers import build_trainer
+        
+        >>> train_dataset = MsDataset.load('chinese-poetry-collection', split='train'). remap_columns({'text1': 'src_txt'})
+        >>> eval_dataset = MsDataset.load('chinese-poetry-collection', split='test').remap_columns({'text1': 'src_txt'})
+        >>> max_epochs = 10
+        >>> tmp_dir = './gpt3_poetry'
+        
+        >>> kwargs = dict(
+             model='damo/nlp_gpt3_text-generation_1.3B',
+             train_dataset=train_dataset,
+             eval_dataset=eval_dataset,
+             max_epochs=max_epochs,
+             work_dir=tmp_dir)
+        
+        >>> trainer = build_trainer(name=Trainers.gpt3_trainer, default_args=kwargs)
+        >>> trainer.train()
+        ```
+        
+        # Why should I use ModelScope library
+        
+        1. A unified and concise user interface is abstracted for different tasks and different models. Model inferences and training can be implemented by as few as 3 and 10 lines of code, respectively. It is convenient for users to explore models in different fields in the ModelScope community. All models integrated into ModelScope are ready to use, which makes it easy to get started with AI, in both educational and industrial settings.
+        
+        2. ModelScope offers a model-centric development and application experience. It streamlines the support for model training, inference, export and deployment, and facilitates users to build their own MLOps based on the ModelScope ecosystem.
+        
+        3. For the model inference and training process, a modular design is put in place, and a wealth of functional module implementations are provided, which is convenient for users to customize their own model inference, training and other processes.
+        
+        4. For distributed model training, especially for large models, it provides rich training strategy support, including data parallel, model parallel, hybrid parallel and so on.
+        
+        # Installation
+        
+        ## Docker
+        
+        ModelScope Library currently supports popular deep learning framework for model training and inference, including PyTorch, TensorFlow and ONNX. All releases are tested and run on Python 3.7+, Pytorch 1.8+, Tensorflow1.15 or Tensorflow2.0+.
+        
+        To allow out-of-box usage for all the models on ModelScope, official docker images are provided for all releases. Based on the docker image, developers can skip all environment installation and configuration and use it directly. Currently, the latest version of the CPU image and GPU image can be obtained from:
+        
+        CPU docker image
+        ```shell
+        # py37
+        registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-py37-torch1.11.0-tf1.15.5-1.6.1
+        
+        # py38
+        registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-py38-torch1.11.0-tf1.15.5-1.6.1
+        ```
+        
+        GPU docker image
+        ```shell
+        # py37
+        registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.3.0-py37-torch1.11.0-tf1.15.5-1.6.1
+        
+        # py38
+        registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.3.0-py38-torch1.11.0-tf1.15.5-1.6.1
+        ```
+        
+        ## Setup Local Python Environment
+        
+        One can also set up local ModelScope environment using pip and conda.  We suggest [anaconda](https://docs.anaconda.com/anaconda/install/) for creating local python environment:
+        
+        ```shell
+        conda create -n modelscope python=3.7
+        conda activate modelscope
+        ```
+        
+        PyTorch or TensorFlow can be installed separately according to each model's requirements.
+        * Install pytorch [doc](https://pytorch.org/get-started/locally/)
+        * Install tensorflow [doc](https://www.tensorflow.org/install/pip)
+        
+        After installing the necessary machine-learning framework, you can install modelscope library as follows:
+        
+        If you only want to play around with the modelscope framework, of trying out model/dataset download, you can install the core modelscope components:
+        ```shell
+        pip install modelscope
+        ```
+        
+        If you want to use multi-modal models:
+        ```shell
+        pip install modelscope[multi-modal]
+        ```
+        
+        If you want to use nlp models:
+        ```shell
+        pip install modelscope[nlp] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html
+        ```
+        
+        If you want to use cv models:
+        ```shell
+        pip install modelscope[cv] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html
+        ```
+        
+        If you want to use audio models:
+        ```shell
+        pip install modelscope[audio] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html
+        ```
+        
+        If you want to use science models:
+        ```shell
+        pip install modelscope[science] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html
+        ```
+        
+        `Notes`:
+        1. Currently, some audio-task models only support python3.7, tensorflow1.15.4 Linux environments. Most other models can be installed and used on Windows and Mac (x86).
+        
+        2. Some models in the audio field use the third-party library SoundFile for wav file processing. On the Linux system, users need to manually install libsndfile of SoundFile([doc link](https://github.com/bastibe/python-soundfile#installation)). On Windows and MacOS, it will be installed automatically without user operation. For example, on Ubuntu, you can use following commands:
+            ```shell
+            sudo apt-get update
+            sudo apt-get install libsndfile1
+            ```
+        
+        3. Some models in computer vision need mmcv-full, you can refer to mmcv [installation guide](https://github.com/open-mmlab/mmcv#installation), a minimal installation is as follows:
+        
+            ```shell
+            pip uninstall mmcv # if you have installed mmcv, uninstall it
+            pip install -U openmim
+            mim install mmcv-full
+            ```
+        
+        
+        
+        # Learn More
+        
+        We  provide additional documentations including:
+        * [More detailed Installation Guide](https://modelscope.cn/docs/%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85)
+        * [Introduction to tasks](https://modelscope.cn/docs/%E4%BB%BB%E5%8A%A1%E7%9A%84%E4%BB%8B%E7%BB%8D)
+        * [Use pipeline for model inference](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8E%A8%E7%90%86Pipeline)
+        * [Finetuning example](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83Train)
+        * [Preprocessing of data](https://modelscope.cn/docs/%E6%95%B0%E6%8D%AE%E7%9A%84%E9%A2%84%E5%A4%84%E7%90%86)
+        * [Evaluation](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0)
+        * [Contribute your own model to ModelScope](https://modelscope.cn/docs/ModelScope%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%85%A5%E6%B5%81%E7%A8%8B%E6%A6%82%E8%A7%88)
+        
+        # License
+        
+        This project is licensed under the [Apache License (Version 2.0)](https://github.com/modelscope/modelscope/blob/master/LICENSE).
+        
 Keywords: python,nlp,science,cv,speech,multi-modal
+Platform: UNKNOWN
 Classifier: Development Status :: 4 - Beta
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Operating System :: OS Independent
 Classifier: Programming Language :: Python :: 3
 Classifier: Programming Language :: Python :: 3.7
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
@@ -22,308 +324,7 @@
 Provides-Extra: nlp
 Provides-Extra: science
 Provides-Extra: audio_asr
 Provides-Extra: audio_kws
 Provides-Extra: audio_signal
 Provides-Extra: audio_tts
 Provides-Extra: all
-
-
-<p align="center">
-    <br>
-    <img src="https://modelscope.oss-cn-beijing.aliyuncs.com/modelscope.gif" width="400"/>
-    <br>
-<p>
-
-<div align="center">
-
-[![PyPI](https://img.shields.io/pypi/v/modelscope)](https://pypi.org/project/modelscope/)
-<!-- [![Documentation Status](https://readthedocs.org/projects/easy-cv/badge/?version=latest)](https://easy-cv.readthedocs.io/en/latest/) -->
-[![license](https://img.shields.io/github/license/modelscope/modelscope.svg)](https://github.com/modelscope/modelscope/blob/master/LICENSE)
-[![open issues](https://isitmaintained.com/badge/open/modelscope/modelscope.svg)](https://github.com/modelscope/modelscope/issues)
-[![GitHub pull-requests](https://img.shields.io/github/issues-pr/modelscope/modelscope.svg)](https://GitHub.com/modelscope/modelscope/pull/)
-[![GitHub latest commit](https://badgen.net/github/last-commit/modelscope/modelscope)](https://GitHub.com/modelscope/modelscope/commit/)
-[![Leaderboard](https://img.shields.io/badge/ModelScope-Check%20Your%20Contribution-orange)](https://opensource.alibaba.com/contribution_leaderboard/details?projectValue=modelscope)
-
-<!-- [![GitHub contributors](https://img.shields.io/github/contributors/modelscope/modelscope.svg)](https://GitHub.com/modelscope/modelscope/graphs/contributors/) -->
-<!-- [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com) -->
-
-<h4 align="center">
-    <p>
-        <b>English</b> |
-        <a href="https://github.com/modelscope/modelscope/blob/master/README_zh.md">中文</a> |
-        <a href="https://github.com/modelscope/modelscope/blob/master/README_ja.md">日本語</a>
-    <p>
-</h4>
-
-
-</div>
-
-# Introduction
-
-[ModelScope]( https://www.modelscope.cn) is built upon the notion of “Model-as-a-Service” (MaaS). It seeks to bring together most advanced machine learning models from the AI community, and streamlines the process of leveraging AI models in real-world applications. The core ModelScope library open-sourced in this repository provides the interfaces and implementations that allow developers to perform  model inference, training and evaluation.
-
-
-In particular, with rich layers of API-abstraction, the ModelScope library offers unified experience to explore state-of-the-art models spanning across domains such as CV, NLP, Speech, Multi-Modality, and Scientific-computation. Model contributors of different areas can integrate models into the ModelScope ecosystem through the layered-APIs, allowing easy and unified access to their models. Once integrated, model inference, fine-tuning, and evaluations can be done with only a few lines of codes. In the meantime, flexibilities are also provided so that different components in the model applications can be customized wherever necessary.
-
-Apart from harboring implementations of a wide range of different models, ModelScope library also enables the necessary interactions with ModelScope backend services, particularly with the Model-Hub and Dataset-Hub. Such interactions facilitate management of  various entities (models and datasets) to be performed seamlessly under-the-hood, including entity lookup, version control, cache management, and many others.
-
-# Models and Online Accessibility
-
-Hundreds of models are made publicly available on [ModelScope]( https://www.modelscope.cn)  (700+ and counting), covering the latest development in areas such as NLP, CV, Audio, Multi-modality, and AI for Science, etc. Many of these models represent the SOTA in their specific fields, and made their open-sourced debut on ModelScope. Users can visit ModelScope([modelscope.cn](http://www.modelscope.cn)) and experience first-hand how these models perform via online experience, with just a few clicks. Immediate developer-experience is also possible through the ModelScope Notebook, which is backed by ready-to-use CPU/GPU development environment in the cloud - only one click away on [ModelScope](https://www.modelscope.cn).
-
-
-<p align="center">
-    <br>
-    <img src="data/resource/inference.gif" width="1024"/>
-    <br>
-<p>
-
-Some representative examples include:
-
-NLP:
-
-* [nlp_gpt3_text-generation_2.7B](https://modelscope.cn/models/damo/nlp_gpt3_text-generation_2.7B)
-
-* [ChatYuan-large](https://modelscope.cn/models/ClueAI/ChatYuan-large)
-
-* [mengzi-t5-base](https://modelscope.cn/models/langboat/mengzi-t5-base)
-
-* [nlp_csanmt_translation_en2zh](https://modelscope.cn/models/damo/nlp_csanmt_translation_en2zh)
-
-* [nlp_raner_named-entity-recognition_chinese-base-news](https://modelscope.cn/models/damo/nlp_raner_named-entity-recognition_chinese-base-news)
-
-* [nlp_structbert_word-segmentation_chinese-base](https://modelscope.cn/models/damo/nlp_structbert_word-segmentation_chinese-base)
-
-* [Erlangshen-RoBERTa-330M-Sentiment](https://modelscope.cn/models/fengshenbang/Erlangshen-RoBERTa-330M-Sentiment)
-
-* [nlp_convai_text2sql_pretrain_cn](https://modelscope.cn/models/damo/nlp_convai_text2sql_pretrain_cn)
-
-Multi-Modal:
-
-* [multi-modal_clip-vit-base-patch16_zh](https://modelscope.cn/models/damo/multi-modal_clip-vit-base-patch16_zh)
-
-* [ofa_pretrain_base_zh](https://modelscope.cn/models/damo/ofa_pretrain_base_zh)
-
-* [Taiyi-Stable-Diffusion-1B-Chinese-v0.1](https://modelscope.cn/models/fengshenbang/Taiyi-Stable-Diffusion-1B-Chinese-v0.1)
-
-* [mplug_visual-question-answering_coco_large_en](https://modelscope.cn/models/damo/mplug_visual-question-answering_coco_large_en)
-
-CV:
-
-* [cv_controlnet_controllable-image-generation_nine-annotators](https://modelscope.cn/models/dienstag/cv_controlnet_controllable-image-generation_nine-annotators/summary)
-
-* [cv_tinynas_object-detection_damoyolo](https://modelscope.cn/models/damo/cv_tinynas_object-detection_damoyolo)
-
-* [cv_unet_person-image-cartoon_compound-models](https://modelscope.cn/models/damo/cv_unet_person-image-cartoon_compound-models)
-
-* [cv_convnextTiny_ocr-recognition-general_damo](https://modelscope.cn/models/damo/cv_convnextTiny_ocr-recognition-general_damo)
-
-* [cv_resnet18_human-detection](https://modelscope.cn/models/damo/cv_resnet18_human-detection)
-
-* [cv_resnet50_face-detection_retinaface](https://modelscope.cn/models/damo/cv_resnet50_face-detection_retinaface)
-
-* [cv_unet_image-matting](https://modelscope.cn/models/damo/cv_unet_image-matting)
-
-* [cv_F3Net_product-segmentation](https://modelscope.cn/models/damo/cv_F3Net_product-segmentation)
-
-* [cv_resnest101_general_recognition](https://modelscope.cn/models/damo/cv_resnest101_general_recognition)
-
-
-Audio:
-
-* [speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch](https://modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch)
-
-* [speech_sambert-hifigan_tts_zh-cn_16k](https://modelscope.cn/models/damo/speech_sambert-hifigan_tts_zh-cn_16k)
-
-* [speech_charctc_kws_phone-xiaoyun](https://modelscope.cn/models/damo/speech_charctc_kws_phone-xiaoyun)
-
-* [u2pp_conformer-asr-cn-16k-online](https://modelscope.cn/models/wenet/u2pp_conformer-asr-cn-16k-online)
-
-* [speech_fsmn_vad_zh-cn-16k-common-pytorch](https://modelscope.cn/models/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch/summary)
-
-* [punc_ct-transformer_zh-cn-common-vocab272727-pytorch](https://modelscope.cn/models/damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch/summary)
-
-* [speech_frcrn_ans_cirm_16k](https://modelscope.cn/models/damo/speech_frcrn_ans_cirm_16k)
-
-* [speech_dfsmn_aec_psm_16k](https://modelscope.cn/models/damo/speech_dfsmn_aec_psm_16k)
-
-
-
-AI for Science:
-
-* [uni-fold-monomer](https://modelscope.cn/models/DPTech/uni-fold-monomer/summary)
-
-* [uni-fold-multimer](https://modelscope.cn/models/DPTech/uni-fold-multimer/summary)
-
-**Note:** Most models on ModelScope are public and can be downloaded without account registration on modelscope website([www.modelscope.cn](www.modelscope.cn)), please refer to instructions for [model download](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8B%E8%BD%BD), for dowloading models with api provided by modelscope library or git.
-
-# QuickTour
-
-We provide unified interface for inference using `pipeline`, fine-tuning and evaluation using `Trainer` for different tasks.
-
-For any given task with any type of input (image, text, audio, video...), inference pipeline can be implemented with only a few lines of code, which will automatically load the underlying model to get inference result, as is exemplified below:
-
-```python
->>> from modelscope.pipelines import pipeline
->>> word_segmentation = pipeline('word-segmentation',model='damo/nlp_structbert_word-segmentation_chinese-base')
->>> word_segmentation('今天天气不错，适合出去游玩')
-{'output': '今天 天气 不错 ， 适合 出去 游玩'}
-```
-
-Given an image, portrait matting (aka. background-removal) can be accomplished with the following code snippet:
-
-![image](data/resource/portrait_input.png)
-
-```python
->>> import cv2
->>> from modelscope.pipelines import pipeline
-
->>> portrait_matting = pipeline('portrait-matting')
->>> result = portrait_matting('https://modelscope.oss-cn-beijing.aliyuncs.com/test/images/image_matting.png')
->>> cv2.imwrite('result.png', result['output_img'])
-```
-
-The output image with the background removed is:
-![image](data/resource/portrait_output.png)
-
-
-Fine-tuning and evaluation can also be done with a few more lines of code to set up training dataset and trainer, with the heavy-lifting work of training and evaluation a model encapsulated in the implementation of  `traner.train()` and
-`trainer.evaluate()`  interfaces.
-
-For example, the gpt3 base model (1.3B) can be fine-tuned with the chinese-poetry dataset, resulting in a model that can be used for chinese-poetry generation.
-
-```python
->>> from modelscope.metainfo import Trainers
->>> from modelscope.msdatasets import MsDataset
->>> from modelscope.trainers import build_trainer
-
->>> train_dataset = MsDataset.load('chinese-poetry-collection', split='train'). remap_columns({'text1': 'src_txt'})
->>> eval_dataset = MsDataset.load('chinese-poetry-collection', split='test').remap_columns({'text1': 'src_txt'})
->>> max_epochs = 10
->>> tmp_dir = './gpt3_poetry'
-
->>> kwargs = dict(
-     model='damo/nlp_gpt3_text-generation_1.3B',
-     train_dataset=train_dataset,
-     eval_dataset=eval_dataset,
-     max_epochs=max_epochs,
-     work_dir=tmp_dir)
-
->>> trainer = build_trainer(name=Trainers.gpt3_trainer, default_args=kwargs)
->>> trainer.train()
-```
-
-# Why should I use ModelScope library
-
-1. A unified and concise user interface is abstracted for different tasks and different models. Model inferences and training can be implemented by as few as 3 and 10 lines of code, respectively. It is convenient for users to explore models in different fields in the ModelScope community. All models integrated into ModelScope are ready to use, which makes it easy to get started with AI, in both educational and industrial settings.
-
-2. ModelScope offers a model-centric development and application experience. It streamlines the support for model training, inference, export and deployment, and facilitates users to build their own MLOps based on the ModelScope ecosystem.
-
-3. For the model inference and training process, a modular design is put in place, and a wealth of functional module implementations are provided, which is convenient for users to customize their own model inference, training and other processes.
-
-4. For distributed model training, especially for large models, it provides rich training strategy support, including data parallel, model parallel, hybrid parallel and so on.
-
-# Installation
-
-## Docker
-
-ModelScope Library currently supports popular deep learning framework for model training and inference, including PyTorch, TensorFlow and ONNX. All releases are tested and run on Python 3.7+, Pytorch 1.8+, Tensorflow1.15 or Tensorflow2.0+.
-
-To allow out-of-box usage for all the models on ModelScope, official docker images are provided for all releases. Based on the docker image, developers can skip all environment installation and configuration and use it directly. Currently, the latest version of the CPU image and GPU image can be obtained from:
-
-CPU docker image
-```shell
-# py37
-registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-py37-torch1.11.0-tf1.15.5-1.6.1
-
-# py38
-registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-py38-torch1.11.0-tf1.15.5-1.6.1
-```
-
-GPU docker image
-```shell
-# py37
-registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.3.0-py37-torch1.11.0-tf1.15.5-1.6.1
-
-# py38
-registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.3.0-py38-torch1.11.0-tf1.15.5-1.6.1
-```
-
-## Setup Local Python Environment
-
-One can also set up local ModelScope environment using pip and conda.  We suggest [anaconda](https://docs.anaconda.com/anaconda/install/) for creating local python environment:
-
-```shell
-conda create -n modelscope python=3.7
-conda activate modelscope
-```
-
-PyTorch or TensorFlow can be installed separately according to each model's requirements.
-* Install pytorch [doc](https://pytorch.org/get-started/locally/)
-* Install tensorflow [doc](https://www.tensorflow.org/install/pip)
-
-After installing the necessary machine-learning framework, you can install modelscope library as follows:
-
-If you only want to play around with the modelscope framework, of trying out model/dataset download, you can install the core modelscope components:
-```shell
-pip install modelscope
-```
-
-If you want to use multi-modal models:
-```shell
-pip install modelscope[multi-modal]
-```
-
-If you want to use nlp models:
-```shell
-pip install modelscope[nlp] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html
-```
-
-If you want to use cv models:
-```shell
-pip install modelscope[cv] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html
-```
-
-If you want to use audio models:
-```shell
-pip install modelscope[audio] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html
-```
-
-If you want to use science models:
-```shell
-pip install modelscope[science] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html
-```
-
-`Notes`:
-1. Currently, some audio-task models only support python3.7, tensorflow1.15.4 Linux environments. Most other models can be installed and used on Windows and Mac (x86).
-
-2. Some models in the audio field use the third-party library SoundFile for wav file processing. On the Linux system, users need to manually install libsndfile of SoundFile([doc link](https://github.com/bastibe/python-soundfile#installation)). On Windows and MacOS, it will be installed automatically without user operation. For example, on Ubuntu, you can use following commands:
-    ```shell
-    sudo apt-get update
-    sudo apt-get install libsndfile1
-    ```
-
-3. Some models in computer vision need mmcv-full, you can refer to mmcv [installation guide](https://github.com/open-mmlab/mmcv#installation), a minimal installation is as follows:
-
-    ```shell
-    pip uninstall mmcv # if you have installed mmcv, uninstall it
-    pip install -U openmim
-    mim install mmcv-full
-    ```
-
-
-
-# Learn More
-
-We  provide additional documentations including:
-* [More detailed Installation Guide](https://modelscope.cn/docs/%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85)
-* [Introduction to tasks](https://modelscope.cn/docs/%E4%BB%BB%E5%8A%A1%E7%9A%84%E4%BB%8B%E7%BB%8D)
-* [Use pipeline for model inference](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8E%A8%E7%90%86Pipeline)
-* [Finetuning example](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83Train)
-* [Preprocessing of data](https://modelscope.cn/docs/%E6%95%B0%E6%8D%AE%E7%9A%84%E9%A2%84%E5%A4%84%E7%90%86)
-* [Evaluation](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0)
-* [Contribute your own model to ModelScope](https://modelscope.cn/docs/ModelScope%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%85%A5%E6%B5%81%E7%A8%8B%E6%A6%82%E8%A7%88)
-
-# License
-
-This project is licensed under the [Apache License (Version 2.0)](https://github.com/modelscope/modelscope/blob/master/LICENSE).
```

#### html2text {}

```diff
@@ -1,21 +1,11 @@
-Metadata-Version: 2.1 Name: modelscope Version: 1.8.0 Summary: ModelScope:
+Metadata-Version: 2.1 Name: modelscope Version: 1.8.0rc0 Summary: ModelScope:
 bring the notion of Model-as-a-Service to life. Home-page: https://github.com/
 modelscope/modelscope Author: ModelScope team Author-email:
-contact@modelscope.cn License: Apache License 2.0 Keywords:
-python,nlp,science,cv,speech,multi-modal Classifier: Development Status :: 4 -
-Beta Classifier: License :: OSI Approved :: Apache Software License Classifier:
-Operating System :: OS Independent Classifier: Programming Language :: Python
-:: 3 Classifier: Programming Language :: Python :: 3.7 Classifier: Programming
-Language :: Python :: 3.8 Classifier: Programming Language :: Python :: 3.9
-Classifier: Programming Language :: Python :: 3.10 Description-Content-Type:
-text/markdown Provides-Extra: audio Provides-Extra: cv Provides-Extra: multi-
-modal Provides-Extra: nlp Provides-Extra: science Provides-Extra: audio_asr
-Provides-Extra: audio_kws Provides-Extra: audio_signal Provides-Extra:
-audio_tts Provides-Extra: all
+contact@modelscope.cn License: Apache License 2.0 Description:
 
        [https://modelscope.oss-cn-beijing.aliyuncs.com/modelscope.gif]
  [![PyPI](https://img.shields.io/pypi/v/modelscope)](https://pypi.org/project/
   modelscope/)  [![license](https://img.shields.io/github/license/modelscope/
 modelscope.svg)](https://github.com/modelscope/modelscope/blob/master/LICENSE)
        [![open issues](https://isitmaintained.com/badge/open/modelscope/
   modelscope.svg)](https://github.com/modelscope/modelscope/issues) [![GitHub
@@ -216,8 +206,18 @@
 (https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83Train)
 * [Preprocessing of data](https://modelscope.cn/docs/
 %E6%95%B0%E6%8D%AE%E7%9A%84%E9%A2%84%E5%A4%84%E7%90%86) * [Evaluation](https://
 modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0) * [Contribute
 your own model to ModelScope](https://modelscope.cn/docs/
 ModelScope%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%85%A5%E6%B5%81%E7%A8%8B%E6%A6%82%E8%A7%88)
 # License This project is licensed under the [Apache License (Version 2.0)]
-(https://github.com/modelscope/modelscope/blob/master/LICENSE).
+(https://github.com/modelscope/modelscope/blob/master/LICENSE). Keywords:
+python,nlp,science,cv,speech,multi-modal Platform: UNKNOWN Classifier:
+Development Status :: 4 - Beta Classifier: License :: OSI Approved :: Apache
+Software License Classifier: Operating System :: OS Independent Classifier:
+Programming Language :: Python :: 3 Classifier: Programming Language :: Python
+:: 3.7 Classifier: Programming Language :: Python :: 3.8 Classifier:
+Programming Language :: Python :: 3.9 Classifier: Programming Language ::
+Python :: 3.10 Description-Content-Type: text/markdown Provides-Extra: audio
+Provides-Extra: cv Provides-Extra: multi-modal Provides-Extra: nlp Provides-
+Extra: science Provides-Extra: audio_asr Provides-Extra: audio_kws Provides-
+Extra: audio_signal Provides-Extra: audio_tts Provides-Extra: all
```

### Comparing `modelscope-1.8.0/modelscope.egg-info/SOURCES.txt` & `modelscope-1.8.0rc0/modelscope.egg-info/SOURCES.txt`

 * *Files 0% similar despite different names*

```diff
@@ -1558,20 +1558,14 @@
 modelscope/models/nlp/polylm/text_generation.py
 modelscope/models/nlp/ponet/__init__.py
 modelscope/models/nlp/ponet/backbone.py
 modelscope/models/nlp/ponet/configuration.py
 modelscope/models/nlp/ponet/document_segmentation.py
 modelscope/models/nlp/ponet/fill_mask.py
 modelscope/models/nlp/ponet/tokenization.py
-modelscope/models/nlp/qwen/__init__.py
-modelscope/models/nlp/qwen/backbone.py
-modelscope/models/nlp/qwen/configuration.py
-modelscope/models/nlp/qwen/qwen_generation_utils.py
-modelscope/models/nlp/qwen/text_generation.py
-modelscope/models/nlp/qwen/tokenization.py
 modelscope/models/nlp/space/__init__.py
 modelscope/models/nlp/space/configuration.py
 modelscope/models/nlp/space/dialog_intent_prediction.py
 modelscope/models/nlp/space/dialog_modeling.py
 modelscope/models/nlp/space/dialog_state_tracking.py
 modelscope/models/nlp/space/model/__init__.py
 modelscope/models/nlp/space/model/gen_unified_transformer.py
@@ -1777,42 +1771,24 @@
 modelscope/msdatasets/utils/dataset_utils.py
 modelscope/msdatasets/utils/delete_utils.py
 modelscope/msdatasets/utils/maxcompute_utils.py
 modelscope/msdatasets/utils/oss_utils.py
 modelscope/msdatasets/utils/upload_utils.py
 modelscope/ops/__init__.py
 modelscope/ops/4knerf/__init__.py
-modelscope/ops/4knerf/adam_upd.cpp
-modelscope/ops/4knerf/adam_upd_kernel.cu
-modelscope/ops/4knerf/render_utils.cpp
-modelscope/ops/4knerf/render_utils_kernel.cu
-modelscope/ops/4knerf/total_variation.cpp
-modelscope/ops/4knerf/total_variation_kernel.cu
-modelscope/ops/4knerf/ub360_utils.cpp
-modelscope/ops/4knerf/ub360_utils_kernel.cu
 modelscope/ops/ailut/__init__.py
 modelscope/ops/ailut/pyinterfaces.py
 modelscope/ops/ailut/Ailut/__init__.py
 modelscope/ops/ailut/Ailut/csrc/__init__.py
-modelscope/ops/ailut/Ailut/csrc/ailut_transform.cpp
-modelscope/ops/ailut/Ailut/csrc/ailut_transform_cpu.cpp
-modelscope/ops/ailut/Ailut/csrc/ailut_transform_cuda.cu
 modelscope/ops/quadtree_attention/__init__.py
 modelscope/ops/quadtree_attention/functions/__init__.py
 modelscope/ops/quadtree_attention/functions/quadtree_attention.py
 modelscope/ops/quadtree_attention/modules/__init__.py
 modelscope/ops/quadtree_attention/modules/quadtree_attention.py
 modelscope/ops/quadtree_attention/src/__init__.py
-modelscope/ops/quadtree_attention/src/score_computation.cpp
-modelscope/ops/quadtree_attention/src/score_computation.h
-modelscope/ops/quadtree_attention/src/score_computation_kernal.cu
-modelscope/ops/quadtree_attention/src/utils.h
-modelscope/ops/quadtree_attention/src/value_aggregation.cpp
-modelscope/ops/quadtree_attention/src/value_aggregation.h
-modelscope/ops/quadtree_attention/src/value_aggregation_kernel.cu
 modelscope/outputs/__init__.py
 modelscope/outputs/cv_outputs.py
 modelscope/outputs/nlp_outputs.py
 modelscope/outputs/outputs.py
 modelscope/pipelines/__init__.py
 modelscope/pipelines/base.py
 modelscope/pipelines/builder.py
```

### Comparing `modelscope-1.8.0/modelscope.egg-info/requires.txt` & `modelscope-1.8.0rc0/modelscope.egg-info/requires.txt`

 * *Files 4% similar despite different names*

```diff
@@ -67,14 +67,15 @@
 pandas
 panopticapi
 plyfile>=0.7.4
 psutil
 pyclipper
 PyMCubes
 pytorch-lightning
+realesrgan==0.3.0
 regex
 scikit-image<0.20.0,>=0.19.3
 scikit-learn>=0.20.1
 shapely
 shotdetect_scenedetect_lgss>=0.0.4
 smplx
 tensorflow-estimator>=1.15.1
@@ -88,60 +89,72 @@
 tqdm
 transformers>=4.26.0
 trimesh
 ujson
 utils
 videofeatures_clipit>=1.0
 yacs
+accelerate
 cloudpickle
 decord>=0.6.0
+diffusers==0.18.0
 fairseq
 ftfy>=6.0.3
 librosa==0.9.2
+opencv-python
 pycocoevalcap>=1.2
 pycocotools>=2.0.4
 pydot
 pytorch_lightning<=1.7.7
 rapidfuzz
 rouge_score<=0.0.4
 sacrebleu
 safetensors
 soundfile
 taming-transformers-rom1504
 timm
 tokenizers
+torchvision
 transformers>=4.27.1
 unicodedata2
 zhconv
 boto3
 embeddings
 en_core_web_sm>=2.3.5
 filelock
+ftfy
 jieba>=0.42.1
 matplotlib
 megatron_util
 nltk
+pandas
 protobuf<3.21.0,>=3.19.0
 pythainlp
 pyvi
+regex
 rouge
 sacremoses>=0.0.41
 scikit_learn
 sentencepiece
 seqeval
 spacy>=2.3.5
 stanza
 subword_nmt>=0.3.8
 termcolor
+tokenizers
 transformers<=4.30.2,>=4.12.0
+zhconv
 biopython
 iopath
 ipdb
+lmdb
+ml_collections
 scipy
 tensorboardX
+tokenizers
 
 [audio]
 funasr>=0.6.5
 kaldiio
 kwsbp>=0.0.6
 matplotlib
 py_sound_connect>=0.1
@@ -150,38 +163,44 @@
 tensorboardX
 hdbscan
 hyperpyyaml
 librosa==0.9.2
 MinDAEC
 mir_eval>=0.7
 rotary_embedding_torch>=0.1.5
+scipy
+SoundFile>0.10
 speechbrain>=0.5.12
 torchaudio
 tqdm
 umap-learn
 bitstring
 greenlet>=1.1.2
 inflect
 jedi>=0.18.1
 kantts
+librosa==0.9.2
 lxml
+matplotlib
 msgpack>=1.0.4
 parso>=0.8.3
 pexpect>=4.8.0
 pickleshare>=0.7.5
 prompt-toolkit>=3.0.30
 protobuf
 ptflops
 ptyprocess>=0.7.0
 pygments>=2.12.0
 pysptk<0.1.19,>=0.1.15
 pytorch_wavelets
 PyWavelets>=1.0.0
 scikit-learn
 sox
+tensorboardx
+tqdm
 traitlets>=5.3.0
 ttsfrd>=0.1.2
 unidecode
 wcwidth>=0.2.5
 
 [audio_asr]
 funasr>=0.6.5
@@ -286,14 +305,15 @@
 pandas
 panopticapi
 plyfile>=0.7.4
 psutil
 pyclipper
 PyMCubes
 pytorch-lightning
+realesrgan==0.3.0
 regex
 scikit-image<0.20.0,>=0.19.3
 scikit-learn>=0.20.1
 shapely
 shotdetect_scenedetect_lgss>=0.0.4
 smplx
 tensorflow-estimator>=1.15.1
```

