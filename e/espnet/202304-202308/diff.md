# Comparing `tmp/espnet-202304.tar.gz` & `tmp/espnet-202308.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "espnet-202304.tar", last modified: Mon May  1 13:09:06 2023, max compression
+gzip compressed data, was "espnet-202308.tar", last modified: Thu Aug  3 13:36:56 2023, max compression
```

## Comparing `espnet-202304.tar` & `espnet-202308.tar`

### file list

```diff
@@ -1,900 +1,994 @@
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.812660 espnet-202304/
--rw-r--r--   0 runner    (1001) docker     (123)    11372 2023-05-01 13:08:54.000000 espnet-202304/LICENSE
--rw-r--r--   0 runner    (1001) docker     (123)    68321 2023-05-01 13:09:06.812660 espnet-202304/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)    60357 2023-05-01 13:08:54.000000 espnet-202304/README.md
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.736658 espnet-202304/espnet/
--rw-r--r--   0 runner    (1001) docker     (123)      203 2023-05-01 13:08:54.000000 espnet-202304/espnet/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.736658 espnet-202304/espnet/asr/
--rw-r--r--   0 runner    (1001) docker     (123)       30 2023-05-01 13:08:54.000000 espnet-202304/espnet/asr/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6407 2023-05-01 13:08:54.000000 espnet-202304/espnet/asr/asr_mix_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    34453 2023-05-01 13:08:54.000000 espnet-202304/espnet/asr/asr_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.736658 espnet-202304/espnet/asr/chainer_backend/
--rw-r--r--   0 runner    (1001) docker     (123)       30 2023-05-01 13:08:54.000000 espnet-202304/espnet/asr/chainer_backend/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    19506 2023-05-01 13:08:54.000000 espnet-202304/espnet/asr/chainer_backend/asr.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.736658 espnet-202304/espnet/asr/pytorch_backend/
--rw-r--r--   0 runner    (1001) docker     (123)       30 2023-05-01 13:08:54.000000 espnet-202304/espnet/asr/pytorch_backend/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    61311 2023-05-01 13:08:54.000000 espnet-202304/espnet/asr/pytorch_backend/asr.py
--rw-r--r--   0 runner    (1001) docker     (123)    10707 2023-05-01 13:08:54.000000 espnet-202304/espnet/asr/pytorch_backend/asr_init.py
--rw-r--r--   0 runner    (1001) docker     (123)    22054 2023-05-01 13:08:54.000000 espnet-202304/espnet/asr/pytorch_backend/asr_mix.py
--rw-r--r--   0 runner    (1001) docker     (123)     6635 2023-05-01 13:08:54.000000 espnet-202304/espnet/asr/pytorch_backend/recog.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.740658 espnet-202304/espnet/bin/
--rw-r--r--   0 runner    (1001) docker     (123)       30 2023-05-01 13:08:54.000000 espnet-202304/espnet/bin/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    13210 2023-05-01 13:08:54.000000 espnet-202304/espnet/bin/asr_align.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     5778 2023-05-01 13:08:54.000000 espnet-202304/espnet/bin/asr_enhance.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    13800 2023-05-01 13:08:54.000000 espnet-202304/espnet/bin/asr_recog.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    22288 2023-05-01 13:08:54.000000 espnet-202304/espnet/bin/asr_train.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     8939 2023-05-01 13:08:54.000000 espnet-202304/espnet/bin/lm_train.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    14923 2023-05-01 13:08:54.000000 espnet-202304/espnet/bin/mt_train.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     5956 2023-05-01 13:08:54.000000 espnet-202304/espnet/bin/mt_trans.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    17332 2023-05-01 13:08:54.000000 espnet-202304/espnet/bin/st_train.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     5855 2023-05-01 13:08:54.000000 espnet-202304/espnet/bin/st_trans.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     5267 2023-05-01 13:08:54.000000 espnet-202304/espnet/bin/tts_decode.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    10680 2023-05-01 13:08:54.000000 espnet-202304/espnet/bin/tts_train.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     5094 2023-05-01 13:08:54.000000 espnet-202304/espnet/bin/vc_decode.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    10830 2023-05-01 13:08:54.000000 espnet-202304/espnet/bin/vc_train.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.740658 espnet-202304/espnet/distributed/
--rw-r--r--   0 runner    (1001) docker     (123)      176 2023-05-01 13:08:54.000000 espnet-202304/espnet/distributed/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.740658 espnet-202304/espnet/lm/
--rw-r--r--   0 runner    (1001) docker     (123)       30 2023-05-01 13:08:54.000000 espnet-202304/espnet/lm/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.740658 espnet-202304/espnet/lm/chainer_backend/
--rw-r--r--   0 runner    (1001) docker     (123)       30 2023-05-01 13:08:54.000000 espnet-202304/espnet/lm/chainer_backend/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8768 2023-05-01 13:08:54.000000 espnet-202304/espnet/lm/chainer_backend/extlm.py
--rw-r--r--   0 runner    (1001) docker     (123)    18196 2023-05-01 13:08:54.000000 espnet-202304/espnet/lm/chainer_backend/lm.py
--rw-r--r--   0 runner    (1001) docker     (123)    10853 2023-05-01 13:08:54.000000 espnet-202304/espnet/lm/lm_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.740658 espnet-202304/espnet/lm/pytorch_backend/
--rw-r--r--   0 runner    (1001) docker     (123)       30 2023-05-01 13:08:54.000000 espnet-202304/espnet/lm/pytorch_backend/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     9546 2023-05-01 13:08:54.000000 espnet-202304/espnet/lm/pytorch_backend/extlm.py
--rw-r--r--   0 runner    (1001) docker     (123)    14856 2023-05-01 13:08:54.000000 espnet-202304/espnet/lm/pytorch_backend/lm.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.740658 espnet-202304/espnet/mt/
--rw-r--r--   0 runner    (1001) docker     (123)       30 2023-05-01 13:08:54.000000 espnet-202304/espnet/mt/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2571 2023-05-01 13:08:54.000000 espnet-202304/espnet/mt/mt_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.740658 espnet-202304/espnet/mt/pytorch_backend/
--rw-r--r--   0 runner    (1001) docker     (123)       30 2023-05-01 13:08:54.000000 espnet-202304/espnet/mt/pytorch_backend/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    19744 2023-05-01 13:08:54.000000 espnet-202304/espnet/mt/pytorch_backend/mt.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.744658 espnet-202304/espnet/nets/
--rw-r--r--   0 runner    (1001) docker     (123)       30 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5957 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/asr_interface.py
--rw-r--r--   0 runner    (1001) docker     (123)    13532 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/batch_beam_search.py
--rw-r--r--   0 runner    (1001) docker     (123)    11320 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/batch_beam_search_online.py
--rw-r--r--   0 runner    (1001) docker     (123)    10567 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/batch_beam_search_online_sim.py
--rw-r--r--   0 runner    (1001) docker     (123)    20199 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/beam_search.py
--rw-r--r--   0 runner    (1001) docker     (123)     8986 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/beam_search_timesync.py
--rw-r--r--   0 runner    (1001) docker     (123)    30731 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/beam_search_transducer.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.744658 espnet-202304/espnet/nets/chainer_backend/
--rw-r--r--   0 runner    (1001) docker     (123)       30 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/chainer_backend/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1033 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/chainer_backend/asr_interface.py
--rw-r--r--   0 runner    (1001) docker     (123)     3004 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/chainer_backend/ctc.py
--rw-r--r--   0 runner    (1001) docker     (123)     8531 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/chainer_backend/deterministic_embed_id.py
--rw-r--r--   0 runner    (1001) docker     (123)     7619 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/chainer_backend/e2e_asr.py
--rw-r--r--   0 runner    (1001) docker     (123)    23226 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/chainer_backend/e2e_asr_transformer.py
--rw-r--r--   0 runner    (1001) docker     (123)      187 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/chainer_backend/nets_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.744658 espnet-202304/espnet/nets/chainer_backend/rnn/
--rw-r--r--   0 runner    (1001) docker     (123)       30 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/chainer_backend/rnn/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     9422 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/chainer_backend/rnn/attentions.py
--rw-r--r--   0 runner    (1001) docker     (123)    19976 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/chainer_backend/rnn/decoders.py
--rw-r--r--   0 runner    (1001) docker     (123)    11030 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/chainer_backend/rnn/encoders.py
--rw-r--r--   0 runner    (1001) docker     (123)     9245 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/chainer_backend/rnn/training.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.744658 espnet-202304/espnet/nets/chainer_backend/transformer/
--rw-r--r--   0 runner    (1001) docker     (123)       30 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/chainer_backend/transformer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3423 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/chainer_backend/transformer/attention.py
--rw-r--r--   0 runner    (1001) docker     (123)     2677 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/chainer_backend/transformer/ctc.py
--rw-r--r--   0 runner    (1001) docker     (123)     4319 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/chainer_backend/transformer/decoder.py
--rw-r--r--   0 runner    (1001) docker     (123)     2526 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/chainer_backend/transformer/decoder_layer.py
--rw-r--r--   0 runner    (1001) docker     (123)     1204 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/chainer_backend/transformer/embedding.py
--rw-r--r--   0 runner    (1001) docker     (123)     4911 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/chainer_backend/transformer/encoder.py
--rw-r--r--   0 runner    (1001) docker     (123)     1903 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/chainer_backend/transformer/encoder_layer.py
--rw-r--r--   0 runner    (1001) docker     (123)     2455 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/chainer_backend/transformer/label_smoothing_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)      441 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/chainer_backend/transformer/layer_norm.py
--rw-r--r--   0 runner    (1001) docker     (123)      489 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/chainer_backend/transformer/mask.py
--rw-r--r--   0 runner    (1001) docker     (123)     1946 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/chainer_backend/transformer/positionwise_feed_forward.py
--rw-r--r--   0 runner    (1001) docker     (123)     3464 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/chainer_backend/transformer/subsampling.py
--rw-r--r--   0 runner    (1001) docker     (123)    11798 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/chainer_backend/transformer/training.py
--rw-r--r--   0 runner    (1001) docker     (123)    13899 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/ctc_prefix_score.py
--rw-r--r--   0 runner    (1001) docker     (123)     8773 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/e2e_asr_common.py
--rw-r--r--   0 runner    (1001) docker     (123)     2731 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/e2e_mt_common.py
--rw-r--r--   0 runner    (1001) docker     (123)     2590 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/lm_interface.py
--rw-r--r--   0 runner    (1001) docker     (123)     3428 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/mt_interface.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.748658 espnet-202304/espnet/nets/pytorch_backend/
--rw-r--r--   0 runner    (1001) docker     (123)       30 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.748658 espnet-202304/espnet/nets/pytorch_backend/conformer/
--rw-r--r--   0 runner    (1001) docker     (123)       30 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/conformer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2799 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/conformer/argument.py
--rw-r--r--   0 runner    (1001) docker     (123)    11220 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/conformer/contextual_block_encoder_layer.py
--rw-r--r--   0 runner    (1001) docker     (123)     2193 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/conformer/convolution.py
--rw-r--r--   0 runner    (1001) docker     (123)    12154 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/conformer/encoder.py
--rw-r--r--   0 runner    (1001) docker     (123)     6761 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/conformer/encoder_layer.py
--rw-r--r--   0 runner    (1001) docker     (123)      483 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/conformer/swish.py
--rw-r--r--   0 runner    (1001) docker     (123)     9877 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/ctc.py
--rw-r--r--   0 runner    (1001) docker     (123)    19358 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/e2e_asr.py
--rw-r--r--   0 runner    (1001) docker     (123)     2955 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/e2e_asr_conformer.py
--rw-r--r--   0 runner    (1001) docker     (123)    10739 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/e2e_asr_maskctc.py
--rw-r--r--   0 runner    (1001) docker     (123)    30819 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/e2e_asr_mix.py
--rw-r--r--   0 runner    (1001) docker     (123)    18143 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/e2e_asr_mix_transformer.py
--rw-r--r--   0 runner    (1001) docker     (123)    31487 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/e2e_asr_mulenc.py
--rw-r--r--   0 runner    (1001) docker     (123)    18123 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/e2e_asr_transducer.py
--rw-r--r--   0 runner    (1001) docker     (123)    22522 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/e2e_asr_transformer.py
--rw-r--r--   0 runner    (1001) docker     (123)    13630 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/e2e_mt.py
--rw-r--r--   0 runner    (1001) docker     (123)    16007 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/e2e_mt_transformer.py
--rw-r--r--   0 runner    (1001) docker     (123)    24036 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/e2e_st.py
--rw-r--r--   0 runner    (1001) docker     (123)     2594 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/e2e_st_conformer.py
--rw-r--r--   0 runner    (1001) docker     (123)    22657 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/e2e_st_transformer.py
--rw-r--r--   0 runner    (1001) docker     (123)    34348 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/e2e_tts_fastspeech.py
--rw-r--r--   0 runner    (1001) docker     (123)    34041 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/e2e_tts_tacotron2.py
--rw-r--r--   0 runner    (1001) docker     (123)    45641 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/e2e_tts_transformer.py
--rw-r--r--   0 runner    (1001) docker     (123)    30409 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/e2e_vc_tacotron2.py
--rw-r--r--   0 runner    (1001) docker     (123)    46109 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/e2e_vc_transformer.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.748658 espnet-202304/espnet/nets/pytorch_backend/fastspeech/
--rw-r--r--   0 runner    (1001) docker     (123)       30 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/fastspeech/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3600 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/fastspeech/duration_calculator.py
--rw-r--r--   0 runner    (1001) docker     (123)     4992 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/fastspeech/duration_predictor.py
--rw-r--r--   0 runner    (1001) docker     (123)     2171 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/fastspeech/length_regulator.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.748658 espnet-202304/espnet/nets/pytorch_backend/frontends/
--rw-r--r--   0 runner    (1001) docker     (123)       30 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/frontends/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2731 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/frontends/beamformer.py
--rw-r--r--   0 runner    (1001) docker     (123)     5480 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/frontends/dnn_beamformer.py
--rw-r--r--   0 runner    (1001) docker     (123)     2851 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/frontends/dnn_wpe.py
--rw-r--r--   0 runner    (1001) docker     (123)     7935 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/frontends/feature_transform.py
--rw-r--r--   0 runner    (1001) docker     (123)     4557 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/frontends/frontend.py
--rw-r--r--   0 runner    (1001) docker     (123)     2637 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/frontends/mask_estimator.py
--rw-r--r--   0 runner    (1001) docker     (123)     3974 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/gtn_ctc.py
--rw-r--r--   0 runner    (1001) docker     (123)     1561 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/initialization.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.748658 espnet-202304/espnet/nets/pytorch_backend/lm/
--rw-r--r--   0 runner    (1001) docker     (123)       30 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/lm/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    14561 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/lm/default.py
--rw-r--r--   0 runner    (1001) docker     (123)     5940 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/lm/seq_rnn.py
--rw-r--r--   0 runner    (1001) docker     (123)     8410 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/lm/transformer.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.748658 espnet-202304/espnet/nets/pytorch_backend/maskctc/
--rw-r--r--   0 runner    (1001) docker     (123)       30 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/maskctc/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1352 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/maskctc/add_mask_token.py
--rw-r--r--   0 runner    (1001) docker     (123)      803 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/maskctc/mask.py
--rw-r--r--   0 runner    (1001) docker     (123)    16551 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/nets_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.752658 espnet-202304/espnet/nets/pytorch_backend/rnn/
--rw-r--r--   0 runner    (1001) docker     (123)       30 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/rnn/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4080 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/rnn/argument.py
--rw-r--r--   0 runner    (1001) docker     (123)    66536 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/rnn/attentions.py
--rw-r--r--   0 runner    (1001) docker     (123)    48709 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/rnn/decoders.py
--rw-r--r--   0 runner    (1001) docker     (123)    14152 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/rnn/encoders.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.752658 espnet-202304/espnet/nets/pytorch_backend/streaming/
--rw-r--r--   0 runner    (1001) docker     (123)       30 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/streaming/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4774 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/streaming/segment.py
--rw-r--r--   0 runner    (1001) docker     (123)     2768 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/streaming/window.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.752658 espnet-202304/espnet/nets/pytorch_backend/tacotron2/
--rw-r--r--   0 runner    (1001) docker     (123)       30 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/tacotron2/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     9068 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/tacotron2/cbhg.py
--rw-r--r--   0 runner    (1001) docker     (123)    24375 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/tacotron2/decoder.py
--rw-r--r--   0 runner    (1001) docker     (123)     6261 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/tacotron2/encoder.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.752658 espnet-202304/espnet/nets/pytorch_backend/transducer/
--rw-r--r--   0 runner    (1001) docker     (123)       30 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/transducer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    10628 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/transducer/arguments.py
--rw-r--r--   0 runner    (1001) docker     (123)    16356 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/transducer/blocks.py
--rw-r--r--   0 runner    (1001) docker     (123)     7246 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/transducer/conv1d_nets.py
--rw-r--r--   0 runner    (1001) docker     (123)     9049 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/transducer/custom_decoder.py
--rw-r--r--   0 runner    (1001) docker     (123)     4661 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/transducer/custom_encoder.py
--rw-r--r--   0 runner    (1001) docker     (123)     4829 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/transducer/error_calculator.py
--rw-r--r--   0 runner    (1001) docker     (123)     1304 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/transducer/initializer.py
--rw-r--r--   0 runner    (1001) docker     (123)     2306 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/transducer/joint_network.py
--rw-r--r--   0 runner    (1001) docker     (123)     9028 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/transducer/rnn_decoder.py
--rw-r--r--   0 runner    (1001) docker     (123)    18464 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/transducer/rnn_encoder.py
--rw-r--r--   0 runner    (1001) docker     (123)    15051 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/transducer/transducer_tasks.py
--rw-r--r--   0 runner    (1001) docker     (123)     2702 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/transducer/transformer_decoder_layer.py
--rw-r--r--   0 runner    (1001) docker     (123)    10508 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/transducer/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     2782 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/transducer/vgg2l.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.756658 espnet-202304/espnet/nets/pytorch_backend/transformer/
--rw-r--r--   0 runner    (1001) docker     (123)       30 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/transformer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      957 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/transformer/add_sos_eos.py
--rw-r--r--   0 runner    (1001) docker     (123)     5280 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/transformer/argument.py
--rw-r--r--   0 runner    (1001) docker     (123)    11646 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/transformer/attention.py
--rw-r--r--   0 runner    (1001) docker     (123)     8899 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/transformer/contextual_block_encoder_layer.py
--rw-r--r--   0 runner    (1001) docker     (123)    13734 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/transformer/decoder.py
--rw-r--r--   0 runner    (1001) docker     (123)     4877 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/transformer/decoder_layer.py
--rw-r--r--   0 runner    (1001) docker     (123)     4243 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/transformer/dynamic_conv.py
--rw-r--r--   0 runner    (1001) docker     (123)     4862 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/transformer/dynamic_conv2d.py
--rw-r--r--   0 runner    (1001) docker     (123)    12758 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/transformer/embedding.py
--rw-r--r--   0 runner    (1001) docker     (123)    15107 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/transformer/encoder.py
--rw-r--r--   0 runner    (1001) docker     (123)     4261 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/transformer/encoder_layer.py
--rw-r--r--   0 runner    (1001) docker     (123)     6407 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/transformer/encoder_mix.py
--rw-r--r--   0 runner    (1001) docker     (123)     1383 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/transformer/initializer.py
--rw-r--r--   0 runner    (1001) docker     (123)     2164 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/transformer/label_smoothing_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)      958 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/transformer/layer_norm.py
--rw-r--r--   0 runner    (1001) docker     (123)     3589 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/transformer/lightconv.py
--rw-r--r--   0 runner    (1001) docker     (123)     4229 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/transformer/lightconv2d.py
--rw-r--r--   0 runner    (1001) docker     (123)     2070 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/transformer/longformer_attention.py
--rw-r--r--   0 runner    (1001) docker     (123)     1037 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/transformer/mask.py
--rw-r--r--   0 runner    (1001) docker     (123)     3164 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/transformer/multi_layer_conv.py
--rw-r--r--   0 runner    (1001) docker     (123)     2094 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/transformer/optimizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     5718 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/transformer/plot.py
--rw-r--r--   0 runner    (1001) docker     (123)      983 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/transformer/positionwise_feed_forward.py
--rw-r--r--   0 runner    (1001) docker     (123)     1299 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/transformer/repeat.py
--rw-r--r--   0 runner    (1001) docker     (123)    10413 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/transformer/subsampling.py
--rw-r--r--   0 runner    (1001) docker     (123)     1899 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/transformer/subsampling_without_posenc.py
--rw-r--r--   0 runner    (1001) docker     (123)    13938 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/pytorch_backend/wavenet.py
--rw-r--r--   0 runner    (1001) docker     (123)     5902 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/scorer_interface.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.756658 espnet-202304/espnet/nets/scorers/
--rw-r--r--   0 runner    (1001) docker     (123)       30 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/scorers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4953 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/scorers/ctc.py
--rw-r--r--   0 runner    (1001) docker     (123)     1740 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/scorers/length_bonus.py
--rw-r--r--   0 runner    (1001) docker     (123)     3080 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/scorers/ngram.py
--rw-r--r--   0 runner    (1001) docker     (123)     1469 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/scorers/uasr.py
--rw-r--r--   0 runner    (1001) docker     (123)     2271 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/st_interface.py
--rw-r--r--   0 runner    (1001) docker     (123)     4282 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/transducer_decoder_interface.py
--rw-r--r--   0 runner    (1001) docker     (123)     2582 2023-05-01 13:08:54.000000 espnet-202304/espnet/nets/tts_interface.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.756658 espnet-202304/espnet/optimizer/
--rw-r--r--   0 runner    (1001) docker     (123)       30 2023-05-01 13:08:54.000000 espnet-202304/espnet/optimizer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2588 2023-05-01 13:08:54.000000 espnet-202304/espnet/optimizer/chainer.py
--rw-r--r--   0 runner    (1001) docker     (123)     2013 2023-05-01 13:08:54.000000 espnet-202304/espnet/optimizer/factory.py
--rw-r--r--   0 runner    (1001) docker     (123)      968 2023-05-01 13:08:54.000000 espnet-202304/espnet/optimizer/parser.py
--rw-r--r--   0 runner    (1001) docker     (123)     2399 2023-05-01 13:08:54.000000 espnet-202304/espnet/optimizer/pytorch.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.756658 espnet-202304/espnet/scheduler/
--rw-r--r--   0 runner    (1001) docker     (123)       30 2023-05-01 13:08:54.000000 espnet-202304/espnet/scheduler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      781 2023-05-01 13:08:54.000000 espnet-202304/espnet/scheduler/chainer.py
--rw-r--r--   0 runner    (1001) docker     (123)      801 2023-05-01 13:08:54.000000 espnet-202304/espnet/scheduler/pytorch.py
--rw-r--r--   0 runner    (1001) docker     (123)     4695 2023-05-01 13:08:54.000000 espnet-202304/espnet/scheduler/scheduler.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.756658 espnet-202304/espnet/st/
--rw-r--r--   0 runner    (1001) docker     (123)       30 2023-05-01 13:08:54.000000 espnet-202304/espnet/st/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.756658 espnet-202304/espnet/st/pytorch_backend/
--rw-r--r--   0 runner    (1001) docker     (123)       30 2023-05-01 13:08:54.000000 espnet-202304/espnet/st/pytorch_backend/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    22845 2023-05-01 13:08:54.000000 espnet-202304/espnet/st/pytorch_backend/st.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.756658 espnet-202304/espnet/transform/
--rw-r--r--   0 runner    (1001) docker     (123)       31 2023-05-01 13:08:54.000000 espnet-202304/espnet/transform/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      953 2023-05-01 13:08:54.000000 espnet-202304/espnet/transform/add_deltas.py
--rw-r--r--   0 runner    (1001) docker     (123)     1355 2023-05-01 13:08:54.000000 espnet-202304/espnet/transform/channel_selector.py
--rw-r--r--   0 runner    (1001) docker     (123)     4581 2023-05-01 13:08:54.000000 espnet-202304/espnet/transform/cmvn.py
--rw-r--r--   0 runner    (1001) docker     (123)     1964 2023-05-01 13:08:54.000000 espnet-202304/espnet/transform/functional.py
--rw-r--r--   0 runner    (1001) docker     (123)    11299 2023-05-01 13:08:54.000000 espnet-202304/espnet/transform/perturb.py
--rw-r--r--   0 runner    (1001) docker     (123)     5933 2023-05-01 13:08:54.000000 espnet-202304/espnet/transform/spec_augment.py
--rw-r--r--   0 runner    (1001) docker     (123)     8080 2023-05-01 13:08:54.000000 espnet-202304/espnet/transform/spectrogram.py
--rw-r--r--   0 runner    (1001) docker     (123)      466 2023-05-01 13:08:54.000000 espnet-202304/espnet/transform/transform_interface.py
--rw-r--r--   0 runner    (1001) docker     (123)     5871 2023-05-01 13:08:54.000000 espnet-202304/espnet/transform/transformation.py
--rw-r--r--   0 runner    (1001) docker     (123)     1344 2023-05-01 13:08:54.000000 espnet-202304/espnet/transform/wpe.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.756658 espnet-202304/espnet/tts/
--rw-r--r--   0 runner    (1001) docker     (123)       30 2023-05-01 13:08:54.000000 espnet-202304/espnet/tts/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.760658 espnet-202304/espnet/tts/pytorch_backend/
--rw-r--r--   0 runner    (1001) docker     (123)       30 2023-05-01 13:08:54.000000 espnet-202304/espnet/tts/pytorch_backend/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    26532 2023-05-01 13:08:54.000000 espnet-202304/espnet/tts/pytorch_backend/tts.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.760658 espnet-202304/espnet/utils/
--rw-r--r--   0 runner    (1001) docker     (123)       30 2023-05-01 13:08:54.000000 espnet-202304/espnet/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      633 2023-05-01 13:08:54.000000 espnet-202304/espnet/utils/check_kwargs.py
--rw-r--r--   0 runner    (1001) docker     (123)     8273 2023-05-01 13:08:54.000000 espnet-202304/espnet/utils/cli_readers.py
--rw-r--r--   0 runner    (1001) docker     (123)     1380 2023-05-01 13:08:54.000000 espnet-202304/espnet/utils/cli_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     8821 2023-05-01 13:08:54.000000 espnet-202304/espnet/utils/cli_writers.py
--rw-r--r--   0 runner    (1001) docker     (123)     3703 2023-05-01 13:08:54.000000 espnet-202304/espnet/utils/dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)     1794 2023-05-01 13:08:54.000000 espnet-202304/espnet/utils/deterministic_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)      778 2023-05-01 13:08:54.000000 espnet-202304/espnet/utils/dynamic_import.py
--rw-r--r--   0 runner    (1001) docker     (123)     1426 2023-05-01 13:08:54.000000 espnet-202304/espnet/utils/fill_missing_args.py
--rw-r--r--   0 runner    (1001) docker     (123)    24831 2023-05-01 13:08:54.000000 espnet-202304/espnet/utils/io_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    18458 2023-05-01 13:08:54.000000 espnet-202304/espnet/utils/spec_augment.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.760658 espnet-202304/espnet/utils/training/
--rw-r--r--   0 runner    (1001) docker     (123)       30 2023-05-01 13:08:54.000000 espnet-202304/espnet/utils/training/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    18371 2023-05-01 13:08:54.000000 espnet-202304/espnet/utils/training/batchfy.py
--rw-r--r--   0 runner    (1001) docker     (123)      560 2023-05-01 13:08:54.000000 espnet-202304/espnet/utils/training/evaluator.py
--rw-r--r--   0 runner    (1001) docker     (123)     3526 2023-05-01 13:08:54.000000 espnet-202304/espnet/utils/training/iterators.py
--rw-r--r--   0 runner    (1001) docker     (123)     1915 2023-05-01 13:08:54.000000 espnet-202304/espnet/utils/training/tensorboard_logger.py
--rw-r--r--   0 runner    (1001) docker     (123)     1220 2023-05-01 13:08:54.000000 espnet-202304/espnet/utils/training/train_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)        7 2023-05-01 13:08:54.000000 espnet-202304/espnet/version.txt
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.736658 espnet-202304/espnet.egg-info/
--rw-r--r--   0 runner    (1001) docker     (123)    68321 2023-05-01 13:09:06.000000 espnet-202304/espnet.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)    28474 2023-05-01 13:09:06.000000 espnet-202304/espnet.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (123)        1 2023-05-01 13:09:06.000000 espnet-202304/espnet.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (123)     1507 2023-05-01 13:09:06.000000 espnet-202304/espnet.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (123)       15 2023-05-01 13:09:06.000000 espnet-202304/espnet.egg-info/top_level.txt
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.760658 espnet-202304/espnet2/
--rw-r--r--   0 runner    (1001) docker     (123)       74 2023-05-01 13:08:54.000000 espnet-202304/espnet2/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.760658 espnet-202304/espnet2/asr/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4223 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/ctc.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.760658 espnet-202304/espnet2/asr/decoder/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/decoder/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      447 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/decoder/abs_decoder.py
--rw-r--r--   0 runner    (1001) docker     (123)     3571 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/decoder/hugging_face_transformers_decoder.py
--rw-r--r--   0 runner    (1001) docker     (123)     4749 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/decoder/mlm_decoder.py
--rw-r--r--   0 runner    (1001) docker     (123)    12120 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/decoder/rnn_decoder.py
--rw-r--r--   0 runner    (1001) docker     (123)     5441 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/decoder/s4_decoder.py
--rw-r--r--   0 runner    (1001) docker     (123)     9252 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/decoder/transducer_decoder.py
--rw-r--r--   0 runner    (1001) docker     (123)    19739 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/decoder/transformer_decoder.py
--rw-r--r--   0 runner    (1001) docker     (123)     6090 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/decoder/whisper_decoder.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.764659 espnet-202304/espnet2/asr/encoder/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/encoder/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      470 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/encoder/abs_encoder.py
--rw-r--r--   0 runner    (1001) docker     (123)    21255 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/encoder/branchformer_encoder.py
--rw-r--r--   0 runner    (1001) docker     (123)    15429 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/encoder/conformer_encoder.py
--rw-r--r--   0 runner    (1001) docker     (123)    23191 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/encoder/contextual_block_conformer_encoder.py
--rw-r--r--   0 runner    (1001) docker     (123)    21748 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/encoder/contextual_block_transformer_encoder.py
--rw-r--r--   0 runner    (1001) docker     (123)    17609 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/encoder/e_branchformer_encoder.py
--rw-r--r--   0 runner    (1001) docker     (123)    25130 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/encoder/hubert_encoder.py
--rw-r--r--   0 runner    (1001) docker     (123)    15394 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/encoder/longformer_encoder.py
--rw-r--r--   0 runner    (1001) docker     (123)     3587 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/encoder/rnn_encoder.py
--rw-r--r--   0 runner    (1001) docker     (123)     9066 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/encoder/transformer_encoder.py
--rw-r--r--   0 runner    (1001) docker     (123)     8820 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/encoder/transformer_encoder_multispkr.py
--rw-r--r--   0 runner    (1001) docker     (123)     3415 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/encoder/vgg_rnn_encoder.py
--rw-r--r--   0 runner    (1001) docker     (123)     5628 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/encoder/wav2vec2_encoder.py
--rw-r--r--   0 runner    (1001) docker     (123)     5541 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/encoder/whisper_encoder.py
--rw-r--r--   0 runner    (1001) docker     (123)    24373 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/espnet_model.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.764659 espnet-202304/espnet2/asr/frontend/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/frontend/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      385 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/frontend/abs_frontend.py
--rw-r--r--   0 runner    (1001) docker     (123)     4417 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/frontend/default.py
--rw-r--r--   0 runner    (1001) docker     (123)     5752 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/frontend/fused.py
--rw-r--r--   0 runner    (1001) docker     (123)     4366 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/frontend/s3prl.py
--rw-r--r--   0 runner    (1001) docker     (123)     3963 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/frontend/whisper.py
--rw-r--r--   0 runner    (1001) docker     (123)     2817 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/frontend/windowing.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.764659 espnet-202304/espnet2/asr/layers/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/layers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3518 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/layers/cgmlp.py
--rw-r--r--   0 runner    (1001) docker     (123)     5282 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/layers/fastformer.py
--rw-r--r--   0 runner    (1001) docker     (123)    12277 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/maskctc_model.py
--rw-r--r--   0 runner    (1001) docker     (123)    11847 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/pit_espnet_model.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.764659 espnet-202304/espnet2/asr/postencoder/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/postencoder/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      388 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/postencoder/abs_postencoder.py
--rw-r--r--   0 runner    (1001) docker     (123)     6584 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/postencoder/hugging_face_transformers_postencoder.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.764659 espnet-202304/espnet2/asr/preencoder/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/preencoder/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      387 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/preencoder/abs_preencoder.py
--rw-r--r--   0 runner    (1001) docker     (123)     1095 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/preencoder/linear.py
--rw-r--r--   0 runner    (1001) docker     (123)    10226 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/preencoder/sinc.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.764659 espnet-202304/espnet2/asr/specaug/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/specaug/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      408 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/specaug/abs_specaug.py
--rw-r--r--   0 runner    (1001) docker     (123)     3435 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/specaug/specaug.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.768658 espnet-202304/espnet2/asr/state_spaces/
--rw-r--r--   0 runner    (1001) docker     (123)       30 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/state_spaces/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4629 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/state_spaces/attention.py
--rw-r--r--   0 runner    (1001) docker     (123)     5487 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/state_spaces/base.py
--rw-r--r--   0 runner    (1001) docker     (123)     5330 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/state_spaces/block.py
--rw-r--r--   0 runner    (1001) docker     (123)     4022 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/state_spaces/cauchy.py
--rw-r--r--   0 runner    (1001) docker     (123)    13449 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/state_spaces/components.py
--rw-r--r--   0 runner    (1001) docker     (123)     1923 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/state_spaces/ff.py
--rw-r--r--   0 runner    (1001) docker     (123)     6438 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/state_spaces/model.py
--rw-r--r--   0 runner    (1001) docker     (123)    11292 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/state_spaces/pool.py
--rw-r--r--   0 runner    (1001) docker     (123)      170 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/state_spaces/registry.py
--rw-r--r--   0 runner    (1001) docker     (123)     3422 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/state_spaces/residual.py
--rw-r--r--   0 runner    (1001) docker     (123)    61257 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/state_spaces/s4.py
--rw-r--r--   0 runner    (1001) docker     (123)     3469 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/state_spaces/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.768658 espnet-202304/espnet2/asr/transducer/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/transducer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    31972 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/transducer/beam_search_transducer.py
--rw-r--r--   0 runner    (1001) docker     (123)     4907 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/transducer/error_calculator.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.768658 espnet-202304/espnet2/asr/transducer/rnnt_multi_blank/
--rw-r--r--   0 runner    (1001) docker     (123)      750 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/transducer/rnnt_multi_blank/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    13479 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/transducer/rnnt_multi_blank/rnnt.py
--rw-r--r--   0 runner    (1001) docker     (123)    18251 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/transducer/rnnt_multi_blank/rnnt_multi_blank.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.768658 espnet-202304/espnet2/asr/transducer/rnnt_multi_blank/utils/
--rw-r--r--   0 runner    (1001) docker     (123)      610 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/transducer/rnnt_multi_blank/utils/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.768658 espnet-202304/espnet2/asr/transducer/rnnt_multi_blank/utils/cpu_utils/
--rw-r--r--   0 runner    (1001) docker     (123)     1194 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/transducer/rnnt_multi_blank/utils/cpu_utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    16277 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/transducer/rnnt_multi_blank/utils/cpu_utils/cpu_rnnt.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.768658 espnet-202304/espnet2/asr/transducer/rnnt_multi_blank/utils/cuda_utils/
--rw-r--r--   0 runner    (1001) docker     (123)     1194 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/transducer/rnnt_multi_blank/utils/cuda_utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    21906 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/transducer/rnnt_multi_blank/utils/cuda_utils/gpu_rnnt.py
--rw-r--r--   0 runner    (1001) docker     (123)    46645 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/transducer/rnnt_multi_blank/utils/cuda_utils/gpu_rnnt_kernel.py
--rw-r--r--   0 runner    (1001) docker     (123)    12677 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/transducer/rnnt_multi_blank/utils/cuda_utils/reduce.py
--rw-r--r--   0 runner    (1001) docker     (123)     1727 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/transducer/rnnt_multi_blank/utils/global_constants.py
--rw-r--r--   0 runner    (1001) docker     (123)     3802 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr/transducer/rnnt_multi_blank/utils/rnnt_helper.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.768658 espnet-202304/espnet2/asr_transducer/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr_transducer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6730 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr_transducer/activation.py
--rw-r--r--   0 runner    (1001) docker     (123)    23075 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr_transducer/beam_search_transducer.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.768658 espnet-202304/espnet2/asr_transducer/decoder/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr_transducer/decoder/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3079 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr_transducer/decoder/abs_decoder.py
--rw-r--r--   0 runner    (1001) docker     (123)     7887 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr_transducer/decoder/rnn_decoder.py
--rw-r--r--   0 runner    (1001) docker     (123)     3727 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr_transducer/decoder/stateless_decoder.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.768658 espnet-202304/espnet2/asr_transducer/encoder/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr_transducer/encoder/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.768658 espnet-202304/espnet2/asr_transducer/encoder/blocks/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr_transducer/encoder/blocks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5180 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr_transducer/encoder/blocks/branchformer.py
--rw-r--r--   0 runner    (1001) docker     (123)     5763 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr_transducer/encoder/blocks/conformer.py
--rw-r--r--   0 runner    (1001) docker     (123)     6482 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr_transducer/encoder/blocks/conv1d.py
--rw-r--r--   0 runner    (1001) docker     (123)     3700 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr_transducer/encoder/blocks/conv_input.py
--rw-r--r--   0 runner    (1001) docker     (123)    10310 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr_transducer/encoder/building.py
--rw-r--r--   0 runner    (1001) docker     (123)     5673 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr_transducer/encoder/encoder.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.772659 espnet-202304/espnet2/asr_transducer/encoder/modules/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr_transducer/encoder/modules/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7964 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr_transducer/encoder/modules/attention.py
--rw-r--r--   0 runner    (1001) docker     (123)     5757 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr_transducer/encoder/modules/convolution.py
--rw-r--r--   0 runner    (1001) docker     (123)     3001 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr_transducer/encoder/modules/multi_blocks.py
--rw-r--r--   0 runner    (1001) docker     (123)     4454 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr_transducer/encoder/modules/normalization.py
--rw-r--r--   0 runner    (1001) docker     (123)     2748 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr_transducer/encoder/modules/positional_encoding.py
--rw-r--r--   0 runner    (1001) docker     (123)     5237 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr_transducer/encoder/validation.py
--rw-r--r--   0 runner    (1001) docker     (123)     5241 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr_transducer/error_calculator.py
--rw-r--r--   0 runner    (1001) docker     (123)    15606 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr_transducer/espnet_transducer_model.py
--rw-r--r--   0 runner    (1001) docker     (123)     1777 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr_transducer/joint_network.py
--rw-r--r--   0 runner    (1001) docker     (123)     5167 2023-05-01 13:08:54.000000 espnet-202304/espnet2/asr_transducer/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.776659 espnet-202304/espnet2/bin/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     3756 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/aggregate_stats_dirs.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    32304 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/asr_align.py
--rw-r--r--   0 runner    (1001) docker     (123)    34289 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/asr_inference.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    26605 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/asr_inference_k2.py
--rw-r--r--   0 runner    (1001) docker     (123)    11952 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/asr_inference_maskctc.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    21853 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/asr_inference_streaming.py
--rwxr-xr-x   0 runner    (1001) docker     (123)      431 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/asr_train.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    26022 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/asr_transducer_inference.py
--rwxr-xr-x   0 runner    (1001) docker     (123)      599 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/asr_transducer_train.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    26815 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/diar_inference.py
--rwxr-xr-x   0 runner    (1001) docker     (123)      477 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/diar_train.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    21910 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/enh_inference.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    14477 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/enh_inference_streaming.py
--rwxr-xr-x   0 runner    (1001) docker     (123)      467 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/enh_s2t_train.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     8251 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/enh_scoring.py
--rwxr-xr-x   0 runner    (1001) docker     (123)      472 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/enh_train.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    22786 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/enh_tse_inference.py
--rwxr-xr-x   0 runner    (1001) docker     (123)      531 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/enh_tse_train.py
--rwxr-xr-x   0 runner    (1001) docker     (123)      413 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/gan_svs_train.py
--rwxr-xr-x   0 runner    (1001) docker     (123)      413 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/gan_tts_train.py
--rwxr-xr-x   0 runner    (1001) docker     (123)      454 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/hubert_train.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     3105 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/hugging_face_export_vocabulary.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    13042 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/launch.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     6595 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/lm_calc_perplexity.py
--rw-r--r--   0 runner    (1001) docker     (123)    17623 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/lm_inference.py
--rwxr-xr-x   0 runner    (1001) docker     (123)      383 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/lm_train.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    17194 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/mt_inference.py
--rwxr-xr-x   0 runner    (1001) docker     (123)      421 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/mt_train.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     3119 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/pack.py
--rw-r--r--   0 runner    (1001) docker     (123)    23605 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/slu_inference.py
--rw-r--r--   0 runner    (1001) docker     (123)      431 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/slu_train.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     3300 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/split_scps.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    17745 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/st_inference.py
--rw-r--r--   0 runner    (1001) docker     (123)    20953 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/st_inference_streaming.py
--rwxr-xr-x   0 runner    (1001) docker     (123)      421 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/st_train.py
--rw-r--r--   0 runner    (1001) docker     (123)    22489 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/svs_inference.py
--rw-r--r--   0 runner    (1001) docker     (123)      389 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/svs_train.py
--rw-r--r--   0 runner    (1001) docker     (123)     8468 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/tokenize_text.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    25855 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/tts_inference.py
--rwxr-xr-x   0 runner    (1001) docker     (123)      389 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/tts_train.py
--rw-r--r--   0 runner    (1001) docker     (123)     4900 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/uasr_extract_feature.py
--rw-r--r--   0 runner    (1001) docker     (123)    19155 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/uasr_inference.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    21667 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/uasr_inference_k2.py
--rw-r--r--   0 runner    (1001) docker     (123)      441 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/uasr_train.py
--rw-r--r--   0 runner    (1001) docker     (123)     2848 2023-05-01 13:08:54.000000 espnet-202304/espnet2/bin/whisper_export_vocabulary.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.776659 espnet-202304/espnet2/diar/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/diar/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      643 2023-05-01 13:08:54.000000 espnet-202304/espnet2/diar/abs_diar.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.776659 espnet-202304/espnet2/diar/attractor/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/diar/attractor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      343 2023-05-01 13:08:54.000000 espnet-202304/espnet2/diar/attractor/abs_attractor.py
--rw-r--r--   0 runner    (1001) docker     (123)     2007 2023-05-01 13:08:54.000000 espnet-202304/espnet2/diar/attractor/rnn_attractor.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.776659 espnet-202304/espnet2/diar/decoder/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/diar/decoder/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      396 2023-05-01 13:08:54.000000 espnet-202304/espnet2/diar/decoder/abs_decoder.py
--rw-r--r--   0 runner    (1001) docker     (123)      754 2023-05-01 13:08:54.000000 espnet-202304/espnet2/diar/decoder/linear_decoder.py
--rw-r--r--   0 runner    (1001) docker     (123)    14924 2023-05-01 13:08:54.000000 espnet-202304/espnet2/diar/espnet_model.py
--rw-r--r--   0 runner    (1001) docker     (123)      749 2023-05-01 13:08:54.000000 espnet-202304/espnet2/diar/label_processor.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.776659 espnet-202304/espnet2/diar/layers/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/diar/layers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      474 2023-05-01 13:08:54.000000 espnet-202304/espnet2/diar/layers/abs_mask.py
--rw-r--r--   0 runner    (1001) docker     (123)     4124 2023-05-01 13:08:54.000000 espnet-202304/espnet2/diar/layers/multi_mask.py
--rw-r--r--   0 runner    (1001) docker     (123)     7703 2023-05-01 13:08:54.000000 espnet-202304/espnet2/diar/layers/tcn_nomask.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.776659 espnet-202304/espnet2/diar/separator/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/diar/separator/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2737 2023-05-01 13:08:54.000000 espnet-202304/espnet2/diar/separator/tcn_separator_nomask.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.776659 espnet-202304/espnet2/enh/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      643 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/abs_enh.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.776659 espnet-202304/espnet2/enh/decoder/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/decoder/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      975 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/decoder/abs_decoder.py
--rw-r--r--   0 runner    (1001) docker     (123)     3014 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/decoder/conv_decoder.py
--rw-r--r--   0 runner    (1001) docker     (123)      493 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/decoder/null_decoder.py
--rw-r--r--   0 runner    (1001) docker     (123)     6263 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/decoder/stft_decoder.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.776659 espnet-202304/espnet2/enh/encoder/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/encoder/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1041 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/encoder/abs_encoder.py
--rw-r--r--   0 runner    (1001) docker     (123)     2731 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/encoder/conv_encoder.py
--rw-r--r--   0 runner    (1001) docker     (123)      503 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/encoder/null_encoder.py
--rw-r--r--   0 runner    (1001) docker     (123)     4672 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/encoder/stft_encoder.py
--rw-r--r--   0 runner    (1001) docker     (123)    19330 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/espnet_enh_s2t_model.py
--rw-r--r--   0 runner    (1001) docker     (123)    21040 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/espnet_model.py
--rw-r--r--   0 runner    (1001) docker     (123)    12814 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/espnet_model_tse.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.780659 espnet-202304/espnet2/enh/extractor/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/extractor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      458 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/extractor/abs_extractor.py
--rw-r--r--   0 runner    (1001) docker     (123)     4935 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/extractor/td_speakerbeam_extractor.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.780659 espnet-202304/espnet2/enh/layers/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/layers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4373 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/layers/adapt_layers.py
--rw-r--r--   0 runner    (1001) docker     (123)    42560 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/layers/beamformer.py
--rw-r--r--   0 runner    (1001) docker     (123)    37809 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/layers/beamformer_th.py
--rw-r--r--   0 runner    (1001) docker     (123)     6751 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/layers/complex_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    14634 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/layers/complexnn.py
--rw-r--r--   0 runner    (1001) docker     (123)     1462 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/layers/conv_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    18544 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/layers/dc_crn.py
--rw-r--r--   0 runner    (1001) docker     (123)    23362 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/layers/dnn_beamformer.py
--rw-r--r--   0 runner    (1001) docker     (123)     5512 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/layers/dnn_wpe.py
--rw-r--r--   0 runner    (1001) docker     (123)     6771 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/layers/dpmulcat.py
--rw-r--r--   0 runner    (1001) docker     (123)    14234 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/layers/dprnn.py
--rw-r--r--   0 runner    (1001) docker     (123)     6545 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/layers/dptnet.py
--rw-r--r--   0 runner    (1001) docker     (123)    14370 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/layers/fasnet.py
--rw-r--r--   0 runner    (1001) docker     (123)     7647 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/layers/ifasnet.py
--rw-r--r--   0 runner    (1001) docker     (123)     3390 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/layers/mask_estimator.py
--rw-r--r--   0 runner    (1001) docker     (123)    13471 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/layers/skim.py
--rw-r--r--   0 runner    (1001) docker     (123)    15483 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/layers/tcn.py
--rw-r--r--   0 runner    (1001) docker     (123)    14465 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/layers/tcndenseunet.py
--rw-r--r--   0 runner    (1001) docker     (123)     7858 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/layers/wpe.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.780659 espnet-202304/espnet2/enh/loss/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/loss/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.780659 espnet-202304/espnet2/enh/loss/criterions/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/loss/criterions/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      705 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/loss/criterions/abs_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)    16625 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/loss/criterions/tf_domain.py
--rw-r--r--   0 runner    (1001) docker     (123)    13715 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/loss/criterions/time_domain.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.780659 espnet-202304/espnet2/enh/loss/wrappers/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/loss/wrappers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      580 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/loss/wrappers/abs_wrapper.py
--rw-r--r--   0 runner    (1001) docker     (123)     1083 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/loss/wrappers/dpcl_solver.py
--rw-r--r--   0 runner    (1001) docker     (123)     1393 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/loss/wrappers/fixed_order.py
--rw-r--r--   0 runner    (1001) docker     (123)     4145 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/loss/wrappers/mixit_solver.py
--rw-r--r--   0 runner    (1001) docker     (123)     3042 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/loss/wrappers/multilayer_pit_solver.py
--rw-r--r--   0 runner    (1001) docker     (123)     4627 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/loss/wrappers/pit_solver.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.784659 espnet-202304/espnet2/enh/separator/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/separator/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      652 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/separator/abs_separator.py
--rw-r--r--   0 runner    (1001) docker     (123)     5364 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/separator/asteroid_models.py
--rw-r--r--   0 runner    (1001) docker     (123)     7130 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/separator/conformer_separator.py
--rw-r--r--   0 runner    (1001) docker     (123)     6012 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/separator/dan_separator.py
--rw-r--r--   0 runner    (1001) docker     (123)     7510 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/separator/dc_crn_separator.py
--rw-r--r--   0 runner    (1001) docker     (123)    13915 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/separator/dccrn_separator.py
--rw-r--r--   0 runner    (1001) docker     (123)     6677 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/separator/dpcl_e2e_separator.py
--rw-r--r--   0 runner    (1001) docker     (123)     4878 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/separator/dpcl_separator.py
--rw-r--r--   0 runner    (1001) docker     (123)     4412 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/separator/dprnn_separator.py
--rw-r--r--   0 runner    (1001) docker     (123)     6828 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/separator/dptnet_separator.py
--rw-r--r--   0 runner    (1001) docker     (123)     3677 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/separator/fasnet_separator.py
--rw-r--r--   0 runner    (1001) docker     (123)    11343 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/separator/ineube_separator.py
--rw-r--r--   0 runner    (1001) docker     (123)    10439 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/separator/neural_beamformer.py
--rw-r--r--   0 runner    (1001) docker     (123)     4807 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/separator/rnn_separator.py
--rw-r--r--   0 runner    (1001) docker     (123)     5293 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/separator/skim_separator.py
--rw-r--r--   0 runner    (1001) docker     (123)     7042 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/separator/svoice_separator.py
--rw-r--r--   0 runner    (1001) docker     (123)     4478 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/separator/tcn_separator.py
--rw-r--r--   0 runner    (1001) docker     (123)    14294 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/separator/tfgridnet_separator.py
--rw-r--r--   0 runner    (1001) docker     (123)     6107 2023-05-01 13:08:54.000000 espnet-202304/espnet2/enh/separator/transformer_separator.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.784659 espnet-202304/espnet2/fileio/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/fileio/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2338 2023-05-01 13:08:54.000000 espnet-202304/espnet2/fileio/datadir_writer.py
--rw-r--r--   0 runner    (1001) docker     (123)     2360 2023-05-01 13:08:54.000000 espnet-202304/espnet2/fileio/npy_scp.py
--rw-r--r--   0 runner    (1001) docker     (123)     2362 2023-05-01 13:08:54.000000 espnet-202304/espnet2/fileio/rand_gen_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)     7281 2023-05-01 13:08:54.000000 espnet-202304/espnet2/fileio/read_text.py
--rw-r--r--   0 runner    (1001) docker     (123)     2825 2023-05-01 13:08:54.000000 espnet-202304/espnet2/fileio/rttm.py
--rw-r--r--   0 runner    (1001) docker     (123)     8583 2023-05-01 13:08:54.000000 espnet-202304/espnet2/fileio/score_scp.py
--rw-r--r--   0 runner    (1001) docker     (123)     8356 2023-05-01 13:08:54.000000 espnet-202304/espnet2/fileio/sound_scp.py
--rw-r--r--   0 runner    (1001) docker     (123)     2795 2023-05-01 13:08:54.000000 espnet-202304/espnet2/fileio/vad_scp.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.784659 espnet-202304/espnet2/fst/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/fst/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7517 2023-05-01 13:08:54.000000 espnet-202304/espnet2/fst/lm_rescore.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.784659 espnet-202304/espnet2/gan_svs/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_svs/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      627 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_svs/abs_gan_svs.py
--rw-r--r--   0 runner    (1001) docker     (123)    18924 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_svs/espnet_model.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.784659 espnet-202304/espnet2/gan_svs/joint/
--rw-r--r--   0 runner    (1001) docker     (123)       73 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_svs/joint/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    33872 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_svs/joint/joint_score2wav.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.784659 espnet-202304/espnet2/gan_svs/vits/
--rw-r--r--   0 runner    (1001) docker     (123)       51 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_svs/vits/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2165 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_svs/vits/duration_predictor.py
--rw-r--r--   0 runner    (1001) docker     (123)     2121 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_svs/vits/frame_prior_net.py
--rw-r--r--   0 runner    (1001) docker     (123)    26479 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_svs/vits/generator.py
--rw-r--r--   0 runner    (1001) docker     (123)     4247 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_svs/vits/length_regulator.py
--rw-r--r--   0 runner    (1001) docker     (123)      873 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_svs/vits/modules.py
--rw-r--r--   0 runner    (1001) docker     (123)     2355 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_svs/vits/phoneme_predictor.py
--rw-r--r--   0 runner    (1001) docker     (123)     2244 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_svs/vits/pitch_predictor.py
--rw-r--r--   0 runner    (1001) docker     (123)     6050 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_svs/vits/text_encoder.py
--rw-r--r--   0 runner    (1001) docker     (123)    34056 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_svs/vits/vits.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.788659 espnet-202304/espnet2/gan_tts/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_tts/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      600 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_tts/abs_gan_tts.py
--rw-r--r--   0 runner    (1001) docker     (123)     9364 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_tts/espnet_model.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.788659 espnet-202304/espnet2/gan_tts/hifigan/
--rw-r--r--   0 runner    (1001) docker     (123)      485 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_tts/hifigan/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    27021 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_tts/hifigan/hifigan.py
--rw-r--r--   0 runner    (1001) docker     (123)    10182 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_tts/hifigan/loss.py
--rw-r--r--   0 runner    (1001) docker     (123)     3313 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_tts/hifigan/residual_block.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.788659 espnet-202304/espnet2/gan_tts/jets/
--rw-r--r--   0 runner    (1001) docker     (123)       51 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_tts/jets/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5279 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_tts/jets/alignments.py
--rw-r--r--   0 runner    (1001) docker     (123)    35386 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_tts/jets/generator.py
--rw-r--r--   0 runner    (1001) docker     (123)    24388 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_tts/jets/jets.py
--rw-r--r--   0 runner    (1001) docker     (123)     2017 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_tts/jets/length_regulator.py
--rw-r--r--   0 runner    (1001) docker     (123)     7779 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_tts/jets/loss.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.788659 espnet-202304/espnet2/gan_tts/joint/
--rw-r--r--   0 runner    (1001) docker     (123)       71 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_tts/joint/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    24009 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_tts/joint/joint_text2wav.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.788659 espnet-202304/espnet2/gan_tts/melgan/
--rw-r--r--   0 runner    (1001) docker     (123)      216 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_tts/melgan/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    16694 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_tts/melgan/melgan.py
--rw-r--r--   0 runner    (1001) docker     (123)     5141 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_tts/melgan/pqmf.py
--rw-r--r--   0 runner    (1001) docker     (123)     2464 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_tts/melgan/residual_stack.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.788659 espnet-202304/espnet2/gan_tts/parallel_wavegan/
--rw-r--r--   0 runner    (1001) docker     (123)      202 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_tts/parallel_wavegan/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    12423 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_tts/parallel_wavegan/parallel_wavegan.py
--rw-r--r--   0 runner    (1001) docker     (123)     6161 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_tts/parallel_wavegan/upsample.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.788659 espnet-202304/espnet2/gan_tts/style_melgan/
--rw-r--r--   0 runner    (1001) docker     (123)      170 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_tts/style_melgan/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    12076 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_tts/style_melgan/style_melgan.py
--rw-r--r--   0 runner    (1001) docker     (123)     5864 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_tts/style_melgan/tade_res_block.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.788659 espnet-202304/espnet2/gan_tts/utils/
--rw-r--r--   0 runner    (1001) docker     (123)      157 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_tts/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1429 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_tts/utils/get_random_segments.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.792659 espnet-202304/espnet2/gan_tts/vits/
--rw-r--r--   0 runner    (1001) docker     (123)       51 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_tts/vits/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6185 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_tts/vits/duration_predictor.py
--rw-r--r--   0 runner    (1001) docker     (123)     9380 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_tts/vits/flow.py
--rw-r--r--   0 runner    (1001) docker     (123)    25456 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_tts/vits/generator.py
--rw-r--r--   0 runner    (1001) docker     (123)     1318 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_tts/vits/loss.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.792659 espnet-202304/espnet2/gan_tts/vits/monotonic_align/
--rw-r--r--   0 runner    (1001) docker     (123)     2493 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_tts/vits/monotonic_align/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      712 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_tts/vits/monotonic_align/setup.py
--rw-r--r--   0 runner    (1001) docker     (123)     4037 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_tts/vits/posterior_encoder.py
--rw-r--r--   0 runner    (1001) docker     (123)     7596 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_tts/vits/residual_coupling.py
--rw-r--r--   0 runner    (1001) docker     (123)     5385 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_tts/vits/text_encoder.py
--rw-r--r--   0 runner    (1001) docker     (123)     7504 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_tts/vits/transform.py
--rw-r--r--   0 runner    (1001) docker     (123)    23742 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_tts/vits/vits.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.792659 espnet-202304/espnet2/gan_tts/wavenet/
--rw-r--r--   0 runner    (1001) docker     (123)       60 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_tts/wavenet/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5352 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_tts/wavenet/residual_block.py
--rw-r--r--   0 runner    (1001) docker     (123)     6901 2023-05-01 13:08:54.000000 espnet-202304/espnet2/gan_tts/wavenet/wavenet.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.792659 espnet-202304/espnet2/hubert/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/hubert/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    16726 2023-05-01 13:08:54.000000 espnet-202304/espnet2/hubert/espnet_model.py
--rw-r--r--   0 runner    (1001) docker     (123)     2827 2023-05-01 13:08:54.000000 espnet-202304/espnet2/hubert/hubert_loss.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.792659 espnet-202304/espnet2/iterators/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/iterators/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      219 2023-05-01 13:08:54.000000 espnet-202304/espnet2/iterators/abs_iter_factory.py
--rw-r--r--   0 runner    (1001) docker     (123)     9057 2023-05-01 13:08:54.000000 espnet-202304/espnet2/iterators/chunk_iter_factory.py
--rw-r--r--   0 runner    (1001) docker     (123)     1105 2023-05-01 13:08:54.000000 espnet-202304/espnet2/iterators/multiple_iter_factory.py
--rw-r--r--   0 runner    (1001) docker     (123)     5509 2023-05-01 13:08:54.000000 espnet-202304/espnet2/iterators/sequence_iter_factory.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.792659 espnet-202304/espnet2/layers/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/layers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      344 2023-05-01 13:08:54.000000 espnet-202304/espnet2/layers/abs_normalize.py
--rw-r--r--   0 runner    (1001) docker     (123)     3746 2023-05-01 13:08:54.000000 espnet-202304/espnet2/layers/global_mvn.py
--rw-r--r--   0 runner    (1001) docker     (123)      334 2023-05-01 13:08:54.000000 espnet-202304/espnet2/layers/inversible_interface.py
--rw-r--r--   0 runner    (1001) docker     (123)     2519 2023-05-01 13:08:54.000000 espnet-202304/espnet2/layers/label_aggregation.py
--rw-r--r--   0 runner    (1001) docker     (123)     2578 2023-05-01 13:08:54.000000 espnet-202304/espnet2/layers/log_mel.py
--rw-r--r--   0 runner    (1001) docker     (123)     6242 2023-05-01 13:08:54.000000 espnet-202304/espnet2/layers/mask_along_axis.py
--rw-r--r--   0 runner    (1001) docker     (123)     9034 2023-05-01 13:08:54.000000 espnet-202304/espnet2/layers/sinc_conv.py
--rw-r--r--   0 runner    (1001) docker     (123)     8787 2023-05-01 13:08:54.000000 espnet-202304/espnet2/layers/stft.py
--rw-r--r--   0 runner    (1001) docker     (123)     2526 2023-05-01 13:08:54.000000 espnet-202304/espnet2/layers/time_warp.py
--rw-r--r--   0 runner    (1001) docker     (123)     2316 2023-05-01 13:08:54.000000 espnet-202304/espnet2/layers/utterance_mvn.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.792659 espnet-202304/espnet2/lm/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/lm/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      747 2023-05-01 13:08:54.000000 espnet-202304/espnet2/lm/abs_model.py
--rw-r--r--   0 runner    (1001) docker     (123)     4785 2023-05-01 13:08:54.000000 espnet-202304/espnet2/lm/espnet_model.py
--rw-r--r--   0 runner    (1001) docker     (123)     5887 2023-05-01 13:08:54.000000 espnet-202304/espnet2/lm/seq_rnn_lm.py
--rw-r--r--   0 runner    (1001) docker     (123)     4239 2023-05-01 13:08:54.000000 espnet-202304/espnet2/lm/transformer_lm.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.792659 espnet-202304/espnet2/main_funcs/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/main_funcs/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3886 2023-05-01 13:08:54.000000 espnet-202304/espnet2/main_funcs/average_nbest_models.py
--rw-r--r--   0 runner    (1001) docker     (123)     5510 2023-05-01 13:08:54.000000 espnet-202304/espnet2/main_funcs/calculate_all_attentions.py
--rw-r--r--   0 runner    (1001) docker     (123)     5164 2023-05-01 13:08:54.000000 espnet-202304/espnet2/main_funcs/collect_stats.py
--rw-r--r--   0 runner    (1001) docker     (123)     9839 2023-05-01 13:08:54.000000 espnet-202304/espnet2/main_funcs/pack_funcs.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.792659 espnet-202304/espnet2/mt/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/mt/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    10123 2023-05-01 13:08:54.000000 espnet-202304/espnet2/mt/espnet_model.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.792659 espnet-202304/espnet2/mt/frontend/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/mt/frontend/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1941 2023-05-01 13:08:54.000000 espnet-202304/espnet2/mt/frontend/embedding.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.796659 espnet-202304/espnet2/optimizers/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/optimizers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2534 2023-05-01 13:08:54.000000 espnet-202304/espnet2/optimizers/optim_groups.py
--rw-r--r--   0 runner    (1001) docker     (123)      828 2023-05-01 13:08:54.000000 espnet-202304/espnet2/optimizers/sgd.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.796659 espnet-202304/espnet2/samplers/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/samplers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      392 2023-05-01 13:08:54.000000 espnet-202304/espnet2/samplers/abs_sampler.py
--rw-r--r--   0 runner    (1001) docker     (123)     6131 2023-05-01 13:08:54.000000 espnet-202304/espnet2/samplers/build_batch_sampler.py
--rw-r--r--   0 runner    (1001) docker     (123)     5613 2023-05-01 13:08:54.000000 espnet-202304/espnet2/samplers/folded_batch_sampler.py
--rw-r--r--   0 runner    (1001) docker     (123)     4979 2023-05-01 13:08:54.000000 espnet-202304/espnet2/samplers/length_batch_sampler.py
--rw-r--r--   0 runner    (1001) docker     (123)     5628 2023-05-01 13:08:54.000000 espnet-202304/espnet2/samplers/num_elements_batch_sampler.py
--rw-r--r--   0 runner    (1001) docker     (123)     3128 2023-05-01 13:08:54.000000 espnet-202304/espnet2/samplers/sorted_batch_sampler.py
--rw-r--r--   0 runner    (1001) docker     (123)     2927 2023-05-01 13:08:54.000000 espnet-202304/espnet2/samplers/unsorted_batch_sampler.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.796659 espnet-202304/espnet2/schedulers/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/schedulers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1664 2023-05-01 13:08:54.000000 espnet-202304/espnet2/schedulers/abs_scheduler.py
--rw-r--r--   0 runner    (1001) docker     (123)     2068 2023-05-01 13:08:54.000000 espnet-202304/espnet2/schedulers/noam_lr.py
--rw-r--r--   0 runner    (1001) docker     (123)     1495 2023-05-01 13:08:54.000000 espnet-202304/espnet2/schedulers/warmup_lr.py
--rw-r--r--   0 runner    (1001) docker     (123)     2699 2023-05-01 13:08:54.000000 espnet-202304/espnet2/schedulers/warmup_step_lr.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.796659 espnet-202304/espnet2/slu/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/slu/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    16575 2023-05-01 13:08:54.000000 espnet-202304/espnet2/slu/espnet_model.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.796659 espnet-202304/espnet2/slu/postdecoder/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/slu/postdecoder/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      662 2023-05-01 13:08:54.000000 espnet-202304/espnet2/slu/postdecoder/abs_postdecoder.py
--rw-r--r--   0 runner    (1001) docker     (123)     3807 2023-05-01 13:08:54.000000 espnet-202304/espnet2/slu/postdecoder/hugging_face_transformers_postdecoder.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.796659 espnet-202304/espnet2/slu/postencoder/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/slu/postencoder/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    10132 2023-05-01 13:08:54.000000 espnet-202304/espnet2/slu/postencoder/conformer_postencoder.py
--rw-r--r--   0 runner    (1001) docker     (123)     5830 2023-05-01 13:08:54.000000 espnet-202304/espnet2/slu/postencoder/transformer_postencoder.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.796659 espnet-202304/espnet2/st/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/st/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    16065 2023-05-01 13:08:54.000000 espnet-202304/espnet2/st/espnet_model.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.796659 espnet-202304/espnet2/svs/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/svs/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1182 2023-05-01 13:08:54.000000 espnet-202304/espnet2/svs/abs_svs.py
--rw-r--r--   0 runner    (1001) docker     (123)    25874 2023-05-01 13:08:54.000000 espnet-202304/espnet2/svs/espnet_model.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.796659 espnet-202304/espnet2/svs/feats_extract/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/svs/feats_extract/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    11227 2023-05-01 13:08:54.000000 espnet-202304/espnet2/svs/feats_extract/score_feats_extract.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.796659 espnet-202304/espnet2/svs/naive_rnn/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/svs/naive_rnn/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    21298 2023-05-01 13:08:54.000000 espnet-202304/espnet2/svs/naive_rnn/naive_rnn.py
--rw-r--r--   0 runner    (1001) docker     (123)    23024 2023-05-01 13:08:54.000000 espnet-202304/espnet2/svs/naive_rnn/naive_rnn_dp.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.796659 espnet-202304/espnet2/svs/xiaoice/
--rw-r--r--   0 runner    (1001) docker     (123)    33874 2023-05-01 13:08:54.000000 espnet-202304/espnet2/svs/xiaoice/XiaoiceSing.py
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/svs/xiaoice/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5424 2023-05-01 13:08:54.000000 espnet-202304/espnet2/svs/xiaoice/loss.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.800659 espnet-202304/espnet2/tasks/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tasks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    71205 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tasks/abs_task.py
--rw-r--r--   0 runner    (1001) docker     (123)    21301 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tasks/asr.py
--rw-r--r--   0 runner    (1001) docker     (123)    13021 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tasks/asr_transducer.py
--rw-r--r--   0 runner    (1001) docker     (123)     9963 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tasks/diar.py
--rw-r--r--   0 runner    (1001) docker     (123)    18733 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tasks/enh.py
--rw-r--r--   0 runner    (1001) docker     (123)    19866 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tasks/enh_s2t.py
--rw-r--r--   0 runner    (1001) docker     (123)    11772 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tasks/enh_tse.py
--rw-r--r--   0 runner    (1001) docker     (123)    15000 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tasks/gan_svs.py
--rw-r--r--   0 runner    (1001) docker     (123)    13798 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tasks/gan_tts.py
--rw-r--r--   0 runner    (1001) docker     (123)    14204 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tasks/hubert.py
--rw-r--r--   0 runner    (1001) docker     (123)     6804 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tasks/lm.py
--rw-r--r--   0 runner    (1001) docker     (123)    13499 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tasks/mt.py
--rw-r--r--   0 runner    (1001) docker     (123)    20979 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tasks/slu.py
--rw-r--r--   0 runner    (1001) docker     (123)    20374 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tasks/st.py
--rw-r--r--   0 runner    (1001) docker     (123)    16282 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tasks/svs.py
--rw-r--r--   0 runner    (1001) docker     (123)    14328 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tasks/tts.py
--rw-r--r--   0 runner    (1001) docker     (123)    15031 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tasks/uasr.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.800659 espnet-202304/espnet2/text/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/text/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      314 2023-05-01 13:08:54.000000 espnet-202304/espnet2/text/abs_tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     2901 2023-05-01 13:08:54.000000 espnet-202304/espnet2/text/build_tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     2599 2023-05-01 13:08:54.000000 espnet-202304/espnet2/text/char_tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     2140 2023-05-01 13:08:54.000000 espnet-202304/espnet2/text/cleaner.py
--rw-r--r--   0 runner    (1001) docker     (123)     1729 2023-05-01 13:08:54.000000 espnet-202304/espnet2/text/hugging_face_tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     1968 2023-05-01 13:08:54.000000 espnet-202304/espnet2/text/korean_cleaner.py
--rw-r--r--   0 runner    (1001) docker     (123)    21080 2023-05-01 13:08:54.000000 espnet-202304/espnet2/text/phoneme_tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     1259 2023-05-01 13:08:54.000000 espnet-202304/espnet2/text/sentencepiece_tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     2060 2023-05-01 13:08:54.000000 espnet-202304/espnet2/text/token_id_converter.py
--rw-r--r--   0 runner    (1001) docker     (123)     1899 2023-05-01 13:08:54.000000 espnet-202304/espnet2/text/whisper_token_id_converter.py
--rw-r--r--   0 runner    (1001) docker     (123)     1457 2023-05-01 13:08:54.000000 espnet-202304/espnet2/text/whisper_tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     2039 2023-05-01 13:08:54.000000 espnet-202304/espnet2/text/word_tokenizer.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.800659 espnet-202304/espnet2/torch_utils/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/torch_utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      987 2023-05-01 13:08:54.000000 espnet-202304/espnet2/torch_utils/add_gradient_noise.py
--rw-r--r--   0 runner    (1001) docker     (123)     2681 2023-05-01 13:08:54.000000 espnet-202304/espnet2/torch_utils/device_funcs.py
--rw-r--r--   0 runner    (1001) docker     (123)     1052 2023-05-01 13:08:54.000000 espnet-202304/espnet2/torch_utils/forward_adaptor.py
--rw-r--r--   0 runner    (1001) docker     (123)     1411 2023-05-01 13:08:54.000000 espnet-202304/espnet2/torch_utils/get_layer_from_string.py
--rw-r--r--   0 runner    (1001) docker     (123)     4844 2023-05-01 13:08:54.000000 espnet-202304/espnet2/torch_utils/initialize.py
--rw-r--r--   0 runner    (1001) docker     (123)     3500 2023-05-01 13:08:54.000000 espnet-202304/espnet2/torch_utils/load_pretrained_model.py
--rw-r--r--   0 runner    (1001) docker     (123)     2498 2023-05-01 13:08:54.000000 espnet-202304/espnet2/torch_utils/model_summary.py
--rw-r--r--   0 runner    (1001) docker     (123)      468 2023-05-01 13:08:54.000000 espnet-202304/espnet2/torch_utils/pytorch_version.py
--rw-r--r--   0 runner    (1001) docker     (123)     1615 2023-05-01 13:08:54.000000 espnet-202304/espnet2/torch_utils/recursive_op.py
--rw-r--r--   0 runner    (1001) docker     (123)      167 2023-05-01 13:08:54.000000 espnet-202304/espnet2/torch_utils/set_all_random_seed.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.804659 espnet-202304/espnet2/train/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/train/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1366 2023-05-01 13:08:54.000000 espnet-202304/espnet2/train/abs_espnet_model.py
--rw-r--r--   0 runner    (1001) docker     (123)     2913 2023-05-01 13:08:54.000000 espnet-202304/espnet2/train/abs_gan_espnet_model.py
--rw-r--r--   0 runner    (1001) docker     (123)     2962 2023-05-01 13:08:54.000000 espnet-202304/espnet2/train/class_choices.py
--rw-r--r--   0 runner    (1001) docker     (123)     7460 2023-05-01 13:08:54.000000 espnet-202304/espnet2/train/collate_fn.py
--rw-r--r--   0 runner    (1001) docker     (123)    18511 2023-05-01 13:08:54.000000 espnet-202304/espnet2/train/dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)    13914 2023-05-01 13:08:54.000000 espnet-202304/espnet2/train/distributed_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    15057 2023-05-01 13:08:54.000000 espnet-202304/espnet2/train/gan_trainer.py
--rw-r--r--   0 runner    (1001) docker     (123)     8251 2023-05-01 13:08:54.000000 espnet-202304/espnet2/train/iterable_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)    51643 2023-05-01 13:08:54.000000 espnet-202304/espnet2/train/preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (123)    19602 2023-05-01 13:08:54.000000 espnet-202304/espnet2/train/reporter.py
--rw-r--r--   0 runner    (1001) docker     (123)    35902 2023-05-01 13:08:54.000000 espnet-202304/espnet2/train/trainer.py
--rw-r--r--   0 runner    (1001) docker     (123)    16645 2023-05-01 13:08:54.000000 espnet-202304/espnet2/train/uasr_trainer.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.804659 espnet-202304/espnet2/tts/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tts/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1113 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tts/abs_tts.py
--rw-r--r--   0 runner    (1001) docker     (123)    12407 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tts/espnet_model.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.804659 espnet-202304/espnet2/tts/fastspeech/
--rw-r--r--   0 runner    (1001) docker     (123)       65 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tts/fastspeech/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    29711 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tts/fastspeech/fastspeech.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.804659 espnet-202304/espnet2/tts/fastspeech2/
--rw-r--r--   0 runner    (1001) docker     (123)       68 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tts/fastspeech2/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    35997 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tts/fastspeech2/fastspeech2.py
--rw-r--r--   0 runner    (1001) docker     (123)     5331 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tts/fastspeech2/loss.py
--rw-r--r--   0 runner    (1001) docker     (123)     2624 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tts/fastspeech2/variance_predictor.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.804659 espnet-202304/espnet2/tts/feats_extract/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tts/feats_extract/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      503 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tts/feats_extract/abs_feats_extract.py
--rw-r--r--   0 runner    (1001) docker     (123)     6224 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tts/feats_extract/dio.py
--rw-r--r--   0 runner    (1001) docker     (123)     4733 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tts/feats_extract/energy.py
--rw-r--r--   0 runner    (1001) docker     (123)     2092 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tts/feats_extract/linear_spectrogram.py
--rw-r--r--   0 runner    (1001) docker     (123)     3044 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tts/feats_extract/log_mel_fbank.py
--rw-r--r--   0 runner    (1001) docker     (123)     2244 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tts/feats_extract/log_spectrogram.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.804659 espnet-202304/espnet2/tts/gst/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tts/gst/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    10101 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tts/gst/style_encoder.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.804659 espnet-202304/espnet2/tts/prodiff/
--rw-r--r--   0 runner    (1001) docker     (123)       56 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tts/prodiff/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    12143 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tts/prodiff/denoiser.py
--rw-r--r--   0 runner    (1001) docker     (123)     9540 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tts/prodiff/loss.py
--rw-r--r--   0 runner    (1001) docker     (123)    35170 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tts/prodiff/prodiff.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.804659 espnet-202304/espnet2/tts/tacotron2/
--rw-r--r--   0 runner    (1001) docker     (123)       62 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tts/tacotron2/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    21020 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tts/tacotron2/tacotron2.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.804659 espnet-202304/espnet2/tts/transformer/
--rw-r--r--   0 runner    (1001) docker     (123)       68 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tts/transformer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    34927 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tts/transformer/transformer.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.804659 espnet-202304/espnet2/tts/utils/
--rw-r--r--   0 runner    (1001) docker     (123)      247 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tts/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2291 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tts/utils/duration_calculator.py
--rw-r--r--   0 runner    (1001) docker     (123)     1921 2023-05-01 13:08:54.000000 espnet-202304/espnet2/tts/utils/parallel_wavegan_pretrained_vocoder.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.804659 espnet-202304/espnet2/uasr/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/uasr/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.804659 espnet-202304/espnet2/uasr/discriminator/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/uasr/discriminator/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      272 2023-05-01 13:08:54.000000 espnet-202304/espnet2/uasr/discriminator/abs_discriminator.py
--rw-r--r--   0 runner    (1001) docker     (123)     5634 2023-05-01 13:08:54.000000 espnet-202304/espnet2/uasr/discriminator/conv_discriminator.py
--rw-r--r--   0 runner    (1001) docker     (123)    15490 2023-05-01 13:08:54.000000 espnet-202304/espnet2/uasr/espnet_model.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.804659 espnet-202304/espnet2/uasr/generator/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/uasr/generator/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      430 2023-05-01 13:08:54.000000 espnet-202304/espnet2/uasr/generator/abs_generator.py
--rw-r--r--   0 runner    (1001) docker     (123)     5254 2023-05-01 13:08:54.000000 espnet-202304/espnet2/uasr/generator/conv_generator.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.808660 espnet-202304/espnet2/uasr/loss/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/uasr/loss/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      499 2023-05-01 13:08:54.000000 espnet-202304/espnet2/uasr/loss/abs_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)     1994 2023-05-01 13:08:54.000000 espnet-202304/espnet2/uasr/loss/discriminator_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)     3160 2023-05-01 13:08:54.000000 espnet-202304/espnet2/uasr/loss/gradient_penalty.py
--rw-r--r--   0 runner    (1001) docker     (123)     1316 2023-05-01 13:08:54.000000 espnet-202304/espnet2/uasr/loss/phoneme_diversity_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)     1847 2023-05-01 13:08:54.000000 espnet-202304/espnet2/uasr/loss/pseudo_label_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)     1308 2023-05-01 13:08:54.000000 espnet-202304/espnet2/uasr/loss/smoothness_penalty.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.808660 espnet-202304/espnet2/uasr/segmenter/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/uasr/segmenter/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      736 2023-05-01 13:08:54.000000 espnet-202304/espnet2/uasr/segmenter/abs_segmenter.py
--rw-r--r--   0 runner    (1001) docker     (123)     3165 2023-05-01 13:08:54.000000 espnet-202304/espnet2/uasr/segmenter/join_segmenter.py
--rw-r--r--   0 runner    (1001) docker     (123)     1222 2023-05-01 13:08:54.000000 espnet-202304/espnet2/uasr/segmenter/random_segmenter.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.808660 espnet-202304/espnet2/utils/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-01 13:08:54.000000 espnet-202304/espnet2/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      582 2023-05-01 13:08:54.000000 espnet-202304/espnet2/utils/build_dataclass.py
--rw-r--r--   0 runner    (1001) docker     (123)     1760 2023-05-01 13:08:54.000000 espnet-202304/espnet2/utils/config_argparse.py
--rw-r--r--   0 runner    (1001) docker     (123)     1830 2023-05-01 13:08:54.000000 espnet-202304/espnet2/utils/get_default_kwargs.py
--rw-r--r--   0 runner    (1001) docker     (123)     5607 2023-05-01 13:08:54.000000 espnet-202304/espnet2/utils/griffin_lim.py
--rw-r--r--   0 runner    (1001) docker     (123)      427 2023-05-01 13:08:54.000000 espnet-202304/espnet2/utils/kwargs2args.py
--rw-r--r--   0 runner    (1001) docker     (123)     3598 2023-05-01 13:08:54.000000 espnet-202304/espnet2/utils/nested_dict_action.py
--rw-r--r--   0 runner    (1001) docker     (123)     2027 2023-05-01 13:08:54.000000 espnet-202304/espnet2/utils/sized_dict.py
--rw-r--r--   0 runner    (1001) docker     (123)     4149 2023-05-01 13:08:54.000000 espnet-202304/espnet2/utils/types.py
--rw-r--r--   0 runner    (1001) docker     (123)      380 2023-05-01 13:08:54.000000 espnet-202304/espnet2/utils/yaml_no_alias_safe_dump.py
--rw-r--r--   0 runner    (1001) docker     (123)      371 2023-05-01 13:09:06.812660 espnet-202304/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (123)     5171 2023-05-01 13:08:54.000000 espnet-202304/setup.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-01 13:09:06.812660 espnet-202304/test/
--rw-r--r--   0 runner    (1001) docker     (123)     7583 2023-05-01 13:08:54.000000 espnet-202304/test/test_asr_init.py
--rw-r--r--   0 runner    (1001) docker     (123)      415 2023-05-01 13:08:54.000000 espnet-202304/test/test_asr_interface.py
--rw-r--r--   0 runner    (1001) docker     (123)      642 2023-05-01 13:08:54.000000 espnet-202304/test/test_asr_quantize.py
--rw-r--r--   0 runner    (1001) docker     (123)     5863 2023-05-01 13:08:54.000000 espnet-202304/test/test_batch_beam_search.py
--rw-r--r--   0 runner    (1001) docker     (123)     6366 2023-05-01 13:08:54.000000 espnet-202304/test/test_beam_search.py
--rw-r--r--   0 runner    (1001) docker     (123)     5619 2023-05-01 13:08:54.000000 espnet-202304/test/test_beam_search_timesync.py
--rw-r--r--   0 runner    (1001) docker     (123)     2348 2023-05-01 13:08:54.000000 espnet-202304/test/test_cli.py
--rw-r--r--   0 runner    (1001) docker     (123)    21623 2023-05-01 13:08:54.000000 espnet-202304/test/test_custom_transducer.py
--rw-r--r--   0 runner    (1001) docker     (123)     4110 2023-05-01 13:08:54.000000 espnet-202304/test/test_distributed_launch.py
--rw-r--r--   0 runner    (1001) docker     (123)    26225 2023-05-01 13:08:54.000000 espnet-202304/test/test_e2e_asr.py
--rw-r--r--   0 runner    (1001) docker     (123)     4895 2023-05-01 13:08:54.000000 espnet-202304/test/test_e2e_asr_conformer.py
--rw-r--r--   0 runner    (1001) docker     (123)     3708 2023-05-01 13:08:54.000000 espnet-202304/test/test_e2e_asr_maskctc.py
--rw-r--r--   0 runner    (1001) docker     (123)    21848 2023-05-01 13:08:54.000000 espnet-202304/test/test_e2e_asr_mulenc.py
--rw-r--r--   0 runner    (1001) docker     (123)    14894 2023-05-01 13:08:54.000000 espnet-202304/test/test_e2e_asr_transducer.py
--rw-r--r--   0 runner    (1001) docker     (123)     8385 2023-05-01 13:08:54.000000 espnet-202304/test/test_e2e_asr_transformer.py
--rw-r--r--   0 runner    (1001) docker     (123)     3521 2023-05-01 13:08:54.000000 espnet-202304/test/test_e2e_compatibility.py
--rw-r--r--   0 runner    (1001) docker     (123)    12864 2023-05-01 13:08:54.000000 espnet-202304/test/test_e2e_mt.py
--rw-r--r--   0 runner    (1001) docker     (123)     3754 2023-05-01 13:08:54.000000 espnet-202304/test/test_e2e_mt_transformer.py
--rw-r--r--   0 runner    (1001) docker     (123)    20475 2023-05-01 13:08:54.000000 espnet-202304/test/test_e2e_st.py
--rw-r--r--   0 runner    (1001) docker     (123)     4216 2023-05-01 13:08:54.000000 espnet-202304/test/test_e2e_st_conformer.py
--rw-r--r--   0 runner    (1001) docker     (123)     5352 2023-05-01 13:08:54.000000 espnet-202304/test/test_e2e_st_transformer.py
--rw-r--r--   0 runner    (1001) docker     (123)    21140 2023-05-01 13:08:54.000000 espnet-202304/test/test_e2e_tts_fastspeech.py
--rw-r--r--   0 runner    (1001) docker     (123)     8798 2023-05-01 13:08:54.000000 espnet-202304/test/test_e2e_tts_tacotron2.py
--rw-r--r--   0 runner    (1001) docker     (123)    15609 2023-05-01 13:08:54.000000 espnet-202304/test/test_e2e_tts_transformer.py
--rw-r--r--   0 runner    (1001) docker     (123)     8638 2023-05-01 13:08:54.000000 espnet-202304/test/test_e2e_vc_tacotron2.py
--rw-r--r--   0 runner    (1001) docker     (123)    16029 2023-05-01 13:08:54.000000 espnet-202304/test/test_e2e_vc_transformer.py
--rw-r--r--   0 runner    (1001) docker     (123)     3390 2023-05-01 13:08:54.000000 espnet-202304/test/test_initialization.py
--rw-r--r--   0 runner    (1001) docker     (123)      774 2023-05-01 13:08:54.000000 espnet-202304/test/test_io_voxforge.py
--rw-r--r--   0 runner    (1001) docker     (123)     6647 2023-05-01 13:08:54.000000 espnet-202304/test/test_lm.py
--rw-r--r--   0 runner    (1001) docker     (123)     5405 2023-05-01 13:08:54.000000 espnet-202304/test/test_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)     7289 2023-05-01 13:08:54.000000 espnet-202304/test/test_multi_spkrs.py
--rw-r--r--   0 runner    (1001) docker     (123)      464 2023-05-01 13:08:54.000000 espnet-202304/test/test_ngram.py
--rw-r--r--   0 runner    (1001) docker     (123)     3023 2023-05-01 13:08:54.000000 espnet-202304/test/test_optimizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     5074 2023-05-01 13:08:54.000000 espnet-202304/test/test_positional_encoding.py
--rw-r--r--   0 runner    (1001) docker     (123)     4644 2023-05-01 13:08:54.000000 espnet-202304/test/test_recog.py
--rw-r--r--   0 runner    (1001) docker     (123)     1247 2023-05-01 13:08:54.000000 espnet-202304/test/test_scheduler.py
--rw-r--r--   0 runner    (1001) docker     (123)     1171 2023-05-01 13:08:54.000000 espnet-202304/test/test_sentencepiece.py
--rw-r--r--   0 runner    (1001) docker     (123)     2079 2023-05-01 13:08:54.000000 espnet-202304/test/test_tensorboard.py
--rw-r--r--   0 runner    (1001) docker     (123)      852 2023-05-01 13:08:54.000000 espnet-202304/test/test_torch.py
--rw-r--r--   0 runner    (1001) docker     (123)     2795 2023-05-01 13:08:54.000000 espnet-202304/test/test_train_dtype.py
--rw-r--r--   0 runner    (1001) docker     (123)     2973 2023-05-01 13:08:54.000000 espnet-202304/test/test_transform.py
--rw-r--r--   0 runner    (1001) docker     (123)     4486 2023-05-01 13:08:54.000000 espnet-202304/test/test_transformer_decode.py
--rw-r--r--   0 runner    (1001) docker     (123)     9830 2023-05-01 13:08:54.000000 espnet-202304/test/test_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.746057 espnet-202308/
+-rw-r--r--   0 runner    (1001) docker     (123)    11372 2023-08-03 13:36:36.000000 espnet-202308/LICENSE
+-rw-r--r--   0 runner    (1001) docker     (123)    69724 2023-08-03 13:36:56.746057 espnet-202308/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)    61688 2023-08-03 13:36:36.000000 espnet-202308/README.md
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.602048 espnet-202308/espnet/
+-rw-r--r--   0 runner    (1001) docker     (123)      203 2023-08-03 13:36:37.000000 espnet-202308/espnet/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.602048 espnet-202308/espnet/asr/
+-rw-r--r--   0 runner    (1001) docker     (123)       30 2023-08-03 13:36:37.000000 espnet-202308/espnet/asr/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6407 2023-08-03 13:36:37.000000 espnet-202308/espnet/asr/asr_mix_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    34453 2023-08-03 13:36:37.000000 espnet-202308/espnet/asr/asr_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.602048 espnet-202308/espnet/asr/chainer_backend/
+-rw-r--r--   0 runner    (1001) docker     (123)       30 2023-08-03 13:36:37.000000 espnet-202308/espnet/asr/chainer_backend/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19506 2023-08-03 13:36:37.000000 espnet-202308/espnet/asr/chainer_backend/asr.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.606049 espnet-202308/espnet/asr/pytorch_backend/
+-rw-r--r--   0 runner    (1001) docker     (123)       30 2023-08-03 13:36:37.000000 espnet-202308/espnet/asr/pytorch_backend/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    61311 2023-08-03 13:36:37.000000 espnet-202308/espnet/asr/pytorch_backend/asr.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10707 2023-08-03 13:36:37.000000 espnet-202308/espnet/asr/pytorch_backend/asr_init.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22054 2023-08-03 13:36:37.000000 espnet-202308/espnet/asr/pytorch_backend/asr_mix.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6635 2023-08-03 13:36:37.000000 espnet-202308/espnet/asr/pytorch_backend/recog.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.606049 espnet-202308/espnet/bin/
+-rw-r--r--   0 runner    (1001) docker     (123)       30 2023-08-03 13:36:37.000000 espnet-202308/espnet/bin/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    13210 2023-08-03 13:36:37.000000 espnet-202308/espnet/bin/asr_align.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     5778 2023-08-03 13:36:37.000000 espnet-202308/espnet/bin/asr_enhance.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    13800 2023-08-03 13:36:37.000000 espnet-202308/espnet/bin/asr_recog.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    22288 2023-08-03 13:36:37.000000 espnet-202308/espnet/bin/asr_train.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     8939 2023-08-03 13:36:37.000000 espnet-202308/espnet/bin/lm_train.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    14923 2023-08-03 13:36:37.000000 espnet-202308/espnet/bin/mt_train.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     5956 2023-08-03 13:36:37.000000 espnet-202308/espnet/bin/mt_trans.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    17332 2023-08-03 13:36:37.000000 espnet-202308/espnet/bin/st_train.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     5855 2023-08-03 13:36:37.000000 espnet-202308/espnet/bin/st_trans.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     5267 2023-08-03 13:36:37.000000 espnet-202308/espnet/bin/tts_decode.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    10680 2023-08-03 13:36:37.000000 espnet-202308/espnet/bin/tts_train.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     5094 2023-08-03 13:36:37.000000 espnet-202308/espnet/bin/vc_decode.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    10830 2023-08-03 13:36:37.000000 espnet-202308/espnet/bin/vc_train.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.606049 espnet-202308/espnet/distributed/
+-rw-r--r--   0 runner    (1001) docker     (123)      176 2023-08-03 13:36:37.000000 espnet-202308/espnet/distributed/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.606049 espnet-202308/espnet/lm/
+-rw-r--r--   0 runner    (1001) docker     (123)       30 2023-08-03 13:36:37.000000 espnet-202308/espnet/lm/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.606049 espnet-202308/espnet/lm/chainer_backend/
+-rw-r--r--   0 runner    (1001) docker     (123)       30 2023-08-03 13:36:37.000000 espnet-202308/espnet/lm/chainer_backend/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8768 2023-08-03 13:36:37.000000 espnet-202308/espnet/lm/chainer_backend/extlm.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18196 2023-08-03 13:36:37.000000 espnet-202308/espnet/lm/chainer_backend/lm.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10853 2023-08-03 13:36:37.000000 espnet-202308/espnet/lm/lm_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.606049 espnet-202308/espnet/lm/pytorch_backend/
+-rw-r--r--   0 runner    (1001) docker     (123)       30 2023-08-03 13:36:37.000000 espnet-202308/espnet/lm/pytorch_backend/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9546 2023-08-03 13:36:37.000000 espnet-202308/espnet/lm/pytorch_backend/extlm.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14856 2023-08-03 13:36:37.000000 espnet-202308/espnet/lm/pytorch_backend/lm.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.610049 espnet-202308/espnet/mt/
+-rw-r--r--   0 runner    (1001) docker     (123)       30 2023-08-03 13:36:37.000000 espnet-202308/espnet/mt/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2571 2023-08-03 13:36:37.000000 espnet-202308/espnet/mt/mt_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.610049 espnet-202308/espnet/mt/pytorch_backend/
+-rw-r--r--   0 runner    (1001) docker     (123)       30 2023-08-03 13:36:37.000000 espnet-202308/espnet/mt/pytorch_backend/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19744 2023-08-03 13:36:37.000000 espnet-202308/espnet/mt/pytorch_backend/mt.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.610049 espnet-202308/espnet/nets/
+-rw-r--r--   0 runner    (1001) docker     (123)       30 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5957 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/asr_interface.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15991 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/batch_beam_search.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19374 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/batch_beam_search_online.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10567 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/batch_beam_search_online_sim.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21843 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/beam_search.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8986 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/beam_search_timesync.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12681 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/beam_search_timesync_streaming.py
+-rw-r--r--   0 runner    (1001) docker     (123)    30731 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/beam_search_transducer.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.614049 espnet-202308/espnet/nets/chainer_backend/
+-rw-r--r--   0 runner    (1001) docker     (123)       30 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/chainer_backend/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1033 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/chainer_backend/asr_interface.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3004 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/chainer_backend/ctc.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8531 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/chainer_backend/deterministic_embed_id.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7619 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/chainer_backend/e2e_asr.py
+-rw-r--r--   0 runner    (1001) docker     (123)    23226 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/chainer_backend/e2e_asr_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)      187 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/chainer_backend/nets_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.614049 espnet-202308/espnet/nets/chainer_backend/rnn/
+-rw-r--r--   0 runner    (1001) docker     (123)       30 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/chainer_backend/rnn/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9422 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/chainer_backend/rnn/attentions.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19976 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/chainer_backend/rnn/decoders.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11030 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/chainer_backend/rnn/encoders.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9245 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/chainer_backend/rnn/training.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.618049 espnet-202308/espnet/nets/chainer_backend/transformer/
+-rw-r--r--   0 runner    (1001) docker     (123)       30 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/chainer_backend/transformer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3423 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/chainer_backend/transformer/attention.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2677 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/chainer_backend/transformer/ctc.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4319 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/chainer_backend/transformer/decoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2526 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/chainer_backend/transformer/decoder_layer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1204 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/chainer_backend/transformer/embedding.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4911 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/chainer_backend/transformer/encoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1903 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/chainer_backend/transformer/encoder_layer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2455 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/chainer_backend/transformer/label_smoothing_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)      441 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/chainer_backend/transformer/layer_norm.py
+-rw-r--r--   0 runner    (1001) docker     (123)      489 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/chainer_backend/transformer/mask.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1946 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/chainer_backend/transformer/positionwise_feed_forward.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3464 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/chainer_backend/transformer/subsampling.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11798 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/chainer_backend/transformer/training.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13899 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/ctc_prefix_score.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8773 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/e2e_asr_common.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4216 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/e2e_mt_common.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2590 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/lm_interface.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3428 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/mt_interface.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.622050 espnet-202308/espnet/nets/pytorch_backend/
+-rw-r--r--   0 runner    (1001) docker     (123)       30 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.622050 espnet-202308/espnet/nets/pytorch_backend/conformer/
+-rw-r--r--   0 runner    (1001) docker     (123)       30 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/conformer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2799 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/conformer/argument.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11220 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/conformer/contextual_block_encoder_layer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2193 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/conformer/convolution.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12154 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/conformer/encoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6761 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/conformer/encoder_layer.py
+-rw-r--r--   0 runner    (1001) docker     (123)      483 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/conformer/swish.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9877 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/ctc.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19358 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/e2e_asr.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2955 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/e2e_asr_conformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10739 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/e2e_asr_maskctc.py
+-rw-r--r--   0 runner    (1001) docker     (123)    30819 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/e2e_asr_mix.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18143 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/e2e_asr_mix_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    31487 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/e2e_asr_mulenc.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18123 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/e2e_asr_transducer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22522 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/e2e_asr_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13630 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/e2e_mt.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16007 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/e2e_mt_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24036 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/e2e_st.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2594 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/e2e_st_conformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22657 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/e2e_st_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    34348 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/e2e_tts_fastspeech.py
+-rw-r--r--   0 runner    (1001) docker     (123)    34041 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/e2e_tts_tacotron2.py
+-rw-r--r--   0 runner    (1001) docker     (123)    45641 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/e2e_tts_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    30409 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/e2e_vc_tacotron2.py
+-rw-r--r--   0 runner    (1001) docker     (123)    46109 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/e2e_vc_transformer.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.622050 espnet-202308/espnet/nets/pytorch_backend/fastspeech/
+-rw-r--r--   0 runner    (1001) docker     (123)       30 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/fastspeech/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3600 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/fastspeech/duration_calculator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4992 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/fastspeech/duration_predictor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2171 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/fastspeech/length_regulator.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.622050 espnet-202308/espnet/nets/pytorch_backend/frontends/
+-rw-r--r--   0 runner    (1001) docker     (123)       30 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/frontends/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2731 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/frontends/beamformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5480 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/frontends/dnn_beamformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2851 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/frontends/dnn_wpe.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7935 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/frontends/feature_transform.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4557 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/frontends/frontend.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2637 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/frontends/mask_estimator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3974 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/gtn_ctc.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1561 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/initialization.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.626050 espnet-202308/espnet/nets/pytorch_backend/lm/
+-rw-r--r--   0 runner    (1001) docker     (123)       30 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/lm/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14561 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/lm/default.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5940 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/lm/seq_rnn.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8410 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/lm/transformer.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.626050 espnet-202308/espnet/nets/pytorch_backend/maskctc/
+-rw-r--r--   0 runner    (1001) docker     (123)       30 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/maskctc/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1352 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/maskctc/add_mask_token.py
+-rw-r--r--   0 runner    (1001) docker     (123)      803 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/maskctc/mask.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19467 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/nets_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.626050 espnet-202308/espnet/nets/pytorch_backend/rnn/
+-rw-r--r--   0 runner    (1001) docker     (123)       30 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/rnn/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4080 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/rnn/argument.py
+-rw-r--r--   0 runner    (1001) docker     (123)    73205 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/rnn/attentions.py
+-rw-r--r--   0 runner    (1001) docker     (123)    48709 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/rnn/decoders.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14152 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/rnn/encoders.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.626050 espnet-202308/espnet/nets/pytorch_backend/streaming/
+-rw-r--r--   0 runner    (1001) docker     (123)       30 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/streaming/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4774 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/streaming/segment.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2768 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/streaming/window.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.626050 espnet-202308/espnet/nets/pytorch_backend/tacotron2/
+-rw-r--r--   0 runner    (1001) docker     (123)       30 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/tacotron2/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9068 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/tacotron2/cbhg.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24375 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/tacotron2/decoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6261 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/tacotron2/encoder.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.630050 espnet-202308/espnet/nets/pytorch_backend/transducer/
+-rw-r--r--   0 runner    (1001) docker     (123)       30 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/transducer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10628 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/transducer/arguments.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16356 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/transducer/blocks.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7246 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/transducer/conv1d_nets.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9049 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/transducer/custom_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4661 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/transducer/custom_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4829 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/transducer/error_calculator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1304 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/transducer/initializer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2306 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/transducer/joint_network.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9028 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/transducer/rnn_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18464 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/transducer/rnn_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15051 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/transducer/transducer_tasks.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2702 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/transducer/transformer_decoder_layer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10508 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/transducer/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2782 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/transducer/vgg2l.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.634050 espnet-202308/espnet/nets/pytorch_backend/transformer/
+-rw-r--r--   0 runner    (1001) docker     (123)       30 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/transformer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      957 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/transformer/add_sos_eos.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5280 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/transformer/argument.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11646 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/transformer/attention.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8899 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/transformer/contextual_block_encoder_layer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13734 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/transformer/decoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6458 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/transformer/decoder_layer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4243 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/transformer/dynamic_conv.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4862 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/transformer/dynamic_conv2d.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12758 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/transformer/embedding.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15107 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/transformer/encoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4261 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/transformer/encoder_layer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6407 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/transformer/encoder_mix.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1383 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/transformer/initializer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2164 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/transformer/label_smoothing_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)      958 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/transformer/layer_norm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3589 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/transformer/lightconv.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4229 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/transformer/lightconv2d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2070 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/transformer/longformer_attention.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1037 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/transformer/mask.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3164 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/transformer/multi_layer_conv.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2094 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/transformer/optimizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5718 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/transformer/plot.py
+-rw-r--r--   0 runner    (1001) docker     (123)      983 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/transformer/positionwise_feed_forward.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1299 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/transformer/repeat.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14351 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/transformer/subsampling.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1899 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/transformer/subsampling_without_posenc.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13938 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/pytorch_backend/wavenet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5902 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/scorer_interface.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.634050 espnet-202308/espnet/nets/scorers/
+-rw-r--r--   0 runner    (1001) docker     (123)       30 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/scorers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4953 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/scorers/ctc.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1740 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/scorers/length_bonus.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3080 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/scorers/ngram.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1469 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/scorers/uasr.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2271 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/st_interface.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4282 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/transducer_decoder_interface.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2582 2023-08-03 13:36:37.000000 espnet-202308/espnet/nets/tts_interface.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.634050 espnet-202308/espnet/optimizer/
+-rw-r--r--   0 runner    (1001) docker     (123)       30 2023-08-03 13:36:37.000000 espnet-202308/espnet/optimizer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2588 2023-08-03 13:36:37.000000 espnet-202308/espnet/optimizer/chainer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2013 2023-08-03 13:36:37.000000 espnet-202308/espnet/optimizer/factory.py
+-rw-r--r--   0 runner    (1001) docker     (123)      968 2023-08-03 13:36:37.000000 espnet-202308/espnet/optimizer/parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2399 2023-08-03 13:36:37.000000 espnet-202308/espnet/optimizer/pytorch.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.638051 espnet-202308/espnet/scheduler/
+-rw-r--r--   0 runner    (1001) docker     (123)       30 2023-08-03 13:36:37.000000 espnet-202308/espnet/scheduler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      781 2023-08-03 13:36:37.000000 espnet-202308/espnet/scheduler/chainer.py
+-rw-r--r--   0 runner    (1001) docker     (123)      801 2023-08-03 13:36:37.000000 espnet-202308/espnet/scheduler/pytorch.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4695 2023-08-03 13:36:37.000000 espnet-202308/espnet/scheduler/scheduler.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.638051 espnet-202308/espnet/st/
+-rw-r--r--   0 runner    (1001) docker     (123)       30 2023-08-03 13:36:37.000000 espnet-202308/espnet/st/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.638051 espnet-202308/espnet/st/pytorch_backend/
+-rw-r--r--   0 runner    (1001) docker     (123)       30 2023-08-03 13:36:37.000000 espnet-202308/espnet/st/pytorch_backend/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22845 2023-08-03 13:36:37.000000 espnet-202308/espnet/st/pytorch_backend/st.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.638051 espnet-202308/espnet/transform/
+-rw-r--r--   0 runner    (1001) docker     (123)       31 2023-08-03 13:36:37.000000 espnet-202308/espnet/transform/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      953 2023-08-03 13:36:37.000000 espnet-202308/espnet/transform/add_deltas.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1355 2023-08-03 13:36:37.000000 espnet-202308/espnet/transform/channel_selector.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4581 2023-08-03 13:36:37.000000 espnet-202308/espnet/transform/cmvn.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1964 2023-08-03 13:36:37.000000 espnet-202308/espnet/transform/functional.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11299 2023-08-03 13:36:37.000000 espnet-202308/espnet/transform/perturb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5933 2023-08-03 13:36:37.000000 espnet-202308/espnet/transform/spec_augment.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8080 2023-08-03 13:36:37.000000 espnet-202308/espnet/transform/spectrogram.py
+-rw-r--r--   0 runner    (1001) docker     (123)      466 2023-08-03 13:36:37.000000 espnet-202308/espnet/transform/transform_interface.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5871 2023-08-03 13:36:37.000000 espnet-202308/espnet/transform/transformation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1344 2023-08-03 13:36:37.000000 espnet-202308/espnet/transform/wpe.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.638051 espnet-202308/espnet/tts/
+-rw-r--r--   0 runner    (1001) docker     (123)       30 2023-08-03 13:36:37.000000 espnet-202308/espnet/tts/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.638051 espnet-202308/espnet/tts/pytorch_backend/
+-rw-r--r--   0 runner    (1001) docker     (123)       30 2023-08-03 13:36:37.000000 espnet-202308/espnet/tts/pytorch_backend/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    26532 2023-08-03 13:36:37.000000 espnet-202308/espnet/tts/pytorch_backend/tts.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.642051 espnet-202308/espnet/utils/
+-rw-r--r--   0 runner    (1001) docker     (123)       30 2023-08-03 13:36:37.000000 espnet-202308/espnet/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      633 2023-08-03 13:36:37.000000 espnet-202308/espnet/utils/check_kwargs.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8273 2023-08-03 13:36:37.000000 espnet-202308/espnet/utils/cli_readers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1380 2023-08-03 13:36:37.000000 espnet-202308/espnet/utils/cli_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8821 2023-08-03 13:36:37.000000 espnet-202308/espnet/utils/cli_writers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3703 2023-08-03 13:36:37.000000 espnet-202308/espnet/utils/dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1794 2023-08-03 13:36:37.000000 espnet-202308/espnet/utils/deterministic_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)      778 2023-08-03 13:36:37.000000 espnet-202308/espnet/utils/dynamic_import.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1426 2023-08-03 13:36:37.000000 espnet-202308/espnet/utils/fill_missing_args.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24831 2023-08-03 13:36:37.000000 espnet-202308/espnet/utils/io_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18458 2023-08-03 13:36:37.000000 espnet-202308/espnet/utils/spec_augment.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.642051 espnet-202308/espnet/utils/training/
+-rw-r--r--   0 runner    (1001) docker     (123)       30 2023-08-03 13:36:37.000000 espnet-202308/espnet/utils/training/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18371 2023-08-03 13:36:37.000000 espnet-202308/espnet/utils/training/batchfy.py
+-rw-r--r--   0 runner    (1001) docker     (123)      560 2023-08-03 13:36:37.000000 espnet-202308/espnet/utils/training/evaluator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3526 2023-08-03 13:36:37.000000 espnet-202308/espnet/utils/training/iterators.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1915 2023-08-03 13:36:37.000000 espnet-202308/espnet/utils/training/tensorboard_logger.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1220 2023-08-03 13:36:37.000000 espnet-202308/espnet/utils/training/train_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)        7 2023-08-03 13:36:37.000000 espnet-202308/espnet/version.txt
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.602048 espnet-202308/espnet.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (123)    69724 2023-08-03 13:36:56.000000 espnet-202308/espnet.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)    31499 2023-08-03 13:36:56.000000 espnet-202308/espnet.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-08-03 13:36:56.000000 espnet-202308/espnet.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (123)     1538 2023-08-03 13:36:56.000000 espnet-202308/espnet.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       15 2023-08-03 13:36:56.000000 espnet-202308/espnet.egg-info/top_level.txt
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.642051 espnet-202308/espnet2/
+-rw-r--r--   0 runner    (1001) docker     (123)       74 2023-08-03 13:36:37.000000 espnet-202308/espnet2/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.642051 espnet-202308/espnet2/asr/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6600 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/ctc.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.646051 espnet-202308/espnet2/asr/decoder/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/decoder/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      447 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/decoder/abs_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10432 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/decoder/hugging_face_transformers_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4749 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/decoder/mlm_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12120 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/decoder/rnn_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5441 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/decoder/s4_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9252 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/decoder/transducer_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)    29252 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/decoder/transformer_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6090 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/decoder/whisper_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11037 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/discrete_asr_espnet_model.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.646051 espnet-202308/espnet2/asr/encoder/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/encoder/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      470 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/encoder/abs_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21255 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/encoder/branchformer_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15439 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/encoder/conformer_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)    23410 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/encoder/contextual_block_conformer_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21748 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/encoder/contextual_block_transformer_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18444 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/encoder/e_branchformer_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)    25119 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/encoder/hubert_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2785 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/encoder/hugging_face_transformers_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15394 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/encoder/longformer_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3587 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/encoder/rnn_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9402 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/encoder/transformer_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8820 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/encoder/transformer_encoder_multispkr.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3415 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/encoder/vgg_rnn_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5628 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/encoder/wav2vec2_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5639 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/encoder/whisper_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24373 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/espnet_model.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.650051 espnet-202308/espnet2/asr/frontend/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/frontend/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      385 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/frontend/abs_frontend.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4417 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/frontend/default.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5752 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/frontend/fused.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4306 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/frontend/s3prl.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3963 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/frontend/whisper.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2817 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/frontend/windowing.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.650051 espnet-202308/espnet2/asr/layers/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/layers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3518 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/layers/cgmlp.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5282 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/layers/fastformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12296 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/maskctc_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11924 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/pit_espnet_model.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.650051 espnet-202308/espnet2/asr/postencoder/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/postencoder/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      388 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/postencoder/abs_postencoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6584 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/postencoder/hugging_face_transformers_postencoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2869 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/postencoder/length_adaptor_postencoder.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.650051 espnet-202308/espnet2/asr/preencoder/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/preencoder/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      387 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/preencoder/abs_preencoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1095 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/preencoder/linear.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10226 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/preencoder/sinc.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.650051 espnet-202308/espnet2/asr/specaug/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/specaug/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      408 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/specaug/abs_specaug.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3435 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/specaug/specaug.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.654052 espnet-202308/espnet2/asr/state_spaces/
+-rw-r--r--   0 runner    (1001) docker     (123)       30 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/state_spaces/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4629 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/state_spaces/attention.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5487 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/state_spaces/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5330 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/state_spaces/block.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4022 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/state_spaces/cauchy.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13449 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/state_spaces/components.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1923 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/state_spaces/ff.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6438 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/state_spaces/model.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11292 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/state_spaces/pool.py
+-rw-r--r--   0 runner    (1001) docker     (123)      170 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/state_spaces/registry.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3422 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/state_spaces/residual.py
+-rw-r--r--   0 runner    (1001) docker     (123)    61257 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/state_spaces/s4.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3469 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/state_spaces/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.654052 espnet-202308/espnet2/asr/transducer/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/transducer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    33087 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/transducer/beam_search_transducer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    33195 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/transducer/beam_search_transducer_streaming.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4907 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/transducer/error_calculator.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.654052 espnet-202308/espnet2/asr/transducer/rnnt_multi_blank/
+-rw-r--r--   0 runner    (1001) docker     (123)      750 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/transducer/rnnt_multi_blank/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13479 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/transducer/rnnt_multi_blank/rnnt.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18251 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/transducer/rnnt_multi_blank/rnnt_multi_blank.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.654052 espnet-202308/espnet2/asr/transducer/rnnt_multi_blank/utils/
+-rw-r--r--   0 runner    (1001) docker     (123)      610 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/transducer/rnnt_multi_blank/utils/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.654052 espnet-202308/espnet2/asr/transducer/rnnt_multi_blank/utils/cpu_utils/
+-rw-r--r--   0 runner    (1001) docker     (123)     1194 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/transducer/rnnt_multi_blank/utils/cpu_utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16277 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/transducer/rnnt_multi_blank/utils/cpu_utils/cpu_rnnt.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.654052 espnet-202308/espnet2/asr/transducer/rnnt_multi_blank/utils/cuda_utils/
+-rw-r--r--   0 runner    (1001) docker     (123)     1194 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/transducer/rnnt_multi_blank/utils/cuda_utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21906 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/transducer/rnnt_multi_blank/utils/cuda_utils/gpu_rnnt.py
+-rw-r--r--   0 runner    (1001) docker     (123)    46645 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/transducer/rnnt_multi_blank/utils/cuda_utils/gpu_rnnt_kernel.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12677 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/transducer/rnnt_multi_blank/utils/cuda_utils/reduce.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1727 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/transducer/rnnt_multi_blank/utils/global_constants.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3802 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr/transducer/rnnt_multi_blank/utils/rnnt_helper.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.658052 espnet-202308/espnet2/asr_transducer/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr_transducer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6737 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr_transducer/activation.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22742 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr_transducer/beam_search_transducer.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.658052 espnet-202308/espnet2/asr_transducer/decoder/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr_transducer/decoder/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3755 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr_transducer/decoder/abs_decoder.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.658052 espnet-202308/espnet2/asr_transducer/decoder/blocks/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr_transducer/decoder/blocks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9571 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr_transducer/decoder/blocks/mega.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2602 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr_transducer/decoder/blocks/rwkv.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11982 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr_transducer/decoder/mega_decoder.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.658052 espnet-202308/espnet2/asr_transducer/decoder/modules/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr_transducer/decoder/modules/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.658052 espnet-202308/espnet2/asr_transducer/decoder/modules/mega/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr_transducer/decoder/modules/mega/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2076 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr_transducer/decoder/modules/mega/feed_forward.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7009 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr_transducer/decoder/modules/mega/multi_head_damped_ema.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5037 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr_transducer/decoder/modules/mega/positional_bias.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.658052 espnet-202308/espnet2/asr_transducer/decoder/modules/rwkv/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr_transducer/decoder/modules/rwkv/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11396 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr_transducer/decoder/modules/rwkv/attention.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3038 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr_transducer/decoder/modules/rwkv/feed_forward.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7707 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr_transducer/decoder/rnn_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8252 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr_transducer/decoder/rwkv_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4095 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr_transducer/decoder/stateless_decoder.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.662052 espnet-202308/espnet2/asr_transducer/encoder/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr_transducer/encoder/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.662052 espnet-202308/espnet2/asr_transducer/encoder/blocks/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr_transducer/encoder/blocks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5017 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr_transducer/encoder/blocks/branchformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5560 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr_transducer/encoder/blocks/conformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6398 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr_transducer/encoder/blocks/conv1d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3412 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr_transducer/encoder/blocks/conv_input.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6771 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr_transducer/encoder/blocks/ebranchformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12946 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr_transducer/encoder/building.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5102 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr_transducer/encoder/encoder.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.662052 espnet-202308/espnet2/asr_transducer/encoder/modules/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr_transducer/encoder/modules/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8228 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr_transducer/encoder/modules/attention.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7416 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr_transducer/encoder/modules/convolution.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3435 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr_transducer/encoder/modules/multi_blocks.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2878 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr_transducer/encoder/modules/positional_encoding.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5360 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr_transducer/encoder/validation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6166 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr_transducer/error_calculator.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21402 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr_transducer/espnet_transducer_model.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.662052 espnet-202308/espnet2/asr_transducer/frontend/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr_transducer/frontend/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5265 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr_transducer/frontend/online_audio_processor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2173 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr_transducer/joint_network.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4449 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr_transducer/normalization.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5616 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asr_transducer/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.662052 espnet-202308/espnet2/asvspoof/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asvspoof/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.662052 espnet-202308/espnet2/asvspoof/decoder/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asvspoof/decoder/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      304 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asvspoof/decoder/abs_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)      805 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asvspoof/decoder/linear_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6487 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asvspoof/espnet_model.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.662052 espnet-202308/espnet2/asvspoof/loss/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asvspoof/loss/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      646 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asvspoof/loss/abs_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2180 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asvspoof/loss/am_softmax_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)      854 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asvspoof/loss/binary_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1850 2023-08-03 13:36:37.000000 espnet-202308/espnet2/asvspoof/loss/oc_softmax_loss.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.674053 espnet-202308/espnet2/bin/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3756 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/aggregate_stats_dirs.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    32304 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/asr_align.py
+-rw-r--r--   0 runner    (1001) docker     (123)    37090 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/asr_inference.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    26605 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/asr_inference_k2.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11952 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/asr_inference_maskctc.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    21829 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/asr_inference_streaming.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)      431 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/asr_train.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    23357 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/asr_transducer_inference.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)      599 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/asr_transducer_train.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7971 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/asvspoof_inference.py
+-rw-r--r--   0 runner    (1001) docker     (123)      474 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/asvspoof_train.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    26815 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/diar_inference.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)      477 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/diar_train.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    21910 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/enh_inference.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    14477 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/enh_inference_streaming.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)      467 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/enh_s2t_train.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    13380 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/enh_scoring.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)      472 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/enh_train.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    22792 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/enh_tse_inference.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)      531 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/enh_tse_train.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)      413 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/gan_svs_train.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)      413 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/gan_tts_train.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)      454 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/hubert_train.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3173 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/hugging_face_export_vocabulary.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    13042 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/launch.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     6595 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/lm_calc_perplexity.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17623 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/lm_inference.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)      383 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/lm_train.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    17802 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/mt_inference.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)      421 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/mt_train.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3119 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/pack.py
+-rw-r--r--   0 runner    (1001) docker     (123)    23605 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/slu_inference.py
+-rw-r--r--   0 runner    (1001) docker     (123)      431 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/slu_train.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)      643 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/spk_train.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3300 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/split_scps.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    36073 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/st_inference.py
+-rw-r--r--   0 runner    (1001) docker     (123)    28072 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/st_inference_streaming.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)      421 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/st_train.py
+-rw-r--r--   0 runner    (1001) docker     (123)    23443 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/svs_inference.py
+-rw-r--r--   0 runner    (1001) docker     (123)      389 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/svs_train.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8468 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/tokenize_text.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    25855 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/tts_inference.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)      389 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/tts_train.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4900 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/uasr_extract_feature.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19155 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/uasr_inference.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    21667 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/uasr_inference_k2.py
+-rw-r--r--   0 runner    (1001) docker     (123)      441 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/uasr_train.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2848 2023-08-03 13:36:37.000000 espnet-202308/espnet2/bin/whisper_export_vocabulary.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.674053 espnet-202308/espnet2/diar/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/diar/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      643 2023-08-03 13:36:37.000000 espnet-202308/espnet2/diar/abs_diar.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.674053 espnet-202308/espnet2/diar/attractor/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/diar/attractor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      343 2023-08-03 13:36:37.000000 espnet-202308/espnet2/diar/attractor/abs_attractor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2007 2023-08-03 13:36:37.000000 espnet-202308/espnet2/diar/attractor/rnn_attractor.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.674053 espnet-202308/espnet2/diar/decoder/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/diar/decoder/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      396 2023-08-03 13:36:37.000000 espnet-202308/espnet2/diar/decoder/abs_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)      754 2023-08-03 13:36:37.000000 espnet-202308/espnet2/diar/decoder/linear_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14924 2023-08-03 13:36:37.000000 espnet-202308/espnet2/diar/espnet_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)      749 2023-08-03 13:36:37.000000 espnet-202308/espnet2/diar/label_processor.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.674053 espnet-202308/espnet2/diar/layers/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/diar/layers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      474 2023-08-03 13:36:37.000000 espnet-202308/espnet2/diar/layers/abs_mask.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4124 2023-08-03 13:36:37.000000 espnet-202308/espnet2/diar/layers/multi_mask.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7703 2023-08-03 13:36:37.000000 espnet-202308/espnet2/diar/layers/tcn_nomask.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.674053 espnet-202308/espnet2/diar/separator/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/diar/separator/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2737 2023-08-03 13:36:37.000000 espnet-202308/espnet2/diar/separator/tcn_separator_nomask.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.678053 espnet-202308/espnet2/enh/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      643 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/abs_enh.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.678053 espnet-202308/espnet2/enh/decoder/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/decoder/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      975 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/decoder/abs_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3014 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/decoder/conv_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)      493 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/decoder/null_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6263 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/decoder/stft_decoder.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.678053 espnet-202308/espnet2/enh/encoder/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/encoder/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1041 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/encoder/abs_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2731 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/encoder/conv_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)      503 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/encoder/null_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4672 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/encoder/stft_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19330 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/espnet_enh_s2t_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21569 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/espnet_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13324 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/espnet_model_tse.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.678053 espnet-202308/espnet2/enh/extractor/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/extractor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      458 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/extractor/abs_extractor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6590 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/extractor/td_speakerbeam_extractor.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.682053 espnet-202308/espnet2/enh/layers/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/layers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4373 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/layers/adapt_layers.py
+-rw-r--r--   0 runner    (1001) docker     (123)    42560 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/layers/beamformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    37809 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/layers/beamformer_th.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6751 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/layers/complex_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14634 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/layers/complexnn.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1462 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/layers/conv_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18544 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/layers/dc_crn.py
+-rw-r--r--   0 runner    (1001) docker     (123)    23362 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/layers/dnn_beamformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5512 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/layers/dnn_wpe.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5332 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/layers/dnsmos.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6771 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/layers/dpmulcat.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14234 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/layers/dprnn.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6545 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/layers/dptnet.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14370 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/layers/fasnet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7647 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/layers/ifasnet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3390 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/layers/mask_estimator.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13471 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/layers/skim.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16167 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/layers/tcn.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14465 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/layers/tcndenseunet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7858 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/layers/wpe.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.682053 espnet-202308/espnet2/enh/loss/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/loss/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.682053 espnet-202308/espnet2/enh/loss/criterions/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/loss/criterions/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      705 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/loss/criterions/abs_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16625 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/loss/criterions/tf_domain.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13715 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/loss/criterions/time_domain.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.682053 espnet-202308/espnet2/enh/loss/wrappers/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/loss/wrappers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/loss/wrappers/abs_wrapper.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1083 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/loss/wrappers/dpcl_solver.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1393 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/loss/wrappers/fixed_order.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4145 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/loss/wrappers/mixit_solver.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3042 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/loss/wrappers/multilayer_pit_solver.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4627 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/loss/wrappers/pit_solver.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.686053 espnet-202308/espnet2/enh/separator/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/separator/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      652 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/separator/abs_separator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5364 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/separator/asteroid_models.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7130 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/separator/conformer_separator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6012 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/separator/dan_separator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7510 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/separator/dc_crn_separator.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13915 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/separator/dccrn_separator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6677 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/separator/dpcl_e2e_separator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4878 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/separator/dpcl_separator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4412 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/separator/dprnn_separator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6828 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/separator/dptnet_separator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3677 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/separator/fasnet_separator.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11343 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/separator/ineube_separator.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10439 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/separator/neural_beamformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4807 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/separator/rnn_separator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5293 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/separator/skim_separator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7042 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/separator/svoice_separator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4478 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/separator/tcn_separator.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14294 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/separator/tfgridnet_separator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6107 2023-08-03 13:36:37.000000 espnet-202308/espnet2/enh/separator/transformer_separator.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.690054 espnet-202308/espnet2/fileio/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/fileio/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2338 2023-08-03 13:36:37.000000 espnet-202308/espnet2/fileio/datadir_writer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2360 2023-08-03 13:36:37.000000 espnet-202308/espnet2/fileio/npy_scp.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2362 2023-08-03 13:36:37.000000 espnet-202308/espnet2/fileio/rand_gen_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7281 2023-08-03 13:36:37.000000 espnet-202308/espnet2/fileio/read_text.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2825 2023-08-03 13:36:37.000000 espnet-202308/espnet2/fileio/rttm.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11314 2023-08-03 13:36:37.000000 espnet-202308/espnet2/fileio/score_scp.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8356 2023-08-03 13:36:37.000000 espnet-202308/espnet2/fileio/sound_scp.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2795 2023-08-03 13:36:37.000000 espnet-202308/espnet2/fileio/vad_scp.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.690054 espnet-202308/espnet2/fst/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/fst/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7517 2023-08-03 13:36:37.000000 espnet-202308/espnet2/fst/lm_rescore.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.690054 espnet-202308/espnet2/gan_svs/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_svs/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      627 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_svs/abs_gan_svs.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.690054 espnet-202308/espnet2/gan_svs/avocodo/
+-rw-r--r--   0 runner    (1001) docker     (123)      353 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_svs/avocodo/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    29046 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_svs/avocodo/avocodo.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19103 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_svs/espnet_model.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.690054 espnet-202308/espnet2/gan_svs/joint/
+-rw-r--r--   0 runner    (1001) docker     (123)       73 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_svs/joint/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    31014 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_svs/joint/joint_score2wav.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.690054 espnet-202308/espnet2/gan_svs/uhifigan/
+-rw-r--r--   0 runner    (1001) docker     (123)      179 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_svs/uhifigan/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5613 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_svs/uhifigan/sine_generator.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19291 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_svs/uhifigan/uhifigan.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.690054 espnet-202308/espnet2/gan_svs/utils/
+-rw-r--r--   0 runner    (1001) docker     (123)       54 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_svs/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1165 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_svs/utils/expand_f0.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.690054 espnet-202308/espnet2/gan_svs/visinger2/
+-rw-r--r--   0 runner    (1001) docker     (123)      283 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_svs/visinger2/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5159 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_svs/visinger2/ddsp.py
+-rw-r--r--   0 runner    (1001) docker     (123)    35285 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_svs/visinger2/visinger2_vocoder.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.694054 espnet-202308/espnet2/gan_svs/vits/
+-rw-r--r--   0 runner    (1001) docker     (123)       51 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_svs/vits/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2729 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_svs/vits/duration_predictor.py
+-rw-r--r--   0 runner    (1001) docker     (123)    41328 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_svs/vits/generator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3570 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_svs/vits/length_regulator.py
+-rw-r--r--   0 runner    (1001) docker     (123)      873 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_svs/vits/modules.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4212 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_svs/vits/phoneme_predictor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4739 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_svs/vits/pitch_predictor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5294 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_svs/vits/prior_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7323 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_svs/vits/text_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)    40571 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_svs/vits/vits.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.694054 espnet-202308/espnet2/gan_tts/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_tts/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      600 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_tts/abs_gan_tts.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9766 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_tts/espnet_model.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.694054 espnet-202308/espnet2/gan_tts/hifigan/
+-rw-r--r--   0 runner    (1001) docker     (123)      485 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_tts/hifigan/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    31567 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_tts/hifigan/hifigan.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10208 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_tts/hifigan/loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3313 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_tts/hifigan/residual_block.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.694054 espnet-202308/espnet2/gan_tts/jets/
+-rw-r--r--   0 runner    (1001) docker     (123)       51 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_tts/jets/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7515 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_tts/jets/alignments.py
+-rw-r--r--   0 runner    (1001) docker     (123)    35610 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_tts/jets/generator.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24388 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_tts/jets/jets.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2017 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_tts/jets/length_regulator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5920 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_tts/jets/loss.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.694054 espnet-202308/espnet2/gan_tts/joint/
+-rw-r--r--   0 runner    (1001) docker     (123)       71 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_tts/joint/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24009 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_tts/joint/joint_text2wav.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.698054 espnet-202308/espnet2/gan_tts/melgan/
+-rw-r--r--   0 runner    (1001) docker     (123)      216 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_tts/melgan/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16694 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_tts/melgan/melgan.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5141 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_tts/melgan/pqmf.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2464 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_tts/melgan/residual_stack.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.698054 espnet-202308/espnet2/gan_tts/parallel_wavegan/
+-rw-r--r--   0 runner    (1001) docker     (123)      202 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_tts/parallel_wavegan/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12423 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_tts/parallel_wavegan/parallel_wavegan.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6161 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_tts/parallel_wavegan/upsample.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.698054 espnet-202308/espnet2/gan_tts/style_melgan/
+-rw-r--r--   0 runner    (1001) docker     (123)      170 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_tts/style_melgan/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12076 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_tts/style_melgan/style_melgan.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5864 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_tts/style_melgan/tade_res_block.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.698054 espnet-202308/espnet2/gan_tts/utils/
+-rw-r--r--   0 runner    (1001) docker     (123)      157 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_tts/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1481 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_tts/utils/get_random_segments.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.702054 espnet-202308/espnet2/gan_tts/vits/
+-rw-r--r--   0 runner    (1001) docker     (123)       51 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_tts/vits/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6185 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_tts/vits/duration_predictor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9380 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_tts/vits/flow.py
+-rw-r--r--   0 runner    (1001) docker     (123)    25456 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_tts/vits/generator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2215 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_tts/vits/loss.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.702054 espnet-202308/espnet2/gan_tts/vits/monotonic_align/
+-rw-r--r--   0 runner    (1001) docker     (123)     2493 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_tts/vits/monotonic_align/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      712 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_tts/vits/monotonic_align/setup.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4037 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_tts/vits/posterior_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7596 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_tts/vits/residual_coupling.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5385 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_tts/vits/text_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7504 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_tts/vits/transform.py
+-rw-r--r--   0 runner    (1001) docker     (123)    23742 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_tts/vits/vits.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.702054 espnet-202308/espnet2/gan_tts/wavenet/
+-rw-r--r--   0 runner    (1001) docker     (123)       60 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_tts/wavenet/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5352 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_tts/wavenet/residual_block.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6901 2023-08-03 13:36:37.000000 espnet-202308/espnet2/gan_tts/wavenet/wavenet.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.702054 espnet-202308/espnet2/hubert/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/hubert/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16726 2023-08-03 13:36:37.000000 espnet-202308/espnet2/hubert/espnet_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2827 2023-08-03 13:36:37.000000 espnet-202308/espnet2/hubert/hubert_loss.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.702054 espnet-202308/espnet2/iterators/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/iterators/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      219 2023-08-03 13:36:37.000000 espnet-202308/espnet2/iterators/abs_iter_factory.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9683 2023-08-03 13:36:37.000000 espnet-202308/espnet2/iterators/chunk_iter_factory.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1105 2023-08-03 13:36:37.000000 espnet-202308/espnet2/iterators/multiple_iter_factory.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6126 2023-08-03 13:36:37.000000 espnet-202308/espnet2/iterators/sequence_iter_factory.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.706055 espnet-202308/espnet2/layers/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/layers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      344 2023-08-03 13:36:37.000000 espnet-202308/espnet2/layers/abs_normalize.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3746 2023-08-03 13:36:37.000000 espnet-202308/espnet2/layers/global_mvn.py
+-rw-r--r--   0 runner    (1001) docker     (123)      334 2023-08-03 13:36:37.000000 espnet-202308/espnet2/layers/inversible_interface.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2519 2023-08-03 13:36:37.000000 espnet-202308/espnet2/layers/label_aggregation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2578 2023-08-03 13:36:37.000000 espnet-202308/espnet2/layers/log_mel.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6242 2023-08-03 13:36:37.000000 espnet-202308/espnet2/layers/mask_along_axis.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9034 2023-08-03 13:36:37.000000 espnet-202308/espnet2/layers/sinc_conv.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8787 2023-08-03 13:36:37.000000 espnet-202308/espnet2/layers/stft.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2526 2023-08-03 13:36:37.000000 espnet-202308/espnet2/layers/time_warp.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2316 2023-08-03 13:36:37.000000 espnet-202308/espnet2/layers/utterance_mvn.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.706055 espnet-202308/espnet2/lm/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/lm/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      747 2023-08-03 13:36:37.000000 espnet-202308/espnet2/lm/abs_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4785 2023-08-03 13:36:37.000000 espnet-202308/espnet2/lm/espnet_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5887 2023-08-03 13:36:37.000000 espnet-202308/espnet2/lm/seq_rnn_lm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4239 2023-08-03 13:36:37.000000 espnet-202308/espnet2/lm/transformer_lm.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.706055 espnet-202308/espnet2/main_funcs/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/main_funcs/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3965 2023-08-03 13:36:37.000000 espnet-202308/espnet2/main_funcs/average_nbest_models.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5510 2023-08-03 13:36:37.000000 espnet-202308/espnet2/main_funcs/calculate_all_attentions.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5164 2023-08-03 13:36:37.000000 espnet-202308/espnet2/main_funcs/collect_stats.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9839 2023-08-03 13:36:37.000000 espnet-202308/espnet2/main_funcs/pack_funcs.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.706055 espnet-202308/espnet2/mt/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/mt/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10123 2023-08-03 13:36:37.000000 espnet-202308/espnet2/mt/espnet_model.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.706055 espnet-202308/espnet2/mt/frontend/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/mt/frontend/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1941 2023-08-03 13:36:37.000000 espnet-202308/espnet2/mt/frontend/embedding.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.706055 espnet-202308/espnet2/optimizers/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/optimizers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2534 2023-08-03 13:36:37.000000 espnet-202308/espnet2/optimizers/optim_groups.py
+-rw-r--r--   0 runner    (1001) docker     (123)      828 2023-08-03 13:36:37.000000 espnet-202308/espnet2/optimizers/sgd.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.706055 espnet-202308/espnet2/samplers/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/samplers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      392 2023-08-03 13:36:37.000000 espnet-202308/espnet2/samplers/abs_sampler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6131 2023-08-03 13:36:37.000000 espnet-202308/espnet2/samplers/build_batch_sampler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5613 2023-08-03 13:36:37.000000 espnet-202308/espnet2/samplers/folded_batch_sampler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4979 2023-08-03 13:36:37.000000 espnet-202308/espnet2/samplers/length_batch_sampler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5628 2023-08-03 13:36:37.000000 espnet-202308/espnet2/samplers/num_elements_batch_sampler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3128 2023-08-03 13:36:37.000000 espnet-202308/espnet2/samplers/sorted_batch_sampler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2927 2023-08-03 13:36:37.000000 espnet-202308/espnet2/samplers/unsorted_batch_sampler.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.710055 espnet-202308/espnet2/schedulers/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/schedulers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1664 2023-08-03 13:36:37.000000 espnet-202308/espnet2/schedulers/abs_scheduler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5023 2023-08-03 13:36:37.000000 espnet-202308/espnet2/schedulers/cosine_anneal_warmup_restart.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2068 2023-08-03 13:36:37.000000 espnet-202308/espnet2/schedulers/noam_lr.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1495 2023-08-03 13:36:37.000000 espnet-202308/espnet2/schedulers/warmup_lr.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6832 2023-08-03 13:36:37.000000 espnet-202308/espnet2/schedulers/warmup_reducelronplateau.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2699 2023-08-03 13:36:37.000000 espnet-202308/espnet2/schedulers/warmup_step_lr.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.710055 espnet-202308/espnet2/slu/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/slu/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16575 2023-08-03 13:36:37.000000 espnet-202308/espnet2/slu/espnet_model.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.710055 espnet-202308/espnet2/slu/postdecoder/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/slu/postdecoder/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      662 2023-08-03 13:36:37.000000 espnet-202308/espnet2/slu/postdecoder/abs_postdecoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3807 2023-08-03 13:36:37.000000 espnet-202308/espnet2/slu/postdecoder/hugging_face_transformers_postdecoder.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.710055 espnet-202308/espnet2/slu/postencoder/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/slu/postencoder/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10132 2023-08-03 13:36:37.000000 espnet-202308/espnet2/slu/postencoder/conformer_postencoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5830 2023-08-03 13:36:37.000000 espnet-202308/espnet2/slu/postencoder/transformer_postencoder.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.710055 espnet-202308/espnet2/spk/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/spk/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.710055 espnet-202308/espnet2/spk/encoder/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/spk/encoder/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2851 2023-08-03 13:36:37.000000 espnet-202308/espnet2/spk/encoder/rawnet3_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5667 2023-08-03 13:36:37.000000 espnet-202308/espnet2/spk/espnet_model.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.710055 espnet-202308/espnet2/spk/layers/
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3940 2023-08-03 13:36:37.000000 espnet-202308/espnet2/spk/layers/RawNetBasicBlock.py
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/spk/layers/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.710055 espnet-202308/espnet2/spk/loss/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/spk/loss/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2540 2023-08-03 13:36:37.000000 espnet-202308/espnet2/spk/loss/aamsoftmax.py
+-rw-r--r--   0 runner    (1001) docker     (123)      500 2023-08-03 13:36:37.000000 espnet-202308/espnet2/spk/loss/abs_loss.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.714055 espnet-202308/espnet2/spk/pooling/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/spk/pooling/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      206 2023-08-03 13:36:37.000000 espnet-202308/espnet2/spk/pooling/abs_pooling.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1548 2023-08-03 13:36:37.000000 espnet-202308/espnet2/spk/pooling/chn_attn_stat_pooling.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.714055 espnet-202308/espnet2/spk/projector/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/spk/projector/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      300 2023-08-03 13:36:37.000000 espnet-202308/espnet2/spk/projector/abs_projector.py
+-rw-r--r--   0 runner    (1001) docker     (123)      465 2023-08-03 13:36:37.000000 espnet-202308/espnet2/spk/projector/rawnet3_projector.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.714055 espnet-202308/espnet2/st/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/st/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    27019 2023-08-03 13:36:37.000000 espnet-202308/espnet2/st/espnet_model.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.714055 espnet-202308/espnet2/svs/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/svs/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1182 2023-08-03 13:36:37.000000 espnet-202308/espnet2/svs/abs_svs.py
+-rw-r--r--   0 runner    (1001) docker     (123)    28411 2023-08-03 13:36:37.000000 espnet-202308/espnet2/svs/espnet_model.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.714055 espnet-202308/espnet2/svs/feats_extract/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/svs/feats_extract/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11227 2023-08-03 13:36:37.000000 espnet-202308/espnet2/svs/feats_extract/score_feats_extract.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.714055 espnet-202308/espnet2/svs/naive_rnn/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/svs/naive_rnn/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    23773 2023-08-03 13:36:37.000000 espnet-202308/espnet2/svs/naive_rnn/naive_rnn.py
+-rw-r--r--   0 runner    (1001) docker     (123)    26242 2023-08-03 13:36:37.000000 espnet-202308/espnet2/svs/naive_rnn/naive_rnn_dp.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.714055 espnet-202308/espnet2/svs/singing_tacotron/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/svs/singing_tacotron/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14973 2023-08-03 13:36:37.000000 espnet-202308/espnet2/svs/singing_tacotron/decoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8752 2023-08-03 13:36:37.000000 espnet-202308/espnet2/svs/singing_tacotron/encoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)    28523 2023-08-03 13:36:37.000000 espnet-202308/espnet2/svs/singing_tacotron/singing_tacotron.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.714055 espnet-202308/espnet2/svs/xiaoice/
+-rw-r--r--   0 runner    (1001) docker     (123)    34216 2023-08-03 13:36:37.000000 espnet-202308/espnet2/svs/xiaoice/XiaoiceSing.py
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/svs/xiaoice/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5946 2023-08-03 13:36:37.000000 espnet-202308/espnet2/svs/xiaoice/loss.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.718055 espnet-202308/espnet2/tasks/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tasks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    72004 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tasks/abs_task.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21438 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tasks/asr.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13389 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tasks/asr_transducer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10200 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tasks/asvspoof.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9963 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tasks/diar.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18429 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tasks/enh.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19866 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tasks/enh_s2t.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11924 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tasks/enh_tse.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15786 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tasks/gan_svs.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13798 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tasks/gan_tts.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15233 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tasks/hubert.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6804 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tasks/lm.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15892 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tasks/mt.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20979 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tasks/slu.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9552 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tasks/spk.py
+-rw-r--r--   0 runner    (1001) docker     (123)    25801 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tasks/st.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17151 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tasks/svs.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14328 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tasks/tts.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15031 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tasks/uasr.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.722056 espnet-202308/espnet2/text/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/text/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      314 2023-08-03 13:36:37.000000 espnet-202308/espnet2/text/abs_tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3111 2023-08-03 13:36:37.000000 espnet-202308/espnet2/text/build_tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2599 2023-08-03 13:36:37.000000 espnet-202308/espnet2/text/char_tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2140 2023-08-03 13:36:37.000000 espnet-202308/espnet2/text/cleaner.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1152 2023-08-03 13:36:37.000000 espnet-202308/espnet2/text/hugging_face_token_id_converter.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1814 2023-08-03 13:36:37.000000 espnet-202308/espnet2/text/hugging_face_tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1968 2023-08-03 13:36:37.000000 espnet-202308/espnet2/text/korean_cleaner.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21080 2023-08-03 13:36:37.000000 espnet-202308/espnet2/text/phoneme_tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1360 2023-08-03 13:36:37.000000 espnet-202308/espnet2/text/sentencepiece_tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2060 2023-08-03 13:36:37.000000 espnet-202308/espnet2/text/token_id_converter.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1899 2023-08-03 13:36:37.000000 espnet-202308/espnet2/text/whisper_token_id_converter.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1457 2023-08-03 13:36:37.000000 espnet-202308/espnet2/text/whisper_tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2039 2023-08-03 13:36:37.000000 espnet-202308/espnet2/text/word_tokenizer.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.726056 espnet-202308/espnet2/torch_utils/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/torch_utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      987 2023-08-03 13:36:37.000000 espnet-202308/espnet2/torch_utils/add_gradient_noise.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2681 2023-08-03 13:36:37.000000 espnet-202308/espnet2/torch_utils/device_funcs.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1052 2023-08-03 13:36:37.000000 espnet-202308/espnet2/torch_utils/forward_adaptor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1411 2023-08-03 13:36:37.000000 espnet-202308/espnet2/torch_utils/get_layer_from_string.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4844 2023-08-03 13:36:37.000000 espnet-202308/espnet2/torch_utils/initialize.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3500 2023-08-03 13:36:37.000000 espnet-202308/espnet2/torch_utils/load_pretrained_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2498 2023-08-03 13:36:37.000000 espnet-202308/espnet2/torch_utils/model_summary.py
+-rw-r--r--   0 runner    (1001) docker     (123)      468 2023-08-03 13:36:37.000000 espnet-202308/espnet2/torch_utils/pytorch_version.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1615 2023-08-03 13:36:37.000000 espnet-202308/espnet2/torch_utils/recursive_op.py
+-rw-r--r--   0 runner    (1001) docker     (123)      167 2023-08-03 13:36:37.000000 espnet-202308/espnet2/torch_utils/set_all_random_seed.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.726056 espnet-202308/espnet2/train/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/train/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1366 2023-08-03 13:36:37.000000 espnet-202308/espnet2/train/abs_espnet_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2913 2023-08-03 13:36:37.000000 espnet-202308/espnet2/train/abs_gan_espnet_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2962 2023-08-03 13:36:37.000000 espnet-202308/espnet2/train/class_choices.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8311 2023-08-03 13:36:37.000000 espnet-202308/espnet2/train/collate_fn.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18511 2023-08-03 13:36:37.000000 espnet-202308/espnet2/train/dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13914 2023-08-03 13:36:37.000000 espnet-202308/espnet2/train/distributed_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15057 2023-08-03 13:36:37.000000 espnet-202308/espnet2/train/gan_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8251 2023-08-03 13:36:37.000000 espnet-202308/espnet2/train/iterable_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)    68738 2023-08-03 13:36:37.000000 espnet-202308/espnet2/train/preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19602 2023-08-03 13:36:37.000000 espnet-202308/espnet2/train/reporter.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8155 2023-08-03 13:36:37.000000 espnet-202308/espnet2/train/spk_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    35902 2023-08-03 13:36:37.000000 espnet-202308/espnet2/train/trainer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16645 2023-08-03 13:36:37.000000 espnet-202308/espnet2/train/uasr_trainer.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.726056 espnet-202308/espnet2/tts/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tts/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1113 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tts/abs_tts.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12407 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tts/espnet_model.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.726056 espnet-202308/espnet2/tts/fastspeech/
+-rw-r--r--   0 runner    (1001) docker     (123)       65 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tts/fastspeech/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    29711 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tts/fastspeech/fastspeech.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.730056 espnet-202308/espnet2/tts/fastspeech2/
+-rw-r--r--   0 runner    (1001) docker     (123)       68 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tts/fastspeech2/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    35997 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tts/fastspeech2/fastspeech2.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5331 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tts/fastspeech2/loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2624 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tts/fastspeech2/variance_predictor.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.730056 espnet-202308/espnet2/tts/feats_extract/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tts/feats_extract/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      503 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tts/feats_extract/abs_feats_extract.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6224 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tts/feats_extract/dio.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4733 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tts/feats_extract/energy.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2092 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tts/feats_extract/linear_spectrogram.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3044 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tts/feats_extract/log_mel_fbank.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2244 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tts/feats_extract/log_spectrogram.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5723 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tts/feats_extract/yin.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8001 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tts/feats_extract/ying.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.730056 espnet-202308/espnet2/tts/gst/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tts/gst/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10101 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tts/gst/style_encoder.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.730056 espnet-202308/espnet2/tts/prodiff/
+-rw-r--r--   0 runner    (1001) docker     (123)       56 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tts/prodiff/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12143 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tts/prodiff/denoiser.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9540 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tts/prodiff/loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)    35170 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tts/prodiff/prodiff.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.730056 espnet-202308/espnet2/tts/tacotron2/
+-rw-r--r--   0 runner    (1001) docker     (123)       62 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tts/tacotron2/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21020 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tts/tacotron2/tacotron2.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.730056 espnet-202308/espnet2/tts/transformer/
+-rw-r--r--   0 runner    (1001) docker     (123)       68 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tts/transformer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    34927 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tts/transformer/transformer.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.734056 espnet-202308/espnet2/tts/utils/
+-rw-r--r--   0 runner    (1001) docker     (123)      247 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tts/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2291 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tts/utils/duration_calculator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1921 2023-08-03 13:36:37.000000 espnet-202308/espnet2/tts/utils/parallel_wavegan_pretrained_vocoder.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.734056 espnet-202308/espnet2/uasr/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/uasr/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.734056 espnet-202308/espnet2/uasr/discriminator/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/uasr/discriminator/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      272 2023-08-03 13:36:37.000000 espnet-202308/espnet2/uasr/discriminator/abs_discriminator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5634 2023-08-03 13:36:37.000000 espnet-202308/espnet2/uasr/discriminator/conv_discriminator.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15490 2023-08-03 13:36:37.000000 espnet-202308/espnet2/uasr/espnet_model.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.734056 espnet-202308/espnet2/uasr/generator/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/uasr/generator/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      430 2023-08-03 13:36:37.000000 espnet-202308/espnet2/uasr/generator/abs_generator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5254 2023-08-03 13:36:37.000000 espnet-202308/espnet2/uasr/generator/conv_generator.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.734056 espnet-202308/espnet2/uasr/loss/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/uasr/loss/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      499 2023-08-03 13:36:37.000000 espnet-202308/espnet2/uasr/loss/abs_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1994 2023-08-03 13:36:37.000000 espnet-202308/espnet2/uasr/loss/discriminator_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3160 2023-08-03 13:36:37.000000 espnet-202308/espnet2/uasr/loss/gradient_penalty.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1316 2023-08-03 13:36:37.000000 espnet-202308/espnet2/uasr/loss/phoneme_diversity_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1847 2023-08-03 13:36:37.000000 espnet-202308/espnet2/uasr/loss/pseudo_label_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1308 2023-08-03 13:36:37.000000 espnet-202308/espnet2/uasr/loss/smoothness_penalty.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.734056 espnet-202308/espnet2/uasr/segmenter/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/uasr/segmenter/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      736 2023-08-03 13:36:37.000000 espnet-202308/espnet2/uasr/segmenter/abs_segmenter.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3165 2023-08-03 13:36:37.000000 espnet-202308/espnet2/uasr/segmenter/join_segmenter.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1222 2023-08-03 13:36:37.000000 espnet-202308/espnet2/uasr/segmenter/random_segmenter.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.738057 espnet-202308/espnet2/utils/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:37.000000 espnet-202308/espnet2/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      582 2023-08-03 13:36:37.000000 espnet-202308/espnet2/utils/build_dataclass.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1760 2023-08-03 13:36:37.000000 espnet-202308/espnet2/utils/config_argparse.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3374 2023-08-03 13:36:37.000000 espnet-202308/espnet2/utils/eer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1830 2023-08-03 13:36:37.000000 espnet-202308/espnet2/utils/get_default_kwargs.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5607 2023-08-03 13:36:37.000000 espnet-202308/espnet2/utils/griffin_lim.py
+-rw-r--r--   0 runner    (1001) docker     (123)      427 2023-08-03 13:36:37.000000 espnet-202308/espnet2/utils/kwargs2args.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3598 2023-08-03 13:36:37.000000 espnet-202308/espnet2/utils/nested_dict_action.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2027 2023-08-03 13:36:37.000000 espnet-202308/espnet2/utils/sized_dict.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4149 2023-08-03 13:36:37.000000 espnet-202308/espnet2/utils/types.py
+-rw-r--r--   0 runner    (1001) docker     (123)      380 2023-08-03 13:36:37.000000 espnet-202308/espnet2/utils/yaml_no_alias_safe_dump.py
+-rw-r--r--   0 runner    (1001) docker     (123)      371 2023-08-03 13:36:56.746057 espnet-202308/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (123)     5288 2023-08-03 13:36:37.000000 espnet-202308/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-08-03 13:36:56.746057 espnet-202308/test/
+-rw-r--r--   0 runner    (1001) docker     (123)     7583 2023-08-03 13:36:37.000000 espnet-202308/test/test_asr_init.py
+-rw-r--r--   0 runner    (1001) docker     (123)      415 2023-08-03 13:36:37.000000 espnet-202308/test/test_asr_interface.py
+-rw-r--r--   0 runner    (1001) docker     (123)      642 2023-08-03 13:36:37.000000 espnet-202308/test/test_asr_quantize.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5863 2023-08-03 13:36:37.000000 espnet-202308/test/test_batch_beam_search.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6366 2023-08-03 13:36:37.000000 espnet-202308/test/test_beam_search.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5622 2023-08-03 13:36:37.000000 espnet-202308/test/test_beam_search_timesync.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5771 2023-08-03 13:36:37.000000 espnet-202308/test/test_beam_search_timesync_streaming.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2348 2023-08-03 13:36:37.000000 espnet-202308/test/test_cli.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21623 2023-08-03 13:36:37.000000 espnet-202308/test/test_custom_transducer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4110 2023-08-03 13:36:37.000000 espnet-202308/test/test_distributed_launch.py
+-rw-r--r--   0 runner    (1001) docker     (123)    26225 2023-08-03 13:36:37.000000 espnet-202308/test/test_e2e_asr.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4895 2023-08-03 13:36:37.000000 espnet-202308/test/test_e2e_asr_conformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3708 2023-08-03 13:36:37.000000 espnet-202308/test/test_e2e_asr_maskctc.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21848 2023-08-03 13:36:37.000000 espnet-202308/test/test_e2e_asr_mulenc.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14894 2023-08-03 13:36:37.000000 espnet-202308/test/test_e2e_asr_transducer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8385 2023-08-03 13:36:37.000000 espnet-202308/test/test_e2e_asr_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3521 2023-08-03 13:36:37.000000 espnet-202308/test/test_e2e_compatibility.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12864 2023-08-03 13:36:37.000000 espnet-202308/test/test_e2e_mt.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3754 2023-08-03 13:36:37.000000 espnet-202308/test/test_e2e_mt_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20475 2023-08-03 13:36:37.000000 espnet-202308/test/test_e2e_st.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4216 2023-08-03 13:36:37.000000 espnet-202308/test/test_e2e_st_conformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5352 2023-08-03 13:36:37.000000 espnet-202308/test/test_e2e_st_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21140 2023-08-03 13:36:37.000000 espnet-202308/test/test_e2e_tts_fastspeech.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8798 2023-08-03 13:36:37.000000 espnet-202308/test/test_e2e_tts_tacotron2.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15609 2023-08-03 13:36:37.000000 espnet-202308/test/test_e2e_tts_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8638 2023-08-03 13:36:37.000000 espnet-202308/test/test_e2e_vc_tacotron2.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16029 2023-08-03 13:36:37.000000 espnet-202308/test/test_e2e_vc_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3390 2023-08-03 13:36:37.000000 espnet-202308/test/test_initialization.py
+-rw-r--r--   0 runner    (1001) docker     (123)      774 2023-08-03 13:36:37.000000 espnet-202308/test/test_io_voxforge.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6647 2023-08-03 13:36:37.000000 espnet-202308/test/test_lm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5405 2023-08-03 13:36:37.000000 espnet-202308/test/test_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7289 2023-08-03 13:36:37.000000 espnet-202308/test/test_multi_spkrs.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4224 2023-08-03 13:36:37.000000 espnet-202308/test/test_nets_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)      464 2023-08-03 13:36:37.000000 espnet-202308/test/test_ngram.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3023 2023-08-03 13:36:37.000000 espnet-202308/test/test_optimizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5074 2023-08-03 13:36:37.000000 espnet-202308/test/test_positional_encoding.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4644 2023-08-03 13:36:37.000000 espnet-202308/test/test_recog.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1247 2023-08-03 13:36:37.000000 espnet-202308/test/test_scheduler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1171 2023-08-03 13:36:37.000000 espnet-202308/test/test_sentencepiece.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2079 2023-08-03 13:36:37.000000 espnet-202308/test/test_tensorboard.py
+-rw-r--r--   0 runner    (1001) docker     (123)      852 2023-08-03 13:36:37.000000 espnet-202308/test/test_torch.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2795 2023-08-03 13:36:37.000000 espnet-202308/test/test_train_dtype.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2973 2023-08-03 13:36:37.000000 espnet-202308/test/test_transform.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4486 2023-08-03 13:36:37.000000 espnet-202308/test/test_transformer_decode.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9830 2023-08-03 13:36:37.000000 espnet-202308/test/test_utils.py
```

### Comparing `espnet-202304/LICENSE` & `espnet-202308/LICENSE`

 * *Files identical despite different names*

### Comparing `espnet-202304/PKG-INFO` & `espnet-202308/PKG-INFO`

 * *Files 5% similar despite different names*

```diff
@@ -1,30 +1,30 @@
 Metadata-Version: 2.1
 Name: espnet
-Version: 202304
+Version: 202308
 Summary: ESPnet: end-to-end speech processing toolkit
 Home-page: http://github.com/espnet/espnet
 Author: Shinji Watanabe
 Author-email: shinjiw@ieee.org
 License: Apache Software License
 Description: <div align="left"><img src="doc/image/espnet_logo1.png" width="550"/></div>
         
         # ESPnet: end-to-end speech processing toolkit
         
         |system/pytorch ver.|1.10.2|1.11.0|1.12.1|1.13.1|
-        | :---: | :---: | :---: | :---: | :---: |
-        |ubuntu/python3.10/pip||||[![Github Actions](https://github.com/espnet/espnet/workflows/CI/badge.svg)](https://github.com/espnet/espnet/actions)|
-        |ubuntu/python3.9/pip|[![Github Actions](https://github.com/espnet/espnet/workflows/CI/badge.svg)](https://github.com/espnet/espnet/actions)|[![Github Actions](https://github.com/espnet/espnet/workflows/CI/badge.svg)](https://github.com/espnet/espnet/actions)|[![Github Actions](https://github.com/espnet/espnet/workflows/CI/badge.svg)](https://github.com/espnet/espnet/actions)|[![Github Actions](https://github.com/espnet/espnet/workflows/CI/badge.svg)](https://github.com/espnet/espnet/actions)|
-        |ubuntu/python3.8/pip|[![Github Actions](https://github.com/espnet/espnet/workflows/CI/badge.svg)](https://github.com/espnet/espnet/actions)|[![Github Actions](https://github.com/espnet/espnet/workflows/CI/badge.svg)](https://github.com/espnet/espnet/actions)|[![Github Actions](https://github.com/espnet/espnet/workflows/CI/badge.svg)](https://github.com/espnet/espnet/actions)|[![Github Actions](https://github.com/espnet/espnet/workflows/CI/badge.svg)](https://github.com/espnet/espnet/actions)|
-        |ubuntu/python3.7/pip|[![Github Actions](https://github.com/espnet/espnet/workflows/CI/badge.svg)](https://github.com/espnet/espnet/actions)|[![Github Actions](https://github.com/espnet/espnet/workflows/CI/badge.svg)](https://github.com/espnet/espnet/actions)|[![Github Actions](https://github.com/espnet/espnet/workflows/CI/badge.svg)](https://github.com/espnet/espnet/actions)|[![Github Actions](https://github.com/espnet/espnet/workflows/CI/badge.svg)](https://github.com/espnet/espnet/actions)|
-        |debian11/python3.7/conda||||[![debian11](https://github.com/espnet/espnet/workflows/debian11/badge.svg)](https://github.com/espnet/espnet/actions?query=workflow%3Adebian11)|
-        |centos7/python3.7/conda||||[![centos7](https://github.com/espnet/espnet/workflows/centos7/badge.svg)](https://github.com/espnet/espnet/actions?query=workflow%3Acentos7)|
-        |ubuntu/doc/python3.8||||[![doc](https://github.com/espnet/espnet/workflows/doc/badge.svg)](https://github.com/espnet/espnet/actions?query=workflow%3Adoc)|
-        
-        
+        | :---- | :---: | :---: | :---: | :---: |
+        |ubuntu/python3.10/pip||||[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|
+        |ubuntu/python3.9/pip||||[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|
+        |ubuntu/python3.8/pip||||[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|
+        |ubuntu/python3.7/pip|[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|
+        |debian11/python3.7/conda||||[![ci on debian11](https://github.com/espnet/espnet/actions/workflows/ci_on_debian11.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_debian11.yml?query=branch%3Amaster)|
+        |centos7/python3.7/conda||||[![ci on centos7](https://github.com/espnet/espnet/actions/workflows/ci_on_centos7.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_centos7.yml?query=branch%3Amaster)|
+        |windows/python3.10/pip||||[![ci on windows](https://github.com/espnet/espnet/actions/workflows/ci_on_windows.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_windows.yml?query=branch%3Amaster)|
+        |macos/python3.10/pip||||[![ci on macos](https://github.com/espnet/espnet/actions/workflows/ci_on_macos.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_macos.yml?query=branch%3Amaster)|
+        |macos/python3.10/conda||||[![ci on macos](https://github.com/espnet/espnet/actions/workflows/ci_on_macos.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_macos.yml?query=branch%3Amaster)|
         
         [![PyPI version](https://badge.fury.io/py/espnet.svg)](https://badge.fury.io/py/espnet)
         [![Python Versions](https://img.shields.io/pypi/pyversions/espnet.svg)](https://pypi.org/project/espnet/)
         [![Downloads](https://pepy.tech/badge/espnet)](https://pepy.tech/project/espnet)
         [![GitHub license](https://img.shields.io/github/license/espnet/espnet.svg)](https://github.com/espnet/espnet)
         [![codecov](https://codecov.io/gh/espnet/espnet/branch/master/graph/badge.svg)](https://codecov.io/gh/espnet/espnet)
         [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
@@ -55,64 +55,65 @@
           - Add new models/tasks to ESPnet
             - [Online video](https://youtu.be/Css3XAes7SU)
             - [Material](https://colab.research.google.com/github/espnet/notebook/blob/master/espnet2_new_task_tutorial_CMU_11751_18781_Fall2022.ipynb)
         
         
         ## Key Features
         
-        ### Kaldi style complete recipe
-        - Support numbers of `ASR` recipes (WSJ, Switchboard, CHiME-4/5, Librispeech, TED, CSJ, AMI, HKUST, Voxforge, REVERB, etc.)
-        - Support numbers of `TTS` recipes with a similar manner to the ASR recipe (LJSpeech, LibriTTS, M-AILABS, etc.)
+        ### Kaldi-style complete recipe
+        - Support numbers of `ASR` recipes (WSJ, Switchboard, CHiME-4/5, Librispeech, TED, CSJ, AMI, HKUST, Voxforge, REVERB, Gigaspeech, etc.)
+        - Support numbers of `TTS` recipes in a similar manner to the ASR recipe (LJSpeech, LibriTTS, M-AILABS, etc.)
         - Support numbers of `ST` recipes (Fisher-CallHome Spanish, Libri-trans, IWSLT'18, How2, Must-C, Mboshi-French, etc.)
         - Support numbers of `MT` recipes (IWSLT'14, IWSLT'16, the above ST recipes etc.)
         - Support numbers of `SLU` recipes (CATSLU-MAPS, FSC, Grabo, IEMOCAP, JDCINAL, SNIPS, SLURP, SWBD-DA, etc.)
         - Support numbers of `SE/SS` recipes (DNS-IS2020, LibriMix, SMS-WSJ, VCTK-noisyreverb, WHAM!, WHAMR!, WSJ-2mix, etc.)
         - Support voice conversion recipe (VCC2020 baseline)
         - Support speaker diarization recipe (mini_librispeech, librimix)
-        - Support singing voice synthesis recipe (ofuton_p_utagoe_db)
+        - Support singing voice synthesis recipe (ofuton_p_utagoe_db, opencpop, m4singer, etc.)
         
         ### ASR: Automatic Speech Recognition
         - **State-of-the-art performance** in several ASR benchmarks (comparable/superior to hybrid DNN/HMM and CTC)
         - **Hybrid CTC/attention** based end-to-end ASR
           - Fast/accurate training with CTC/attention multitask training
           - CTC/attention joint decoding to boost monotonic alignment decoding
           - Encoder: VGG-like CNN + BiRNN (LSTM/GRU), sub-sampling BiRNN (LSTM/GRU), Transformer, Conformer, [Branchformer](https://proceedings.mlr.press/v162/peng22a.html), or [E-Branchformer](https://arxiv.org/abs/2210.00077)
           - Decoder: RNN (LSTM/GRU), Transformer, or S4
         - Attention: Dot product, location-aware attention, variants of multi-head
         - Incorporate RNNLM/LSTMLM/TransformerLM/N-gram trained only with text data
         - Batch GPU decoding
         - Data augmentation
         - **Transducer** based end-to-end ASR
           - Architecture:
-            - RNN-based encoder and decoder.
-            - Custom encoder and decoder supporting Transformer, Conformer (encoder), 1D Conv / TDNN (encoder) and causal 1D Conv (decoder) blocks.
-            - VGG2L (RNN/custom encoder) and Conv2D (custom encoder) bottlenecks.
+            - Custom encoder supporting RNNs, Conformer, Branchformer (w/ variants), 1D Conv / TDNN.
+            - Decoder w/ parameters shared across blocks supporting RNN, stateless w/ 1D Conv, [MEGA](https://arxiv.org/abs/2209.10655), and [RWKV](https://arxiv.org/abs/2305.13048).
+            - Pre-encoder: VGG2L or Conv2D available.
           - Search algorithms:
             - Greedy search constrained to one emission by timestep.
             - Default beam search algorithm [[Graves, 2012]](https://arxiv.org/abs/1211.3711) without prefix search.
             - Alignment-Length Synchronous decoding [[Saon et al., 2020]](https://ieeexplore.ieee.org/abstract/document/9053040).
             - Time Synchronous Decoding [[Saon et al., 2020]](https://ieeexplore.ieee.org/abstract/document/9053040).
             - N-step Constrained beam search modified from [[Kim et al., 2020]](https://arxiv.org/abs/2002.03577).
             - modified Adaptive Expansion Search based on [[Kim et al., 2021]](https://ieeexplore.ieee.org/abstract/document/9250505) and NSC.
           - Features:
+            - Unified interface for offline and streaming speech recognition.
             - Multi-task learning with various auxiliary losses:
               - Encoder: CTC, auxiliary Transducer and symmetric KL divergence.
               - Decoder: cross-entropy w/ label smoothing.
-            - Transfer learning with acoustic model and/or language model.
+            - Transfer learning with an acoustic model and/or language model.
             - Training with FastEmit regularization method [[Yu et al., 2021]](https://arxiv.org/abs/2010.11148).
           > Please refer to the [tutorial page](https://espnet.github.io/espnet/tutorial.html#transducer) for complete documentation.
         - CTC segmentation
         - Non-autoregressive model based on Mask-CTC
         - ASR examples for supporting endangered language documentation (Please refer to egs/puebla_nahuatl and egs/yoloxochitl_mixtec for details)
-        - Wav2Vec2.0 pretrained model as Encoder, imported from [FairSeq](https://github.com/pytorch/fairseq/tree/master/fairseq).
+        - Wav2Vec2.0 pre-trained model as Encoder, imported from [FairSeq](https://github.com/pytorch/fairseq/tree/master/fairseq).
         - Self-supervised learning representations as features, using upstream models in [S3PRL](https://github.com/s3prl/s3prl) in frontend.
-          - Set `frontend` to be `s3prl`
+          - Set `frontend` to `s3prl`
           - Select any upstream model by setting the `frontend_conf` to the corresponding name.
         - Transfer Learning :
-          - easy usage and transfers from models previously trained by your group, or models from [ESPnet Hugging Face repository](https://huggingface.co/espnet).
+          - easy usage and transfers from models previously trained by your group or models from [ESPnet Hugging Face repository](https://huggingface.co/espnet).
           - [Documentation](https://github.com/espnet/espnet/tree/master/egs2/mini_an4/asr1/transfer_learning.md) and [toy example runnable on colab](https://github.com/espnet/notebook/blob/master/espnet2_asr_transfer_learning_demo.ipynb).
         - Streaming Transformer/Conformer ASR with blockwise synchronous beam search.
         - Restricted Self-Attention based on [Longformer](https://arxiv.org/abs/2004.05150) as an encoder for long sequences
         - OpenAI [Whisper](https://openai.com/blog/whisper/) model, robust ASR based on large-scale, weakly-supervised multitask learning
         
         Demonstration
         - Real-time ASR demo with ESPnet2  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/espnet/notebook/blob/master/espnet2_asr_realtime_demo.ipynb)
@@ -125,15 +126,15 @@
             - Transformer-TTS
             - FastSpeech
             - FastSpeech2
             - Conformer FastSpeech & FastSpeech2
             - VITS
             - JETS
         - Multi-speaker & multi-language extention
-            - Pretrained speaker embedding (e.g., X-vector)
+            - Pre-trained speaker embedding (e.g., X-vector)
             - Speaker ID embedding
             - Language ID embedding
             - Global style token (GST) embedding
             - Mix of the above embeddings
         - End-to-end training
             - End-to-end text-to-wav model (e.g., VITS, JETS, etc.)
             - Joint training of text2mel and vocoder
@@ -151,165 +152,154 @@
         - Real-time TTS demo with ESPnet2  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/espnet/notebook/blob/master/espnet2_tts_realtime_demo.ipynb)
         - Integrated to [Hugging Face Spaces](https://huggingface.co/spaces) with [Gradio](https://github.com/gradio-app/gradio). See demo: [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/akhaliq/ESPnet2-TTS)
         
         To train the neural vocoder, please check the following repositories:
         - [kan-bayashi/ParallelWaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN)
         - [r9y9/wavenet_vocoder](https://github.com/r9y9/wavenet_vocoder)
         
-        > **NOTE**:
-        > - We are moving on ESPnet2-based development for TTS.
-        > - The use of ESPnet1-TTS is deprecated, please use [ESPnet2-TTS](https://github.com/espnet/espnet/tree/master/egs2/TEMPLATE/tts1).
-        
         ### SE: Speech enhancement (and separation)
         
         - Single-speaker speech enhancement
         - Multi-speaker speech separation
         - Unified encoder-separator-decoder structure for time-domain and frequency-domain models
           - Encoder/Decoder: STFT/iSTFT, Convolution/Transposed-Convolution
           - Separators: BLSTM, Transformer, Conformer, [TasNet](https://arxiv.org/abs/1809.07454), [DPRNN](https://arxiv.org/abs/1910.06379), [SkiM](https://arxiv.org/abs/2201.10800), [SVoice](https://arxiv.org/abs/2011.02329), [DC-CRN](https://web.cse.ohio-state.edu/~wang.77/papers/TZW.taslp21.pdf), [DCCRN](https://arxiv.org/abs/2008.00264), [Deep Clustering](https://ieeexplore.ieee.org/document/7471631), [Deep Attractor Network](https://pubmed.ncbi.nlm.nih.gov/29430212/), [FaSNet](https://arxiv.org/abs/1909.13387), [iFaSNet](https://arxiv.org/abs/1910.14104), Neural Beamformers, etc.
         - Flexible ASR integration: working as an individual task or as the ASR frontend
-        - Easy to import pretrained models from [Asteroid](https://github.com/asteroid-team/asteroid)
+        - Easy to import pre-trained models from [Asteroid](https://github.com/asteroid-team/asteroid)
           - Both the pre-trained models from Asteroid and the specific configuration are supported.
         
         Demonstration
         - Interactive SE demo with ESPnet2 [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1fjRJCh96SoYLZPRxsjF9VDv4Q2VoIckI?usp=sharing)
+        - Streaming SE demo with ESPnet2 [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/17vd1V78eJpp3PHBnbFE5aVY5uMxQFL6o?usp=sharing)
         
         ### ST: Speech Translation & MT: Machine Translation
         - **State-of-the-art performance** in several ST benchmarks (comparable/superior to cascaded ASR and MT)
-        - Transformer based end-to-end ST (new!)
-        - Transformer based end-to-end MT (new!)
+        - Transformer-based end-to-end ST (new!)
+        - Transformer-based end-to-end MT (new!)
         
         ### VC: Voice conversion
-        - Transformer and Tacotron2 based parallel VC using melspectrogram (new!)
+        - Transformer and Tacotron2-based parallel VC using Mel spectrogram
         - End-to-end VC based on cascaded ASR+TTS (Baseline system for Voice Conversion Challenge 2020!)
         
         ### SLU: Spoken Language Understanding
         - Architecture
-            - Transformer based Encoder
-            - Conformer based Encoder
+            - Transformer-based Encoder
+            - Conformer-based Encoder
             - [Branchformer](https://proceedings.mlr.press/v162/peng22a.html) based Encoder
             - [E-Branchformer](https://arxiv.org/abs/2210.00077) based Encoder
             - RNN based Decoder
-            - Transformer based Decoder
+            - Transformer-based Decoder
         - Support Multitasking with ASR
             - Predict both intent and ASR transcript
         - Support Multitasking with NLU
             - Deliberation encoder based 2 pass model
-        - Support using pretrained ASR models
+        - Support using pre-trained ASR models
             - Hubert
             - Wav2vec2
             - VQ-APC
             - TERA and more ...
-        - Support using pretrained NLP models
+        - Support using pre-trained NLP models
             - BERT
             - MPNet And more...
         - Various language support
             - En / Jp / Zn / Nl / And more...
         - Supports using context from previous utterances
-        - Supports using other tasks like SE in pipeline manner
+        - Supports using other tasks like SE in a pipeline manner
         - Supports Two Pass SLU that combines audio and ASR transcript
         Demonstration
-        - Performing noisy spoken language understanding using speech enhancement model followed by spoken language understanding model.  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/14nCrJ05vJcQX0cJuXjbMVFWUHJ3Wfb6N?usp=sharing)
-        - Performing two pass spoken language understanding where the second pass model attends on both acoustic and semantic information.  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1p2cbGIPpIIcynuDl4ZVHDpmNPl8Nh_ci?usp=sharing)
+        - Performing noisy spoken language understanding using a speech enhancement model followed by a spoken language understanding model.  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/14nCrJ05vJcQX0cJuXjbMVFWUHJ3Wfb6N?usp=sharing)
+        - Performing two-pass spoken language understanding where the second pass model attends to both acoustic and semantic information.  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1p2cbGIPpIIcynuDl4ZVHDpmNPl8Nh_ci?usp=sharing)
         - Integrated to [Hugging Face Spaces](https://huggingface.co/spaces) with [Gradio](https://github.com/gradio-app/gradio). See SLU demo on multiple languages: [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/Siddhant/ESPnet2-SLU)
         
         
         ### SUM: Speech Summarization
         - End to End Speech Summarization Recipe for Instructional Videos using Restricted Self-Attention [[Sharma et al., 2022]](https://arxiv.org/abs/2110.06263)
         
         ### SVS: Singing Voice Synthesis
         - Framework merge from [Muskits](https://github.com/SJTMusicTeam/Muskits)
         - Architecture
           - RNN-based non-autoregressive model
           - Xiaoice
-          - Sequence-to-sequence Transformer (with GLU-based encoder)
-          - MLP singer (in progress)
-          - Tacotron-singing (in progress)
+          - Tacotron-singing
           - DiffSinger (in progress)
           - VISinger
+          - VISinger 2 (its variations with different vocoders-architecture)
         - Support multi-speaker & multilingual singing synthesis
           - Speaker ID embedding
           - Language ID embedding
         - Various language support
           - Jp / En / Kr / Zh
         - Tight integration with neural vocoders (the same as TTS)
         
         ### SSL: Self-supervised Learning
-        - Support HuBERT Pretraining:
+        - Support HuBERT Pre-training:
           * Example recipe: [egs2/LibriSpeech/ssl1](egs2/LibriSpeech/ssl1)
         
         ### UASR: Unsupervised ASR (EURO: ESPnet Unsupervised Recognition - Open-source)
         - Architecture
           - wav2vec-U (with different self-supervised models)
           - wav2vec-U 2.0 (in progress)
         - Support PrefixBeamSearch and K2-based WFST decoding
         
         ### DNN Framework
-        - Flexible network architecture thanks to chainer and pytorch
+        - Flexible network architecture thanks to Chainer and PyTorch
         - Flexible front-end processing thanks to [kaldiio](https://github.com/nttcslab-sp/kaldiio) and HDF5 support
-        - Tensorboard based monitoring
+        - Tensorboard-based monitoring
         
         ### ESPnet2
         See [ESPnet2](https://espnet.github.io/espnet/espnet2_tutorial.html).
         
         - Independent from Kaldi/Chainer, unlike ESPnet1
-        - On the fly feature extraction and text processing when training
+        - On-the-fly feature extraction and text processing when training
         - Supporting DistributedDataParallel and DaraParallel both
         - Supporting multiple nodes training and integrated with [Slurm](https://slurm.schedmd.com/) or MPI
         - Supporting Sharded Training provided by [fairscale](https://github.com/facebookresearch/fairscale)
-        - A template recipe which can be applied for all corpora
+        - A template recipe that can be applied to all corpora
         - Possible to train any size of corpus without CPU memory error
         - [ESPnet Model Zoo](https://github.com/espnet/espnet_model_zoo)
         - Integrated with [wandb](https://espnet.github.io/espnet/espnet2_training_option.html#weights-biases-integration)
         
         ## Installation
-        - If you intend to do full experiments including DNN training, then see [Installation](https://espnet.github.io/espnet/installation.html).
+        - If you intend to do full experiments, including DNN training, then see [Installation](https://espnet.github.io/espnet/installation.html).
         - If you just need the Python module only:
             ```sh
-            # We recommend you installing pytorch before installing espnet following https://pytorch.org/get-started/locally/
+            # We recommend you install PyTorch before installing espnet following https://pytorch.org/get-started/locally/
             pip install espnet
-            # To install latest
+            # To install the latest
             # pip install git+https://github.com/espnet/espnet
             # To install additional packages
             # pip install "espnet[all]"
             ```
         
-            If you'll use ESPnet1, please install chainer and cupy.
+            If you use ESPnet1, please install chainer and cupy.
         
             ```sh
             pip install chainer==6.0.0 cupy==6.0.0    # [Option]
             ```
         
             You might need to install some packages depending on each task. We prepared various installation scripts at [tools/installers](tools/installers).
         
         - (ESPnet2) Once installed, run `wandb login` and set `--use_wandb true` to enable tracking runs using W&B.
         
-        ## Usage
-        See [Usage](https://espnet.github.io/espnet/tutorial.html).
-        
         ## Docker Container
         
         go to [docker/](docker/) and follow [instructions](https://espnet.github.io/espnet/docker.html).
         
         ## Contribution
-        Thank you for taking times for ESPnet! Any contributions to ESPnet are welcome and feel free to ask any questions or requests to [issues](https://github.com/espnet/espnet/issues).
-        If it's the first contribution to ESPnet for you,  please follow the [contribution guide](CONTRIBUTING.md).
-        
-        ## Results and demo
-        
-        You can find useful tutorials and demos in [Interspeech 2019 Tutorial](https://github.com/espnet/interspeech2019-tutorial)
+        Thank you for taking the time for ESPnet! Any contributions to ESPnet are welcome, and feel free to ask any questions or requests to [issues](https://github.com/espnet/espnet/issues).
+        If it's your first ESPnet contribution,  please follow the [contribution guide](CONTRIBUTING.md).
         
         ### ASR results
         
         <details><summary>expand</summary><div>
         
         
         We list the character error rate (CER) and word error rate (WER) of major ASR tasks.
         
-        | Task                                                              |     CER (%)     |     WER (%)     |                                                                              Pretrained model                                                                               |
+        | Task                                                              |     CER (%)     |     WER (%)     |                                                                              Pre-trained model                                                                               |
         | ----------------------------------------------------------------- | :-------------: | :-------------: | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
         | Aishell dev/test                                                  |     4.6/5.1     |       N/A       |                [link](https://github.com/espnet/espnet/blob/master/egs/aishell/asr1/RESULTS.md#conformer-kernel-size--15--specaugment--lm-weight--00-result)                |
         | **ESPnet2** Aishell dev/test                                      |     4.1/4.4     |       N/A       |                [link](https://github.com/espnet/espnet/tree/master/egs2/aishell/asr1#branchformer-initial)                                                                  |
         | Common Voice dev/test                                             |     1.7/1.8     |     2.2/2.3     |    [link](https://github.com/espnet/espnet/blob/master/egs/commonvoice/asr1/RESULTS.md#first-results-default-pytorch-transformer-setting-with-bpe-100-epochs-single-gpu)    |
         | CSJ eval1/eval2/eval3                                             |   5.7/3.8/4.2   |       N/A       |                 [link](https://github.com/espnet/espnet/blob/master/egs/csj/asr1/RESULTS.md#pytorch-backend-transformer-without-any-hyperparameter-tuning)                  |
         | **ESPnet2** CSJ eval1/eval2/eval3                                 |   4.5/3.3/3.6   |       N/A       |                                        [link](https://github.com/espnet/espnet/tree/master/egs2/csj/asr1#initial-conformer-results)                                         |
         | **ESPnet2** GigaSpeech dev/test                                   |       N/A       |    10.6/10.5    |                                          [link](https://github.com/espnet/espnet/tree/master/egs2/gigaspeech/asr1#e-branchformer)                                           |
@@ -332,26 +322,26 @@
         </div></details>
         
         
         ### ASR demo
         
         <details><summary>expand</summary><div>
         
-        You can recognize speech in a WAV file using pretrained models.
+        You can recognize speech in a WAV file using pre-trained models.
         Go to a recipe directory and run `utils/recog_wav.sh` as follows:
         ```sh
-        # go to recipe directory and source path of espnet tools
+        # go to the recipe directory and source path of espnet tools
         cd egs/tedlium2/asr1 && . ./path.sh
         # let's recognize speech!
         recog_wav.sh --models tedlium2.transformer.v1 example.wav
         ```
         where `example.wav` is a WAV file to be recognized.
         The sampling rate must be consistent with that of data used in training.
         
-        Available pretrained models in the demo script are listed as below.
+        Available pre-trained models in the demo script are listed below.
         
         | Model                                                                                            | Notes                                                      |
         | :----------------------------------------------------------------------------------------------- | :--------------------------------------------------------- |
         | [tedlium2.rnn.v1](https://drive.google.com/open?id=1UqIY6WJMZ4sxNxSugUqp3mrGb3j6h7xe)            | Streaming decoding based on CTC-based VAD                  |
         | [tedlium2.rnn.v2](https://drive.google.com/open?id=1cac5Uc09lJrCYfWkLQsF8eapQcxZnYdf)            | Streaming decoding based on CTC-based VAD (batch decoding) |
         | [tedlium2.transformer.v1](https://drive.google.com/open?id=1cVeSOYY1twOfL9Gns7Z3ZDnkrJqNwPow)    | Joint-CTC attention Transformer trained on Tedlium 2       |
         | [tedlium3.transformer.v1](https://drive.google.com/open?id=1zcPglHAKILwVgfACoMWWERiyIquzSYuU)    | Joint-CTC attention Transformer trained on Tedlium 3       |
@@ -378,36 +368,42 @@
         ### SE demos
         <details><summary>expand</summary><div>
         You can try the interactive demo with Google Colab. Please click the following button to get access to the demos.
         
         [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1fjRJCh96SoYLZPRxsjF9VDv4Q2VoIckI?usp=sharing)
         
         
-        It is based on ESPnet2. Pretrained models are available for both speech enhancement and speech separation tasks.
+        It is based on ESPnet2. Pre-trained models are available for both speech enhancement and speech separation tasks.
+        
+        Speech separation streaming demos:
+        
+        [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/17vd1V78eJpp3PHBnbFE5aVY5uMxQFL6o?usp=sharing)
+        
+        
         
         </div></details>
         
         ### ST results
         
         <details><summary>expand</summary><div>
         
         We list 4-gram BLEU of major ST tasks.
         
         #### end-to-end system
-        | Task                                              | BLEU  |                                                                                         Pretrained model                                                                                          |
+        | Task                                              | BLEU  |                                                                                         Pre-trained model                                                                                          |
         | ------------------------------------------------- | :---: | :-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
         | Fisher-CallHome Spanish fisher_test (Es->En)      | 51.03 | [link](https://github.com/espnet/espnet/blob/master/egs/fisher_callhome_spanish/st1/RESULTS.md#train_spen_lcrm_pytorch_train_pytorch_transformer_bpe_short_long_bpe1000_specaug_asrtrans_mttrans) |
         | Fisher-CallHome Spanish callhome_evltest (Es->En) | 20.44 | [link](https://github.com/espnet/espnet/blob/master/egs/fisher_callhome_spanish/st1/RESULTS.md#train_spen_lcrm_pytorch_train_pytorch_transformer_bpe_short_long_bpe1000_specaug_asrtrans_mttrans) |
         | Libri-trans test (En->Fr)                         | 16.70 |       [link](https://github.com/espnet/espnet/blob/master/egs/libri_trans/st1/RESULTS.md#train_spfr_lc_pytorch_train_pytorch_transformer_bpe_short_long_bpe1000_specaug_asrtrans_mttrans-1)       |
         | How2 dev5 (En->Pt)                                | 45.68 |              [link](https://github.com/espnet/espnet/blob/master/egs/how2/st1/RESULTS.md#trainpt_tc_pytorch_train_pytorch_transformer_short_long_bpe8000_specaug_asrtrans_mttrans-1)              |
         | Must-C tst-COMMON (En->De)                        | 22.91 |          [link](https://github.com/espnet/espnet/blob/master/egs/must_c/st1/RESULTS.md#train_spen-dede_tc_pytorch_train_pytorch_transformer_short_long_bpe8000_specaug_asrtrans_mttrans)          |
         | Mboshi-French dev (Fr->Mboshi)                    | 6.18  |                                                                                                N/A                                                                                                |
         
         #### cascaded system
-        | Task                                              | BLEU  | Pretrained model |
+        | Task                                              | BLEU  | Pre-trained model |
         | ------------------------------------------------- | :---: | :--------------: |
         | Fisher-CallHome Spanish fisher_test (Es->En)      | 42.16 |       N/A        |
         | Fisher-CallHome Spanish callhome_evltest (Es->En) | 19.82 |       N/A        |
         | Libri-trans test (En->Fr)                         | 16.96 |       N/A        |
         | How2 dev5 (En->Pt)                                | 44.90 |       N/A        |
         | Must-C tst-COMMON (En->De)                        | 23.65 |       N/A        |
         
@@ -423,41 +419,41 @@
         (**New!**) We made a new real-time E2E-ST + TTS demonstration in Google Colab.
         Please access the notebook from the following button and enjoy the real-time speech-to-speech translation!
         
         [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/espnet/notebook/blob/master/st_demo.ipynb)
         
         ---
         
-        You can translate speech in a WAV file using pretrained models.
+        You can translate speech in a WAV file using pre-trained models.
         Go to a recipe directory and run `utils/translate_wav.sh` as follows:
         ```sh
-        # go to recipe directory and source path of espnet tools
+        # Go to recipe directory and source path of espnet tools
         cd egs/fisher_callhome_spanish/st1 && . ./path.sh
         # download example wav file
         wget -O - https://github.com/espnet/espnet/files/4100928/test.wav.tar.gz | tar zxvf -
         # let's translate speech!
         translate_wav.sh --models fisher_callhome_spanish.transformer.v1.es-en test.wav
         ```
         where `test.wav` is a WAV file to be translated.
         The sampling rate must be consistent with that of data used in training.
         
-        Available pretrained models in the demo script are listed as below.
+        Available pre-trained models in the demo script are listed as below.
         
         | Model                                                                                                        | Notes                                                    |
         | :----------------------------------------------------------------------------------------------------------- | :------------------------------------------------------- |
         | [fisher_callhome_spanish.transformer.v1](https://drive.google.com/open?id=1hawp5ZLw4_SIHIT3edglxbKIIkPVe8n3) | Transformer-ST trained on Fisher-CallHome Spanish Es->En |
         
         </div></details>
         
         
         ### MT results
         
         <details><summary>expand</summary><div>
         
-        | Task                                              | BLEU  |                                                                        Pretrained model                                                                         |
+        | Task                                              | BLEU  |                                                                        Pre-trained model                                                                         |
         | ------------------------------------------------- | :---: | :-------------------------------------------------------------------------------------------------------------------------------------------------------------: |
         | Fisher-CallHome Spanish fisher_test (Es->En)      | 61.45 | [link](https://github.com/espnet/espnet/blob/master/egs/fisher_callhome_spanish/mt1/RESULTS.md#trainen_lcrm_lcrm_pytorch_train_pytorch_transformer_bpe_bpe1000) |
         | Fisher-CallHome Spanish callhome_evltest (Es->En) | 29.86 | [link](https://github.com/espnet/espnet/blob/master/egs/fisher_callhome_spanish/mt1/RESULTS.md#trainen_lcrm_lcrm_pytorch_train_pytorch_transformer_bpe_bpe1000) |
         | Libri-trans test (En->Fr)                         | 18.09 |          [link](https://github.com/espnet/espnet/blob/master/egs/libri_trans/mt1/RESULTS.md#trainfr_lcrm_tc_pytorch_train_pytorch_transformer_bpe1000)          |
         | How2 dev5 (En->Pt)                                | 58.61 |              [link](https://github.com/espnet/espnet/blob/master/egs/how2/mt1/RESULTS.md#trainpt_tc_tc_pytorch_train_pytorch_transformer_bpe8000)               |
         | Must-C tst-COMMON (En->De)                        | 27.63 |                               [link](https://github.com/espnet/espnet/blob/master/egs/must_c/mt1/RESULTS.md#summary-4-gram-bleu)                                |
         | IWSLT'14 test2014 (En->De)                        | 24.70 |                                     [link](https://github.com/espnet/espnet/blob/master/egs/iwslt16/mt1/RESULTS.md#result)                                      |
@@ -471,23 +467,23 @@
         ### TTS results
         
         <details><summary>ESPnet2</summary><div>
         
         You can listen to the generated samples in the following URL.
         - [ESPnet2 TTS generated samples](https://drive.google.com/drive/folders/1H3fnlBbWMEkQUfrHqosKN_ZX_WjO29ma?usp=sharing)
         
-        > Note that in the generation we use Griffin-Lim (`wav/`) and [Parallel WaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN) (`wav_pwg/`).
+        > Note that in the generation, we use Griffin-Lim (`wav/`) and [Parallel WaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN) (`wav_pwg/`).
         
-        You can download pretrained models via `espnet_model_zoo`.
+        You can download pre-trained models via `espnet_model_zoo`.
         - [ESPnet model zoo](https://github.com/espnet/espnet_model_zoo)
-        - [Pretrained model list](https://github.com/espnet/espnet_model_zoo/blob/master/espnet_model_zoo/table.csv)
+        - [Pre-trained model list](https://github.com/espnet/espnet_model_zoo/blob/master/espnet_model_zoo/table.csv)
         
-        You can download pretrained vocoders via `kan-bayashi/ParallelWaveGAN`.
+        You can download pre-trained vocoders via `kan-bayashi/ParallelWaveGAN`.
         - [kan-bayashi/ParallelWaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN)
-        - [Pretrained vocoder list](https://github.com/kan-bayashi/ParallelWaveGAN#results)
+        - [Pre-trained vocoder list](https://github.com/kan-bayashi/ParallelWaveGAN#results)
         
         </div></details>
         
         <details><summary>ESPnet1</summary><div>
         
         > NOTE: We are moving on ESPnet2-based development for TTS. Please check the latest results in the above ESPnet2 results.
         
@@ -504,28 +500,28 @@
         - [Single Italian speaker FastSpeech](https://drive.google.com/open?id=13I5V2w7deYFX4DlVk1-0JfaXmUR2rNOv)
         - [Single Mandarin speaker Transformer](https://drive.google.com/open?id=1mEnZfBKqA4eT6Bn0eRZuP6lNzL-IL3VD)
         - [Single Mandarin speaker FastSpeech](https://drive.google.com/open?id=1Ol_048Tuy6BgvYm1RpjhOX4HfhUeBqdK)
         - [Multi Japanese speaker Transformer](https://drive.google.com/open?id=1fFMQDF6NV5Ysz48QLFYE8fEvbAxCsMBw)
         - [Single English speaker models with Parallel WaveGAN](https://drive.google.com/open?id=1HvB0_LDf1PVinJdehiuCt5gWmXGguqtx)
         - [Single English speaker knowledge distillation-based FastSpeech](https://drive.google.com/open?id=1wG-Y0itVYalxuLAHdkAHO7w1CWFfRPF4)
         
-        You can download all of the pretrained models and generated samples:
-        - [All of the pretrained E2E-TTS models](https://drive.google.com/open?id=1k9RRyc06Zl0mM2A7mi-hxNiNMFb_YzTF)
+        You can download all of the pre-trained models and generated samples:
+        - [All of the pre-trained E2E-TTS models](https://drive.google.com/open?id=1k9RRyc06Zl0mM2A7mi-hxNiNMFb_YzTF)
         - [All of the generated samples](https://drive.google.com/open?id=1bQGuqH92xuxOX__reWLP4-cif0cbpMLX)
         
-        Note that in the generated samples we use the following vocoders: Griffin-Lim (**GL**), WaveNet vocoder (**WaveNet**), Parallel WaveGAN (**ParallelWaveGAN**), and MelGAN (**MelGAN**).
-        The neural vocoders are based on following repositories.
+        Note that in the generated samples, we use the following vocoders: Griffin-Lim (**GL**), WaveNet vocoder (**WaveNet**), Parallel WaveGAN (**ParallelWaveGAN**), and MelGAN (**MelGAN**).
+        The neural vocoders are based on the following repositories.
         - [kan-bayashi/ParallelWaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN): Parallel WaveGAN / MelGAN / Multi-band MelGAN
         - [r9y9/wavenet_vocoder](https://github.com/r9y9/wavenet_vocoder): 16 bit mixture of Logistics WaveNet vocoder
         - [kan-bayashi/PytorchWaveNetVocoder](https://github.com/kan-bayashi/PytorchWaveNetVocoder): 8 bit Softmax WaveNet Vocoder with the noise shaping
         
         If you want to build your own neural vocoder, please check the above repositories.
         [kan-bayashi/ParallelWaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN) provides [the manual](https://github.com/kan-bayashi/ParallelWaveGAN#decoding-with-espnet-tts-models-features) about how to decode ESPnet-TTS model's features with neural vocoders. Please check it.
         
-        Here we list all of the pretrained neural vocoders. Please download and enjoy the generation of high quality speech!
+        Here we list all of the pre-trained neural vocoders. Please download and enjoy the generation of high-quality speech!
         
         | Model link                                                                                           | Lang  | Fs [Hz] | Mel range [Hz] | FFT / Shift / Win [pt] | Model type                                                              |
         | :--------------------------------------------------------------------------------------------------- | :---: | :-----: | :------------: | :--------------------: | :---------------------------------------------------------------------- |
         | [ljspeech.wavenet.softmax.ns.v1](https://drive.google.com/open?id=1eA1VcRS9jzFa-DovyTgJLQ_jmwOLIi8L) |  EN   | 22.05k  |      None      |   1024 / 256 / None    | [Softmax WaveNet](https://github.com/kan-bayashi/PytorchWaveNetVocoder) |
         | [ljspeech.wavenet.mol.v1](https://drive.google.com/open?id=1sY7gEUg39QaO1szuN62-Llst9TrFno2t)        |  EN   | 22.05k  |      None      |   1024 / 256 / None    | [MoL WaveNet](https://github.com/r9y9/wavenet_vocoder)                  |
         | [ljspeech.parallel_wavegan.v1](https://drive.google.com/open?id=1tv9GKyRT4CDsvUWKwH3s_OfXkiTi0gw7)   |  EN   | 22.05k  |      None      |   1024 / 256 / None    | [Parallel WaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN)      |
         | [ljspeech.wavenet.mol.v2](https://drive.google.com/open?id=1es2HuKUeKVtEdq6YDtAsLNpqCy4fhIXr)        |  EN   | 22.05k  |    80-7600     |   1024 / 256 / None    | [MoL WaveNet](https://github.com/r9y9/wavenet_vocoder)                  |
@@ -534,15 +530,15 @@
         | [ljspeech.melgan.v3](https://drive.google.com/open?id=1_a8faVA5OGCzIcJNw4blQYjfG4oA9VEt)             |  EN   | 22.05k  |    80-7600     |   1024 / 256 / None    | [MelGAN](https://github.com/kan-bayashi/ParallelWaveGAN)                |
         | [libritts.wavenet.mol.v1](https://drive.google.com/open?id=1jHUUmQFjWiQGyDd7ZeiCThSjjpbF_B4h)        |  EN   |   24k   |      None      |   1024 / 256 / None    | [MoL WaveNet](https://github.com/r9y9/wavenet_vocoder)                  |
         | [jsut.wavenet.mol.v1](https://drive.google.com/open?id=187xvyNbmJVZ0EZ1XHCdyjZHTXK9EcfkK)            |  JP   |   24k   |    80-7600     |   2048 / 300 / 1200    | [MoL WaveNet](https://github.com/r9y9/wavenet_vocoder)                  |
         | [jsut.parallel_wavegan.v1](https://drive.google.com/open?id=1OwrUQzAmvjj1x9cDhnZPp6dqtsEqGEJM)       |  JP   |   24k   |    80-7600     |   2048 / 300 / 1200    | [Parallel WaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN)      |
         | [csmsc.wavenet.mol.v1](https://drive.google.com/open?id=1PsjFRV5eUP0HHwBaRYya9smKy5ghXKzj)           |  ZH   |   24k   |    80-7600     |   2048 / 300 / 1200    | [MoL WaveNet](https://github.com/r9y9/wavenet_vocoder)                  |
         | [csmsc.parallel_wavegan.v1](https://drive.google.com/open?id=10M6H88jEUGbRWBmU1Ff2VaTmOAeL8CEy)      |  ZH   |   24k   |    80-7600     |   2048 / 300 / 1200    | [Parallel WaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN)      |
         
-        If you want to use the above pretrained vocoders, please exactly match the feature setting with them.
+        If you want to use the above pre-trained vocoders, please exactly match the feature setting with them.
         
         </div></details>
         
         ### TTS demo
         
         <details><summary>ESPnet2</summary><div>
         
@@ -560,78 +556,78 @@
         > NOTE: We are moving on ESPnet2-based development for TTS. Please check the latest demo in the above ESPnet2 demo.
         
         You can try the real-time demo in Google Colab.
         Please access the notebook from the following button and enjoy the real-time synthesis.
         
         - Real-time TTS demo with ESPnet1  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/espnet/notebook/blob/master/tts_realtime_demo.ipynb)
         
-        We also provide shell script to perform synthesize.
+        We also provide a shell script to perform synthesis.
         Go to a recipe directory and run `utils/synth_wav.sh` as follows:
         
         ```sh
-        # go to recipe directory and source path of espnet tools
+        # Go to recipe directory and source path of espnet tools
         cd egs/ljspeech/tts1 && . ./path.sh
-        # we use upper-case char sequence for the default model.
+        # We use an upper-case char sequence for the default model.
         echo "THIS IS A DEMONSTRATION OF TEXT TO SPEECH." > example.txt
         # let's synthesize speech!
         synth_wav.sh example.txt
         
-        # also you can use multiple sentences
+        # Also, you can use multiple sentences
         echo "THIS IS A DEMONSTRATION OF TEXT TO SPEECH." > example_multi.txt
         echo "TEXT TO SPEECH IS A TECHNIQUE TO CONVERT TEXT INTO SPEECH." >> example_multi.txt
         synth_wav.sh example_multi.txt
         ```
         
-        You can change the pretrained model as follows:
+        You can change the pre-trained model as follows:
         
         ```sh
         synth_wav.sh --models ljspeech.fastspeech.v1 example.txt
         ```
         
-        Waveform synthesis is performed with Griffin-Lim algorithm and neural vocoders (WaveNet and ParallelWaveGAN).
-        You can change the pretrained vocoder model as follows:
+        Waveform synthesis is performed with the Griffin-Lim algorithm and neural vocoders (WaveNet and ParallelWaveGAN).
+        You can change the pre-trained vocoder model as follows:
         
         ```sh
         synth_wav.sh --vocoder_models ljspeech.wavenet.mol.v1 example.txt
         ```
         
-        WaveNet vocoder provides very high quality speech but it takes time to generate.
+        WaveNet vocoder provides very high-quality speech, but it takes time to generate.
         
         See more details or available models via `--help`.
         
         ```sh
         synth_wav.sh --help
         ```
         
         </div></details>
         
         ### VC results
         
         <details><summary>expand</summary><div>
         
-        - Transformer and Tacotron2 based VC
+        - Transformer and Tacotron2-based VC
         
         You can listen to some samples on the [demo webpage](https://unilight.github.io/Publication-Demos/publications/transformer-vc/).
         
         - Cascade ASR+TTS as one of the baseline systems of VCC2020
         
         The [Voice Conversion Challenge 2020](http://www.vc-challenge.org/) (VCC2020) adopts ESPnet to build an end-to-end based baseline system.
-        In VCC2020, the objective is intra/cross lingual nonparallel VC.
+        In VCC2020, the objective is intra/cross-lingual nonparallel VC.
         You can download converted samples of the cascade ASR+TTS baseline system [here](https://drive.google.com/drive/folders/1oeZo83GrOgtqxGwF7KagzIrfjr8X59Ue?usp=sharing).
         
         </div></details>
         
         ### SLU results
         
         <details><summary>expand</summary><div>
         
         
-        We list the performance on various SLU tasks and dataset using the metric reported in the original dataset paper
+        We list the performance on various SLU tasks and datasets using the metric reported in the original dataset paper
         
-        | Task                                                              | Dataset                                                              |    Metric     |     Result     |                                                                              Pretrained Model                                         |
+        | Task                                                              | Dataset                                                              |    Metric     |     Result     |                                                                              Pre-trained Model                                         |
         | ----------------------------------------------------------------- | :-------------: | :-------------: | :-------------: | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
         | Intent Classification                                                 |     SLURP     |       Acc       |       86.3       |                [link](https://github.com/espnet/espnet/tree/master/egs2/slurp/asr1/README.md)                |
         | Intent Classification                                                   |     FSC     |       Acc       |       99.6       |                [link](https://github.com/espnet/espnet/tree/master/egs2/fsc/asr1/README.md)                |
         | Intent Classification                                                  |     FSC Unseen Speaker Set     |       Acc       |       98.6       |                [link](https://github.com/espnet/espnet/tree/master/egs2/fsc_unseen/asr1/README.md)                |
         | Intent Classification                                                   |     FSC Unseen Utterance Set     |       Acc       |       86.4       |                [link](https://github.com/espnet/espnet/tree/master/egs2/fsc_unseen/asr1/README.md)                |
         | Intent Classification                                                   |     FSC Challenge Speaker Set     |       Acc       |       97.5       |                [link](https://github.com/espnet/espnet/tree/master/egs2/fsc_challenge/asr1/README.md)                |
         | Intent Classification                                                   |     FSC Challenge Utterance Set     |       Acc       |       78.5       |                [link](https://github.com/espnet/espnet/tree/master/egs2/fsc_challenge/asr1/README.md)                |
@@ -656,15 +652,15 @@
         ### CTC Segmentation demo
         
         <details><summary>ESPnet1</summary><div>
         
         [CTC segmentation](https://arxiv.org/abs/2007.09127) determines utterance segments within audio files.
         Aligned utterance segments constitute the labels of speech datasets.
         
-        As demo, we align start and end of utterances within the audio file `ctc_align_test.wav`, using the example script `utils/asr_align_wav.sh`.
+        As a demo, we align the start and end of utterances within the audio file `ctc_align_test.wav`, using the example script `utils/asr_align_wav.sh`.
         For preparation, set up a data directory:
         
         ```sh
         cd egs/tedlium2/align1/
         # data directory
         align_dir=data/demo
         mkdir -p ${align_dir}
@@ -694,36 +690,36 @@
         ../../../utils/asr_align_wav.sh \
             --models ${model} \
             --align_dir ${align_dir} \
             --align_config ${align_dir}/align.yaml \
             ${wav} ${align_dir}/utt_text
         ```
         
-        Segments are written to `aligned_segments` as a list of file/utterance name, utterance start and end times in seconds and a confidence score.
-        The confidence score is a probability in log space that indicates how good the utterance was aligned. If needed, remove bad utterances:
+        Segments are written to `aligned_segments` as a list of file/utterance names, utterance start and end times in seconds, and a confidence score.
+        The confidence score is a probability in log space that indicates how well the utterance was aligned. If needed, remove bad utterances:
         
         ```sh
         min_confidence_score=-5
         awk -v ms=${min_confidence_score} '{ if ($5 > ms) {print} }' ${align_dir}/aligned_segments
         ```
         
-        The demo script `utils/ctc_align_wav.sh` uses an already pretrained ASR model (see list above for more models).
+        The demo script `utils/ctc_align_wav.sh` uses an already pre-trained ASR model (see the list above for more models).
         It is recommended to use models with RNN-based encoders (such as BLSTMP) for aligning large audio files;
-        rather than using Transformer models that have a high memory consumption on longer audio data.
+        rather than using Transformer models with a high memory consumption on longer audio data.
         The sample rate of the audio must be consistent with that of the data used in training; adjust with `sox` if needed.
         A full example recipe is in `egs/tedlium2/align1/`.
         
         </div></details>
         
         <details><summary>ESPnet2</summary><div>
         
         [CTC segmentation](https://arxiv.org/abs/2007.09127) determines utterance segments within audio files.
         Aligned utterance segments constitute the labels of speech datasets.
         
-        As demo, we align start and end of utterances within the audio file `ctc_align_test.wav`.
+        As a demo, we align the start and end of utterances within the audio file `ctc_align_test.wav`.
         This can be done either directly from the Python command line or using the script `espnet2/bin/asr_align.py`.
         
         From the Python command line interface:
         
         ```python
         # load a model with character tokens
         from espnet_model_zoo.downloader import ModelDownloader
@@ -747,29 +743,29 @@
         # utt2 utt 1.73 3.19 -0.7674 IS PART OF HOLIDAY'S STRATEGY
         # utt3 utt 3.19 4.20 -0.7433 TO SELL OFF ASSETS
         # utt4 utt 4.20 6.10 -0.4899 AND CONCENTRATE ON PROPERTY MANAGEMENT
         ```
         
         Aligning also works with fragments of the text.
         For this, set the `gratis_blank` option that allows skipping unrelated audio sections without penalty.
-        It's also possible to omit the utterance names at the beginning of each line, by setting `kaldi_style_text` to False.
+        It's also possible to omit the utterance names at the beginning of each line by setting `kaldi_style_text` to False.
         
         ```python
         aligner.set_config( gratis_blank=True, kaldi_style_text=False )
         text = ["SALE OF THE HOTELS", "PROPERTY MANAGEMENT"]
         segments = aligner(speech, text)
         print(segments)
         # utt_0000 utt 0.37 1.72 -2.0651 SALE OF THE HOTELS
         # utt_0001 utt 4.70 6.10 -5.0566 PROPERTY MANAGEMENT
         ```
         
         The script `espnet2/bin/asr_align.py` uses a similar interface. To align utterances:
         
         ```sh
-        # ASR model and config files from pretrained model (e.g. from cachedir):
+        # ASR model and config files from pre-trained model (e.g., from cachedir):
         asr_config=<path-to-model>/config.yaml
         asr_model=<path-to-model>/valid.*best.pth
         # prepare the text file
         wav="test_utils/ctc_align_test.wav"
         text="test_utils/ctc_align_text.txt"
         cat << EOF > ${text}
         utt1 THE SALE OF THE HOTELS
@@ -784,16 +780,16 @@
         # utt2 ctc_align_test 1.73 3.19 -0.7674 IS PART OF HOLIDAY'S STRATEGY
         # utt3 ctc_align_test 3.19 4.20 -0.7433 TO SELL OFF ASSETS
         # utt4 ctc_align_test 4.20 4.97 -0.6017 AND CONCENTRATE
         # utt5 ctc_align_test 4.97 6.10 -0.3477 ON PROPERTY MANAGEMENT
         ```
         
         The output of the script can be redirected to a `segments` file by adding the argument `--output segments`.
-        Each line contains file/utterance name, utterance start and end times in seconds and a confidence score; optionally also the utterance text.
-        The confidence score is a probability in log space that indicates how good the utterance was aligned. If needed, remove bad utterances:
+        Each line contains the file/utterance name, utterance start and end times in seconds, and a confidence score; optionally also the utterance text.
+        The confidence score is a probability in log space that indicates how well the utterance was aligned. If needed, remove bad utterances:
         
         ```sh
         min_confidence_score=-7
         # here, we assume that the output was written to the file `segments`
         awk -v ms=${min_confidence_score} '{ if ($5 > ms) {print} }' segments
         ```
         
@@ -839,14 +835,20 @@
             month = jul,
             year = "2020",
             address = "Online",
             publisher = "Association for Computational Linguistics",
             url = "https://www.aclweb.org/anthology/2020.acl-demos.34",
             pages = "302--311",
         }
+        @article{hayashi2021espnet2,
+          title={Espnet2-tts: Extending the edge of tts research},
+          author={Hayashi, Tomoki and Yamamoto, Ryuichi and Yoshimura, Takenori and Wu, Peter and Shi, Jiatong and Saeki, Takaaki and Ju, Yooncheol and Yasuda, Yusuke and Takamichi, Shinnosuke and Watanabe, Shinji},
+          journal={arXiv preprint arXiv:2110.07840},
+          year={2021}
+        }
         @inproceedings{li2020espnet,
           title={{ESPnet-SE}: End-to-End Speech Enhancement and Separation Toolkit Designed for {ASR} Integration},
           author={Chenda Li and Jing Shi and Wangyou Zhang and Aswin Shanmugam Subramanian and Xuankai Chang and Naoyuki Kamo and Moto Hira and Tomoki Hayashi and Christoph Boeddeker and Zhuo Chen and Shinji Watanabe},
           booktitle={Proceedings of IEEE Spoken Language Technology Workshop (SLT)},
           pages={785--792},
           year={2021},
           organization={IEEE},
@@ -863,14 +865,21 @@
           author={Shi, Jiatong and Guo, Shuai and Qian, Tao and Huo, Nan and Hayashi, Tomoki and Wu, Yuning and Xu, Frank and Chang, Xuankai and Li, Huazhe and Wu, Peter and Watanabe, Shinji and Jin, Qin},
           title={{Muskits}: an End-to-End Music Processing Toolkit for Singing Voice Synthesis},
           year={2022},
           booktitle={Proceedings of Interspeech},
           pages={4277-4281},
           url={https://www.isca-speech.org/archive/pdfs/interspeech_2022/shi22d_interspeech.pdf}
         }
+        @inproceedings{lu22c_interspeech,
+          author={Yen-Ju Lu and Xuankai Chang and Chenda Li and Wangyou Zhang and Samuele Cornell and Zhaoheng Ni and Yoshiki Masuyama and Brian Yan and Robin Scheibler and Zhong-Qiu Wang and Yu Tsao and Yanmin Qian and Shinji Watanabe},
+          title={{ESPnet-SE++: Speech Enhancement for Robust Speech Recognition, Translation, and Understanding}},
+          year=2022,
+          booktitle={Proc. Interspeech 2022},
+          pages={5458--5462},
+        }
         @article{gao2022euro,
           title={{EURO}: {ESPnet} Unsupervised ASR Open-source Toolkit},
           author={Gao, Dongji and Shi, Jiatong and Chuang, Shun-Po and Garcia, Leibny Paola and Lee, Hung-yi and Watanabe, Shinji and Khudanpur, Sanjeev},
           journal={arXiv preprint arXiv:2211.17196},
           year={2022}
         }
         ```
```

### Comparing `espnet-202304/README.md` & `espnet-202308/README.md`

 * *Files 4% similar despite different names*

```diff
@@ -1,22 +1,22 @@
 <div align="left"><img src="doc/image/espnet_logo1.png" width="550"/></div>
 
 # ESPnet: end-to-end speech processing toolkit
 
 |system/pytorch ver.|1.10.2|1.11.0|1.12.1|1.13.1|
-| :---: | :---: | :---: | :---: | :---: |
-|ubuntu/python3.10/pip||||[![Github Actions](https://github.com/espnet/espnet/workflows/CI/badge.svg)](https://github.com/espnet/espnet/actions)|
-|ubuntu/python3.9/pip|[![Github Actions](https://github.com/espnet/espnet/workflows/CI/badge.svg)](https://github.com/espnet/espnet/actions)|[![Github Actions](https://github.com/espnet/espnet/workflows/CI/badge.svg)](https://github.com/espnet/espnet/actions)|[![Github Actions](https://github.com/espnet/espnet/workflows/CI/badge.svg)](https://github.com/espnet/espnet/actions)|[![Github Actions](https://github.com/espnet/espnet/workflows/CI/badge.svg)](https://github.com/espnet/espnet/actions)|
-|ubuntu/python3.8/pip|[![Github Actions](https://github.com/espnet/espnet/workflows/CI/badge.svg)](https://github.com/espnet/espnet/actions)|[![Github Actions](https://github.com/espnet/espnet/workflows/CI/badge.svg)](https://github.com/espnet/espnet/actions)|[![Github Actions](https://github.com/espnet/espnet/workflows/CI/badge.svg)](https://github.com/espnet/espnet/actions)|[![Github Actions](https://github.com/espnet/espnet/workflows/CI/badge.svg)](https://github.com/espnet/espnet/actions)|
-|ubuntu/python3.7/pip|[![Github Actions](https://github.com/espnet/espnet/workflows/CI/badge.svg)](https://github.com/espnet/espnet/actions)|[![Github Actions](https://github.com/espnet/espnet/workflows/CI/badge.svg)](https://github.com/espnet/espnet/actions)|[![Github Actions](https://github.com/espnet/espnet/workflows/CI/badge.svg)](https://github.com/espnet/espnet/actions)|[![Github Actions](https://github.com/espnet/espnet/workflows/CI/badge.svg)](https://github.com/espnet/espnet/actions)|
-|debian11/python3.7/conda||||[![debian11](https://github.com/espnet/espnet/workflows/debian11/badge.svg)](https://github.com/espnet/espnet/actions?query=workflow%3Adebian11)|
-|centos7/python3.7/conda||||[![centos7](https://github.com/espnet/espnet/workflows/centos7/badge.svg)](https://github.com/espnet/espnet/actions?query=workflow%3Acentos7)|
-|ubuntu/doc/python3.8||||[![doc](https://github.com/espnet/espnet/workflows/doc/badge.svg)](https://github.com/espnet/espnet/actions?query=workflow%3Adoc)|
-
-
+| :---- | :---: | :---: | :---: | :---: |
+|ubuntu/python3.10/pip||||[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|
+|ubuntu/python3.9/pip||||[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|
+|ubuntu/python3.8/pip||||[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|
+|ubuntu/python3.7/pip|[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|
+|debian11/python3.7/conda||||[![ci on debian11](https://github.com/espnet/espnet/actions/workflows/ci_on_debian11.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_debian11.yml?query=branch%3Amaster)|
+|centos7/python3.7/conda||||[![ci on centos7](https://github.com/espnet/espnet/actions/workflows/ci_on_centos7.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_centos7.yml?query=branch%3Amaster)|
+|windows/python3.10/pip||||[![ci on windows](https://github.com/espnet/espnet/actions/workflows/ci_on_windows.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_windows.yml?query=branch%3Amaster)|
+|macos/python3.10/pip||||[![ci on macos](https://github.com/espnet/espnet/actions/workflows/ci_on_macos.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_macos.yml?query=branch%3Amaster)|
+|macos/python3.10/conda||||[![ci on macos](https://github.com/espnet/espnet/actions/workflows/ci_on_macos.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_macos.yml?query=branch%3Amaster)|
 
 [![PyPI version](https://badge.fury.io/py/espnet.svg)](https://badge.fury.io/py/espnet)
 [![Python Versions](https://img.shields.io/pypi/pyversions/espnet.svg)](https://pypi.org/project/espnet/)
 [![Downloads](https://pepy.tech/badge/espnet)](https://pepy.tech/project/espnet)
 [![GitHub license](https://img.shields.io/github/license/espnet/espnet.svg)](https://github.com/espnet/espnet)
 [![codecov](https://codecov.io/gh/espnet/espnet/branch/master/graph/badge.svg)](https://codecov.io/gh/espnet/espnet)
 [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
@@ -47,64 +47,65 @@
   - Add new models/tasks to ESPnet
     - [Online video](https://youtu.be/Css3XAes7SU)
     - [Material](https://colab.research.google.com/github/espnet/notebook/blob/master/espnet2_new_task_tutorial_CMU_11751_18781_Fall2022.ipynb)
 
 
 ## Key Features
 
-### Kaldi style complete recipe
-- Support numbers of `ASR` recipes (WSJ, Switchboard, CHiME-4/5, Librispeech, TED, CSJ, AMI, HKUST, Voxforge, REVERB, etc.)
-- Support numbers of `TTS` recipes with a similar manner to the ASR recipe (LJSpeech, LibriTTS, M-AILABS, etc.)
+### Kaldi-style complete recipe
+- Support numbers of `ASR` recipes (WSJ, Switchboard, CHiME-4/5, Librispeech, TED, CSJ, AMI, HKUST, Voxforge, REVERB, Gigaspeech, etc.)
+- Support numbers of `TTS` recipes in a similar manner to the ASR recipe (LJSpeech, LibriTTS, M-AILABS, etc.)
 - Support numbers of `ST` recipes (Fisher-CallHome Spanish, Libri-trans, IWSLT'18, How2, Must-C, Mboshi-French, etc.)
 - Support numbers of `MT` recipes (IWSLT'14, IWSLT'16, the above ST recipes etc.)
 - Support numbers of `SLU` recipes (CATSLU-MAPS, FSC, Grabo, IEMOCAP, JDCINAL, SNIPS, SLURP, SWBD-DA, etc.)
 - Support numbers of `SE/SS` recipes (DNS-IS2020, LibriMix, SMS-WSJ, VCTK-noisyreverb, WHAM!, WHAMR!, WSJ-2mix, etc.)
 - Support voice conversion recipe (VCC2020 baseline)
 - Support speaker diarization recipe (mini_librispeech, librimix)
-- Support singing voice synthesis recipe (ofuton_p_utagoe_db)
+- Support singing voice synthesis recipe (ofuton_p_utagoe_db, opencpop, m4singer, etc.)
 
 ### ASR: Automatic Speech Recognition
 - **State-of-the-art performance** in several ASR benchmarks (comparable/superior to hybrid DNN/HMM and CTC)
 - **Hybrid CTC/attention** based end-to-end ASR
   - Fast/accurate training with CTC/attention multitask training
   - CTC/attention joint decoding to boost monotonic alignment decoding
   - Encoder: VGG-like CNN + BiRNN (LSTM/GRU), sub-sampling BiRNN (LSTM/GRU), Transformer, Conformer, [Branchformer](https://proceedings.mlr.press/v162/peng22a.html), or [E-Branchformer](https://arxiv.org/abs/2210.00077)
   - Decoder: RNN (LSTM/GRU), Transformer, or S4
 - Attention: Dot product, location-aware attention, variants of multi-head
 - Incorporate RNNLM/LSTMLM/TransformerLM/N-gram trained only with text data
 - Batch GPU decoding
 - Data augmentation
 - **Transducer** based end-to-end ASR
   - Architecture:
-    - RNN-based encoder and decoder.
-    - Custom encoder and decoder supporting Transformer, Conformer (encoder), 1D Conv / TDNN (encoder) and causal 1D Conv (decoder) blocks.
-    - VGG2L (RNN/custom encoder) and Conv2D (custom encoder) bottlenecks.
+    - Custom encoder supporting RNNs, Conformer, Branchformer (w/ variants), 1D Conv / TDNN.
+    - Decoder w/ parameters shared across blocks supporting RNN, stateless w/ 1D Conv, [MEGA](https://arxiv.org/abs/2209.10655), and [RWKV](https://arxiv.org/abs/2305.13048).
+    - Pre-encoder: VGG2L or Conv2D available.
   - Search algorithms:
     - Greedy search constrained to one emission by timestep.
     - Default beam search algorithm [[Graves, 2012]](https://arxiv.org/abs/1211.3711) without prefix search.
     - Alignment-Length Synchronous decoding [[Saon et al., 2020]](https://ieeexplore.ieee.org/abstract/document/9053040).
     - Time Synchronous Decoding [[Saon et al., 2020]](https://ieeexplore.ieee.org/abstract/document/9053040).
     - N-step Constrained beam search modified from [[Kim et al., 2020]](https://arxiv.org/abs/2002.03577).
     - modified Adaptive Expansion Search based on [[Kim et al., 2021]](https://ieeexplore.ieee.org/abstract/document/9250505) and NSC.
   - Features:
+    - Unified interface for offline and streaming speech recognition.
     - Multi-task learning with various auxiliary losses:
       - Encoder: CTC, auxiliary Transducer and symmetric KL divergence.
       - Decoder: cross-entropy w/ label smoothing.
-    - Transfer learning with acoustic model and/or language model.
+    - Transfer learning with an acoustic model and/or language model.
     - Training with FastEmit regularization method [[Yu et al., 2021]](https://arxiv.org/abs/2010.11148).
   > Please refer to the [tutorial page](https://espnet.github.io/espnet/tutorial.html#transducer) for complete documentation.
 - CTC segmentation
 - Non-autoregressive model based on Mask-CTC
 - ASR examples for supporting endangered language documentation (Please refer to egs/puebla_nahuatl and egs/yoloxochitl_mixtec for details)
-- Wav2Vec2.0 pretrained model as Encoder, imported from [FairSeq](https://github.com/pytorch/fairseq/tree/master/fairseq).
+- Wav2Vec2.0 pre-trained model as Encoder, imported from [FairSeq](https://github.com/pytorch/fairseq/tree/master/fairseq).
 - Self-supervised learning representations as features, using upstream models in [S3PRL](https://github.com/s3prl/s3prl) in frontend.
-  - Set `frontend` to be `s3prl`
+  - Set `frontend` to `s3prl`
   - Select any upstream model by setting the `frontend_conf` to the corresponding name.
 - Transfer Learning :
-  - easy usage and transfers from models previously trained by your group, or models from [ESPnet Hugging Face repository](https://huggingface.co/espnet).
+  - easy usage and transfers from models previously trained by your group or models from [ESPnet Hugging Face repository](https://huggingface.co/espnet).
   - [Documentation](https://github.com/espnet/espnet/tree/master/egs2/mini_an4/asr1/transfer_learning.md) and [toy example runnable on colab](https://github.com/espnet/notebook/blob/master/espnet2_asr_transfer_learning_demo.ipynb).
 - Streaming Transformer/Conformer ASR with blockwise synchronous beam search.
 - Restricted Self-Attention based on [Longformer](https://arxiv.org/abs/2004.05150) as an encoder for long sequences
 - OpenAI [Whisper](https://openai.com/blog/whisper/) model, robust ASR based on large-scale, weakly-supervised multitask learning
 
 Demonstration
 - Real-time ASR demo with ESPnet2  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/espnet/notebook/blob/master/espnet2_asr_realtime_demo.ipynb)
@@ -117,15 +118,15 @@
     - Transformer-TTS
     - FastSpeech
     - FastSpeech2
     - Conformer FastSpeech & FastSpeech2
     - VITS
     - JETS
 - Multi-speaker & multi-language extention
-    - Pretrained speaker embedding (e.g., X-vector)
+    - Pre-trained speaker embedding (e.g., X-vector)
     - Speaker ID embedding
     - Language ID embedding
     - Global style token (GST) embedding
     - Mix of the above embeddings
 - End-to-end training
     - End-to-end text-to-wav model (e.g., VITS, JETS, etc.)
     - Joint training of text2mel and vocoder
@@ -143,165 +144,154 @@
 - Real-time TTS demo with ESPnet2  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/espnet/notebook/blob/master/espnet2_tts_realtime_demo.ipynb)
 - Integrated to [Hugging Face Spaces](https://huggingface.co/spaces) with [Gradio](https://github.com/gradio-app/gradio). See demo: [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/akhaliq/ESPnet2-TTS)
 
 To train the neural vocoder, please check the following repositories:
 - [kan-bayashi/ParallelWaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN)
 - [r9y9/wavenet_vocoder](https://github.com/r9y9/wavenet_vocoder)
 
-> **NOTE**:
-> - We are moving on ESPnet2-based development for TTS.
-> - The use of ESPnet1-TTS is deprecated, please use [ESPnet2-TTS](https://github.com/espnet/espnet/tree/master/egs2/TEMPLATE/tts1).
-
 ### SE: Speech enhancement (and separation)
 
 - Single-speaker speech enhancement
 - Multi-speaker speech separation
 - Unified encoder-separator-decoder structure for time-domain and frequency-domain models
   - Encoder/Decoder: STFT/iSTFT, Convolution/Transposed-Convolution
   - Separators: BLSTM, Transformer, Conformer, [TasNet](https://arxiv.org/abs/1809.07454), [DPRNN](https://arxiv.org/abs/1910.06379), [SkiM](https://arxiv.org/abs/2201.10800), [SVoice](https://arxiv.org/abs/2011.02329), [DC-CRN](https://web.cse.ohio-state.edu/~wang.77/papers/TZW.taslp21.pdf), [DCCRN](https://arxiv.org/abs/2008.00264), [Deep Clustering](https://ieeexplore.ieee.org/document/7471631), [Deep Attractor Network](https://pubmed.ncbi.nlm.nih.gov/29430212/), [FaSNet](https://arxiv.org/abs/1909.13387), [iFaSNet](https://arxiv.org/abs/1910.14104), Neural Beamformers, etc.
 - Flexible ASR integration: working as an individual task or as the ASR frontend
-- Easy to import pretrained models from [Asteroid](https://github.com/asteroid-team/asteroid)
+- Easy to import pre-trained models from [Asteroid](https://github.com/asteroid-team/asteroid)
   - Both the pre-trained models from Asteroid and the specific configuration are supported.
 
 Demonstration
 - Interactive SE demo with ESPnet2 [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1fjRJCh96SoYLZPRxsjF9VDv4Q2VoIckI?usp=sharing)
+- Streaming SE demo with ESPnet2 [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/17vd1V78eJpp3PHBnbFE5aVY5uMxQFL6o?usp=sharing)
 
 ### ST: Speech Translation & MT: Machine Translation
 - **State-of-the-art performance** in several ST benchmarks (comparable/superior to cascaded ASR and MT)
-- Transformer based end-to-end ST (new!)
-- Transformer based end-to-end MT (new!)
+- Transformer-based end-to-end ST (new!)
+- Transformer-based end-to-end MT (new!)
 
 ### VC: Voice conversion
-- Transformer and Tacotron2 based parallel VC using melspectrogram (new!)
+- Transformer and Tacotron2-based parallel VC using Mel spectrogram
 - End-to-end VC based on cascaded ASR+TTS (Baseline system for Voice Conversion Challenge 2020!)
 
 ### SLU: Spoken Language Understanding
 - Architecture
-    - Transformer based Encoder
-    - Conformer based Encoder
+    - Transformer-based Encoder
+    - Conformer-based Encoder
     - [Branchformer](https://proceedings.mlr.press/v162/peng22a.html) based Encoder
     - [E-Branchformer](https://arxiv.org/abs/2210.00077) based Encoder
     - RNN based Decoder
-    - Transformer based Decoder
+    - Transformer-based Decoder
 - Support Multitasking with ASR
     - Predict both intent and ASR transcript
 - Support Multitasking with NLU
     - Deliberation encoder based 2 pass model
-- Support using pretrained ASR models
+- Support using pre-trained ASR models
     - Hubert
     - Wav2vec2
     - VQ-APC
     - TERA and more ...
-- Support using pretrained NLP models
+- Support using pre-trained NLP models
     - BERT
     - MPNet And more...
 - Various language support
     - En / Jp / Zn / Nl / And more...
 - Supports using context from previous utterances
-- Supports using other tasks like SE in pipeline manner
+- Supports using other tasks like SE in a pipeline manner
 - Supports Two Pass SLU that combines audio and ASR transcript
 Demonstration
-- Performing noisy spoken language understanding using speech enhancement model followed by spoken language understanding model.  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/14nCrJ05vJcQX0cJuXjbMVFWUHJ3Wfb6N?usp=sharing)
-- Performing two pass spoken language understanding where the second pass model attends on both acoustic and semantic information.  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1p2cbGIPpIIcynuDl4ZVHDpmNPl8Nh_ci?usp=sharing)
+- Performing noisy spoken language understanding using a speech enhancement model followed by a spoken language understanding model.  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/14nCrJ05vJcQX0cJuXjbMVFWUHJ3Wfb6N?usp=sharing)
+- Performing two-pass spoken language understanding where the second pass model attends to both acoustic and semantic information.  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1p2cbGIPpIIcynuDl4ZVHDpmNPl8Nh_ci?usp=sharing)
 - Integrated to [Hugging Face Spaces](https://huggingface.co/spaces) with [Gradio](https://github.com/gradio-app/gradio). See SLU demo on multiple languages: [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/Siddhant/ESPnet2-SLU)
 
 
 ### SUM: Speech Summarization
 - End to End Speech Summarization Recipe for Instructional Videos using Restricted Self-Attention [[Sharma et al., 2022]](https://arxiv.org/abs/2110.06263)
 
 ### SVS: Singing Voice Synthesis
 - Framework merge from [Muskits](https://github.com/SJTMusicTeam/Muskits)
 - Architecture
   - RNN-based non-autoregressive model
   - Xiaoice
-  - Sequence-to-sequence Transformer (with GLU-based encoder)
-  - MLP singer (in progress)
-  - Tacotron-singing (in progress)
+  - Tacotron-singing
   - DiffSinger (in progress)
   - VISinger
+  - VISinger 2 (its variations with different vocoders-architecture)
 - Support multi-speaker & multilingual singing synthesis
   - Speaker ID embedding
   - Language ID embedding
 - Various language support
   - Jp / En / Kr / Zh
 - Tight integration with neural vocoders (the same as TTS)
 
 ### SSL: Self-supervised Learning
-- Support HuBERT Pretraining:
+- Support HuBERT Pre-training:
   * Example recipe: [egs2/LibriSpeech/ssl1](egs2/LibriSpeech/ssl1)
 
 ### UASR: Unsupervised ASR (EURO: ESPnet Unsupervised Recognition - Open-source)
 - Architecture
   - wav2vec-U (with different self-supervised models)
   - wav2vec-U 2.0 (in progress)
 - Support PrefixBeamSearch and K2-based WFST decoding
 
 ### DNN Framework
-- Flexible network architecture thanks to chainer and pytorch
+- Flexible network architecture thanks to Chainer and PyTorch
 - Flexible front-end processing thanks to [kaldiio](https://github.com/nttcslab-sp/kaldiio) and HDF5 support
-- Tensorboard based monitoring
+- Tensorboard-based monitoring
 
 ### ESPnet2
 See [ESPnet2](https://espnet.github.io/espnet/espnet2_tutorial.html).
 
 - Independent from Kaldi/Chainer, unlike ESPnet1
-- On the fly feature extraction and text processing when training
+- On-the-fly feature extraction and text processing when training
 - Supporting DistributedDataParallel and DaraParallel both
 - Supporting multiple nodes training and integrated with [Slurm](https://slurm.schedmd.com/) or MPI
 - Supporting Sharded Training provided by [fairscale](https://github.com/facebookresearch/fairscale)
-- A template recipe which can be applied for all corpora
+- A template recipe that can be applied to all corpora
 - Possible to train any size of corpus without CPU memory error
 - [ESPnet Model Zoo](https://github.com/espnet/espnet_model_zoo)
 - Integrated with [wandb](https://espnet.github.io/espnet/espnet2_training_option.html#weights-biases-integration)
 
 ## Installation
-- If you intend to do full experiments including DNN training, then see [Installation](https://espnet.github.io/espnet/installation.html).
+- If you intend to do full experiments, including DNN training, then see [Installation](https://espnet.github.io/espnet/installation.html).
 - If you just need the Python module only:
     ```sh
-    # We recommend you installing pytorch before installing espnet following https://pytorch.org/get-started/locally/
+    # We recommend you install PyTorch before installing espnet following https://pytorch.org/get-started/locally/
     pip install espnet
-    # To install latest
+    # To install the latest
     # pip install git+https://github.com/espnet/espnet
     # To install additional packages
     # pip install "espnet[all]"
     ```
 
-    If you'll use ESPnet1, please install chainer and cupy.
+    If you use ESPnet1, please install chainer and cupy.
 
     ```sh
     pip install chainer==6.0.0 cupy==6.0.0    # [Option]
     ```
 
     You might need to install some packages depending on each task. We prepared various installation scripts at [tools/installers](tools/installers).
 
 - (ESPnet2) Once installed, run `wandb login` and set `--use_wandb true` to enable tracking runs using W&B.
 
-## Usage
-See [Usage](https://espnet.github.io/espnet/tutorial.html).
-
 ## Docker Container
 
 go to [docker/](docker/) and follow [instructions](https://espnet.github.io/espnet/docker.html).
 
 ## Contribution
-Thank you for taking times for ESPnet! Any contributions to ESPnet are welcome and feel free to ask any questions or requests to [issues](https://github.com/espnet/espnet/issues).
-If it's the first contribution to ESPnet for you,  please follow the [contribution guide](CONTRIBUTING.md).
-
-## Results and demo
-
-You can find useful tutorials and demos in [Interspeech 2019 Tutorial](https://github.com/espnet/interspeech2019-tutorial)
+Thank you for taking the time for ESPnet! Any contributions to ESPnet are welcome, and feel free to ask any questions or requests to [issues](https://github.com/espnet/espnet/issues).
+If it's your first ESPnet contribution,  please follow the [contribution guide](CONTRIBUTING.md).
 
 ### ASR results
 
 <details><summary>expand</summary><div>
 
 
 We list the character error rate (CER) and word error rate (WER) of major ASR tasks.
 
-| Task                                                              |     CER (%)     |     WER (%)     |                                                                              Pretrained model                                                                               |
+| Task                                                              |     CER (%)     |     WER (%)     |                                                                              Pre-trained model                                                                               |
 | ----------------------------------------------------------------- | :-------------: | :-------------: | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
 | Aishell dev/test                                                  |     4.6/5.1     |       N/A       |                [link](https://github.com/espnet/espnet/blob/master/egs/aishell/asr1/RESULTS.md#conformer-kernel-size--15--specaugment--lm-weight--00-result)                |
 | **ESPnet2** Aishell dev/test                                      |     4.1/4.4     |       N/A       |                [link](https://github.com/espnet/espnet/tree/master/egs2/aishell/asr1#branchformer-initial)                                                                  |
 | Common Voice dev/test                                             |     1.7/1.8     |     2.2/2.3     |    [link](https://github.com/espnet/espnet/blob/master/egs/commonvoice/asr1/RESULTS.md#first-results-default-pytorch-transformer-setting-with-bpe-100-epochs-single-gpu)    |
 | CSJ eval1/eval2/eval3                                             |   5.7/3.8/4.2   |       N/A       |                 [link](https://github.com/espnet/espnet/blob/master/egs/csj/asr1/RESULTS.md#pytorch-backend-transformer-without-any-hyperparameter-tuning)                  |
 | **ESPnet2** CSJ eval1/eval2/eval3                                 |   4.5/3.3/3.6   |       N/A       |                                        [link](https://github.com/espnet/espnet/tree/master/egs2/csj/asr1#initial-conformer-results)                                         |
 | **ESPnet2** GigaSpeech dev/test                                   |       N/A       |    10.6/10.5    |                                          [link](https://github.com/espnet/espnet/tree/master/egs2/gigaspeech/asr1#e-branchformer)                                           |
@@ -324,26 +314,26 @@
 </div></details>
 
 
 ### ASR demo
 
 <details><summary>expand</summary><div>
 
-You can recognize speech in a WAV file using pretrained models.
+You can recognize speech in a WAV file using pre-trained models.
 Go to a recipe directory and run `utils/recog_wav.sh` as follows:
 ```sh
-# go to recipe directory and source path of espnet tools
+# go to the recipe directory and source path of espnet tools
 cd egs/tedlium2/asr1 && . ./path.sh
 # let's recognize speech!
 recog_wav.sh --models tedlium2.transformer.v1 example.wav
 ```
 where `example.wav` is a WAV file to be recognized.
 The sampling rate must be consistent with that of data used in training.
 
-Available pretrained models in the demo script are listed as below.
+Available pre-trained models in the demo script are listed below.
 
 | Model                                                                                            | Notes                                                      |
 | :----------------------------------------------------------------------------------------------- | :--------------------------------------------------------- |
 | [tedlium2.rnn.v1](https://drive.google.com/open?id=1UqIY6WJMZ4sxNxSugUqp3mrGb3j6h7xe)            | Streaming decoding based on CTC-based VAD                  |
 | [tedlium2.rnn.v2](https://drive.google.com/open?id=1cac5Uc09lJrCYfWkLQsF8eapQcxZnYdf)            | Streaming decoding based on CTC-based VAD (batch decoding) |
 | [tedlium2.transformer.v1](https://drive.google.com/open?id=1cVeSOYY1twOfL9Gns7Z3ZDnkrJqNwPow)    | Joint-CTC attention Transformer trained on Tedlium 2       |
 | [tedlium3.transformer.v1](https://drive.google.com/open?id=1zcPglHAKILwVgfACoMWWERiyIquzSYuU)    | Joint-CTC attention Transformer trained on Tedlium 3       |
@@ -370,36 +360,42 @@
 ### SE demos
 <details><summary>expand</summary><div>
 You can try the interactive demo with Google Colab. Please click the following button to get access to the demos.
 
 [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1fjRJCh96SoYLZPRxsjF9VDv4Q2VoIckI?usp=sharing)
 
 
-It is based on ESPnet2. Pretrained models are available for both speech enhancement and speech separation tasks.
+It is based on ESPnet2. Pre-trained models are available for both speech enhancement and speech separation tasks.
+
+Speech separation streaming demos:
+
+[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/17vd1V78eJpp3PHBnbFE5aVY5uMxQFL6o?usp=sharing)
+
+
 
 </div></details>
 
 ### ST results
 
 <details><summary>expand</summary><div>
 
 We list 4-gram BLEU of major ST tasks.
 
 #### end-to-end system
-| Task                                              | BLEU  |                                                                                         Pretrained model                                                                                          |
+| Task                                              | BLEU  |                                                                                         Pre-trained model                                                                                          |
 | ------------------------------------------------- | :---: | :-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
 | Fisher-CallHome Spanish fisher_test (Es->En)      | 51.03 | [link](https://github.com/espnet/espnet/blob/master/egs/fisher_callhome_spanish/st1/RESULTS.md#train_spen_lcrm_pytorch_train_pytorch_transformer_bpe_short_long_bpe1000_specaug_asrtrans_mttrans) |
 | Fisher-CallHome Spanish callhome_evltest (Es->En) | 20.44 | [link](https://github.com/espnet/espnet/blob/master/egs/fisher_callhome_spanish/st1/RESULTS.md#train_spen_lcrm_pytorch_train_pytorch_transformer_bpe_short_long_bpe1000_specaug_asrtrans_mttrans) |
 | Libri-trans test (En->Fr)                         | 16.70 |       [link](https://github.com/espnet/espnet/blob/master/egs/libri_trans/st1/RESULTS.md#train_spfr_lc_pytorch_train_pytorch_transformer_bpe_short_long_bpe1000_specaug_asrtrans_mttrans-1)       |
 | How2 dev5 (En->Pt)                                | 45.68 |              [link](https://github.com/espnet/espnet/blob/master/egs/how2/st1/RESULTS.md#trainpt_tc_pytorch_train_pytorch_transformer_short_long_bpe8000_specaug_asrtrans_mttrans-1)              |
 | Must-C tst-COMMON (En->De)                        | 22.91 |          [link](https://github.com/espnet/espnet/blob/master/egs/must_c/st1/RESULTS.md#train_spen-dede_tc_pytorch_train_pytorch_transformer_short_long_bpe8000_specaug_asrtrans_mttrans)          |
 | Mboshi-French dev (Fr->Mboshi)                    | 6.18  |                                                                                                N/A                                                                                                |
 
 #### cascaded system
-| Task                                              | BLEU  | Pretrained model |
+| Task                                              | BLEU  | Pre-trained model |
 | ------------------------------------------------- | :---: | :--------------: |
 | Fisher-CallHome Spanish fisher_test (Es->En)      | 42.16 |       N/A        |
 | Fisher-CallHome Spanish callhome_evltest (Es->En) | 19.82 |       N/A        |
 | Libri-trans test (En->Fr)                         | 16.96 |       N/A        |
 | How2 dev5 (En->Pt)                                | 44.90 |       N/A        |
 | Must-C tst-COMMON (En->De)                        | 23.65 |       N/A        |
 
@@ -415,41 +411,41 @@
 (**New!**) We made a new real-time E2E-ST + TTS demonstration in Google Colab.
 Please access the notebook from the following button and enjoy the real-time speech-to-speech translation!
 
 [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/espnet/notebook/blob/master/st_demo.ipynb)
 
 ---
 
-You can translate speech in a WAV file using pretrained models.
+You can translate speech in a WAV file using pre-trained models.
 Go to a recipe directory and run `utils/translate_wav.sh` as follows:
 ```sh
-# go to recipe directory and source path of espnet tools
+# Go to recipe directory and source path of espnet tools
 cd egs/fisher_callhome_spanish/st1 && . ./path.sh
 # download example wav file
 wget -O - https://github.com/espnet/espnet/files/4100928/test.wav.tar.gz | tar zxvf -
 # let's translate speech!
 translate_wav.sh --models fisher_callhome_spanish.transformer.v1.es-en test.wav
 ```
 where `test.wav` is a WAV file to be translated.
 The sampling rate must be consistent with that of data used in training.
 
-Available pretrained models in the demo script are listed as below.
+Available pre-trained models in the demo script are listed as below.
 
 | Model                                                                                                        | Notes                                                    |
 | :----------------------------------------------------------------------------------------------------------- | :------------------------------------------------------- |
 | [fisher_callhome_spanish.transformer.v1](https://drive.google.com/open?id=1hawp5ZLw4_SIHIT3edglxbKIIkPVe8n3) | Transformer-ST trained on Fisher-CallHome Spanish Es->En |
 
 </div></details>
 
 
 ### MT results
 
 <details><summary>expand</summary><div>
 
-| Task                                              | BLEU  |                                                                        Pretrained model                                                                         |
+| Task                                              | BLEU  |                                                                        Pre-trained model                                                                         |
 | ------------------------------------------------- | :---: | :-------------------------------------------------------------------------------------------------------------------------------------------------------------: |
 | Fisher-CallHome Spanish fisher_test (Es->En)      | 61.45 | [link](https://github.com/espnet/espnet/blob/master/egs/fisher_callhome_spanish/mt1/RESULTS.md#trainen_lcrm_lcrm_pytorch_train_pytorch_transformer_bpe_bpe1000) |
 | Fisher-CallHome Spanish callhome_evltest (Es->En) | 29.86 | [link](https://github.com/espnet/espnet/blob/master/egs/fisher_callhome_spanish/mt1/RESULTS.md#trainen_lcrm_lcrm_pytorch_train_pytorch_transformer_bpe_bpe1000) |
 | Libri-trans test (En->Fr)                         | 18.09 |          [link](https://github.com/espnet/espnet/blob/master/egs/libri_trans/mt1/RESULTS.md#trainfr_lcrm_tc_pytorch_train_pytorch_transformer_bpe1000)          |
 | How2 dev5 (En->Pt)                                | 58.61 |              [link](https://github.com/espnet/espnet/blob/master/egs/how2/mt1/RESULTS.md#trainpt_tc_tc_pytorch_train_pytorch_transformer_bpe8000)               |
 | Must-C tst-COMMON (En->De)                        | 27.63 |                               [link](https://github.com/espnet/espnet/blob/master/egs/must_c/mt1/RESULTS.md#summary-4-gram-bleu)                                |
 | IWSLT'14 test2014 (En->De)                        | 24.70 |                                     [link](https://github.com/espnet/espnet/blob/master/egs/iwslt16/mt1/RESULTS.md#result)                                      |
@@ -463,23 +459,23 @@
 ### TTS results
 
 <details><summary>ESPnet2</summary><div>
 
 You can listen to the generated samples in the following URL.
 - [ESPnet2 TTS generated samples](https://drive.google.com/drive/folders/1H3fnlBbWMEkQUfrHqosKN_ZX_WjO29ma?usp=sharing)
 
-> Note that in the generation we use Griffin-Lim (`wav/`) and [Parallel WaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN) (`wav_pwg/`).
+> Note that in the generation, we use Griffin-Lim (`wav/`) and [Parallel WaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN) (`wav_pwg/`).
 
-You can download pretrained models via `espnet_model_zoo`.
+You can download pre-trained models via `espnet_model_zoo`.
 - [ESPnet model zoo](https://github.com/espnet/espnet_model_zoo)
-- [Pretrained model list](https://github.com/espnet/espnet_model_zoo/blob/master/espnet_model_zoo/table.csv)
+- [Pre-trained model list](https://github.com/espnet/espnet_model_zoo/blob/master/espnet_model_zoo/table.csv)
 
-You can download pretrained vocoders via `kan-bayashi/ParallelWaveGAN`.
+You can download pre-trained vocoders via `kan-bayashi/ParallelWaveGAN`.
 - [kan-bayashi/ParallelWaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN)
-- [Pretrained vocoder list](https://github.com/kan-bayashi/ParallelWaveGAN#results)
+- [Pre-trained vocoder list](https://github.com/kan-bayashi/ParallelWaveGAN#results)
 
 </div></details>
 
 <details><summary>ESPnet1</summary><div>
 
 > NOTE: We are moving on ESPnet2-based development for TTS. Please check the latest results in the above ESPnet2 results.
 
@@ -496,28 +492,28 @@
 - [Single Italian speaker FastSpeech](https://drive.google.com/open?id=13I5V2w7deYFX4DlVk1-0JfaXmUR2rNOv)
 - [Single Mandarin speaker Transformer](https://drive.google.com/open?id=1mEnZfBKqA4eT6Bn0eRZuP6lNzL-IL3VD)
 - [Single Mandarin speaker FastSpeech](https://drive.google.com/open?id=1Ol_048Tuy6BgvYm1RpjhOX4HfhUeBqdK)
 - [Multi Japanese speaker Transformer](https://drive.google.com/open?id=1fFMQDF6NV5Ysz48QLFYE8fEvbAxCsMBw)
 - [Single English speaker models with Parallel WaveGAN](https://drive.google.com/open?id=1HvB0_LDf1PVinJdehiuCt5gWmXGguqtx)
 - [Single English speaker knowledge distillation-based FastSpeech](https://drive.google.com/open?id=1wG-Y0itVYalxuLAHdkAHO7w1CWFfRPF4)
 
-You can download all of the pretrained models and generated samples:
-- [All of the pretrained E2E-TTS models](https://drive.google.com/open?id=1k9RRyc06Zl0mM2A7mi-hxNiNMFb_YzTF)
+You can download all of the pre-trained models and generated samples:
+- [All of the pre-trained E2E-TTS models](https://drive.google.com/open?id=1k9RRyc06Zl0mM2A7mi-hxNiNMFb_YzTF)
 - [All of the generated samples](https://drive.google.com/open?id=1bQGuqH92xuxOX__reWLP4-cif0cbpMLX)
 
-Note that in the generated samples we use the following vocoders: Griffin-Lim (**GL**), WaveNet vocoder (**WaveNet**), Parallel WaveGAN (**ParallelWaveGAN**), and MelGAN (**MelGAN**).
-The neural vocoders are based on following repositories.
+Note that in the generated samples, we use the following vocoders: Griffin-Lim (**GL**), WaveNet vocoder (**WaveNet**), Parallel WaveGAN (**ParallelWaveGAN**), and MelGAN (**MelGAN**).
+The neural vocoders are based on the following repositories.
 - [kan-bayashi/ParallelWaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN): Parallel WaveGAN / MelGAN / Multi-band MelGAN
 - [r9y9/wavenet_vocoder](https://github.com/r9y9/wavenet_vocoder): 16 bit mixture of Logistics WaveNet vocoder
 - [kan-bayashi/PytorchWaveNetVocoder](https://github.com/kan-bayashi/PytorchWaveNetVocoder): 8 bit Softmax WaveNet Vocoder with the noise shaping
 
 If you want to build your own neural vocoder, please check the above repositories.
 [kan-bayashi/ParallelWaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN) provides [the manual](https://github.com/kan-bayashi/ParallelWaveGAN#decoding-with-espnet-tts-models-features) about how to decode ESPnet-TTS model's features with neural vocoders. Please check it.
 
-Here we list all of the pretrained neural vocoders. Please download and enjoy the generation of high quality speech!
+Here we list all of the pre-trained neural vocoders. Please download and enjoy the generation of high-quality speech!
 
 | Model link                                                                                           | Lang  | Fs [Hz] | Mel range [Hz] | FFT / Shift / Win [pt] | Model type                                                              |
 | :--------------------------------------------------------------------------------------------------- | :---: | :-----: | :------------: | :--------------------: | :---------------------------------------------------------------------- |
 | [ljspeech.wavenet.softmax.ns.v1](https://drive.google.com/open?id=1eA1VcRS9jzFa-DovyTgJLQ_jmwOLIi8L) |  EN   | 22.05k  |      None      |   1024 / 256 / None    | [Softmax WaveNet](https://github.com/kan-bayashi/PytorchWaveNetVocoder) |
 | [ljspeech.wavenet.mol.v1](https://drive.google.com/open?id=1sY7gEUg39QaO1szuN62-Llst9TrFno2t)        |  EN   | 22.05k  |      None      |   1024 / 256 / None    | [MoL WaveNet](https://github.com/r9y9/wavenet_vocoder)                  |
 | [ljspeech.parallel_wavegan.v1](https://drive.google.com/open?id=1tv9GKyRT4CDsvUWKwH3s_OfXkiTi0gw7)   |  EN   | 22.05k  |      None      |   1024 / 256 / None    | [Parallel WaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN)      |
 | [ljspeech.wavenet.mol.v2](https://drive.google.com/open?id=1es2HuKUeKVtEdq6YDtAsLNpqCy4fhIXr)        |  EN   | 22.05k  |    80-7600     |   1024 / 256 / None    | [MoL WaveNet](https://github.com/r9y9/wavenet_vocoder)                  |
@@ -526,15 +522,15 @@
 | [ljspeech.melgan.v3](https://drive.google.com/open?id=1_a8faVA5OGCzIcJNw4blQYjfG4oA9VEt)             |  EN   | 22.05k  |    80-7600     |   1024 / 256 / None    | [MelGAN](https://github.com/kan-bayashi/ParallelWaveGAN)                |
 | [libritts.wavenet.mol.v1](https://drive.google.com/open?id=1jHUUmQFjWiQGyDd7ZeiCThSjjpbF_B4h)        |  EN   |   24k   |      None      |   1024 / 256 / None    | [MoL WaveNet](https://github.com/r9y9/wavenet_vocoder)                  |
 | [jsut.wavenet.mol.v1](https://drive.google.com/open?id=187xvyNbmJVZ0EZ1XHCdyjZHTXK9EcfkK)            |  JP   |   24k   |    80-7600     |   2048 / 300 / 1200    | [MoL WaveNet](https://github.com/r9y9/wavenet_vocoder)                  |
 | [jsut.parallel_wavegan.v1](https://drive.google.com/open?id=1OwrUQzAmvjj1x9cDhnZPp6dqtsEqGEJM)       |  JP   |   24k   |    80-7600     |   2048 / 300 / 1200    | [Parallel WaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN)      |
 | [csmsc.wavenet.mol.v1](https://drive.google.com/open?id=1PsjFRV5eUP0HHwBaRYya9smKy5ghXKzj)           |  ZH   |   24k   |    80-7600     |   2048 / 300 / 1200    | [MoL WaveNet](https://github.com/r9y9/wavenet_vocoder)                  |
 | [csmsc.parallel_wavegan.v1](https://drive.google.com/open?id=10M6H88jEUGbRWBmU1Ff2VaTmOAeL8CEy)      |  ZH   |   24k   |    80-7600     |   2048 / 300 / 1200    | [Parallel WaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN)      |
 
-If you want to use the above pretrained vocoders, please exactly match the feature setting with them.
+If you want to use the above pre-trained vocoders, please exactly match the feature setting with them.
 
 </div></details>
 
 ### TTS demo
 
 <details><summary>ESPnet2</summary><div>
 
@@ -552,78 +548,78 @@
 > NOTE: We are moving on ESPnet2-based development for TTS. Please check the latest demo in the above ESPnet2 demo.
 
 You can try the real-time demo in Google Colab.
 Please access the notebook from the following button and enjoy the real-time synthesis.
 
 - Real-time TTS demo with ESPnet1  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/espnet/notebook/blob/master/tts_realtime_demo.ipynb)
 
-We also provide shell script to perform synthesize.
+We also provide a shell script to perform synthesis.
 Go to a recipe directory and run `utils/synth_wav.sh` as follows:
 
 ```sh
-# go to recipe directory and source path of espnet tools
+# Go to recipe directory and source path of espnet tools
 cd egs/ljspeech/tts1 && . ./path.sh
-# we use upper-case char sequence for the default model.
+# We use an upper-case char sequence for the default model.
 echo "THIS IS A DEMONSTRATION OF TEXT TO SPEECH." > example.txt
 # let's synthesize speech!
 synth_wav.sh example.txt
 
-# also you can use multiple sentences
+# Also, you can use multiple sentences
 echo "THIS IS A DEMONSTRATION OF TEXT TO SPEECH." > example_multi.txt
 echo "TEXT TO SPEECH IS A TECHNIQUE TO CONVERT TEXT INTO SPEECH." >> example_multi.txt
 synth_wav.sh example_multi.txt
 ```
 
-You can change the pretrained model as follows:
+You can change the pre-trained model as follows:
 
 ```sh
 synth_wav.sh --models ljspeech.fastspeech.v1 example.txt
 ```
 
-Waveform synthesis is performed with Griffin-Lim algorithm and neural vocoders (WaveNet and ParallelWaveGAN).
-You can change the pretrained vocoder model as follows:
+Waveform synthesis is performed with the Griffin-Lim algorithm and neural vocoders (WaveNet and ParallelWaveGAN).
+You can change the pre-trained vocoder model as follows:
 
 ```sh
 synth_wav.sh --vocoder_models ljspeech.wavenet.mol.v1 example.txt
 ```
 
-WaveNet vocoder provides very high quality speech but it takes time to generate.
+WaveNet vocoder provides very high-quality speech, but it takes time to generate.
 
 See more details or available models via `--help`.
 
 ```sh
 synth_wav.sh --help
 ```
 
 </div></details>
 
 ### VC results
 
 <details><summary>expand</summary><div>
 
-- Transformer and Tacotron2 based VC
+- Transformer and Tacotron2-based VC
 
 You can listen to some samples on the [demo webpage](https://unilight.github.io/Publication-Demos/publications/transformer-vc/).
 
 - Cascade ASR+TTS as one of the baseline systems of VCC2020
 
 The [Voice Conversion Challenge 2020](http://www.vc-challenge.org/) (VCC2020) adopts ESPnet to build an end-to-end based baseline system.
-In VCC2020, the objective is intra/cross lingual nonparallel VC.
+In VCC2020, the objective is intra/cross-lingual nonparallel VC.
 You can download converted samples of the cascade ASR+TTS baseline system [here](https://drive.google.com/drive/folders/1oeZo83GrOgtqxGwF7KagzIrfjr8X59Ue?usp=sharing).
 
 </div></details>
 
 ### SLU results
 
 <details><summary>expand</summary><div>
 
 
-We list the performance on various SLU tasks and dataset using the metric reported in the original dataset paper
+We list the performance on various SLU tasks and datasets using the metric reported in the original dataset paper
 
-| Task                                                              | Dataset                                                              |    Metric     |     Result     |                                                                              Pretrained Model                                         |
+| Task                                                              | Dataset                                                              |    Metric     |     Result     |                                                                              Pre-trained Model                                         |
 | ----------------------------------------------------------------- | :-------------: | :-------------: | :-------------: | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
 | Intent Classification                                                 |     SLURP     |       Acc       |       86.3       |                [link](https://github.com/espnet/espnet/tree/master/egs2/slurp/asr1/README.md)                |
 | Intent Classification                                                   |     FSC     |       Acc       |       99.6       |                [link](https://github.com/espnet/espnet/tree/master/egs2/fsc/asr1/README.md)                |
 | Intent Classification                                                  |     FSC Unseen Speaker Set     |       Acc       |       98.6       |                [link](https://github.com/espnet/espnet/tree/master/egs2/fsc_unseen/asr1/README.md)                |
 | Intent Classification                                                   |     FSC Unseen Utterance Set     |       Acc       |       86.4       |                [link](https://github.com/espnet/espnet/tree/master/egs2/fsc_unseen/asr1/README.md)                |
 | Intent Classification                                                   |     FSC Challenge Speaker Set     |       Acc       |       97.5       |                [link](https://github.com/espnet/espnet/tree/master/egs2/fsc_challenge/asr1/README.md)                |
 | Intent Classification                                                   |     FSC Challenge Utterance Set     |       Acc       |       78.5       |                [link](https://github.com/espnet/espnet/tree/master/egs2/fsc_challenge/asr1/README.md)                |
@@ -648,15 +644,15 @@
 ### CTC Segmentation demo
 
 <details><summary>ESPnet1</summary><div>
 
 [CTC segmentation](https://arxiv.org/abs/2007.09127) determines utterance segments within audio files.
 Aligned utterance segments constitute the labels of speech datasets.
 
-As demo, we align start and end of utterances within the audio file `ctc_align_test.wav`, using the example script `utils/asr_align_wav.sh`.
+As a demo, we align the start and end of utterances within the audio file `ctc_align_test.wav`, using the example script `utils/asr_align_wav.sh`.
 For preparation, set up a data directory:
 
 ```sh
 cd egs/tedlium2/align1/
 # data directory
 align_dir=data/demo
 mkdir -p ${align_dir}
@@ -686,36 +682,36 @@
 ../../../utils/asr_align_wav.sh \
     --models ${model} \
     --align_dir ${align_dir} \
     --align_config ${align_dir}/align.yaml \
     ${wav} ${align_dir}/utt_text
 ```
 
-Segments are written to `aligned_segments` as a list of file/utterance name, utterance start and end times in seconds and a confidence score.
-The confidence score is a probability in log space that indicates how good the utterance was aligned. If needed, remove bad utterances:
+Segments are written to `aligned_segments` as a list of file/utterance names, utterance start and end times in seconds, and a confidence score.
+The confidence score is a probability in log space that indicates how well the utterance was aligned. If needed, remove bad utterances:
 
 ```sh
 min_confidence_score=-5
 awk -v ms=${min_confidence_score} '{ if ($5 > ms) {print} }' ${align_dir}/aligned_segments
 ```
 
-The demo script `utils/ctc_align_wav.sh` uses an already pretrained ASR model (see list above for more models).
+The demo script `utils/ctc_align_wav.sh` uses an already pre-trained ASR model (see the list above for more models).
 It is recommended to use models with RNN-based encoders (such as BLSTMP) for aligning large audio files;
-rather than using Transformer models that have a high memory consumption on longer audio data.
+rather than using Transformer models with a high memory consumption on longer audio data.
 The sample rate of the audio must be consistent with that of the data used in training; adjust with `sox` if needed.
 A full example recipe is in `egs/tedlium2/align1/`.
 
 </div></details>
 
 <details><summary>ESPnet2</summary><div>
 
 [CTC segmentation](https://arxiv.org/abs/2007.09127) determines utterance segments within audio files.
 Aligned utterance segments constitute the labels of speech datasets.
 
-As demo, we align start and end of utterances within the audio file `ctc_align_test.wav`.
+As a demo, we align the start and end of utterances within the audio file `ctc_align_test.wav`.
 This can be done either directly from the Python command line or using the script `espnet2/bin/asr_align.py`.
 
 From the Python command line interface:
 
 ```python
 # load a model with character tokens
 from espnet_model_zoo.downloader import ModelDownloader
@@ -739,29 +735,29 @@
 # utt2 utt 1.73 3.19 -0.7674 IS PART OF HOLIDAY'S STRATEGY
 # utt3 utt 3.19 4.20 -0.7433 TO SELL OFF ASSETS
 # utt4 utt 4.20 6.10 -0.4899 AND CONCENTRATE ON PROPERTY MANAGEMENT
 ```
 
 Aligning also works with fragments of the text.
 For this, set the `gratis_blank` option that allows skipping unrelated audio sections without penalty.
-It's also possible to omit the utterance names at the beginning of each line, by setting `kaldi_style_text` to False.
+It's also possible to omit the utterance names at the beginning of each line by setting `kaldi_style_text` to False.
 
 ```python
 aligner.set_config( gratis_blank=True, kaldi_style_text=False )
 text = ["SALE OF THE HOTELS", "PROPERTY MANAGEMENT"]
 segments = aligner(speech, text)
 print(segments)
 # utt_0000 utt 0.37 1.72 -2.0651 SALE OF THE HOTELS
 # utt_0001 utt 4.70 6.10 -5.0566 PROPERTY MANAGEMENT
 ```
 
 The script `espnet2/bin/asr_align.py` uses a similar interface. To align utterances:
 
 ```sh
-# ASR model and config files from pretrained model (e.g. from cachedir):
+# ASR model and config files from pre-trained model (e.g., from cachedir):
 asr_config=<path-to-model>/config.yaml
 asr_model=<path-to-model>/valid.*best.pth
 # prepare the text file
 wav="test_utils/ctc_align_test.wav"
 text="test_utils/ctc_align_text.txt"
 cat << EOF > ${text}
 utt1 THE SALE OF THE HOTELS
@@ -776,16 +772,16 @@
 # utt2 ctc_align_test 1.73 3.19 -0.7674 IS PART OF HOLIDAY'S STRATEGY
 # utt3 ctc_align_test 3.19 4.20 -0.7433 TO SELL OFF ASSETS
 # utt4 ctc_align_test 4.20 4.97 -0.6017 AND CONCENTRATE
 # utt5 ctc_align_test 4.97 6.10 -0.3477 ON PROPERTY MANAGEMENT
 ```
 
 The output of the script can be redirected to a `segments` file by adding the argument `--output segments`.
-Each line contains file/utterance name, utterance start and end times in seconds and a confidence score; optionally also the utterance text.
-The confidence score is a probability in log space that indicates how good the utterance was aligned. If needed, remove bad utterances:
+Each line contains the file/utterance name, utterance start and end times in seconds, and a confidence score; optionally also the utterance text.
+The confidence score is a probability in log space that indicates how well the utterance was aligned. If needed, remove bad utterances:
 
 ```sh
 min_confidence_score=-7
 # here, we assume that the output was written to the file `segments`
 awk -v ms=${min_confidence_score} '{ if ($5 > ms) {print} }' segments
 ```
 
@@ -831,14 +827,20 @@
     month = jul,
     year = "2020",
     address = "Online",
     publisher = "Association for Computational Linguistics",
     url = "https://www.aclweb.org/anthology/2020.acl-demos.34",
     pages = "302--311",
 }
+@article{hayashi2021espnet2,
+  title={Espnet2-tts: Extending the edge of tts research},
+  author={Hayashi, Tomoki and Yamamoto, Ryuichi and Yoshimura, Takenori and Wu, Peter and Shi, Jiatong and Saeki, Takaaki and Ju, Yooncheol and Yasuda, Yusuke and Takamichi, Shinnosuke and Watanabe, Shinji},
+  journal={arXiv preprint arXiv:2110.07840},
+  year={2021}
+}
 @inproceedings{li2020espnet,
   title={{ESPnet-SE}: End-to-End Speech Enhancement and Separation Toolkit Designed for {ASR} Integration},
   author={Chenda Li and Jing Shi and Wangyou Zhang and Aswin Shanmugam Subramanian and Xuankai Chang and Naoyuki Kamo and Moto Hira and Tomoki Hayashi and Christoph Boeddeker and Zhuo Chen and Shinji Watanabe},
   booktitle={Proceedings of IEEE Spoken Language Technology Workshop (SLT)},
   pages={785--792},
   year={2021},
   organization={IEEE},
@@ -855,14 +857,21 @@
   author={Shi, Jiatong and Guo, Shuai and Qian, Tao and Huo, Nan and Hayashi, Tomoki and Wu, Yuning and Xu, Frank and Chang, Xuankai and Li, Huazhe and Wu, Peter and Watanabe, Shinji and Jin, Qin},
   title={{Muskits}: an End-to-End Music Processing Toolkit for Singing Voice Synthesis},
   year={2022},
   booktitle={Proceedings of Interspeech},
   pages={4277-4281},
   url={https://www.isca-speech.org/archive/pdfs/interspeech_2022/shi22d_interspeech.pdf}
 }
+@inproceedings{lu22c_interspeech,
+  author={Yen-Ju Lu and Xuankai Chang and Chenda Li and Wangyou Zhang and Samuele Cornell and Zhaoheng Ni and Yoshiki Masuyama and Brian Yan and Robin Scheibler and Zhong-Qiu Wang and Yu Tsao and Yanmin Qian and Shinji Watanabe},
+  title={{ESPnet-SE++: Speech Enhancement for Robust Speech Recognition, Translation, and Understanding}},
+  year=2022,
+  booktitle={Proc. Interspeech 2022},
+  pages={5458--5462},
+}
 @article{gao2022euro,
   title={{EURO}: {ESPnet} Unsupervised ASR Open-source Toolkit},
   author={Gao, Dongji and Shi, Jiatong and Chuang, Shun-Po and Garcia, Leibny Paola and Lee, Hung-yi and Watanabe, Shinji and Khudanpur, Sanjeev},
   journal={arXiv preprint arXiv:2211.17196},
   year={2022}
 }
 ```
```

### Comparing `espnet-202304/espnet/asr/asr_mix_utils.py` & `espnet-202308/espnet/asr/asr_mix_utils.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/asr/asr_utils.py` & `espnet-202308/espnet/asr/asr_utils.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/asr/chainer_backend/asr.py` & `espnet-202308/espnet/asr/chainer_backend/asr.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/asr/pytorch_backend/asr.py` & `espnet-202308/espnet/asr/pytorch_backend/asr.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/asr/pytorch_backend/asr_init.py` & `espnet-202308/espnet/asr/pytorch_backend/asr_init.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/asr/pytorch_backend/asr_mix.py` & `espnet-202308/espnet/asr/pytorch_backend/asr_mix.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/asr/pytorch_backend/recog.py` & `espnet-202308/espnet/asr/pytorch_backend/recog.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/bin/asr_align.py` & `espnet-202308/espnet/bin/asr_align.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/bin/asr_enhance.py` & `espnet-202308/espnet/bin/asr_enhance.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/bin/asr_recog.py` & `espnet-202308/espnet/bin/asr_recog.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/bin/asr_train.py` & `espnet-202308/espnet/bin/asr_train.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/bin/lm_train.py` & `espnet-202308/espnet/bin/lm_train.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/bin/mt_train.py` & `espnet-202308/espnet/bin/mt_train.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/bin/mt_trans.py` & `espnet-202308/espnet/bin/mt_trans.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/bin/st_train.py` & `espnet-202308/espnet/bin/st_train.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/bin/st_trans.py` & `espnet-202308/espnet/bin/st_trans.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/bin/tts_decode.py` & `espnet-202308/espnet/bin/tts_decode.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/bin/tts_train.py` & `espnet-202308/espnet/bin/tts_train.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/bin/vc_decode.py` & `espnet-202308/espnet/bin/vc_decode.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/bin/vc_train.py` & `espnet-202308/espnet/bin/vc_train.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/lm/chainer_backend/extlm.py` & `espnet-202308/espnet/lm/chainer_backend/extlm.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/lm/chainer_backend/lm.py` & `espnet-202308/espnet/lm/chainer_backend/lm.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/lm/lm_utils.py` & `espnet-202308/espnet/lm/lm_utils.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/lm/pytorch_backend/extlm.py` & `espnet-202308/espnet/lm/pytorch_backend/extlm.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/lm/pytorch_backend/lm.py` & `espnet-202308/espnet/lm/pytorch_backend/lm.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/mt/mt_utils.py` & `espnet-202308/espnet/mt/mt_utils.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/mt/pytorch_backend/mt.py` & `espnet-202308/espnet/mt/pytorch_backend/mt.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/asr_interface.py` & `espnet-202308/espnet/nets/asr_interface.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/batch_beam_search.py` & `espnet-202308/espnet/nets/batch_beam_search.py`

 * *Files 18% similar despite different names*

```diff
@@ -16,70 +16,86 @@
     """Batchfied/Vectorized hypothesis data type."""
 
     yseq: torch.Tensor = torch.tensor([])  # (batch, maxlen)
     score: torch.Tensor = torch.tensor([])  # (batch,)
     length: torch.Tensor = torch.tensor([])  # (batch,)
     scores: Dict[str, torch.Tensor] = dict()  # values: (batch,)
     states: Dict[str, Dict] = dict()
+    hs: List[torch.Tensor] = []  # (batch, maxlen, adim)
 
     def __len__(self) -> int:
         """Return a batch size."""
         return len(self.length)
 
 
 class BatchBeamSearch(BeamSearch):
     """Batch beam search implementation."""
 
     def batchfy(self, hyps: List[Hypothesis]) -> BatchHypothesis:
         """Convert list to batch."""
         if len(hyps) == 0:
             return BatchHypothesis()
+
+        if self.return_hs:
+            hs = [h.hs for h in hyps]
+        else:
+            hs = []
+
         return BatchHypothesis(
             yseq=pad_sequence(
                 [h.yseq for h in hyps], batch_first=True, padding_value=self.eos
             ),
             length=torch.tensor([len(h.yseq) for h in hyps], dtype=torch.int64),
             score=torch.tensor([h.score for h in hyps]),
             scores={k: torch.tensor([h.scores[k] for h in hyps]) for k in self.scorers},
             states={k: [h.states[k] for h in hyps] for k in self.scorers},
+            hs=hs,
         )
 
     def _batch_select(self, hyps: BatchHypothesis, ids: List[int]) -> BatchHypothesis:
+        if self.return_hs:
+            hs = [hyps.hs[i] for i in ids]
+        else:
+            hs = []
+
         return BatchHypothesis(
             yseq=hyps.yseq[ids],
             score=hyps.score[ids],
             length=hyps.length[ids],
             scores={k: v[ids] for k, v in hyps.scores.items()},
             states={
                 k: [self.scorers[k].select_state(v, i) for i in ids]
                 for k, v in hyps.states.items()
             },
+            hs=hs,
         )
 
     def _select(self, hyps: BatchHypothesis, i: int) -> Hypothesis:
         return Hypothesis(
             yseq=hyps.yseq[i, : hyps.length[i]],
             score=hyps.score[i],
             scores={k: v[i] for k, v in hyps.scores.items()},
             states={
                 k: self.scorers[k].select_state(v, i) for k, v in hyps.states.items()
             },
+            hs=hyps.hs[i] if self.return_hs else [],
         )
 
     def unbatchfy(self, batch_hyps: BatchHypothesis) -> List[Hypothesis]:
         """Revert batch to list."""
         return [
             Hypothesis(
                 yseq=batch_hyps.yseq[i][: batch_hyps.length[i]],
                 score=batch_hyps.score[i],
                 scores={k: batch_hyps.scores[k][i] for k in self.scorers},
                 states={
                     k: v.select_state(batch_hyps.states[k], i)
                     for k, v in self.scorers.items()
                 },
+                hs=batch_hyps.hs[i] if self.return_hs else [],
             )
             for i in range(len(batch_hyps.length))
         ]
 
     def batch_beam(
         self, weighted_scores: torch.Tensor, ids: torch.Tensor
     ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
@@ -130,66 +146,95 @@
 
         return self.batchfy(
             [
                 Hypothesis(
                     score=0.0,
                     scores=init_scores,
                     states=init_states,
+                    hs=[],
                     yseq=torch.tensor(primer, device=x.device),
                 )
             ]
         )
 
     def score_full(
-        self, hyp: BatchHypothesis, x: torch.Tensor
+        self,
+        hyp: BatchHypothesis,
+        x: torch.Tensor,
+        pre_x: torch.Tensor = None,
     ) -> Tuple[Dict[str, torch.Tensor], Dict[str, Any]]:
         """Score new hypothesis by `self.full_scorers`.
 
         Args:
             hyp (Hypothesis): Hypothesis with prefix tokens to score
             x (torch.Tensor): Corresponding input feature
+            pre_x (torch.Tensor): Encoded speech feature for sequential attn (T, D)
+                Sequential attn computes attn first on pre_x then on x,
+                thereby attending to two sources in sequence.
 
         Returns:
             Tuple[Dict[str, torch.Tensor], Dict[str, Any]]: Tuple of
                 score dict of `hyp` that has string keys of `self.full_scorers`
                 and tensor score values of shape: `(self.n_vocab,)`,
                 and state dict that has string keys
                 and state values of `self.full_scorers`
 
         """
         scores = dict()
         states = dict()
         for k, d in self.full_scorers.items():
-            scores[k], states[k] = d.batch_score(hyp.yseq, hyp.states[k], x)
+            if "decoder" in k and self.return_hs:
+                scores[k], hs, states[k] = d.batch_score(
+                    hyp.yseq, hyp.states[k], x, return_hs=self.return_hs
+                )
+            elif "decoder" in k and pre_x is not None:
+                scores[k], states[k] = d.batch_score(hyp.yseq, hyp.states[k], x, pre_x)
+            else:
+                scores[k], states[k] = d.batch_score(hyp.yseq, hyp.states[k], x)
+
+        if self.return_hs:
+            return hs, scores, states
         return scores, states
 
     def score_partial(
-        self, hyp: BatchHypothesis, ids: torch.Tensor, x: torch.Tensor
+        self,
+        hyp: BatchHypothesis,
+        ids: torch.Tensor,
+        x: torch.Tensor,
+        pre_x: torch.Tensor = None,
     ) -> Tuple[Dict[str, torch.Tensor], Dict[str, Any]]:
         """Score new hypothesis by `self.full_scorers`.
 
         Args:
             hyp (Hypothesis): Hypothesis with prefix tokens to score
             ids (torch.Tensor): 2D tensor of new partial tokens to score
             x (torch.Tensor): Corresponding input feature
+            pre_x (torch.Tensor): Encoded speech feature for sequential attn (T, D)
+                Sequential attn computes attn first on pre_x then on x,
+                thereby attending to two sources in sequence.
 
         Returns:
             Tuple[Dict[str, torch.Tensor], Dict[str, Any]]: Tuple of
                 score dict of `hyp` that has string keys of `self.full_scorers`
                 and tensor score values of shape: `(self.n_vocab,)`,
                 and state dict that has string keys
                 and state values of `self.full_scorers`
 
         """
         scores = dict()
         states = dict()
         for k, d in self.part_scorers.items():
-            scores[k], states[k] = d.batch_score_partial(
-                hyp.yseq, ids, hyp.states[k], x
-            )
+            if "ctc" in k and pre_x is not None:
+                scores[k], states[k] = d.batch_score_partial(
+                    hyp.yseq, ids, hyp.states[k], pre_x
+                )
+            else:
+                scores[k], states[k] = d.batch_score_partial(
+                    hyp.yseq, ids, hyp.states[k], x
+                )
         return scores, states
 
     def merge_states(self, states: Any, part_states: Any, part_idx: int) -> Any:
         """Merge states for new hypothesis.
 
         Args:
             states: states of `self.full_scorers`
@@ -205,46 +250,68 @@
         new_states = dict()
         for k, v in states.items():
             new_states[k] = v
         for k, v in part_states.items():
             new_states[k] = v
         return new_states
 
-    def search(self, running_hyps: BatchHypothesis, x: torch.Tensor) -> BatchHypothesis:
+    def search(
+        self,
+        running_hyps: BatchHypothesis,
+        x: torch.Tensor,
+        pre_x: torch.Tensor = None,
+    ) -> BatchHypothesis:
         """Search new tokens for running hypotheses and encoded speech x.
 
         Args:
             running_hyps (BatchHypothesis): Running hypotheses on beam
             x (torch.Tensor): Encoded speech feature (T, D)
+            pre_x (torch.Tensor): Encoded speech feature for sequential attention (T, D)
 
         Returns:
             BatchHypothesis: Best sorted hypotheses
 
         """
         n_batch = len(running_hyps)
         part_ids = None  # no pre-beam
         # batch scoring
         weighted_scores = torch.zeros(
             n_batch, self.n_vocab, dtype=x.dtype, device=x.device
         )
-        scores, states = self.score_full(running_hyps, x.expand(n_batch, *x.shape))
+        if self.return_hs:
+            hs, scores, states = self.score_full(
+                running_hyps,
+                x.expand(n_batch, *x.shape),
+                pre_x=pre_x.expand(n_batch, *pre_x.shape)
+                if pre_x is not None
+                else None,
+            )
+        else:
+            scores, states = self.score_full(
+                running_hyps,
+                x.expand(n_batch, *x.shape),
+                pre_x=pre_x.expand(n_batch, *pre_x.shape)
+                if pre_x is not None
+                else None,
+            )
+
         for k in self.full_scorers:
             weighted_scores += self.weights[k] * scores[k]
         # partial scoring
         if self.do_pre_beam:
             pre_beam_scores = (
                 weighted_scores
                 if self.pre_beam_score_key == "full"
                 else scores[self.pre_beam_score_key]
             )
             part_ids = torch.topk(pre_beam_scores, self.pre_beam_size, dim=-1)[1]
         # NOTE(takaaki-hori): Unlike BeamSearch, we assume that score_partial returns
         # full-size score matrices, which has non-zero scores for part_ids and zeros
         # for others.
-        part_scores, part_states = self.score_partial(running_hyps, part_ids, x)
+        part_scores, part_states = self.score_partial(running_hyps, part_ids, x, pre_x)
         for k in self.part_scorers:
             weighted_scores += self.weights[k] * part_scores[k]
         # add previous hyp scores
         weighted_scores += running_hyps.score.to(
             dtype=x.dtype, device=x.device
         ).unsqueeze(1)
 
@@ -256,14 +323,18 @@
         for (
             full_prev_hyp_id,
             full_new_token_id,
             part_prev_hyp_id,
             part_new_token_id,
         ) in zip(*self.batch_beam(weighted_scores, part_ids)):
             prev_hyp = prev_hyps[full_prev_hyp_id]
+            if self.return_hs:
+                new_hs = prev_hyp.hs + [hs[full_prev_hyp_id].squeeze(0)]
+            else:
+                new_hs = []
             best_hyps.append(
                 Hypothesis(
                     score=weighted_scores[full_prev_hyp_id, full_new_token_id],
                     yseq=self.append_token(prev_hyp.yseq, full_new_token_id),
                     scores=self.merge_scores(
                         prev_hyp.scores,
                         {k: v[full_prev_hyp_id] for k, v in scores.items()},
@@ -280,14 +351,15 @@
                             k: self.part_scorers[k].select_state(
                                 v, part_prev_hyp_id, part_new_token_id
                             )
                             for k, v in part_states.items()
                         },
                         part_new_token_id,
                     ),
+                    hs=new_hs,
                 )
             )
         return self.batchfy(best_hyps)
 
     def post_process(
         self,
         i: int,
```

### Comparing `espnet-202304/espnet/nets/batch_beam_search_online.py` & `espnet-202308/espnet/nets/batch_beam_search_online.py`

 * *Files 26% similar despite different names*

```diff
@@ -4,17 +4,21 @@
 from typing import Any  # noqa: H301
 from typing import Dict  # noqa: H301
 from typing import List  # noqa: H301
 from typing import Tuple  # noqa: H301
 
 import torch
 
+from espnet2.asr.transducer.beam_search_transducer_streaming import (
+    BeamSearchTransducerStreaming,
+)
 from espnet.nets.batch_beam_search import BatchBeamSearch  # noqa: H301
 from espnet.nets.batch_beam_search import BatchHypothesis  # noqa: H301
 from espnet.nets.beam_search import Hypothesis
+from espnet.nets.beam_search_timesync_streaming import BeamSearchTimeSyncStreaming
 from espnet.nets.e2e_asr_common import end_detect
 
 
 class BatchBeamSearchOnline(BatchBeamSearch):
     """Online beam search implementation.
 
     This simulates streaming decoding.
@@ -31,39 +35,75 @@
         *args,
         block_size=40,
         hop_size=16,
         look_ahead=16,
         disable_repetition_detection=False,
         encoded_feat_length_limit=0,
         decoder_text_length_limit=0,
+        incremental_decode=False,
+        time_sync=False,
+        ctc=None,
+        hold_n=0,
+        transducer_conf=None,
+        joint_network=None,
         **kwargs,
     ):
         """Initialize beam search."""
         super().__init__(*args, **kwargs)
         self.block_size = block_size
         self.hop_size = hop_size
         self.look_ahead = look_ahead
         self.disable_repetition_detection = disable_repetition_detection
         self.encoded_feat_length_limit = encoded_feat_length_limit
         self.decoder_text_length_limit = decoder_text_length_limit
+        self.incremental_decode = incremental_decode
+        self.time_sync = time_sync
+        self.ctc = ctc
+        self.hold_n = hold_n
+
+        if time_sync:
+            if transducer_conf is not None:
+                self.time_sync_search = BeamSearchTransducerStreaming(
+                    decoder=self.scorers["decoder"],
+                    joint_network=joint_network,
+                    beam_size=self.beam_size,
+                    token_list=self.token_list,
+                    hold_n=hold_n,
+                    **transducer_conf,
+                )
+                del self.scorers["decoder"]
+                self.t = 0
+            else:
+                scorers = self.scorers.copy()
+                scorers["ctc"] = ctc
+                self.time_sync_search = BeamSearchTimeSyncStreaming(
+                    beam_size=self.beam_size,
+                    weights=self.weights,
+                    scorers=scorers,
+                    sos=self.sos,
+                    token_list=self.token_list,
+                    hold_n=hold_n,
+                )
+                self.t = 0
 
         self.reset()
 
     def reset(self):
         """Reset parameters."""
         self.encbuffer = None
         self.running_hyps = None
         self.prev_hyps = []
         self.ended_hyps = []
         self.processed_block = 0
         self.process_idx = 0
         self.prev_output = None
+        self.prev_incremental = None
 
     def score_full(
-        self, hyp: BatchHypothesis, x: torch.Tensor
+        self, hyp: BatchHypothesis, x: torch.Tensor, pre_x: torch.Tensor = None
     ) -> Tuple[Dict[str, torch.Tensor], Dict[str, Any]]:
         """Score new hypothesis by `self.full_scorers`.
 
         Args:
             hyp (Hypothesis): Hypothesis with prefix tokens to score
             x (torch.Tensor): Corresponding input feature
 
@@ -111,74 +151,223 @@
                 to automatically find maximum hypothesis lengths
             minlenratio (float): Input length ratio to obtain min output length.
 
         Returns:
             list[Hypothesis]: N-best decoding results
 
         """
-        if self.encbuffer is None:
+        if self.encbuffer is None or self.block_size == 0:
             self.encbuffer = x
         else:
             self.encbuffer = torch.cat([self.encbuffer, x], axis=0)
 
         x = self.encbuffer
 
         # set length bounds
         if maxlenratio == 0:
             maxlen = x.shape[0]
         else:
             maxlen = max(1, int(maxlenratio * x.size(0)))
 
-        ret = None
-        while True:
-            cur_end_frame = (
-                self.block_size - self.look_ahead + self.hop_size * self.processed_block
-            )
-            if cur_end_frame < x.shape[0]:
-                h = x.narrow(0, 0, cur_end_frame)
-                block_is_final = False
-            else:
-                if is_final:
-                    h = x
-                    block_is_final = True
-                else:
-                    break
-
-            logging.debug("Start processing block: %d", self.processed_block)
+        # set block_size == 0 for recomputing
+        if self.block_size == 0:
+            block_is_final = is_final
+            h = x
             logging.debug(
                 "  Feature length: {}, current position: {}".format(
                     h.shape[0], self.process_idx
                 )
             )
-            if (
-                self.encoded_feat_length_limit > 0
-                and h.shape[0] > self.encoded_feat_length_limit
-            ):
-                h = h.narrow(
-                    0,
-                    h.shape[0] - self.encoded_feat_length_limit,
-                    self.encoded_feat_length_limit,
+
+            if self.running_hyps is None:  # init hyps
+                init_scores = {}
+                init_states = {}
+                if "ctc" in self.scorers.keys():
+                    self.scorers["ctc"].batch_init_state(h)
+                    init_scores["ctc"] = torch.tensor([0.0])
+                    init_states["ctc"] = [None]
+                if "length_bonus" in self.scorers.keys():
+                    init_scores["length_bonus"] = torch.tensor([0.0])
+                    init_states["length_bonus"] = [None]
+                if "decoder" in self.scorers.keys():
+                    init_scores["decoder"] = torch.tensor([0.0])
+                    init_states["decoder"] = [None]
+                self.running_hyps = BatchHypothesis(
+                    score=torch.tensor([0.0]),
+                    scores=init_scores,
+                    states=init_states,
+                    length=torch.tensor([2]),
+                    yseq=torch.tensor([self.hyp_primer], device=x.device),
+                    hs=[],
                 )
+                self.prev_incremental = self.running_hyps
 
-            if self.running_hyps is None:
-                self.running_hyps = self.init_hyp(h)
-            ret = self.process_one_block(h, block_is_final, maxlen, maxlenratio)
-            logging.debug("Finished processing block: %d", self.processed_block)
+            if self.time_sync:
+                ret = self.process_one_block_time_sync(
+                    h, block_is_final, maxlen, maxlenratio
+                )
+            else:
+                ret = self.process_one_block(
+                    h, block_is_final, maxlen - self.process_idx, maxlenratio
+                )
+            logging.debug("Finished processing chunk: %d", self.processed_block)
             self.processed_block += 1
-            if block_is_final:
-                return ret
-        if ret is None:
-            if self.prev_output is None:
+
+            # prune running_hyps, taking top as an incremental decoding
+            if self.incremental_decode and not is_final:
+                if (
+                    self.running_hyps.yseq.shape[0] == 0
+                ):  # running_hyps will be empty if maxlen is reached
+                    logging.info(
+                        "search stopped by maxlen in a non final chunk. \
+                        reverting to prev running hyp"
+                    )
+                    self.running_hyps = self.prev_incremental
+                logging.info(
+                    "Hyps before incremental pruning: %d",
+                    self.running_hyps.yseq.shape[0],
+                )
+                if self.running_hyps.yseq.shape[0] > 0:
+                    self.running_hyps = self._batch_select(self.running_hyps, [0])
+                    self.prev_incremental = self.running_hyps
+                logging.info(
+                    "Hyps after incremental pruning: %d",
+                    self.running_hyps.yseq.shape[0],
+                )
+
+                if self.token_list is not None:
+                    logging.info(
+                        "best running hypo: "
+                        + "".join(
+                            [self.token_list[x] for x in self.running_hyps.yseq[0, 1:]]
+                        )
+                    )
+
+                # hold_n
+                if self.hold_n > 0 and self.running_hyps.length[0] > 2:
+                    self.running_hyps = BatchHypothesis(
+                        score=self.running_hyps.score,
+                        scores=self.running_hyps.scores,
+                        states=self.running_hyps.states,
+                        length=self.running_hyps.length - self.hold_n,
+                        yseq=self.running_hyps.yseq[:, : -self.hold_n],
+                        hs=[],
+                    )
+                    if self.token_list is not None:
+                        logging.info(
+                            "best hypo after hold: "
+                            + "".join(
+                                [
+                                    self.token_list[x]
+                                    for x in self.running_hyps.yseq[0, 1:]
+                                ]
+                            )
+                        )
+
+            if is_final:
+                if len(ret) == 0:
+                    if self.prev_output is None:
+                        return []
+                    else:
+                        return self.prev_output
+                else:
+                    return ret
+            else:
+                # dont return incremental hyps,
+                # check them by grabbing top running_hyp
+                if len(ret) > 0:
+                    self.prev_output = ret
                 return []
+
+        # blockwise processing w/ rewinding
+        else:
+            ret = None
+            while True:
+                cur_end_frame = (
+                    self.block_size
+                    - self.look_ahead
+                    + self.hop_size * self.processed_block
+                )
+                if cur_end_frame < x.shape[0]:
+                    h = x.narrow(0, 0, cur_end_frame)
+                    block_is_final = False
+                else:
+                    if is_final:
+                        h = x
+                        block_is_final = True
+                    else:
+                        break
+
+                logging.debug("Start processing block: %d", self.processed_block)
+                logging.debug(
+                    "  Feature length: {}, current position: {}".format(
+                        h.shape[0], self.process_idx
+                    )
+                )
+                if (
+                    self.encoded_feat_length_limit > 0
+                    and h.shape[0] > self.encoded_feat_length_limit
+                ):
+                    h = h.narrow(
+                        0,
+                        h.shape[0] - self.encoded_feat_length_limit,
+                        self.encoded_feat_length_limit,
+                    )
+
+                if self.running_hyps is None:
+                    self.running_hyps = self.init_hyp(h)
+                if self.time_sync:
+                    ret = self.process_one_block_time_sync(
+                        h, block_is_final, maxlen, maxlenratio
+                    )
+                else:
+                    ret = self.process_one_block(h, block_is_final, maxlen, maxlenratio)
+                logging.debug("Finished processing block: %d", self.processed_block)
+                self.processed_block += 1
+
+                # prune running_hyps, taking top as an incremental decoding
+                if self.incremental_decode:
+                    logging.debug(
+                        "Hyps before incremental pruning: %d",
+                        self.running_hyps.yseq.shape[0],
+                    )
+                    if self.running_hyps.yseq.shape[0] > 0:
+                        self.running_hyps = self._batch_select(self.running_hyps, [0])
+                    logging.debug(
+                        "Hyps after incremental pruning: %d",
+                        self.running_hyps.yseq.shape[0],
+                    )
+
+                if block_is_final:
+                    return ret
+            if ret is None:
+                if self.prev_output is None:
+                    return []
+                else:
+                    return self.prev_output
             else:
-                return self.prev_output
+                self.prev_output = ret
+                # N-best results
+                return ret
+
+    def process_one_block_time_sync(self, h, is_final, maxlen, maxlenratio):
+        """Recognize one block w/ time sync."""
+        hyps = self.time_sync_search(
+            h,
+            start_idx=self.t,
+            is_final=is_final,
+            incremental_decode=self.incremental_decode,
+        )
+        logging.debug("time:" + str(self.t))
+        logging.debug("best_hyp:" + "".join([self.token_list[x] for x in hyps[0].yseq]))
+        if is_final:
+            self.t = 0
         else:
-            self.prev_output = ret
-            # N-best results
-            return ret
+            self.t = len(h)
+        return hyps
 
     def process_one_block(self, h, is_final, maxlen, maxlenratio):
         """Recognize one block."""
         # extend states for ctc
         self.extend(h, self.running_hyps)
         while self.process_idx < maxlen:
             logging.debug("position " + str(self.process_idx))
@@ -226,14 +415,16 @@
             ):
                 logging.info(f"end detected at {self.process_idx}")
                 return self.assemble_hyps(self.ended_hyps)
 
             if len(local_ended_hyps) > 0 and not is_final:
                 logging.info("Detected hyp(s) reaching EOS in this block.")
                 break
+                # breaking here means that prev hyps
+                # is 2 behind the ended hyps, not 1
 
             self.prev_hyps = self.running_hyps
             self.running_hyps = self.post_process(
                 self.process_idx, maxlen, maxlenratio, best, self.ended_hyps
             )
 
             if is_final:
@@ -247,14 +438,19 @@
                 logging.debug(f"remained hypotheses: {len(self.running_hyps)}")
             # increment number
             self.process_idx += 1
 
         if is_final:
             return self.assemble_hyps(self.ended_hyps)
         else:
+            try:
+                local_ended_hyps
+            except Exception:
+                local_ended_hyps = []
+
             for hyp in self.ended_hyps:
                 local_ended_hyps.append(hyp)
             rets = self.assemble_hyps(local_ended_hyps)
 
             if self.process_idx > 1 and len(self.prev_hyps) > 0:
                 self.running_hyps = self.prev_hyps
                 self.process_idx -= 1
```

### Comparing `espnet-202304/espnet/nets/batch_beam_search_online_sim.py` & `espnet-202308/espnet/nets/batch_beam_search_online_sim.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/beam_search.py` & `espnet-202308/espnet/nets/beam_search.py`

 * *Files 4% similar despite different names*

```diff
@@ -13,14 +13,16 @@
 class Hypothesis(NamedTuple):
     """Hypothesis data type."""
 
     yseq: torch.Tensor
     score: Union[float, torch.Tensor] = 0
     scores: Dict[str, Union[float, torch.Tensor]] = dict()
     states: Dict[str, Any] = dict()
+    # dec hidden state corresponding to yseq, used for searchable hidden ints
+    hs: List[torch.Tensor] = []
 
     def asdict(self) -> dict:
         """Convert data to JSON-friendly dict."""
         return self._replace(
             yseq=self.yseq.tolist(),
             score=float(self.score),
             scores={k: float(v) for k, v in self.scores.items()},
@@ -37,14 +39,15 @@
         beam_size: int,
         vocab_size: int,
         sos: int,
         eos: int,
         token_list: List[str] = None,
         pre_beam_ratio: float = 1.5,
         pre_beam_score_key: str = None,
+        return_hs: bool = False,
         hyp_primer: List[int] = None,
     ):
         """Initialize beam search.
 
         Args:
             scorers (dict[str, ScorerInterface]): Dict of decoder modules
                 e.g., Decoder, CTCPrefixScorer, LM
@@ -104,14 +107,15 @@
             raise KeyError(f"{pre_beam_score_key} is not found in {self.full_scorers}")
         self.pre_beam_score_key = pre_beam_score_key
         self.do_pre_beam = (
             self.pre_beam_score_key is not None
             and self.pre_beam_size < self.n_vocab
             and len(self.part_scorers) > 0
         )
+        self.return_hs = return_hs
 
     def set_hyp_primer(self, hyp_primer: List[int] = None) -> None:
         """Set the primer sequence for decoding.
 
         Used for OpenAI Whisper models.
         """
         self.hyp_primer = hyp_primer
@@ -136,14 +140,15 @@
         primer = [self.sos] if self.hyp_primer is None else self.hyp_primer
 
         return [
             Hypothesis(
                 score=0.0,
                 scores=init_scores,
                 states=init_states,
+                hs=[],
                 yseq=torch.tensor(primer, device=x.device),
             )
         ]
 
     @staticmethod
     def append_token(xs: torch.Tensor, x: int) -> torch.Tensor:
         """Append new token to prefix tokens.
@@ -156,34 +161,47 @@
             torch.Tensor: New tensor contains: xs + [x] with xs.dtype and xs.device
 
         """
         x = torch.tensor([x], dtype=xs.dtype, device=xs.device)
         return torch.cat((xs, x))
 
     def score_full(
-        self, hyp: Hypothesis, x: torch.Tensor
+        self, hyp: Hypothesis, x: torch.Tensor, pre_x: torch.Tensor = None
     ) -> Tuple[Dict[str, torch.Tensor], Dict[str, Any]]:
         """Score new hypothesis by `self.full_scorers`.
 
         Args:
             hyp (Hypothesis): Hypothesis with prefix tokens to score
             x (torch.Tensor): Corresponding input feature
+            pre_x (torch.Tensor): Encoded speech feature for sequential attn (T, D)
+                Sequential attn computes attn first on pre_x then on x,
+                thereby attending to two sources in sequence.
 
         Returns:
             Tuple[Dict[str, torch.Tensor], Dict[str, Any]]: Tuple of
                 score dict of `hyp` that has string keys of `self.full_scorers`
                 and tensor score values of shape: `(self.n_vocab,)`,
                 and state dict that has string keys
                 and state values of `self.full_scorers`
 
         """
         scores = dict()
         states = dict()
         for k, d in self.full_scorers.items():
-            scores[k], states[k] = d.score(hyp.yseq, hyp.states[k], x)
+            if "decoder" in k and self.return_hs:
+                scores[k], hs, states[k] = d.score(
+                    hyp.yseq, hyp.states[k], x, return_hs=self.return_hs
+                )
+            elif pre_x is not None:
+                scores[k], states[k] = d.score(hyp.yseq, hyp.states[k], x, pre_x)
+            else:
+                scores[k], states[k] = d.score(hyp.yseq, hyp.states[k], x)
+
+        if self.return_hs:
+            return hs, scores, states
         return scores, states
 
     def score_partial(
         self, hyp: Hypothesis, ids: torch.Tensor, x: torch.Tensor
     ) -> Tuple[Dict[str, torch.Tensor], Dict[str, Any]]:
         """Score new hypothesis by `self.part_scorers`.
 
@@ -285,32 +303,41 @@
         for k, v in states.items():
             new_states[k] = v
         for k, d in self.part_scorers.items():
             new_states[k] = d.select_state(part_states[k], part_idx)
         return new_states
 
     def search(
-        self, running_hyps: List[Hypothesis], x: torch.Tensor
+        self,
+        running_hyps: List[Hypothesis],
+        x: torch.Tensor,
+        pre_x: torch.Tensor = None,
     ) -> List[Hypothesis]:
         """Search new tokens for running hypotheses and encoded speech x.
 
         Args:
             running_hyps (List[Hypothesis]): Running hypotheses on beam
             x (torch.Tensor): Encoded speech feature (T, D)
+            pre_x (torch.Tensor): Encoded speech feature for sequential attn (T, D)
+                Sequential attn computes attn first on pre_x then on x,
+                thereby attending to two sources in sequence.
 
         Returns:
             List[Hypotheses]: Best sorted hypotheses
 
         """
         best_hyps = []
         part_ids = torch.arange(self.n_vocab, device=x.device)  # no pre-beam
         for hyp in running_hyps:
             # scoring
             weighted_scores = torch.zeros(self.n_vocab, dtype=x.dtype, device=x.device)
-            scores, states = self.score_full(hyp, x)
+            if self.return_hs:
+                hs, scores, states = self.score_full(hyp, x, pre_x=pre_x)
+            else:
+                scores, states = self.score_full(hyp, x, pre_x=pre_x)
             for k in self.full_scorers:
                 weighted_scores += self.weights[k] * scores[k]
             # partial scoring
             if self.do_pre_beam:
                 pre_beam_scores = (
                     weighted_scores
                     if self.pre_beam_score_key == "full"
@@ -322,85 +349,97 @@
                 weighted_scores[part_ids] += self.weights[k] * part_scores[k]
             # add previous hyp score
             weighted_scores += hyp.score
 
             # update hyps
             for j, part_j in zip(*self.beam(weighted_scores, part_ids)):
                 # will be (2 x beam at most)
+                if self.return_hs:
+                    new_hs = hyp.hs + [hs.squeeze(0)]
+                else:
+                    new_hs = []
                 best_hyps.append(
                     Hypothesis(
                         score=weighted_scores[j],
                         yseq=self.append_token(hyp.yseq, j),
                         scores=self.merge_scores(
                             hyp.scores, scores, j, part_scores, part_j
                         ),
                         states=self.merge_states(states, part_states, part_j),
+                        hs=new_hs,
                     )
                 )
 
             # sort and prune 2 x beam -> beam
             best_hyps = sorted(best_hyps, key=lambda x: x.score, reverse=True)[
                 : min(len(best_hyps), self.beam_size)
             ]
         return best_hyps
 
     def forward(
-        self, x: torch.Tensor, maxlenratio: float = 0.0, minlenratio: float = 0.0
+        self,
+        x: torch.Tensor,
+        maxlenratio: float = 0.0,
+        minlenratio: float = 0.0,
+        pre_x: torch.Tensor = None,
     ) -> List[Hypothesis]:
         """Perform beam search.
 
         Args:
             x (torch.Tensor): Encoded speech feature (T, D)
             maxlenratio (float): Input length ratio to obtain max output length.
                 If maxlenratio=0.0 (default), it uses a end-detect function
                 to automatically find maximum hypothesis lengths
                 If maxlenratio<0.0, its absolute value is interpreted
                 as a constant max output length.
             minlenratio (float): Input length ratio to obtain min output length.
-                If minlenratio<0.0, its absolute value is interpreted
-                as a constant min output length.
+            pre_x (torch.Tensor): Encoded speech feature for sequential attn (T, D)
+                Sequential attn computes attn first on pre_x then on x,
+                thereby attending to two sources in sequence.
 
         Returns:
             list[Hypothesis]: N-best decoding results
 
         """
         # set length bounds
+        if pre_x is not None:
+            inp = pre_x
+        else:
+            inp = x
         if maxlenratio == 0:
-            maxlen = x.shape[0]
+            maxlen = inp.shape[0]
         elif maxlenratio < 0:
             maxlen = -1 * int(maxlenratio)
         else:
-            maxlen = max(1, int(maxlenratio * x.size(0)))
-        if minlenratio < 0:
-            minlen = -1 * int(minlenratio)
-        else:
-            minlen = int(minlenratio * x.size(0))
-        logging.info("decoder input length: " + str(x.shape[0]))
+            maxlen = max(1, int(maxlenratio * inp.size(0)))
+        minlen = int(minlenratio * inp.size(0))
+        logging.info("decoder input length: " + str(inp.shape[0]))
         logging.info("max output length: " + str(maxlen))
         logging.info("min output length: " + str(minlen))
 
         # main loop of prefix search
-        running_hyps = self.init_hyp(x)
+        running_hyps = self.init_hyp(x if pre_x is None else pre_x)
         ended_hyps = []
         for i in range(maxlen):
             logging.debug("position " + str(i))
-            best = self.search(running_hyps, x)
+            best = self.search(running_hyps, x, pre_x=pre_x)
             # post process of one iteration
             running_hyps = self.post_process(i, maxlen, maxlenratio, best, ended_hyps)
             # end detection
             if maxlenratio == 0.0 and end_detect([h.asdict() for h in ended_hyps], i):
                 logging.info(f"end detected at {i}")
                 break
             if len(running_hyps) == 0:
                 logging.info("no hypothesis. Finish decoding.")
                 break
             else:
                 logging.debug(f"remained hypotheses: {len(running_hyps)}")
 
         nbest_hyps = sorted(ended_hyps, key=lambda x: x.score, reverse=True)
+
         # check the number of hypotheses reaching to eos
         if len(nbest_hyps) == 0:
             logging.warning(
                 "there is no N-best results, perform recognition "
                 "again with smaller minlenratio."
             )
             return (
```

### Comparing `espnet-202304/espnet/nets/beam_search_timesync.py` & `espnet-202308/espnet/nets/beam_search_timesync.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/beam_search_transducer.py` & `espnet-202308/espnet/nets/beam_search_transducer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/chainer_backend/asr_interface.py` & `espnet-202308/espnet/nets/chainer_backend/asr_interface.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/chainer_backend/ctc.py` & `espnet-202308/espnet/nets/chainer_backend/ctc.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/chainer_backend/deterministic_embed_id.py` & `espnet-202308/espnet/nets/chainer_backend/deterministic_embed_id.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/chainer_backend/e2e_asr.py` & `espnet-202308/espnet/nets/chainer_backend/e2e_asr.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/chainer_backend/e2e_asr_transformer.py` & `espnet-202308/espnet/nets/chainer_backend/e2e_asr_transformer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/chainer_backend/rnn/attentions.py` & `espnet-202308/espnet/nets/chainer_backend/rnn/attentions.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/chainer_backend/rnn/decoders.py` & `espnet-202308/espnet/nets/chainer_backend/rnn/decoders.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/chainer_backend/rnn/encoders.py` & `espnet-202308/espnet/nets/chainer_backend/rnn/encoders.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/chainer_backend/rnn/training.py` & `espnet-202308/espnet/nets/chainer_backend/rnn/training.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/chainer_backend/transformer/attention.py` & `espnet-202308/espnet/nets/chainer_backend/transformer/attention.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/chainer_backend/transformer/ctc.py` & `espnet-202308/espnet/nets/chainer_backend/transformer/ctc.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/chainer_backend/transformer/decoder.py` & `espnet-202308/espnet/nets/chainer_backend/transformer/decoder.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/chainer_backend/transformer/decoder_layer.py` & `espnet-202308/espnet/nets/chainer_backend/transformer/decoder_layer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/chainer_backend/transformer/embedding.py` & `espnet-202308/espnet/nets/chainer_backend/transformer/embedding.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/chainer_backend/transformer/encoder.py` & `espnet-202308/espnet/nets/chainer_backend/transformer/encoder.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/chainer_backend/transformer/encoder_layer.py` & `espnet-202308/espnet/nets/chainer_backend/transformer/encoder_layer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/chainer_backend/transformer/label_smoothing_loss.py` & `espnet-202308/espnet/nets/chainer_backend/transformer/label_smoothing_loss.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/chainer_backend/transformer/positionwise_feed_forward.py` & `espnet-202308/espnet/nets/chainer_backend/transformer/positionwise_feed_forward.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/chainer_backend/transformer/subsampling.py` & `espnet-202308/espnet/nets/chainer_backend/transformer/subsampling.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/chainer_backend/transformer/training.py` & `espnet-202308/espnet/nets/chainer_backend/transformer/training.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/ctc_prefix_score.py` & `espnet-202308/espnet/nets/ctc_prefix_score.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/e2e_asr_common.py` & `espnet-202308/espnet/nets/e2e_asr_common.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/e2e_mt_common.py` & `espnet-202308/espnet/nets/e2e_mt_common.py`

 * *Files 20% similar despite different names*

```diff
@@ -2,14 +2,16 @@
 # encoding: utf-8
 
 # Copyright 2019 Kyoto University (Hirofumi Inaguma)
 #  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)
 
 """Common functions for ST and MT."""
 
+from itertools import groupby
+
 import nltk
 import numpy as np
 
 
 class ErrorCalculator(object):
     """Calculate BLEU for ST and MT models during training.
 
@@ -24,29 +26,32 @@
     def __init__(self, char_list, sym_space, sym_pad, report_bleu=False):
         """Construct an ErrorCalculator object."""
         super(ErrorCalculator, self).__init__()
         self.char_list = char_list
         self.space = sym_space
         self.pad = sym_pad
         self.report_bleu = report_bleu
+        self.idx_blank = self.char_list.index(self.pad)
         if self.space in self.char_list:
             self.idx_space = self.char_list.index(self.space)
         else:
             self.idx_space = None
 
-    def __call__(self, ys_hat, ys_pad):
+    def __call__(self, ys_hat, ys_pad, is_ctc=False):
         """Calculate corpus-level BLEU score.
 
         :param torch.Tensor ys_hat: prediction (batch, seqlen)
         :param torch.Tensor ys_pad: reference (batch, seqlen)
         :return: corpus-level BLEU score in a mini-batch
         :rtype float
         """
         bleu = None
-        if not self.report_bleu:
+        if is_ctc:
+            return self.calculate_bleu_ctc(ys_hat, ys_pad)
+        elif not self.report_bleu:
             return bleu
 
         bleu = self.calculate_corpus_bleu(ys_hat, ys_pad)
         return bleu
 
     def calculate_corpus_bleu(self, ys_hat, ys_pad):
         """Calculate corpus-level BLEU score in a mini-batch.
@@ -68,7 +73,39 @@
             seq_hat_text = "".join(seq_hat).replace(self.space, " ")
             seq_hat_text = seq_hat_text.replace(self.pad, "")
             seq_true_text = "".join(seq_true).replace(self.space, " ")
             seqs_hat.append(seq_hat_text)
             seqs_true.append(seq_true_text)
         bleu = nltk.bleu_score.corpus_bleu([[ref] for ref in seqs_true], seqs_hat)
         return bleu * 100
+
+    def calculate_bleu_ctc(self, ys_hat, ys_pad):
+        """Calculate sentence-level BLEU score for CTC.
+
+        :param torch.Tensor ys_hat: prediction (batch, seqlen)
+        :param torch.Tensor ys_pad: reference (batch, seqlen)
+        :return: corpus-level BLEU score
+        :rtype float
+        """
+        seqs_hat, seqs_true = [], []
+        for i, y in enumerate(ys_hat):
+            y_hat = [x[0] for x in groupby(y)]
+            y_true = ys_pad[i]
+            seq_hat, seq_true = [], []
+            for idx in y_hat:
+                idx = int(idx)
+                if idx != -1 and idx != self.idx_blank and idx != self.idx_space:
+                    seq_hat.append(self.char_list[int(idx)])
+
+            for idx in y_true:
+                idx = int(idx)
+                if idx != -1 and idx != self.idx_blank and idx != self.idx_space:
+                    seq_true.append(self.char_list[int(idx)])
+
+            seq_hat_text = "".join(seq_hat).replace(self.space, " ")
+            seq_hat_text = seq_hat_text.replace(self.pad, "")
+            seq_true_text = "".join(seq_true).replace(self.space, " ")
+            seqs_hat.append(seq_hat_text)
+            seqs_true.append(seq_true_text)
+
+        bleu = nltk.bleu_score.corpus_bleu([[ref] for ref in seqs_true], seqs_hat)
+        return bleu
```

### Comparing `espnet-202304/espnet/nets/lm_interface.py` & `espnet-202308/espnet/nets/lm_interface.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/mt_interface.py` & `espnet-202308/espnet/nets/mt_interface.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/conformer/argument.py` & `espnet-202308/espnet/nets/pytorch_backend/conformer/argument.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/conformer/contextual_block_encoder_layer.py` & `espnet-202308/espnet/nets/pytorch_backend/conformer/contextual_block_encoder_layer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/conformer/convolution.py` & `espnet-202308/espnet/nets/pytorch_backend/conformer/convolution.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/conformer/encoder.py` & `espnet-202308/espnet/nets/pytorch_backend/conformer/encoder.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/conformer/encoder_layer.py` & `espnet-202308/espnet/nets/pytorch_backend/conformer/encoder_layer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/ctc.py` & `espnet-202308/espnet/nets/pytorch_backend/ctc.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/e2e_asr.py` & `espnet-202308/espnet/nets/pytorch_backend/e2e_asr.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/e2e_asr_conformer.py` & `espnet-202308/espnet/nets/pytorch_backend/e2e_asr_conformer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/e2e_asr_maskctc.py` & `espnet-202308/espnet/nets/pytorch_backend/e2e_asr_maskctc.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/e2e_asr_mix.py` & `espnet-202308/espnet/nets/pytorch_backend/e2e_asr_mix.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/e2e_asr_mix_transformer.py` & `espnet-202308/espnet/nets/pytorch_backend/e2e_asr_mix_transformer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/e2e_asr_mulenc.py` & `espnet-202308/espnet/nets/pytorch_backend/e2e_asr_mulenc.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/e2e_asr_transducer.py` & `espnet-202308/espnet/nets/pytorch_backend/e2e_asr_transducer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/e2e_asr_transformer.py` & `espnet-202308/espnet/nets/pytorch_backend/e2e_asr_transformer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/e2e_mt.py` & `espnet-202308/espnet/nets/pytorch_backend/e2e_mt.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/e2e_mt_transformer.py` & `espnet-202308/espnet/nets/pytorch_backend/e2e_mt_transformer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/e2e_st.py` & `espnet-202308/espnet/nets/pytorch_backend/e2e_st.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/e2e_st_conformer.py` & `espnet-202308/espnet/nets/pytorch_backend/e2e_st_conformer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/e2e_st_transformer.py` & `espnet-202308/espnet/nets/pytorch_backend/e2e_st_transformer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/e2e_tts_fastspeech.py` & `espnet-202308/espnet/nets/pytorch_backend/e2e_tts_fastspeech.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/e2e_tts_tacotron2.py` & `espnet-202308/espnet/nets/pytorch_backend/e2e_tts_tacotron2.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/e2e_tts_transformer.py` & `espnet-202308/espnet/nets/pytorch_backend/e2e_tts_transformer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/e2e_vc_tacotron2.py` & `espnet-202308/espnet/nets/pytorch_backend/e2e_vc_tacotron2.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/e2e_vc_transformer.py` & `espnet-202308/espnet/nets/pytorch_backend/e2e_vc_transformer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/fastspeech/duration_calculator.py` & `espnet-202308/espnet/nets/pytorch_backend/fastspeech/duration_calculator.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/fastspeech/duration_predictor.py` & `espnet-202308/espnet/nets/pytorch_backend/fastspeech/duration_predictor.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/fastspeech/length_regulator.py` & `espnet-202308/espnet/nets/pytorch_backend/fastspeech/length_regulator.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/frontends/beamformer.py` & `espnet-202308/espnet/nets/pytorch_backend/frontends/beamformer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/frontends/dnn_beamformer.py` & `espnet-202308/espnet/nets/pytorch_backend/frontends/dnn_beamformer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/frontends/dnn_wpe.py` & `espnet-202308/espnet/nets/pytorch_backend/frontends/dnn_wpe.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/frontends/feature_transform.py` & `espnet-202308/espnet/nets/pytorch_backend/frontends/feature_transform.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/frontends/frontend.py` & `espnet-202308/espnet/nets/pytorch_backend/frontends/frontend.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/frontends/mask_estimator.py` & `espnet-202308/espnet/nets/pytorch_backend/frontends/mask_estimator.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/gtn_ctc.py` & `espnet-202308/espnet/nets/pytorch_backend/gtn_ctc.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/initialization.py` & `espnet-202308/espnet/nets/pytorch_backend/initialization.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/lm/default.py` & `espnet-202308/espnet/nets/pytorch_backend/lm/default.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/lm/seq_rnn.py` & `espnet-202308/espnet/nets/pytorch_backend/lm/seq_rnn.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/lm/transformer.py` & `espnet-202308/espnet/nets/pytorch_backend/lm/transformer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/maskctc/add_mask_token.py` & `espnet-202308/espnet/nets/pytorch_backend/maskctc/add_mask_token.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/maskctc/mask.py` & `espnet-202308/espnet/nets/pytorch_backend/maskctc/mask.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/nets_utils.py` & `espnet-202308/espnet/nets/pytorch_backend/nets_utils.py`

 * *Files 15% similar despite different names*

```diff
@@ -146,45 +146,125 @@
                  [0, 0, 1, 1, 1, 1],
                  [0, 0, 1, 1, 1, 1]]], dtype=torch.uint8)
 
     """
     if length_dim == 0:
         raise ValueError("length_dim cannot be 0: {}".format(length_dim))
 
+    # If the input dimension is 2 or 3,
+    # then we use ESPnet-ONNX based implementation for tracable modeling.
+    # otherwise we use the traditional implementation for research use.
+    if isinstance(lengths, list):
+        logging.warn(
+            "Using make_pad_mask with a list of lengths is not tracable. "
+            + "If you try to trace this function with type(lengths) == list, "
+            + "please change the type of lengths to torch.LongTensor."
+        )
+
+    if (
+        (xs is None or xs.dim() in (2, 3))
+        and length_dim <= 2
+        and (not isinstance(lengths, list) and lengths.dim() == 1)
+    ):
+        return _make_pad_mask_traceable(lengths, xs, length_dim, maxlen)
+    else:
+        return _make_pad_mask(lengths, xs, length_dim, maxlen)
+
+
+def _make_pad_mask(lengths, xs=None, length_dim=-1, maxlen=None):
     if not isinstance(lengths, list):
         lengths = lengths.long().tolist()
 
     bs = int(len(lengths))
     if maxlen is None:
         if xs is None:
             maxlen = int(max(lengths))
         else:
             maxlen = xs.size(length_dim)
     else:
-        assert xs is None
-        assert maxlen >= int(max(lengths))
+        assert xs is None, "When maxlen is specified, xs must not be specified."
+        assert maxlen >= int(
+            max(lengths)
+        ), f"maxlen {maxlen} must be >= max(lengths) {max(lengths)}"
 
     seq_range = torch.arange(0, maxlen, dtype=torch.int64)
     seq_range_expand = seq_range.unsqueeze(0).expand(bs, maxlen)
     seq_length_expand = seq_range_expand.new(lengths).unsqueeze(-1)
     mask = seq_range_expand >= seq_length_expand
 
     if xs is not None:
-        assert xs.size(0) == bs, (xs.size(0), bs)
+        assert (
+            xs.size(0) == bs
+        ), f"The size of x.size(0) {xs.size(0)} must match the batch size {bs}"
 
         if length_dim < 0:
             length_dim = xs.dim() + length_dim
         # ind = (:, None, ..., None, :, , None, ..., None)
         ind = tuple(
             slice(None) if i in (0, length_dim) else None for i in range(xs.dim())
         )
         mask = mask[ind].expand_as(xs).to(xs.device)
     return mask
 
 
+def _make_pad_mask_traceable(lengths, xs, length_dim, maxlen=None):
+    """
+    Make mask tensor containing indices of padded part.
+    This is a simplified implementation of make_pad_mask without the xs input
+    that supports JIT tracing for applications like exporting models to ONNX.
+    Dimension length of xs should be 2 or 3
+    This function will create torch.ones(maxlen, maxlen).triu(diagonal=1) and
+    select rows to create mask tensor.
+    """
+
+    if xs is None:
+        device = lengths.device
+    else:
+        device = xs.device
+
+    if xs is not None and len(xs.shape) == 3:
+        if length_dim == 1:
+            lengths = lengths.unsqueeze(1).expand(*xs.transpose(1, 2).shape[:2])
+        else:
+            # Then length_dim is 2 or -1.
+            if length_dim not in (-1, 2):
+                logging.warn(
+                    f"Invalid length_dim {length_dim}."
+                    + "We set it to -1, which is the default value."
+                )
+                length_dim = -1
+            lengths = lengths.unsqueeze(1).expand(*xs.shape[:2])
+
+    if maxlen is not None:
+        assert xs is None
+        assert maxlen >= lengths.max()
+    elif xs is not None:
+        maxlen = xs.shape[length_dim]
+    else:
+        maxlen = lengths.max()
+
+    # clip max(length) to maxlen
+    lengths = torch.clamp(lengths, max=maxlen).type(torch.long)
+
+    mask = torch.ones(maxlen + 1, maxlen + 1, dtype=torch.bool, device=device)
+    mask = triu_onnx(mask)[1:, :-1]  # onnx cannot handle diagonal argument.
+    mask = mask[lengths - 1][..., :maxlen]
+
+    if xs is not None and len(xs.shape) == 3 and length_dim == 1:
+        return mask.transpose(1, 2)
+    else:
+        return mask
+
+
+def triu_onnx(x):
+    arange = torch.arange(x.size(0), device=x.device)
+    mask = arange.unsqueeze(-1).expand(-1, x.size(0)) <= arange
+    return x * mask
+
+
 def make_non_pad_mask(lengths, xs=None, length_dim=-1):
     """Make mask tensor containing indices of non-padded part.
 
     Args:
         lengths (LongTensor or List): Batch of lengths (B,).
         xs (Tensor, optional): The reference tensor.
             If set, masks will be the same shape as this tensor.
```

### Comparing `espnet-202304/espnet/nets/pytorch_backend/rnn/argument.py` & `espnet-202308/espnet/nets/pytorch_backend/rnn/argument.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/rnn/attentions.py` & `espnet-202308/espnet/nets/pytorch_backend/rnn/attentions.py`

 * *Files 2% similar despite different names*

```diff
@@ -1785,7 +1785,182 @@
             att_ws_head = torch.stack([aw[h] for aw in att_ws], dim=1)
             att_ws_sorted_by_head += [att_ws_head]
         att_ws = torch.stack(att_ws_sorted_by_head, dim=1).cpu().numpy()
     else:
         # att_ws => list of attentions
         att_ws = torch.stack(att_ws, dim=1).cpu().numpy()
     return att_ws
+
+
+def _apply_dynamic_filter(p, last_attended_idx, backward_window=1, forward_window=3):
+    """Apply dynamic filter.
+
+    This function apply the dynamic filter
+    introduced in `Singing-Tacotron: Global Duration Control Attention and Dynamic
+    Filter for End-to-end Singing Voice Synthesis`_.
+
+    Args:
+        p (Tensor): probability before applying softmax (1, T).
+        last_attended_idx (int): The index of the inputs of the last attended [0, T].
+        backward_window (int, optional): Backward window size in dynamic filter.
+        forward_window (int, optional): Forward window size in dynamic filter.
+
+    Returns:
+        Tensor: Dynamic filtered probability (1, T).
+
+    .. _`Singing-Tacotron: Global Duration Control Attention and Dynamic
+    Filter for End-to-end Singing Voice Synthesis`:
+        https://arxiv.org/pdf/2202.07907v1.pdf
+
+    """
+
+    if p.size(0) != 1:
+        raise NotImplementedError("Batch dynamic filter is not yet supported.")
+    backward_idx = last_attended_idx - backward_window
+    forward_idx = last_attended_idx + forward_window
+    if backward_idx > 0:
+        p[:, :backward_idx] = 0
+    if forward_idx < p.size(1):
+        p[:, forward_idx:] = 0
+    return p
+
+
+class GDCAttLoc(torch.nn.Module):
+    """Global duration control attention module.
+    Reference: Singing-Tacotron: Global Duration Control Attention and Dynamic
+    Filter for End-to-end Singing Voice Synthesis
+    (https://arxiv.org/abs/2202.07907)
+    :param int eprojs: # projection-units of encoder
+    :param int dunits: # units of decoder
+    :param int att_dim: attention dimension
+    :param int aconv_chans: # channels of attention convolution
+    :param int aconv_filts: filter size of attention convolution
+    :param bool han_mode: flag to swith on mode of hierarchical attention
+        and not store pre_compute_enc_h
+    """
+
+    def __init__(
+        self, eprojs, dunits, att_dim, aconv_chans, aconv_filts, han_mode=False
+    ):
+        super(GDCAttLoc, self).__init__()
+        self.pt_zero_linear = torch.nn.Linear(att_dim, 1)
+        self.mlp_enc = torch.nn.Linear(eprojs, att_dim)
+        self.mlp_dec = torch.nn.Linear(dunits, att_dim, bias=False)
+        self.mlp_att = torch.nn.Linear(aconv_chans, att_dim, bias=False)
+        self.loc_conv = torch.nn.Conv2d(
+            1,
+            aconv_chans,
+            (1, 2 * aconv_filts + 1),
+            padding=(0, aconv_filts),
+            bias=False,
+        )
+        self.gvec = torch.nn.Linear(att_dim, 1)
+
+        self.dunits = dunits
+        self.eprojs = eprojs
+        self.att_dim = att_dim
+        self.h_length = None
+        self.enc_h = None
+        self.pre_compute_enc_h = None
+        self.mask = None
+        self.han_mode = han_mode
+
+    def reset(self):
+        """reset states"""
+        self.h_length = None
+        self.enc_h = None
+        self.pre_compute_enc_h = None
+        self.mask = None
+
+    def forward(
+        self,
+        enc_hs_pad,
+        enc_hs_len,
+        trans_token,
+        dec_z,
+        att_prev,
+        scaling=1.0,
+        last_attended_idx=None,
+        backward_window=1,
+        forward_window=3,
+    ):
+        """Calcualte AttLoc forward propagation.
+        :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc)
+        :param list enc_hs_len: padded encoder hidden state length (B)
+        :param torch.Tensor trans_token: Global transition token
+            for duration (B x T_max x 1)
+        :param torch.Tensor dec_z: decoder hidden state (B x D_dec)
+        :param torch.Tensor att_prev: previous attention weight (B x T_max)
+        :param float scaling: scaling parameter before applying softmax
+        :param torch.Tensor forward_window: forward window size
+            when constraining attention
+        :param int last_attended_idx: index of the inputs of the last attended
+        :param int backward_window: backward window size in attention constraint
+        :param int forward_window: forward window size in attetion constraint
+        :return: attention weighted encoder state (B, D_enc)
+        :rtype: torch.Tensor
+        :return: previous attention weights (B x T_max)
+        :rtype: torch.Tensor
+        """
+        batch = len(enc_hs_pad)
+        # pre-compute all h outside the decoder loop
+        if self.pre_compute_enc_h is None or self.han_mode:
+            self.enc_h = enc_hs_pad  # utt x frame x hdim
+            self.h_length = self.enc_h.size(1)
+            # utt x frame x att_dim
+            self.pre_compute_enc_h = self.mlp_enc(self.enc_h)
+
+        if dec_z is None:
+            dec_z = enc_hs_pad.new_zeros(batch, self.dunits)
+        else:
+            dec_z = dec_z.view(batch, self.dunits)
+
+        # initialize attention weight with uniform dist.
+        if att_prev is None:
+            att_prev = enc_hs_pad.new_zeros(*enc_hs_pad.size()[:2])
+            att_prev[:, 0] = 1.0
+
+        # att_prev: utt x frame -> utt x 1 x 1 x frame
+        # -> utt x att_conv_chans x 1 x frame
+        att_conv = self.loc_conv(att_prev.view(batch, 1, 1, self.h_length))
+        # att_conv: utt x att_conv_chans x 1 x frame -> utt x frame x att_conv_chans
+        att_conv = att_conv.squeeze(2).transpose(1, 2)
+        # att_conv: utt x frame x att_conv_chans -> utt x frame x att_dim
+        att_conv = self.mlp_att(att_conv)
+
+        # dec_z_tiled: utt x frame x att_dim
+        dec_z_tiled = self.mlp_dec(dec_z).view(batch, 1, self.att_dim)
+
+        # dot with gvec
+        # utt x frame x att_dim -> utt x frame
+        e = self.gvec(
+            torch.tanh(att_conv + self.pre_compute_enc_h + dec_z_tiled)
+        ).squeeze(2)
+
+        # NOTE: consider zero padding when compute w.
+        if self.mask is None:
+            self.mask = to_device(enc_hs_pad, make_pad_mask(enc_hs_len))
+        e.masked_fill_(self.mask, -float("inf"))
+
+        w = F.softmax(scaling * e, dim=1)
+
+        # dynamic filter
+        if last_attended_idx is not None:
+            att_prev = _apply_dynamic_filter(
+                att_prev, last_attended_idx, backward_window, forward_window
+            )
+
+        # GDCA attention
+        att_prev_shift = F.pad(att_prev, (1, 0))[:, :-1]
+        trans_token = trans_token.squeeze(-1)
+        trans_token_shift = F.pad(trans_token, (1, 0))[:, :-1]
+        w = ((1 - trans_token_shift) * att_prev_shift + trans_token * att_prev) * w
+
+        # NOTE: clamp is needed to avoid nan gradient
+        w = F.normalize(torch.clamp(w, 1e-6), p=1, dim=1)
+
+        # weighted sum over flames
+        # utt x hdim
+        # NOTE use bmm instead of sum(*)
+        c = torch.sum(self.enc_h * w.view(batch, self.h_length, 1), dim=1)
+
+        return c, w
```

### Comparing `espnet-202304/espnet/nets/pytorch_backend/rnn/decoders.py` & `espnet-202308/espnet/nets/pytorch_backend/rnn/decoders.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/rnn/encoders.py` & `espnet-202308/espnet/nets/pytorch_backend/rnn/encoders.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/streaming/segment.py` & `espnet-202308/espnet/nets/pytorch_backend/streaming/segment.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/streaming/window.py` & `espnet-202308/espnet/nets/pytorch_backend/streaming/window.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/tacotron2/cbhg.py` & `espnet-202308/espnet/nets/pytorch_backend/tacotron2/cbhg.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/tacotron2/decoder.py` & `espnet-202308/espnet/nets/pytorch_backend/tacotron2/decoder.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/tacotron2/encoder.py` & `espnet-202308/espnet/nets/pytorch_backend/tacotron2/encoder.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/transducer/arguments.py` & `espnet-202308/espnet/nets/pytorch_backend/transducer/arguments.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/transducer/blocks.py` & `espnet-202308/espnet/nets/pytorch_backend/transducer/blocks.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/transducer/conv1d_nets.py` & `espnet-202308/espnet/nets/pytorch_backend/transducer/conv1d_nets.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/transducer/custom_decoder.py` & `espnet-202308/espnet/nets/pytorch_backend/transducer/custom_decoder.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/transducer/custom_encoder.py` & `espnet-202308/espnet/nets/pytorch_backend/transducer/custom_encoder.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/transducer/error_calculator.py` & `espnet-202308/espnet/nets/pytorch_backend/transducer/error_calculator.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/transducer/initializer.py` & `espnet-202308/espnet/nets/pytorch_backend/transducer/initializer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/transducer/joint_network.py` & `espnet-202308/espnet/nets/pytorch_backend/transducer/joint_network.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/transducer/rnn_decoder.py` & `espnet-202308/espnet/nets/pytorch_backend/transducer/rnn_decoder.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/transducer/rnn_encoder.py` & `espnet-202308/espnet/nets/pytorch_backend/transducer/rnn_encoder.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/transducer/transducer_tasks.py` & `espnet-202308/espnet/nets/pytorch_backend/transducer/transducer_tasks.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/transducer/transformer_decoder_layer.py` & `espnet-202308/espnet/nets/pytorch_backend/transducer/transformer_decoder_layer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/transducer/utils.py` & `espnet-202308/espnet/nets/pytorch_backend/transducer/utils.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/transducer/vgg2l.py` & `espnet-202308/espnet/nets/pytorch_backend/transducer/vgg2l.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/transformer/add_sos_eos.py` & `espnet-202308/espnet/nets/pytorch_backend/transformer/add_sos_eos.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/transformer/argument.py` & `espnet-202308/espnet/nets/pytorch_backend/transformer/argument.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/transformer/attention.py` & `espnet-202308/espnet/nets/pytorch_backend/transformer/attention.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/transformer/contextual_block_encoder_layer.py` & `espnet-202308/espnet/nets/pytorch_backend/transformer/contextual_block_encoder_layer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/transformer/decoder.py` & `espnet-202308/espnet/nets/pytorch_backend/transformer/decoder.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/transformer/decoder_layer.py` & `espnet-202308/espnet/nets/pytorch_backend/transformer/encoder_layer.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,134 +1,119 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 
 # Copyright 2019 Shigeki Karita
 #  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)
 
-"""Decoder self-attention layer definition."""
+"""Encoder self-attention layer definition."""
 
 import torch
 from torch import nn
 
 from espnet.nets.pytorch_backend.transformer.layer_norm import LayerNorm
 
 
-class DecoderLayer(nn.Module):
-    """Single decoder layer module.
+class EncoderLayer(nn.Module):
+    """Encoder layer module.
 
     Args:
         size (int): Input dimension.
         self_attn (torch.nn.Module): Self-attention module instance.
-            `MultiHeadedAttention` instance can be used as the argument.
-        src_attn (torch.nn.Module): Self-attention module instance.
-            `MultiHeadedAttention` instance can be used as the argument.
+            `MultiHeadedAttention` or `RelPositionMultiHeadedAttention` instance
+            can be used as the argument.
         feed_forward (torch.nn.Module): Feed-forward module instance.
             `PositionwiseFeedForward`, `MultiLayeredConv1d`, or `Conv1dLinear` instance
             can be used as the argument.
         dropout_rate (float): Dropout rate.
         normalize_before (bool): Whether to use layer_norm before the first block.
         concat_after (bool): Whether to concat attention layer's input and output.
             if True, additional linear will be applied.
             i.e. x -> x + linear(concat(x, att(x)))
             if False, no additional linear will be applied. i.e. x -> x + att(x)
-
-
+        stochastic_depth_rate (float): Proability to skip this layer.
+            During training, the layer may skip residual computation and return input
+            as-is with given probability.
     """
 
     def __init__(
         self,
         size,
         self_attn,
-        src_attn,
         feed_forward,
         dropout_rate,
         normalize_before=True,
         concat_after=False,
+        stochastic_depth_rate=0.0,
     ):
-        """Construct an DecoderLayer object."""
-        super(DecoderLayer, self).__init__()
-        self.size = size
+        """Construct an EncoderLayer object."""
+        super(EncoderLayer, self).__init__()
         self.self_attn = self_attn
-        self.src_attn = src_attn
         self.feed_forward = feed_forward
         self.norm1 = LayerNorm(size)
         self.norm2 = LayerNorm(size)
-        self.norm3 = LayerNorm(size)
         self.dropout = nn.Dropout(dropout_rate)
+        self.size = size
         self.normalize_before = normalize_before
         self.concat_after = concat_after
         if self.concat_after:
-            self.concat_linear1 = nn.Linear(size + size, size)
-            self.concat_linear2 = nn.Linear(size + size, size)
+            self.concat_linear = nn.Linear(size + size, size)
+        self.stochastic_depth_rate = stochastic_depth_rate
 
-    def forward(self, tgt, tgt_mask, memory, memory_mask, cache=None):
-        """Compute decoded features.
+    def forward(self, x, mask, cache=None):
+        """Compute encoded features.
 
         Args:
-            tgt (torch.Tensor): Input tensor (#batch, maxlen_out, size).
-            tgt_mask (torch.Tensor): Mask for input tensor (#batch, maxlen_out).
-            memory (torch.Tensor): Encoded memory, float32 (#batch, maxlen_in, size).
-            memory_mask (torch.Tensor): Encoded memory mask (#batch, maxlen_in).
-            cache (List[torch.Tensor]): List of cached tensors.
-                Each tensor shape should be (#batch, maxlen_out - 1, size).
+            x_input (torch.Tensor): Input tensor (#batch, time, size).
+            mask (torch.Tensor): Mask tensor for the input (#batch, 1, time).
+            cache (torch.Tensor): Cache tensor of the input (#batch, time - 1, size).
 
         Returns:
-            torch.Tensor: Output tensor(#batch, maxlen_out, size).
-            torch.Tensor: Mask for output tensor (#batch, maxlen_out).
-            torch.Tensor: Encoded memory (#batch, maxlen_in, size).
-            torch.Tensor: Encoded memory mask (#batch, maxlen_in).
+            torch.Tensor: Output tensor (#batch, time, size).
+            torch.Tensor: Mask tensor (#batch, 1, time).
 
         """
-        residual = tgt
+        skip_layer = False
+        # with stochastic depth, residual connection `x + f(x)` becomes
+        # `x <- x + 1 / (1 - p) * f(x)` at training time.
+        stoch_layer_coeff = 1.0
+        if self.training and self.stochastic_depth_rate > 0:
+            skip_layer = torch.rand(1).item() < self.stochastic_depth_rate
+            stoch_layer_coeff = 1.0 / (1 - self.stochastic_depth_rate)
+
+        if skip_layer:
+            if cache is not None:
+                x = torch.cat([cache, x], dim=1)
+            return x, mask
+
+        residual = x
         if self.normalize_before:
-            tgt = self.norm1(tgt)
+            x = self.norm1(x)
 
         if cache is None:
-            tgt_q = tgt
-            tgt_q_mask = tgt_mask
+            x_q = x
         else:
-            # compute only the last frame query keeping dim: max_time_out -> 1
-            assert cache.shape == (
-                tgt.shape[0],
-                tgt.shape[1] - 1,
-                self.size,
-            ), f"{cache.shape} == {(tgt.shape[0], tgt.shape[1] - 1, self.size)}"
-            tgt_q = tgt[:, -1:, :]
+            assert cache.shape == (x.shape[0], x.shape[1] - 1, self.size)
+            x_q = x[:, -1:, :]
             residual = residual[:, -1:, :]
-            tgt_q_mask = None
-            if tgt_mask is not None:
-                tgt_q_mask = tgt_mask[:, -1:, :]
+            mask = None if mask is None else mask[:, -1:, :]
 
         if self.concat_after:
-            tgt_concat = torch.cat(
-                (tgt_q, self.self_attn(tgt_q, tgt, tgt, tgt_q_mask)), dim=-1
-            )
-            x = residual + self.concat_linear1(tgt_concat)
+            x_concat = torch.cat((x, self.self_attn(x_q, x, x, mask)), dim=-1)
+            x = residual + stoch_layer_coeff * self.concat_linear(x_concat)
         else:
-            x = residual + self.dropout(self.self_attn(tgt_q, tgt, tgt, tgt_q_mask))
+            x = residual + stoch_layer_coeff * self.dropout(
+                self.self_attn(x_q, x, x, mask)
+            )
         if not self.normalize_before:
             x = self.norm1(x)
 
         residual = x
         if self.normalize_before:
             x = self.norm2(x)
-        if self.concat_after:
-            x_concat = torch.cat(
-                (x, self.src_attn(x, memory, memory, memory_mask)), dim=-1
-            )
-            x = residual + self.concat_linear2(x_concat)
-        else:
-            x = residual + self.dropout(self.src_attn(x, memory, memory, memory_mask))
+        x = residual + stoch_layer_coeff * self.dropout(self.feed_forward(x))
         if not self.normalize_before:
             x = self.norm2(x)
 
-        residual = x
-        if self.normalize_before:
-            x = self.norm3(x)
-        x = residual + self.dropout(self.feed_forward(x))
-        if not self.normalize_before:
-            x = self.norm3(x)
-
         if cache is not None:
             x = torch.cat([cache, x], dim=1)
 
-        return x, tgt_mask, memory, memory_mask
+        return x, mask
```

### Comparing `espnet-202304/espnet/nets/pytorch_backend/transformer/dynamic_conv.py` & `espnet-202308/espnet/nets/pytorch_backend/transformer/dynamic_conv.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/transformer/dynamic_conv2d.py` & `espnet-202308/espnet/nets/pytorch_backend/transformer/dynamic_conv2d.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/transformer/embedding.py` & `espnet-202308/espnet/nets/pytorch_backend/transformer/embedding.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/transformer/encoder.py` & `espnet-202308/espnet/nets/pytorch_backend/transformer/encoder.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/transformer/encoder_layer.py` & `espnet-202308/espnet2/enh/separator/rnn_separator.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,119 +1,157 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-
-# Copyright 2019 Shigeki Karita
-#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)
-
-"""Encoder self-attention layer definition."""
+from collections import OrderedDict
+from typing import Dict, List, Optional, Tuple, Union
 
 import torch
-from torch import nn
+from packaging.version import parse as V
+from torch_complex.tensor import ComplexTensor
 
-from espnet.nets.pytorch_backend.transformer.layer_norm import LayerNorm
+from espnet2.enh.layers.complex_utils import is_complex
+from espnet2.enh.separator.abs_separator import AbsSeparator
+from espnet.nets.pytorch_backend.rnn.encoders import RNN
 
+is_torch_1_9_plus = V(torch.__version__) >= V("1.9.0")
 
-class EncoderLayer(nn.Module):
-    """Encoder layer module.
-
-    Args:
-        size (int): Input dimension.
-        self_attn (torch.nn.Module): Self-attention module instance.
-            `MultiHeadedAttention` or `RelPositionMultiHeadedAttention` instance
-            can be used as the argument.
-        feed_forward (torch.nn.Module): Feed-forward module instance.
-            `PositionwiseFeedForward`, `MultiLayeredConv1d`, or `Conv1dLinear` instance
-            can be used as the argument.
-        dropout_rate (float): Dropout rate.
-        normalize_before (bool): Whether to use layer_norm before the first block.
-        concat_after (bool): Whether to concat attention layer's input and output.
-            if True, additional linear will be applied.
-            i.e. x -> x + linear(concat(x, att(x)))
-            if False, no additional linear will be applied. i.e. x -> x + att(x)
-        stochastic_depth_rate (float): Proability to skip this layer.
-            During training, the layer may skip residual computation and return input
-            as-is with given probability.
-    """
 
+class RNNSeparator(AbsSeparator):
     def __init__(
         self,
-        size,
-        self_attn,
-        feed_forward,
-        dropout_rate,
-        normalize_before=True,
-        concat_after=False,
-        stochastic_depth_rate=0.0,
+        input_dim: int,
+        rnn_type: str = "blstm",
+        num_spk: int = 2,
+        predict_noise: bool = False,
+        nonlinear: str = "sigmoid",
+        layer: int = 3,
+        unit: int = 512,
+        dropout: float = 0.0,
     ):
-        """Construct an EncoderLayer object."""
-        super(EncoderLayer, self).__init__()
-        self.self_attn = self_attn
-        self.feed_forward = feed_forward
-        self.norm1 = LayerNorm(size)
-        self.norm2 = LayerNorm(size)
-        self.dropout = nn.Dropout(dropout_rate)
-        self.size = size
-        self.normalize_before = normalize_before
-        self.concat_after = concat_after
-        if self.concat_after:
-            self.concat_linear = nn.Linear(size + size, size)
-        self.stochastic_depth_rate = stochastic_depth_rate
+        """RNN Separator
+
+        Args:
+            input_dim: input feature dimension
+            rnn_type: string, select from 'blstm', 'lstm' etc.
+            bidirectional: bool, whether the inter-chunk RNN layers are bidirectional.
+            num_spk: number of speakers
+            predict_noise: whether to output the estimated noise signal
+            nonlinear: the nonlinear function for mask estimation,
+                       select from 'relu', 'tanh', 'sigmoid'
+            layer: int, number of stacked RNN layers. Default is 3.
+            unit: int, dimension of the hidden state.
+            dropout: float, dropout ratio. Default is 0.
+        """
+        super().__init__()
+
+        self._num_spk = num_spk
+        self.predict_noise = predict_noise
 
-    def forward(self, x, mask, cache=None):
-        """Compute encoded features.
+        self.rnn = RNN(
+            idim=input_dim,
+            elayers=layer,
+            cdim=unit,
+            hdim=unit,
+            dropout=dropout,
+            typ=rnn_type,
+        )
+
+        num_outputs = self.num_spk + 1 if self.predict_noise else self.num_spk
+        self.linear = torch.nn.ModuleList(
+            [torch.nn.Linear(unit, input_dim) for _ in range(num_outputs)]
+        )
+
+        if nonlinear not in ("sigmoid", "relu", "tanh"):
+            raise ValueError("Not supporting nonlinear={}".format(nonlinear))
+
+        self.nonlinear = {
+            "sigmoid": torch.nn.Sigmoid(),
+            "relu": torch.nn.ReLU(),
+            "tanh": torch.nn.Tanh(),
+        }[nonlinear]
+
+    def forward(
+        self,
+        input: Union[torch.Tensor, ComplexTensor],
+        ilens: torch.Tensor,
+        additional: Optional[Dict] = None,
+    ) -> Tuple[List[Union[torch.Tensor, ComplexTensor]], torch.Tensor, OrderedDict]:
+        """Forward.
 
         Args:
-            x_input (torch.Tensor): Input tensor (#batch, time, size).
-            mask (torch.Tensor): Mask tensor for the input (#batch, 1, time).
-            cache (torch.Tensor): Cache tensor of the input (#batch, time - 1, size).
+            input (torch.Tensor or ComplexTensor): Encoded feature [B, T, N]
+            ilens (torch.Tensor): input lengths [Batch]
+            additional (Dict or None): other data included in model
+                NOTE: not used in this model
 
         Returns:
-            torch.Tensor: Output tensor (#batch, time, size).
-            torch.Tensor: Mask tensor (#batch, 1, time).
-
+            masked (List[Union(torch.Tensor, ComplexTensor)]): [(B, T, N), ...]
+            ilens (torch.Tensor): (B,)
+            others predicted data, e.g. masks: OrderedDict[
+                'mask_spk1': torch.Tensor(Batch, Frames, Freq),
+                'mask_spk2': torch.Tensor(Batch, Frames, Freq),
+                ...
+                'mask_spkn': torch.Tensor(Batch, Frames, Freq),
+            ]
         """
-        skip_layer = False
-        # with stochastic depth, residual connection `x + f(x)` becomes
-        # `x <- x + 1 / (1 - p) * f(x)` at training time.
-        stoch_layer_coeff = 1.0
-        if self.training and self.stochastic_depth_rate > 0:
-            skip_layer = torch.rand(1).item() < self.stochastic_depth_rate
-            stoch_layer_coeff = 1.0 / (1 - self.stochastic_depth_rate)
-
-        if skip_layer:
-            if cache is not None:
-                x = torch.cat([cache, x], dim=1)
-            return x, mask
-
-        residual = x
-        if self.normalize_before:
-            x = self.norm1(x)
 
-        if cache is None:
-            x_q = x
+        # if complex spectrum,
+        if is_complex(input):
+            feature = abs(input)
         else:
-            assert cache.shape == (x.shape[0], x.shape[1] - 1, self.size)
-            x_q = x[:, -1:, :]
-            residual = residual[:, -1:, :]
-            mask = None if mask is None else mask[:, -1:, :]
-
-        if self.concat_after:
-            x_concat = torch.cat((x, self.self_attn(x_q, x, x, mask)), dim=-1)
-            x = residual + stoch_layer_coeff * self.concat_linear(x_concat)
+            feature = input
+
+        x, ilens, _ = self.rnn(feature, ilens)
+
+        masks = []
+
+        for linear in self.linear:
+            y = linear(x)
+            y = self.nonlinear(y)
+            masks.append(y)
+
+        if self.predict_noise:
+            *masks, mask_noise = masks
+
+        masked = [input * m for m in masks]
+
+        others = OrderedDict(
+            zip(["mask_spk{}".format(i + 1) for i in range(len(masks))], masks)
+        )
+        if self.predict_noise:
+            others["noise1"] = input * mask_noise
+
+        return masked, ilens, others
+
+    @property
+    def num_spk(self):
+        return self._num_spk
+
+    def forward_streaming(self, input_frame: torch.Tensor, states=None):
+        # input_frame # B, 1, N
+
+        # if complex spectrum,
+        if is_complex(input_frame):
+            feature = abs(input_frame)
         else:
-            x = residual + stoch_layer_coeff * self.dropout(
-                self.self_attn(x_q, x, x, mask)
-            )
-        if not self.normalize_before:
-            x = self.norm1(x)
-
-        residual = x
-        if self.normalize_before:
-            x = self.norm2(x)
-        x = residual + stoch_layer_coeff * self.dropout(self.feed_forward(x))
-        if not self.normalize_before:
-            x = self.norm2(x)
+            feature = input_frame
+
+        ilens = torch.ones(feature.shape[0], device=feature.device)
+
+        x, _, states = self.rnn(feature, ilens, states)
+
+        masks = []
+
+        for linear in self.linear:
+            y = linear(x)
+            y = self.nonlinear(y)
+            masks.append(y)
+
+        if self.predict_noise:
+            *masks, mask_noise = masks
+
+        masked = [input_frame * m for m in masks]
 
-        if cache is not None:
-            x = torch.cat([cache, x], dim=1)
+        others = OrderedDict(
+            zip(["mask_spk{}".format(i + 1) for i in range(len(masks))], masks)
+        )
+        if self.predict_noise:
+            others["noise1"] = input * mask_noise
 
-        return x, mask
+        return masked, states, others
```

### Comparing `espnet-202304/espnet/nets/pytorch_backend/transformer/encoder_mix.py` & `espnet-202308/espnet/nets/pytorch_backend/transformer/encoder_mix.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/transformer/initializer.py` & `espnet-202308/espnet/nets/pytorch_backend/transformer/initializer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/transformer/label_smoothing_loss.py` & `espnet-202308/espnet/nets/pytorch_backend/transformer/label_smoothing_loss.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/transformer/layer_norm.py` & `espnet-202308/espnet/nets/pytorch_backend/transformer/layer_norm.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/transformer/lightconv.py` & `espnet-202308/espnet/nets/pytorch_backend/transformer/lightconv.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/transformer/lightconv2d.py` & `espnet-202308/espnet/nets/pytorch_backend/transformer/lightconv2d.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/transformer/longformer_attention.py` & `espnet-202308/espnet/nets/pytorch_backend/transformer/longformer_attention.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/transformer/mask.py` & `espnet-202308/espnet/nets/pytorch_backend/transformer/mask.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/transformer/multi_layer_conv.py` & `espnet-202308/espnet/nets/pytorch_backend/transformer/multi_layer_conv.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/transformer/optimizer.py` & `espnet-202308/espnet/nets/pytorch_backend/transformer/optimizer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/transformer/plot.py` & `espnet-202308/espnet/nets/pytorch_backend/transformer/plot.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/transformer/positionwise_feed_forward.py` & `espnet-202308/espnet/nets/pytorch_backend/transformer/positionwise_feed_forward.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/transformer/repeat.py` & `espnet-202308/espnet/nets/pytorch_backend/transformer/repeat.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/transformer/subsampling.py` & `espnet-202308/espnet/nets/pytorch_backend/transformer/subsampling.py`

 * *Files 13% similar despite different names*

```diff
@@ -26,27 +26,149 @@
         super().__init__(message)
         self.actual_size = actual_size
         self.limit = limit
 
 
 def check_short_utt(ins, size):
     """Check if the utterance is too short for subsampling."""
+    if isinstance(ins, Conv1dSubsampling2) and size < 5:
+        return True, 5
+    if isinstance(ins, Conv1dSubsampling3) and size < 7:
+        return True, 7
     if isinstance(ins, Conv2dSubsampling1) and size < 5:
         return True, 5
     if isinstance(ins, Conv2dSubsampling2) and size < 7:
         return True, 7
     if isinstance(ins, Conv2dSubsampling) and size < 7:
         return True, 7
     if isinstance(ins, Conv2dSubsampling6) and size < 11:
         return True, 11
     if isinstance(ins, Conv2dSubsampling8) and size < 15:
         return True, 15
     return False, -1
 
 
+class Conv1dSubsampling2(torch.nn.Module):
+    """Convolutional 1D subsampling (to 1/2 length).
+
+    Args:
+        idim (int): Input dimension.
+        odim (int): Output dimension.
+        dropout_rate (float): Dropout rate.
+        pos_enc (torch.nn.Module): Custom position encoding layer.
+
+    """
+
+    def __init__(self, idim, odim, dropout_rate, pos_enc=None):
+        """Construct an Conv1dSubsampling2 object."""
+        super(Conv1dSubsampling2, self).__init__()
+        self.conv = torch.nn.Sequential(
+            torch.nn.Conv1d(idim, odim, 3, 1),
+            torch.nn.ReLU(),
+            torch.nn.Conv1d(odim, odim, 3, 2),
+            torch.nn.ReLU(),
+        )
+        self.out = torch.nn.Sequential(
+            torch.nn.Linear(odim, odim),
+            pos_enc if pos_enc is not None else PositionalEncoding(odim, dropout_rate),
+        )
+
+    def forward(self, x, x_mask):
+        """Subsample x.
+
+        Args:
+            x (torch.Tensor): Input tensor (#batch, time, idim).
+            x_mask (torch.Tensor): Input mask (#batch, 1, time).
+
+        Returns:
+            torch.Tensor: Subsampled tensor (#batch, time', odim),
+                where time' = time // 2.
+            torch.Tensor: Subsampled mask (#batch, 1, time'),
+                where time' = time // 2.
+
+        """
+        x = x.transpose(2, 1)  # (#batch, idim, time)
+        x = self.conv(x)
+        b, c, t = x.size()
+        x = self.out(x.transpose(1, 2).contiguous())
+        if x_mask is None:
+            return x, None
+        return x, x_mask[:, :, :-2:1][:, :, :-2:2]
+
+    def __getitem__(self, key):
+        """Get item.
+
+        When reset_parameters() is called, if use_scaled_pos_enc is used,
+            return the positioning encoding.
+
+        """
+        if key != -1:
+            raise NotImplementedError("Support only `-1` (for `reset_parameters`).")
+        return self.out[key]
+
+
+class Conv1dSubsampling3(torch.nn.Module):
+    """Convolutional 1D subsampling (to 1/3 length).
+
+    Args:
+        idim (int): Input dimension.
+        odim (int): Output dimension.
+        dropout_rate (float): Dropout rate.
+        pos_enc (torch.nn.Module): Custom position encoding layer.
+
+    """
+
+    def __init__(self, idim, odim, dropout_rate, pos_enc=None):
+        """Construct an Conv1dSubsampling3 object."""
+        super(Conv1dSubsampling3, self).__init__()
+        self.conv = torch.nn.Sequential(
+            torch.nn.Conv1d(idim, odim, 3, 1),
+            torch.nn.ReLU(),
+            torch.nn.Conv1d(odim, odim, 5, 3),
+            torch.nn.ReLU(),
+        )
+        self.out = torch.nn.Sequential(
+            torch.nn.Linear(odim, odim),
+            pos_enc if pos_enc is not None else PositionalEncoding(odim, dropout_rate),
+        )
+
+    def forward(self, x, x_mask):
+        """Subsample x.
+
+        Args:
+            x (torch.Tensor): Input tensor (#batch, time, idim).
+            x_mask (torch.Tensor): Input mask (#batch, 1, time).
+
+        Returns:
+            torch.Tensor: Subsampled tensor (#batch, time', odim),
+                where time' = time // 2.
+            torch.Tensor: Subsampled mask (#batch, 1, time'),
+                where time' = time // 2.
+
+        """
+        x = x.transpose(2, 1)  # (#batch, idim, time)
+        x = self.conv(x)
+        b, c, t = x.size()
+        x = self.out(x.transpose(1, 2).contiguous())
+        if x_mask is None:
+            return x, None
+        return x, x_mask[:, :, :-2:1][:, :, :-4:3]
+
+    def __getitem__(self, key):
+        """Get item.
+
+        When reset_parameters() is called, if use_scaled_pos_enc is used,
+            return the positioning encoding.
+
+        """
+        if key != -1:
+            raise NotImplementedError("Support only `-1` (for `reset_parameters`).")
+        return self.out[key]
+
+
 class Conv2dSubsampling(torch.nn.Module):
     """Convolutional 2D subsampling (to 1/4 length).
 
     Args:
         idim (int): Input dimension.
         odim (int): Output dimension.
         dropout_rate (float): Dropout rate.
```

### Comparing `espnet-202304/espnet/nets/pytorch_backend/transformer/subsampling_without_posenc.py` & `espnet-202308/espnet/nets/pytorch_backend/transformer/subsampling_without_posenc.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/pytorch_backend/wavenet.py` & `espnet-202308/espnet/nets/pytorch_backend/wavenet.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/scorer_interface.py` & `espnet-202308/espnet/nets/scorer_interface.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/scorers/ctc.py` & `espnet-202308/espnet/nets/scorers/ctc.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/scorers/length_bonus.py` & `espnet-202308/espnet/nets/scorers/length_bonus.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/scorers/ngram.py` & `espnet-202308/espnet/nets/scorers/ngram.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/scorers/uasr.py` & `espnet-202308/espnet/nets/scorers/uasr.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/st_interface.py` & `espnet-202308/espnet/nets/st_interface.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/transducer_decoder_interface.py` & `espnet-202308/espnet/nets/transducer_decoder_interface.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/nets/tts_interface.py` & `espnet-202308/espnet/nets/tts_interface.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/optimizer/chainer.py` & `espnet-202308/espnet/optimizer/chainer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/optimizer/factory.py` & `espnet-202308/espnet/optimizer/factory.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/optimizer/parser.py` & `espnet-202308/espnet/optimizer/parser.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/optimizer/pytorch.py` & `espnet-202308/espnet/optimizer/pytorch.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/scheduler/chainer.py` & `espnet-202308/espnet/scheduler/chainer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/scheduler/pytorch.py` & `espnet-202308/espnet/scheduler/pytorch.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/scheduler/scheduler.py` & `espnet-202308/espnet/scheduler/scheduler.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/st/pytorch_backend/st.py` & `espnet-202308/espnet/st/pytorch_backend/st.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/transform/add_deltas.py` & `espnet-202308/espnet/transform/add_deltas.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/transform/channel_selector.py` & `espnet-202308/espnet/transform/channel_selector.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/transform/cmvn.py` & `espnet-202308/espnet/transform/cmvn.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/transform/functional.py` & `espnet-202308/espnet/transform/functional.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/transform/perturb.py` & `espnet-202308/espnet/transform/perturb.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/transform/spec_augment.py` & `espnet-202308/espnet/transform/spec_augment.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/transform/spectrogram.py` & `espnet-202308/espnet/transform/spectrogram.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/transform/transformation.py` & `espnet-202308/espnet/transform/transformation.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/transform/wpe.py` & `espnet-202308/espnet/transform/wpe.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/tts/pytorch_backend/tts.py` & `espnet-202308/espnet/tts/pytorch_backend/tts.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/utils/check_kwargs.py` & `espnet-202308/espnet/utils/check_kwargs.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/utils/cli_readers.py` & `espnet-202308/espnet/utils/cli_readers.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/utils/cli_utils.py` & `espnet-202308/espnet/utils/cli_utils.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/utils/cli_writers.py` & `espnet-202308/espnet/utils/cli_writers.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/utils/dataset.py` & `espnet-202308/espnet/utils/dataset.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/utils/deterministic_utils.py` & `espnet-202308/espnet/utils/deterministic_utils.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/utils/dynamic_import.py` & `espnet-202308/espnet/utils/dynamic_import.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/utils/fill_missing_args.py` & `espnet-202308/espnet/utils/fill_missing_args.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/utils/io_utils.py` & `espnet-202308/espnet/utils/io_utils.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/utils/spec_augment.py` & `espnet-202308/espnet/utils/spec_augment.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/utils/training/batchfy.py` & `espnet-202308/espnet/utils/training/batchfy.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/utils/training/evaluator.py` & `espnet-202308/espnet/utils/training/evaluator.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/utils/training/iterators.py` & `espnet-202308/espnet/utils/training/iterators.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/utils/training/tensorboard_logger.py` & `espnet-202308/espnet/utils/training/tensorboard_logger.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet/utils/training/train_utils.py` & `espnet-202308/espnet/utils/training/train_utils.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet.egg-info/PKG-INFO` & `espnet-202308/espnet.egg-info/PKG-INFO`

 * *Files 5% similar despite different names*

```diff
@@ -1,30 +1,30 @@
 Metadata-Version: 2.1
 Name: espnet
-Version: 202304
+Version: 202308
 Summary: ESPnet: end-to-end speech processing toolkit
 Home-page: http://github.com/espnet/espnet
 Author: Shinji Watanabe
 Author-email: shinjiw@ieee.org
 License: Apache Software License
 Description: <div align="left"><img src="doc/image/espnet_logo1.png" width="550"/></div>
         
         # ESPnet: end-to-end speech processing toolkit
         
         |system/pytorch ver.|1.10.2|1.11.0|1.12.1|1.13.1|
-        | :---: | :---: | :---: | :---: | :---: |
-        |ubuntu/python3.10/pip||||[![Github Actions](https://github.com/espnet/espnet/workflows/CI/badge.svg)](https://github.com/espnet/espnet/actions)|
-        |ubuntu/python3.9/pip|[![Github Actions](https://github.com/espnet/espnet/workflows/CI/badge.svg)](https://github.com/espnet/espnet/actions)|[![Github Actions](https://github.com/espnet/espnet/workflows/CI/badge.svg)](https://github.com/espnet/espnet/actions)|[![Github Actions](https://github.com/espnet/espnet/workflows/CI/badge.svg)](https://github.com/espnet/espnet/actions)|[![Github Actions](https://github.com/espnet/espnet/workflows/CI/badge.svg)](https://github.com/espnet/espnet/actions)|
-        |ubuntu/python3.8/pip|[![Github Actions](https://github.com/espnet/espnet/workflows/CI/badge.svg)](https://github.com/espnet/espnet/actions)|[![Github Actions](https://github.com/espnet/espnet/workflows/CI/badge.svg)](https://github.com/espnet/espnet/actions)|[![Github Actions](https://github.com/espnet/espnet/workflows/CI/badge.svg)](https://github.com/espnet/espnet/actions)|[![Github Actions](https://github.com/espnet/espnet/workflows/CI/badge.svg)](https://github.com/espnet/espnet/actions)|
-        |ubuntu/python3.7/pip|[![Github Actions](https://github.com/espnet/espnet/workflows/CI/badge.svg)](https://github.com/espnet/espnet/actions)|[![Github Actions](https://github.com/espnet/espnet/workflows/CI/badge.svg)](https://github.com/espnet/espnet/actions)|[![Github Actions](https://github.com/espnet/espnet/workflows/CI/badge.svg)](https://github.com/espnet/espnet/actions)|[![Github Actions](https://github.com/espnet/espnet/workflows/CI/badge.svg)](https://github.com/espnet/espnet/actions)|
-        |debian11/python3.7/conda||||[![debian11](https://github.com/espnet/espnet/workflows/debian11/badge.svg)](https://github.com/espnet/espnet/actions?query=workflow%3Adebian11)|
-        |centos7/python3.7/conda||||[![centos7](https://github.com/espnet/espnet/workflows/centos7/badge.svg)](https://github.com/espnet/espnet/actions?query=workflow%3Acentos7)|
-        |ubuntu/doc/python3.8||||[![doc](https://github.com/espnet/espnet/workflows/doc/badge.svg)](https://github.com/espnet/espnet/actions?query=workflow%3Adoc)|
-        
-        
+        | :---- | :---: | :---: | :---: | :---: |
+        |ubuntu/python3.10/pip||||[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|
+        |ubuntu/python3.9/pip||||[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|
+        |ubuntu/python3.8/pip||||[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|
+        |ubuntu/python3.7/pip|[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|
+        |debian11/python3.7/conda||||[![ci on debian11](https://github.com/espnet/espnet/actions/workflows/ci_on_debian11.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_debian11.yml?query=branch%3Amaster)|
+        |centos7/python3.7/conda||||[![ci on centos7](https://github.com/espnet/espnet/actions/workflows/ci_on_centos7.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_centos7.yml?query=branch%3Amaster)|
+        |windows/python3.10/pip||||[![ci on windows](https://github.com/espnet/espnet/actions/workflows/ci_on_windows.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_windows.yml?query=branch%3Amaster)|
+        |macos/python3.10/pip||||[![ci on macos](https://github.com/espnet/espnet/actions/workflows/ci_on_macos.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_macos.yml?query=branch%3Amaster)|
+        |macos/python3.10/conda||||[![ci on macos](https://github.com/espnet/espnet/actions/workflows/ci_on_macos.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_macos.yml?query=branch%3Amaster)|
         
         [![PyPI version](https://badge.fury.io/py/espnet.svg)](https://badge.fury.io/py/espnet)
         [![Python Versions](https://img.shields.io/pypi/pyversions/espnet.svg)](https://pypi.org/project/espnet/)
         [![Downloads](https://pepy.tech/badge/espnet)](https://pepy.tech/project/espnet)
         [![GitHub license](https://img.shields.io/github/license/espnet/espnet.svg)](https://github.com/espnet/espnet)
         [![codecov](https://codecov.io/gh/espnet/espnet/branch/master/graph/badge.svg)](https://codecov.io/gh/espnet/espnet)
         [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
@@ -55,64 +55,65 @@
           - Add new models/tasks to ESPnet
             - [Online video](https://youtu.be/Css3XAes7SU)
             - [Material](https://colab.research.google.com/github/espnet/notebook/blob/master/espnet2_new_task_tutorial_CMU_11751_18781_Fall2022.ipynb)
         
         
         ## Key Features
         
-        ### Kaldi style complete recipe
-        - Support numbers of `ASR` recipes (WSJ, Switchboard, CHiME-4/5, Librispeech, TED, CSJ, AMI, HKUST, Voxforge, REVERB, etc.)
-        - Support numbers of `TTS` recipes with a similar manner to the ASR recipe (LJSpeech, LibriTTS, M-AILABS, etc.)
+        ### Kaldi-style complete recipe
+        - Support numbers of `ASR` recipes (WSJ, Switchboard, CHiME-4/5, Librispeech, TED, CSJ, AMI, HKUST, Voxforge, REVERB, Gigaspeech, etc.)
+        - Support numbers of `TTS` recipes in a similar manner to the ASR recipe (LJSpeech, LibriTTS, M-AILABS, etc.)
         - Support numbers of `ST` recipes (Fisher-CallHome Spanish, Libri-trans, IWSLT'18, How2, Must-C, Mboshi-French, etc.)
         - Support numbers of `MT` recipes (IWSLT'14, IWSLT'16, the above ST recipes etc.)
         - Support numbers of `SLU` recipes (CATSLU-MAPS, FSC, Grabo, IEMOCAP, JDCINAL, SNIPS, SLURP, SWBD-DA, etc.)
         - Support numbers of `SE/SS` recipes (DNS-IS2020, LibriMix, SMS-WSJ, VCTK-noisyreverb, WHAM!, WHAMR!, WSJ-2mix, etc.)
         - Support voice conversion recipe (VCC2020 baseline)
         - Support speaker diarization recipe (mini_librispeech, librimix)
-        - Support singing voice synthesis recipe (ofuton_p_utagoe_db)
+        - Support singing voice synthesis recipe (ofuton_p_utagoe_db, opencpop, m4singer, etc.)
         
         ### ASR: Automatic Speech Recognition
         - **State-of-the-art performance** in several ASR benchmarks (comparable/superior to hybrid DNN/HMM and CTC)
         - **Hybrid CTC/attention** based end-to-end ASR
           - Fast/accurate training with CTC/attention multitask training
           - CTC/attention joint decoding to boost monotonic alignment decoding
           - Encoder: VGG-like CNN + BiRNN (LSTM/GRU), sub-sampling BiRNN (LSTM/GRU), Transformer, Conformer, [Branchformer](https://proceedings.mlr.press/v162/peng22a.html), or [E-Branchformer](https://arxiv.org/abs/2210.00077)
           - Decoder: RNN (LSTM/GRU), Transformer, or S4
         - Attention: Dot product, location-aware attention, variants of multi-head
         - Incorporate RNNLM/LSTMLM/TransformerLM/N-gram trained only with text data
         - Batch GPU decoding
         - Data augmentation
         - **Transducer** based end-to-end ASR
           - Architecture:
-            - RNN-based encoder and decoder.
-            - Custom encoder and decoder supporting Transformer, Conformer (encoder), 1D Conv / TDNN (encoder) and causal 1D Conv (decoder) blocks.
-            - VGG2L (RNN/custom encoder) and Conv2D (custom encoder) bottlenecks.
+            - Custom encoder supporting RNNs, Conformer, Branchformer (w/ variants), 1D Conv / TDNN.
+            - Decoder w/ parameters shared across blocks supporting RNN, stateless w/ 1D Conv, [MEGA](https://arxiv.org/abs/2209.10655), and [RWKV](https://arxiv.org/abs/2305.13048).
+            - Pre-encoder: VGG2L or Conv2D available.
           - Search algorithms:
             - Greedy search constrained to one emission by timestep.
             - Default beam search algorithm [[Graves, 2012]](https://arxiv.org/abs/1211.3711) without prefix search.
             - Alignment-Length Synchronous decoding [[Saon et al., 2020]](https://ieeexplore.ieee.org/abstract/document/9053040).
             - Time Synchronous Decoding [[Saon et al., 2020]](https://ieeexplore.ieee.org/abstract/document/9053040).
             - N-step Constrained beam search modified from [[Kim et al., 2020]](https://arxiv.org/abs/2002.03577).
             - modified Adaptive Expansion Search based on [[Kim et al., 2021]](https://ieeexplore.ieee.org/abstract/document/9250505) and NSC.
           - Features:
+            - Unified interface for offline and streaming speech recognition.
             - Multi-task learning with various auxiliary losses:
               - Encoder: CTC, auxiliary Transducer and symmetric KL divergence.
               - Decoder: cross-entropy w/ label smoothing.
-            - Transfer learning with acoustic model and/or language model.
+            - Transfer learning with an acoustic model and/or language model.
             - Training with FastEmit regularization method [[Yu et al., 2021]](https://arxiv.org/abs/2010.11148).
           > Please refer to the [tutorial page](https://espnet.github.io/espnet/tutorial.html#transducer) for complete documentation.
         - CTC segmentation
         - Non-autoregressive model based on Mask-CTC
         - ASR examples for supporting endangered language documentation (Please refer to egs/puebla_nahuatl and egs/yoloxochitl_mixtec for details)
-        - Wav2Vec2.0 pretrained model as Encoder, imported from [FairSeq](https://github.com/pytorch/fairseq/tree/master/fairseq).
+        - Wav2Vec2.0 pre-trained model as Encoder, imported from [FairSeq](https://github.com/pytorch/fairseq/tree/master/fairseq).
         - Self-supervised learning representations as features, using upstream models in [S3PRL](https://github.com/s3prl/s3prl) in frontend.
-          - Set `frontend` to be `s3prl`
+          - Set `frontend` to `s3prl`
           - Select any upstream model by setting the `frontend_conf` to the corresponding name.
         - Transfer Learning :
-          - easy usage and transfers from models previously trained by your group, or models from [ESPnet Hugging Face repository](https://huggingface.co/espnet).
+          - easy usage and transfers from models previously trained by your group or models from [ESPnet Hugging Face repository](https://huggingface.co/espnet).
           - [Documentation](https://github.com/espnet/espnet/tree/master/egs2/mini_an4/asr1/transfer_learning.md) and [toy example runnable on colab](https://github.com/espnet/notebook/blob/master/espnet2_asr_transfer_learning_demo.ipynb).
         - Streaming Transformer/Conformer ASR with blockwise synchronous beam search.
         - Restricted Self-Attention based on [Longformer](https://arxiv.org/abs/2004.05150) as an encoder for long sequences
         - OpenAI [Whisper](https://openai.com/blog/whisper/) model, robust ASR based on large-scale, weakly-supervised multitask learning
         
         Demonstration
         - Real-time ASR demo with ESPnet2  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/espnet/notebook/blob/master/espnet2_asr_realtime_demo.ipynb)
@@ -125,15 +126,15 @@
             - Transformer-TTS
             - FastSpeech
             - FastSpeech2
             - Conformer FastSpeech & FastSpeech2
             - VITS
             - JETS
         - Multi-speaker & multi-language extention
-            - Pretrained speaker embedding (e.g., X-vector)
+            - Pre-trained speaker embedding (e.g., X-vector)
             - Speaker ID embedding
             - Language ID embedding
             - Global style token (GST) embedding
             - Mix of the above embeddings
         - End-to-end training
             - End-to-end text-to-wav model (e.g., VITS, JETS, etc.)
             - Joint training of text2mel and vocoder
@@ -151,165 +152,154 @@
         - Real-time TTS demo with ESPnet2  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/espnet/notebook/blob/master/espnet2_tts_realtime_demo.ipynb)
         - Integrated to [Hugging Face Spaces](https://huggingface.co/spaces) with [Gradio](https://github.com/gradio-app/gradio). See demo: [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/akhaliq/ESPnet2-TTS)
         
         To train the neural vocoder, please check the following repositories:
         - [kan-bayashi/ParallelWaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN)
         - [r9y9/wavenet_vocoder](https://github.com/r9y9/wavenet_vocoder)
         
-        > **NOTE**:
-        > - We are moving on ESPnet2-based development for TTS.
-        > - The use of ESPnet1-TTS is deprecated, please use [ESPnet2-TTS](https://github.com/espnet/espnet/tree/master/egs2/TEMPLATE/tts1).
-        
         ### SE: Speech enhancement (and separation)
         
         - Single-speaker speech enhancement
         - Multi-speaker speech separation
         - Unified encoder-separator-decoder structure for time-domain and frequency-domain models
           - Encoder/Decoder: STFT/iSTFT, Convolution/Transposed-Convolution
           - Separators: BLSTM, Transformer, Conformer, [TasNet](https://arxiv.org/abs/1809.07454), [DPRNN](https://arxiv.org/abs/1910.06379), [SkiM](https://arxiv.org/abs/2201.10800), [SVoice](https://arxiv.org/abs/2011.02329), [DC-CRN](https://web.cse.ohio-state.edu/~wang.77/papers/TZW.taslp21.pdf), [DCCRN](https://arxiv.org/abs/2008.00264), [Deep Clustering](https://ieeexplore.ieee.org/document/7471631), [Deep Attractor Network](https://pubmed.ncbi.nlm.nih.gov/29430212/), [FaSNet](https://arxiv.org/abs/1909.13387), [iFaSNet](https://arxiv.org/abs/1910.14104), Neural Beamformers, etc.
         - Flexible ASR integration: working as an individual task or as the ASR frontend
-        - Easy to import pretrained models from [Asteroid](https://github.com/asteroid-team/asteroid)
+        - Easy to import pre-trained models from [Asteroid](https://github.com/asteroid-team/asteroid)
           - Both the pre-trained models from Asteroid and the specific configuration are supported.
         
         Demonstration
         - Interactive SE demo with ESPnet2 [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1fjRJCh96SoYLZPRxsjF9VDv4Q2VoIckI?usp=sharing)
+        - Streaming SE demo with ESPnet2 [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/17vd1V78eJpp3PHBnbFE5aVY5uMxQFL6o?usp=sharing)
         
         ### ST: Speech Translation & MT: Machine Translation
         - **State-of-the-art performance** in several ST benchmarks (comparable/superior to cascaded ASR and MT)
-        - Transformer based end-to-end ST (new!)
-        - Transformer based end-to-end MT (new!)
+        - Transformer-based end-to-end ST (new!)
+        - Transformer-based end-to-end MT (new!)
         
         ### VC: Voice conversion
-        - Transformer and Tacotron2 based parallel VC using melspectrogram (new!)
+        - Transformer and Tacotron2-based parallel VC using Mel spectrogram
         - End-to-end VC based on cascaded ASR+TTS (Baseline system for Voice Conversion Challenge 2020!)
         
         ### SLU: Spoken Language Understanding
         - Architecture
-            - Transformer based Encoder
-            - Conformer based Encoder
+            - Transformer-based Encoder
+            - Conformer-based Encoder
             - [Branchformer](https://proceedings.mlr.press/v162/peng22a.html) based Encoder
             - [E-Branchformer](https://arxiv.org/abs/2210.00077) based Encoder
             - RNN based Decoder
-            - Transformer based Decoder
+            - Transformer-based Decoder
         - Support Multitasking with ASR
             - Predict both intent and ASR transcript
         - Support Multitasking with NLU
             - Deliberation encoder based 2 pass model
-        - Support using pretrained ASR models
+        - Support using pre-trained ASR models
             - Hubert
             - Wav2vec2
             - VQ-APC
             - TERA and more ...
-        - Support using pretrained NLP models
+        - Support using pre-trained NLP models
             - BERT
             - MPNet And more...
         - Various language support
             - En / Jp / Zn / Nl / And more...
         - Supports using context from previous utterances
-        - Supports using other tasks like SE in pipeline manner
+        - Supports using other tasks like SE in a pipeline manner
         - Supports Two Pass SLU that combines audio and ASR transcript
         Demonstration
-        - Performing noisy spoken language understanding using speech enhancement model followed by spoken language understanding model.  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/14nCrJ05vJcQX0cJuXjbMVFWUHJ3Wfb6N?usp=sharing)
-        - Performing two pass spoken language understanding where the second pass model attends on both acoustic and semantic information.  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1p2cbGIPpIIcynuDl4ZVHDpmNPl8Nh_ci?usp=sharing)
+        - Performing noisy spoken language understanding using a speech enhancement model followed by a spoken language understanding model.  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/14nCrJ05vJcQX0cJuXjbMVFWUHJ3Wfb6N?usp=sharing)
+        - Performing two-pass spoken language understanding where the second pass model attends to both acoustic and semantic information.  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1p2cbGIPpIIcynuDl4ZVHDpmNPl8Nh_ci?usp=sharing)
         - Integrated to [Hugging Face Spaces](https://huggingface.co/spaces) with [Gradio](https://github.com/gradio-app/gradio). See SLU demo on multiple languages: [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/Siddhant/ESPnet2-SLU)
         
         
         ### SUM: Speech Summarization
         - End to End Speech Summarization Recipe for Instructional Videos using Restricted Self-Attention [[Sharma et al., 2022]](https://arxiv.org/abs/2110.06263)
         
         ### SVS: Singing Voice Synthesis
         - Framework merge from [Muskits](https://github.com/SJTMusicTeam/Muskits)
         - Architecture
           - RNN-based non-autoregressive model
           - Xiaoice
-          - Sequence-to-sequence Transformer (with GLU-based encoder)
-          - MLP singer (in progress)
-          - Tacotron-singing (in progress)
+          - Tacotron-singing
           - DiffSinger (in progress)
           - VISinger
+          - VISinger 2 (its variations with different vocoders-architecture)
         - Support multi-speaker & multilingual singing synthesis
           - Speaker ID embedding
           - Language ID embedding
         - Various language support
           - Jp / En / Kr / Zh
         - Tight integration with neural vocoders (the same as TTS)
         
         ### SSL: Self-supervised Learning
-        - Support HuBERT Pretraining:
+        - Support HuBERT Pre-training:
           * Example recipe: [egs2/LibriSpeech/ssl1](egs2/LibriSpeech/ssl1)
         
         ### UASR: Unsupervised ASR (EURO: ESPnet Unsupervised Recognition - Open-source)
         - Architecture
           - wav2vec-U (with different self-supervised models)
           - wav2vec-U 2.0 (in progress)
         - Support PrefixBeamSearch and K2-based WFST decoding
         
         ### DNN Framework
-        - Flexible network architecture thanks to chainer and pytorch
+        - Flexible network architecture thanks to Chainer and PyTorch
         - Flexible front-end processing thanks to [kaldiio](https://github.com/nttcslab-sp/kaldiio) and HDF5 support
-        - Tensorboard based monitoring
+        - Tensorboard-based monitoring
         
         ### ESPnet2
         See [ESPnet2](https://espnet.github.io/espnet/espnet2_tutorial.html).
         
         - Independent from Kaldi/Chainer, unlike ESPnet1
-        - On the fly feature extraction and text processing when training
+        - On-the-fly feature extraction and text processing when training
         - Supporting DistributedDataParallel and DaraParallel both
         - Supporting multiple nodes training and integrated with [Slurm](https://slurm.schedmd.com/) or MPI
         - Supporting Sharded Training provided by [fairscale](https://github.com/facebookresearch/fairscale)
-        - A template recipe which can be applied for all corpora
+        - A template recipe that can be applied to all corpora
         - Possible to train any size of corpus without CPU memory error
         - [ESPnet Model Zoo](https://github.com/espnet/espnet_model_zoo)
         - Integrated with [wandb](https://espnet.github.io/espnet/espnet2_training_option.html#weights-biases-integration)
         
         ## Installation
-        - If you intend to do full experiments including DNN training, then see [Installation](https://espnet.github.io/espnet/installation.html).
+        - If you intend to do full experiments, including DNN training, then see [Installation](https://espnet.github.io/espnet/installation.html).
         - If you just need the Python module only:
             ```sh
-            # We recommend you installing pytorch before installing espnet following https://pytorch.org/get-started/locally/
+            # We recommend you install PyTorch before installing espnet following https://pytorch.org/get-started/locally/
             pip install espnet
-            # To install latest
+            # To install the latest
             # pip install git+https://github.com/espnet/espnet
             # To install additional packages
             # pip install "espnet[all]"
             ```
         
-            If you'll use ESPnet1, please install chainer and cupy.
+            If you use ESPnet1, please install chainer and cupy.
         
             ```sh
             pip install chainer==6.0.0 cupy==6.0.0    # [Option]
             ```
         
             You might need to install some packages depending on each task. We prepared various installation scripts at [tools/installers](tools/installers).
         
         - (ESPnet2) Once installed, run `wandb login` and set `--use_wandb true` to enable tracking runs using W&B.
         
-        ## Usage
-        See [Usage](https://espnet.github.io/espnet/tutorial.html).
-        
         ## Docker Container
         
         go to [docker/](docker/) and follow [instructions](https://espnet.github.io/espnet/docker.html).
         
         ## Contribution
-        Thank you for taking times for ESPnet! Any contributions to ESPnet are welcome and feel free to ask any questions or requests to [issues](https://github.com/espnet/espnet/issues).
-        If it's the first contribution to ESPnet for you,  please follow the [contribution guide](CONTRIBUTING.md).
-        
-        ## Results and demo
-        
-        You can find useful tutorials and demos in [Interspeech 2019 Tutorial](https://github.com/espnet/interspeech2019-tutorial)
+        Thank you for taking the time for ESPnet! Any contributions to ESPnet are welcome, and feel free to ask any questions or requests to [issues](https://github.com/espnet/espnet/issues).
+        If it's your first ESPnet contribution,  please follow the [contribution guide](CONTRIBUTING.md).
         
         ### ASR results
         
         <details><summary>expand</summary><div>
         
         
         We list the character error rate (CER) and word error rate (WER) of major ASR tasks.
         
-        | Task                                                              |     CER (%)     |     WER (%)     |                                                                              Pretrained model                                                                               |
+        | Task                                                              |     CER (%)     |     WER (%)     |                                                                              Pre-trained model                                                                               |
         | ----------------------------------------------------------------- | :-------------: | :-------------: | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
         | Aishell dev/test                                                  |     4.6/5.1     |       N/A       |                [link](https://github.com/espnet/espnet/blob/master/egs/aishell/asr1/RESULTS.md#conformer-kernel-size--15--specaugment--lm-weight--00-result)                |
         | **ESPnet2** Aishell dev/test                                      |     4.1/4.4     |       N/A       |                [link](https://github.com/espnet/espnet/tree/master/egs2/aishell/asr1#branchformer-initial)                                                                  |
         | Common Voice dev/test                                             |     1.7/1.8     |     2.2/2.3     |    [link](https://github.com/espnet/espnet/blob/master/egs/commonvoice/asr1/RESULTS.md#first-results-default-pytorch-transformer-setting-with-bpe-100-epochs-single-gpu)    |
         | CSJ eval1/eval2/eval3                                             |   5.7/3.8/4.2   |       N/A       |                 [link](https://github.com/espnet/espnet/blob/master/egs/csj/asr1/RESULTS.md#pytorch-backend-transformer-without-any-hyperparameter-tuning)                  |
         | **ESPnet2** CSJ eval1/eval2/eval3                                 |   4.5/3.3/3.6   |       N/A       |                                        [link](https://github.com/espnet/espnet/tree/master/egs2/csj/asr1#initial-conformer-results)                                         |
         | **ESPnet2** GigaSpeech dev/test                                   |       N/A       |    10.6/10.5    |                                          [link](https://github.com/espnet/espnet/tree/master/egs2/gigaspeech/asr1#e-branchformer)                                           |
@@ -332,26 +322,26 @@
         </div></details>
         
         
         ### ASR demo
         
         <details><summary>expand</summary><div>
         
-        You can recognize speech in a WAV file using pretrained models.
+        You can recognize speech in a WAV file using pre-trained models.
         Go to a recipe directory and run `utils/recog_wav.sh` as follows:
         ```sh
-        # go to recipe directory and source path of espnet tools
+        # go to the recipe directory and source path of espnet tools
         cd egs/tedlium2/asr1 && . ./path.sh
         # let's recognize speech!
         recog_wav.sh --models tedlium2.transformer.v1 example.wav
         ```
         where `example.wav` is a WAV file to be recognized.
         The sampling rate must be consistent with that of data used in training.
         
-        Available pretrained models in the demo script are listed as below.
+        Available pre-trained models in the demo script are listed below.
         
         | Model                                                                                            | Notes                                                      |
         | :----------------------------------------------------------------------------------------------- | :--------------------------------------------------------- |
         | [tedlium2.rnn.v1](https://drive.google.com/open?id=1UqIY6WJMZ4sxNxSugUqp3mrGb3j6h7xe)            | Streaming decoding based on CTC-based VAD                  |
         | [tedlium2.rnn.v2](https://drive.google.com/open?id=1cac5Uc09lJrCYfWkLQsF8eapQcxZnYdf)            | Streaming decoding based on CTC-based VAD (batch decoding) |
         | [tedlium2.transformer.v1](https://drive.google.com/open?id=1cVeSOYY1twOfL9Gns7Z3ZDnkrJqNwPow)    | Joint-CTC attention Transformer trained on Tedlium 2       |
         | [tedlium3.transformer.v1](https://drive.google.com/open?id=1zcPglHAKILwVgfACoMWWERiyIquzSYuU)    | Joint-CTC attention Transformer trained on Tedlium 3       |
@@ -378,36 +368,42 @@
         ### SE demos
         <details><summary>expand</summary><div>
         You can try the interactive demo with Google Colab. Please click the following button to get access to the demos.
         
         [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1fjRJCh96SoYLZPRxsjF9VDv4Q2VoIckI?usp=sharing)
         
         
-        It is based on ESPnet2. Pretrained models are available for both speech enhancement and speech separation tasks.
+        It is based on ESPnet2. Pre-trained models are available for both speech enhancement and speech separation tasks.
+        
+        Speech separation streaming demos:
+        
+        [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/17vd1V78eJpp3PHBnbFE5aVY5uMxQFL6o?usp=sharing)
+        
+        
         
         </div></details>
         
         ### ST results
         
         <details><summary>expand</summary><div>
         
         We list 4-gram BLEU of major ST tasks.
         
         #### end-to-end system
-        | Task                                              | BLEU  |                                                                                         Pretrained model                                                                                          |
+        | Task                                              | BLEU  |                                                                                         Pre-trained model                                                                                          |
         | ------------------------------------------------- | :---: | :-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
         | Fisher-CallHome Spanish fisher_test (Es->En)      | 51.03 | [link](https://github.com/espnet/espnet/blob/master/egs/fisher_callhome_spanish/st1/RESULTS.md#train_spen_lcrm_pytorch_train_pytorch_transformer_bpe_short_long_bpe1000_specaug_asrtrans_mttrans) |
         | Fisher-CallHome Spanish callhome_evltest (Es->En) | 20.44 | [link](https://github.com/espnet/espnet/blob/master/egs/fisher_callhome_spanish/st1/RESULTS.md#train_spen_lcrm_pytorch_train_pytorch_transformer_bpe_short_long_bpe1000_specaug_asrtrans_mttrans) |
         | Libri-trans test (En->Fr)                         | 16.70 |       [link](https://github.com/espnet/espnet/blob/master/egs/libri_trans/st1/RESULTS.md#train_spfr_lc_pytorch_train_pytorch_transformer_bpe_short_long_bpe1000_specaug_asrtrans_mttrans-1)       |
         | How2 dev5 (En->Pt)                                | 45.68 |              [link](https://github.com/espnet/espnet/blob/master/egs/how2/st1/RESULTS.md#trainpt_tc_pytorch_train_pytorch_transformer_short_long_bpe8000_specaug_asrtrans_mttrans-1)              |
         | Must-C tst-COMMON (En->De)                        | 22.91 |          [link](https://github.com/espnet/espnet/blob/master/egs/must_c/st1/RESULTS.md#train_spen-dede_tc_pytorch_train_pytorch_transformer_short_long_bpe8000_specaug_asrtrans_mttrans)          |
         | Mboshi-French dev (Fr->Mboshi)                    | 6.18  |                                                                                                N/A                                                                                                |
         
         #### cascaded system
-        | Task                                              | BLEU  | Pretrained model |
+        | Task                                              | BLEU  | Pre-trained model |
         | ------------------------------------------------- | :---: | :--------------: |
         | Fisher-CallHome Spanish fisher_test (Es->En)      | 42.16 |       N/A        |
         | Fisher-CallHome Spanish callhome_evltest (Es->En) | 19.82 |       N/A        |
         | Libri-trans test (En->Fr)                         | 16.96 |       N/A        |
         | How2 dev5 (En->Pt)                                | 44.90 |       N/A        |
         | Must-C tst-COMMON (En->De)                        | 23.65 |       N/A        |
         
@@ -423,41 +419,41 @@
         (**New!**) We made a new real-time E2E-ST + TTS demonstration in Google Colab.
         Please access the notebook from the following button and enjoy the real-time speech-to-speech translation!
         
         [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/espnet/notebook/blob/master/st_demo.ipynb)
         
         ---
         
-        You can translate speech in a WAV file using pretrained models.
+        You can translate speech in a WAV file using pre-trained models.
         Go to a recipe directory and run `utils/translate_wav.sh` as follows:
         ```sh
-        # go to recipe directory and source path of espnet tools
+        # Go to recipe directory and source path of espnet tools
         cd egs/fisher_callhome_spanish/st1 && . ./path.sh
         # download example wav file
         wget -O - https://github.com/espnet/espnet/files/4100928/test.wav.tar.gz | tar zxvf -
         # let's translate speech!
         translate_wav.sh --models fisher_callhome_spanish.transformer.v1.es-en test.wav
         ```
         where `test.wav` is a WAV file to be translated.
         The sampling rate must be consistent with that of data used in training.
         
-        Available pretrained models in the demo script are listed as below.
+        Available pre-trained models in the demo script are listed as below.
         
         | Model                                                                                                        | Notes                                                    |
         | :----------------------------------------------------------------------------------------------------------- | :------------------------------------------------------- |
         | [fisher_callhome_spanish.transformer.v1](https://drive.google.com/open?id=1hawp5ZLw4_SIHIT3edglxbKIIkPVe8n3) | Transformer-ST trained on Fisher-CallHome Spanish Es->En |
         
         </div></details>
         
         
         ### MT results
         
         <details><summary>expand</summary><div>
         
-        | Task                                              | BLEU  |                                                                        Pretrained model                                                                         |
+        | Task                                              | BLEU  |                                                                        Pre-trained model                                                                         |
         | ------------------------------------------------- | :---: | :-------------------------------------------------------------------------------------------------------------------------------------------------------------: |
         | Fisher-CallHome Spanish fisher_test (Es->En)      | 61.45 | [link](https://github.com/espnet/espnet/blob/master/egs/fisher_callhome_spanish/mt1/RESULTS.md#trainen_lcrm_lcrm_pytorch_train_pytorch_transformer_bpe_bpe1000) |
         | Fisher-CallHome Spanish callhome_evltest (Es->En) | 29.86 | [link](https://github.com/espnet/espnet/blob/master/egs/fisher_callhome_spanish/mt1/RESULTS.md#trainen_lcrm_lcrm_pytorch_train_pytorch_transformer_bpe_bpe1000) |
         | Libri-trans test (En->Fr)                         | 18.09 |          [link](https://github.com/espnet/espnet/blob/master/egs/libri_trans/mt1/RESULTS.md#trainfr_lcrm_tc_pytorch_train_pytorch_transformer_bpe1000)          |
         | How2 dev5 (En->Pt)                                | 58.61 |              [link](https://github.com/espnet/espnet/blob/master/egs/how2/mt1/RESULTS.md#trainpt_tc_tc_pytorch_train_pytorch_transformer_bpe8000)               |
         | Must-C tst-COMMON (En->De)                        | 27.63 |                               [link](https://github.com/espnet/espnet/blob/master/egs/must_c/mt1/RESULTS.md#summary-4-gram-bleu)                                |
         | IWSLT'14 test2014 (En->De)                        | 24.70 |                                     [link](https://github.com/espnet/espnet/blob/master/egs/iwslt16/mt1/RESULTS.md#result)                                      |
@@ -471,23 +467,23 @@
         ### TTS results
         
         <details><summary>ESPnet2</summary><div>
         
         You can listen to the generated samples in the following URL.
         - [ESPnet2 TTS generated samples](https://drive.google.com/drive/folders/1H3fnlBbWMEkQUfrHqosKN_ZX_WjO29ma?usp=sharing)
         
-        > Note that in the generation we use Griffin-Lim (`wav/`) and [Parallel WaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN) (`wav_pwg/`).
+        > Note that in the generation, we use Griffin-Lim (`wav/`) and [Parallel WaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN) (`wav_pwg/`).
         
-        You can download pretrained models via `espnet_model_zoo`.
+        You can download pre-trained models via `espnet_model_zoo`.
         - [ESPnet model zoo](https://github.com/espnet/espnet_model_zoo)
-        - [Pretrained model list](https://github.com/espnet/espnet_model_zoo/blob/master/espnet_model_zoo/table.csv)
+        - [Pre-trained model list](https://github.com/espnet/espnet_model_zoo/blob/master/espnet_model_zoo/table.csv)
         
-        You can download pretrained vocoders via `kan-bayashi/ParallelWaveGAN`.
+        You can download pre-trained vocoders via `kan-bayashi/ParallelWaveGAN`.
         - [kan-bayashi/ParallelWaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN)
-        - [Pretrained vocoder list](https://github.com/kan-bayashi/ParallelWaveGAN#results)
+        - [Pre-trained vocoder list](https://github.com/kan-bayashi/ParallelWaveGAN#results)
         
         </div></details>
         
         <details><summary>ESPnet1</summary><div>
         
         > NOTE: We are moving on ESPnet2-based development for TTS. Please check the latest results in the above ESPnet2 results.
         
@@ -504,28 +500,28 @@
         - [Single Italian speaker FastSpeech](https://drive.google.com/open?id=13I5V2w7deYFX4DlVk1-0JfaXmUR2rNOv)
         - [Single Mandarin speaker Transformer](https://drive.google.com/open?id=1mEnZfBKqA4eT6Bn0eRZuP6lNzL-IL3VD)
         - [Single Mandarin speaker FastSpeech](https://drive.google.com/open?id=1Ol_048Tuy6BgvYm1RpjhOX4HfhUeBqdK)
         - [Multi Japanese speaker Transformer](https://drive.google.com/open?id=1fFMQDF6NV5Ysz48QLFYE8fEvbAxCsMBw)
         - [Single English speaker models with Parallel WaveGAN](https://drive.google.com/open?id=1HvB0_LDf1PVinJdehiuCt5gWmXGguqtx)
         - [Single English speaker knowledge distillation-based FastSpeech](https://drive.google.com/open?id=1wG-Y0itVYalxuLAHdkAHO7w1CWFfRPF4)
         
-        You can download all of the pretrained models and generated samples:
-        - [All of the pretrained E2E-TTS models](https://drive.google.com/open?id=1k9RRyc06Zl0mM2A7mi-hxNiNMFb_YzTF)
+        You can download all of the pre-trained models and generated samples:
+        - [All of the pre-trained E2E-TTS models](https://drive.google.com/open?id=1k9RRyc06Zl0mM2A7mi-hxNiNMFb_YzTF)
         - [All of the generated samples](https://drive.google.com/open?id=1bQGuqH92xuxOX__reWLP4-cif0cbpMLX)
         
-        Note that in the generated samples we use the following vocoders: Griffin-Lim (**GL**), WaveNet vocoder (**WaveNet**), Parallel WaveGAN (**ParallelWaveGAN**), and MelGAN (**MelGAN**).
-        The neural vocoders are based on following repositories.
+        Note that in the generated samples, we use the following vocoders: Griffin-Lim (**GL**), WaveNet vocoder (**WaveNet**), Parallel WaveGAN (**ParallelWaveGAN**), and MelGAN (**MelGAN**).
+        The neural vocoders are based on the following repositories.
         - [kan-bayashi/ParallelWaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN): Parallel WaveGAN / MelGAN / Multi-band MelGAN
         - [r9y9/wavenet_vocoder](https://github.com/r9y9/wavenet_vocoder): 16 bit mixture of Logistics WaveNet vocoder
         - [kan-bayashi/PytorchWaveNetVocoder](https://github.com/kan-bayashi/PytorchWaveNetVocoder): 8 bit Softmax WaveNet Vocoder with the noise shaping
         
         If you want to build your own neural vocoder, please check the above repositories.
         [kan-bayashi/ParallelWaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN) provides [the manual](https://github.com/kan-bayashi/ParallelWaveGAN#decoding-with-espnet-tts-models-features) about how to decode ESPnet-TTS model's features with neural vocoders. Please check it.
         
-        Here we list all of the pretrained neural vocoders. Please download and enjoy the generation of high quality speech!
+        Here we list all of the pre-trained neural vocoders. Please download and enjoy the generation of high-quality speech!
         
         | Model link                                                                                           | Lang  | Fs [Hz] | Mel range [Hz] | FFT / Shift / Win [pt] | Model type                                                              |
         | :--------------------------------------------------------------------------------------------------- | :---: | :-----: | :------------: | :--------------------: | :---------------------------------------------------------------------- |
         | [ljspeech.wavenet.softmax.ns.v1](https://drive.google.com/open?id=1eA1VcRS9jzFa-DovyTgJLQ_jmwOLIi8L) |  EN   | 22.05k  |      None      |   1024 / 256 / None    | [Softmax WaveNet](https://github.com/kan-bayashi/PytorchWaveNetVocoder) |
         | [ljspeech.wavenet.mol.v1](https://drive.google.com/open?id=1sY7gEUg39QaO1szuN62-Llst9TrFno2t)        |  EN   | 22.05k  |      None      |   1024 / 256 / None    | [MoL WaveNet](https://github.com/r9y9/wavenet_vocoder)                  |
         | [ljspeech.parallel_wavegan.v1](https://drive.google.com/open?id=1tv9GKyRT4CDsvUWKwH3s_OfXkiTi0gw7)   |  EN   | 22.05k  |      None      |   1024 / 256 / None    | [Parallel WaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN)      |
         | [ljspeech.wavenet.mol.v2](https://drive.google.com/open?id=1es2HuKUeKVtEdq6YDtAsLNpqCy4fhIXr)        |  EN   | 22.05k  |    80-7600     |   1024 / 256 / None    | [MoL WaveNet](https://github.com/r9y9/wavenet_vocoder)                  |
@@ -534,15 +530,15 @@
         | [ljspeech.melgan.v3](https://drive.google.com/open?id=1_a8faVA5OGCzIcJNw4blQYjfG4oA9VEt)             |  EN   | 22.05k  |    80-7600     |   1024 / 256 / None    | [MelGAN](https://github.com/kan-bayashi/ParallelWaveGAN)                |
         | [libritts.wavenet.mol.v1](https://drive.google.com/open?id=1jHUUmQFjWiQGyDd7ZeiCThSjjpbF_B4h)        |  EN   |   24k   |      None      |   1024 / 256 / None    | [MoL WaveNet](https://github.com/r9y9/wavenet_vocoder)                  |
         | [jsut.wavenet.mol.v1](https://drive.google.com/open?id=187xvyNbmJVZ0EZ1XHCdyjZHTXK9EcfkK)            |  JP   |   24k   |    80-7600     |   2048 / 300 / 1200    | [MoL WaveNet](https://github.com/r9y9/wavenet_vocoder)                  |
         | [jsut.parallel_wavegan.v1](https://drive.google.com/open?id=1OwrUQzAmvjj1x9cDhnZPp6dqtsEqGEJM)       |  JP   |   24k   |    80-7600     |   2048 / 300 / 1200    | [Parallel WaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN)      |
         | [csmsc.wavenet.mol.v1](https://drive.google.com/open?id=1PsjFRV5eUP0HHwBaRYya9smKy5ghXKzj)           |  ZH   |   24k   |    80-7600     |   2048 / 300 / 1200    | [MoL WaveNet](https://github.com/r9y9/wavenet_vocoder)                  |
         | [csmsc.parallel_wavegan.v1](https://drive.google.com/open?id=10M6H88jEUGbRWBmU1Ff2VaTmOAeL8CEy)      |  ZH   |   24k   |    80-7600     |   2048 / 300 / 1200    | [Parallel WaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN)      |
         
-        If you want to use the above pretrained vocoders, please exactly match the feature setting with them.
+        If you want to use the above pre-trained vocoders, please exactly match the feature setting with them.
         
         </div></details>
         
         ### TTS demo
         
         <details><summary>ESPnet2</summary><div>
         
@@ -560,78 +556,78 @@
         > NOTE: We are moving on ESPnet2-based development for TTS. Please check the latest demo in the above ESPnet2 demo.
         
         You can try the real-time demo in Google Colab.
         Please access the notebook from the following button and enjoy the real-time synthesis.
         
         - Real-time TTS demo with ESPnet1  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/espnet/notebook/blob/master/tts_realtime_demo.ipynb)
         
-        We also provide shell script to perform synthesize.
+        We also provide a shell script to perform synthesis.
         Go to a recipe directory and run `utils/synth_wav.sh` as follows:
         
         ```sh
-        # go to recipe directory and source path of espnet tools
+        # Go to recipe directory and source path of espnet tools
         cd egs/ljspeech/tts1 && . ./path.sh
-        # we use upper-case char sequence for the default model.
+        # We use an upper-case char sequence for the default model.
         echo "THIS IS A DEMONSTRATION OF TEXT TO SPEECH." > example.txt
         # let's synthesize speech!
         synth_wav.sh example.txt
         
-        # also you can use multiple sentences
+        # Also, you can use multiple sentences
         echo "THIS IS A DEMONSTRATION OF TEXT TO SPEECH." > example_multi.txt
         echo "TEXT TO SPEECH IS A TECHNIQUE TO CONVERT TEXT INTO SPEECH." >> example_multi.txt
         synth_wav.sh example_multi.txt
         ```
         
-        You can change the pretrained model as follows:
+        You can change the pre-trained model as follows:
         
         ```sh
         synth_wav.sh --models ljspeech.fastspeech.v1 example.txt
         ```
         
-        Waveform synthesis is performed with Griffin-Lim algorithm and neural vocoders (WaveNet and ParallelWaveGAN).
-        You can change the pretrained vocoder model as follows:
+        Waveform synthesis is performed with the Griffin-Lim algorithm and neural vocoders (WaveNet and ParallelWaveGAN).
+        You can change the pre-trained vocoder model as follows:
         
         ```sh
         synth_wav.sh --vocoder_models ljspeech.wavenet.mol.v1 example.txt
         ```
         
-        WaveNet vocoder provides very high quality speech but it takes time to generate.
+        WaveNet vocoder provides very high-quality speech, but it takes time to generate.
         
         See more details or available models via `--help`.
         
         ```sh
         synth_wav.sh --help
         ```
         
         </div></details>
         
         ### VC results
         
         <details><summary>expand</summary><div>
         
-        - Transformer and Tacotron2 based VC
+        - Transformer and Tacotron2-based VC
         
         You can listen to some samples on the [demo webpage](https://unilight.github.io/Publication-Demos/publications/transformer-vc/).
         
         - Cascade ASR+TTS as one of the baseline systems of VCC2020
         
         The [Voice Conversion Challenge 2020](http://www.vc-challenge.org/) (VCC2020) adopts ESPnet to build an end-to-end based baseline system.
-        In VCC2020, the objective is intra/cross lingual nonparallel VC.
+        In VCC2020, the objective is intra/cross-lingual nonparallel VC.
         You can download converted samples of the cascade ASR+TTS baseline system [here](https://drive.google.com/drive/folders/1oeZo83GrOgtqxGwF7KagzIrfjr8X59Ue?usp=sharing).
         
         </div></details>
         
         ### SLU results
         
         <details><summary>expand</summary><div>
         
         
-        We list the performance on various SLU tasks and dataset using the metric reported in the original dataset paper
+        We list the performance on various SLU tasks and datasets using the metric reported in the original dataset paper
         
-        | Task                                                              | Dataset                                                              |    Metric     |     Result     |                                                                              Pretrained Model                                         |
+        | Task                                                              | Dataset                                                              |    Metric     |     Result     |                                                                              Pre-trained Model                                         |
         | ----------------------------------------------------------------- | :-------------: | :-------------: | :-------------: | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
         | Intent Classification                                                 |     SLURP     |       Acc       |       86.3       |                [link](https://github.com/espnet/espnet/tree/master/egs2/slurp/asr1/README.md)                |
         | Intent Classification                                                   |     FSC     |       Acc       |       99.6       |                [link](https://github.com/espnet/espnet/tree/master/egs2/fsc/asr1/README.md)                |
         | Intent Classification                                                  |     FSC Unseen Speaker Set     |       Acc       |       98.6       |                [link](https://github.com/espnet/espnet/tree/master/egs2/fsc_unseen/asr1/README.md)                |
         | Intent Classification                                                   |     FSC Unseen Utterance Set     |       Acc       |       86.4       |                [link](https://github.com/espnet/espnet/tree/master/egs2/fsc_unseen/asr1/README.md)                |
         | Intent Classification                                                   |     FSC Challenge Speaker Set     |       Acc       |       97.5       |                [link](https://github.com/espnet/espnet/tree/master/egs2/fsc_challenge/asr1/README.md)                |
         | Intent Classification                                                   |     FSC Challenge Utterance Set     |       Acc       |       78.5       |                [link](https://github.com/espnet/espnet/tree/master/egs2/fsc_challenge/asr1/README.md)                |
@@ -656,15 +652,15 @@
         ### CTC Segmentation demo
         
         <details><summary>ESPnet1</summary><div>
         
         [CTC segmentation](https://arxiv.org/abs/2007.09127) determines utterance segments within audio files.
         Aligned utterance segments constitute the labels of speech datasets.
         
-        As demo, we align start and end of utterances within the audio file `ctc_align_test.wav`, using the example script `utils/asr_align_wav.sh`.
+        As a demo, we align the start and end of utterances within the audio file `ctc_align_test.wav`, using the example script `utils/asr_align_wav.sh`.
         For preparation, set up a data directory:
         
         ```sh
         cd egs/tedlium2/align1/
         # data directory
         align_dir=data/demo
         mkdir -p ${align_dir}
@@ -694,36 +690,36 @@
         ../../../utils/asr_align_wav.sh \
             --models ${model} \
             --align_dir ${align_dir} \
             --align_config ${align_dir}/align.yaml \
             ${wav} ${align_dir}/utt_text
         ```
         
-        Segments are written to `aligned_segments` as a list of file/utterance name, utterance start and end times in seconds and a confidence score.
-        The confidence score is a probability in log space that indicates how good the utterance was aligned. If needed, remove bad utterances:
+        Segments are written to `aligned_segments` as a list of file/utterance names, utterance start and end times in seconds, and a confidence score.
+        The confidence score is a probability in log space that indicates how well the utterance was aligned. If needed, remove bad utterances:
         
         ```sh
         min_confidence_score=-5
         awk -v ms=${min_confidence_score} '{ if ($5 > ms) {print} }' ${align_dir}/aligned_segments
         ```
         
-        The demo script `utils/ctc_align_wav.sh` uses an already pretrained ASR model (see list above for more models).
+        The demo script `utils/ctc_align_wav.sh` uses an already pre-trained ASR model (see the list above for more models).
         It is recommended to use models with RNN-based encoders (such as BLSTMP) for aligning large audio files;
-        rather than using Transformer models that have a high memory consumption on longer audio data.
+        rather than using Transformer models with a high memory consumption on longer audio data.
         The sample rate of the audio must be consistent with that of the data used in training; adjust with `sox` if needed.
         A full example recipe is in `egs/tedlium2/align1/`.
         
         </div></details>
         
         <details><summary>ESPnet2</summary><div>
         
         [CTC segmentation](https://arxiv.org/abs/2007.09127) determines utterance segments within audio files.
         Aligned utterance segments constitute the labels of speech datasets.
         
-        As demo, we align start and end of utterances within the audio file `ctc_align_test.wav`.
+        As a demo, we align the start and end of utterances within the audio file `ctc_align_test.wav`.
         This can be done either directly from the Python command line or using the script `espnet2/bin/asr_align.py`.
         
         From the Python command line interface:
         
         ```python
         # load a model with character tokens
         from espnet_model_zoo.downloader import ModelDownloader
@@ -747,29 +743,29 @@
         # utt2 utt 1.73 3.19 -0.7674 IS PART OF HOLIDAY'S STRATEGY
         # utt3 utt 3.19 4.20 -0.7433 TO SELL OFF ASSETS
         # utt4 utt 4.20 6.10 -0.4899 AND CONCENTRATE ON PROPERTY MANAGEMENT
         ```
         
         Aligning also works with fragments of the text.
         For this, set the `gratis_blank` option that allows skipping unrelated audio sections without penalty.
-        It's also possible to omit the utterance names at the beginning of each line, by setting `kaldi_style_text` to False.
+        It's also possible to omit the utterance names at the beginning of each line by setting `kaldi_style_text` to False.
         
         ```python
         aligner.set_config( gratis_blank=True, kaldi_style_text=False )
         text = ["SALE OF THE HOTELS", "PROPERTY MANAGEMENT"]
         segments = aligner(speech, text)
         print(segments)
         # utt_0000 utt 0.37 1.72 -2.0651 SALE OF THE HOTELS
         # utt_0001 utt 4.70 6.10 -5.0566 PROPERTY MANAGEMENT
         ```
         
         The script `espnet2/bin/asr_align.py` uses a similar interface. To align utterances:
         
         ```sh
-        # ASR model and config files from pretrained model (e.g. from cachedir):
+        # ASR model and config files from pre-trained model (e.g., from cachedir):
         asr_config=<path-to-model>/config.yaml
         asr_model=<path-to-model>/valid.*best.pth
         # prepare the text file
         wav="test_utils/ctc_align_test.wav"
         text="test_utils/ctc_align_text.txt"
         cat << EOF > ${text}
         utt1 THE SALE OF THE HOTELS
@@ -784,16 +780,16 @@
         # utt2 ctc_align_test 1.73 3.19 -0.7674 IS PART OF HOLIDAY'S STRATEGY
         # utt3 ctc_align_test 3.19 4.20 -0.7433 TO SELL OFF ASSETS
         # utt4 ctc_align_test 4.20 4.97 -0.6017 AND CONCENTRATE
         # utt5 ctc_align_test 4.97 6.10 -0.3477 ON PROPERTY MANAGEMENT
         ```
         
         The output of the script can be redirected to a `segments` file by adding the argument `--output segments`.
-        Each line contains file/utterance name, utterance start and end times in seconds and a confidence score; optionally also the utterance text.
-        The confidence score is a probability in log space that indicates how good the utterance was aligned. If needed, remove bad utterances:
+        Each line contains the file/utterance name, utterance start and end times in seconds, and a confidence score; optionally also the utterance text.
+        The confidence score is a probability in log space that indicates how well the utterance was aligned. If needed, remove bad utterances:
         
         ```sh
         min_confidence_score=-7
         # here, we assume that the output was written to the file `segments`
         awk -v ms=${min_confidence_score} '{ if ($5 > ms) {print} }' segments
         ```
         
@@ -839,14 +835,20 @@
             month = jul,
             year = "2020",
             address = "Online",
             publisher = "Association for Computational Linguistics",
             url = "https://www.aclweb.org/anthology/2020.acl-demos.34",
             pages = "302--311",
         }
+        @article{hayashi2021espnet2,
+          title={Espnet2-tts: Extending the edge of tts research},
+          author={Hayashi, Tomoki and Yamamoto, Ryuichi and Yoshimura, Takenori and Wu, Peter and Shi, Jiatong and Saeki, Takaaki and Ju, Yooncheol and Yasuda, Yusuke and Takamichi, Shinnosuke and Watanabe, Shinji},
+          journal={arXiv preprint arXiv:2110.07840},
+          year={2021}
+        }
         @inproceedings{li2020espnet,
           title={{ESPnet-SE}: End-to-End Speech Enhancement and Separation Toolkit Designed for {ASR} Integration},
           author={Chenda Li and Jing Shi and Wangyou Zhang and Aswin Shanmugam Subramanian and Xuankai Chang and Naoyuki Kamo and Moto Hira and Tomoki Hayashi and Christoph Boeddeker and Zhuo Chen and Shinji Watanabe},
           booktitle={Proceedings of IEEE Spoken Language Technology Workshop (SLT)},
           pages={785--792},
           year={2021},
           organization={IEEE},
@@ -863,14 +865,21 @@
           author={Shi, Jiatong and Guo, Shuai and Qian, Tao and Huo, Nan and Hayashi, Tomoki and Wu, Yuning and Xu, Frank and Chang, Xuankai and Li, Huazhe and Wu, Peter and Watanabe, Shinji and Jin, Qin},
           title={{Muskits}: an End-to-End Music Processing Toolkit for Singing Voice Synthesis},
           year={2022},
           booktitle={Proceedings of Interspeech},
           pages={4277-4281},
           url={https://www.isca-speech.org/archive/pdfs/interspeech_2022/shi22d_interspeech.pdf}
         }
+        @inproceedings{lu22c_interspeech,
+          author={Yen-Ju Lu and Xuankai Chang and Chenda Li and Wangyou Zhang and Samuele Cornell and Zhaoheng Ni and Yoshiki Masuyama and Brian Yan and Robin Scheibler and Zhong-Qiu Wang and Yu Tsao and Yanmin Qian and Shinji Watanabe},
+          title={{ESPnet-SE++: Speech Enhancement for Robust Speech Recognition, Translation, and Understanding}},
+          year=2022,
+          booktitle={Proc. Interspeech 2022},
+          pages={5458--5462},
+        }
         @article{gao2022euro,
           title={{EURO}: {ESPnet} Unsupervised ASR Open-source Toolkit},
           author={Gao, Dongji and Shi, Jiatong and Chuang, Shun-Po and Garcia, Leibny Paola and Lee, Hung-yi and Watanabe, Shinji and Khudanpur, Sanjeev},
           journal={arXiv preprint arXiv:2211.17196},
           year={2022}
         }
         ```
```

### Comparing `espnet-202304/espnet.egg-info/SOURCES.txt` & `espnet-202308/espnet.egg-info/SOURCES.txt`

 * *Files 4% similar despite different names*

```diff
@@ -49,14 +49,15 @@
 espnet/nets/__init__.py
 espnet/nets/asr_interface.py
 espnet/nets/batch_beam_search.py
 espnet/nets/batch_beam_search_online.py
 espnet/nets/batch_beam_search_online_sim.py
 espnet/nets/beam_search.py
 espnet/nets/beam_search_timesync.py
+espnet/nets/beam_search_timesync_streaming.py
 espnet/nets/beam_search_transducer.py
 espnet/nets/ctc_prefix_score.py
 espnet/nets/e2e_asr_common.py
 espnet/nets/e2e_mt_common.py
 espnet/nets/lm_interface.py
 espnet/nets/mt_interface.py
 espnet/nets/scorer_interface.py
@@ -239,14 +240,15 @@
 espnet/utils/training/evaluator.py
 espnet/utils/training/iterators.py
 espnet/utils/training/tensorboard_logger.py
 espnet/utils/training/train_utils.py
 espnet2/__init__.py
 espnet2/asr/__init__.py
 espnet2/asr/ctc.py
+espnet2/asr/discrete_asr_espnet_model.py
 espnet2/asr/espnet_model.py
 espnet2/asr/maskctc_model.py
 espnet2/asr/pit_espnet_model.py
 espnet2/asr/decoder/__init__.py
 espnet2/asr/decoder/abs_decoder.py
 espnet2/asr/decoder/hugging_face_transformers_decoder.py
 espnet2/asr/decoder/mlm_decoder.py
@@ -259,14 +261,15 @@
 espnet2/asr/encoder/abs_encoder.py
 espnet2/asr/encoder/branchformer_encoder.py
 espnet2/asr/encoder/conformer_encoder.py
 espnet2/asr/encoder/contextual_block_conformer_encoder.py
 espnet2/asr/encoder/contextual_block_transformer_encoder.py
 espnet2/asr/encoder/e_branchformer_encoder.py
 espnet2/asr/encoder/hubert_encoder.py
+espnet2/asr/encoder/hugging_face_transformers_encoder.py
 espnet2/asr/encoder/longformer_encoder.py
 espnet2/asr/encoder/rnn_encoder.py
 espnet2/asr/encoder/transformer_encoder.py
 espnet2/asr/encoder/transformer_encoder_multispkr.py
 espnet2/asr/encoder/vgg_rnn_encoder.py
 espnet2/asr/encoder/wav2vec2_encoder.py
 espnet2/asr/encoder/whisper_encoder.py
@@ -279,14 +282,15 @@
 espnet2/asr/frontend/windowing.py
 espnet2/asr/layers/__init__.py
 espnet2/asr/layers/cgmlp.py
 espnet2/asr/layers/fastformer.py
 espnet2/asr/postencoder/__init__.py
 espnet2/asr/postencoder/abs_postencoder.py
 espnet2/asr/postencoder/hugging_face_transformers_postencoder.py
+espnet2/asr/postencoder/length_adaptor_postencoder.py
 espnet2/asr/preencoder/__init__.py
 espnet2/asr/preencoder/abs_preencoder.py
 espnet2/asr/preencoder/linear.py
 espnet2/asr/preencoder/sinc.py
 espnet2/asr/specaug/__init__.py
 espnet2/asr/specaug/abs_specaug.py
 espnet2/asr/specaug/specaug.py
@@ -301,14 +305,15 @@
 espnet2/asr/state_spaces/pool.py
 espnet2/asr/state_spaces/registry.py
 espnet2/asr/state_spaces/residual.py
 espnet2/asr/state_spaces/s4.py
 espnet2/asr/state_spaces/utils.py
 espnet2/asr/transducer/__init__.py
 espnet2/asr/transducer/beam_search_transducer.py
+espnet2/asr/transducer/beam_search_transducer_streaming.py
 espnet2/asr/transducer/error_calculator.py
 espnet2/asr/transducer/rnnt_multi_blank/__init__.py
 espnet2/asr/transducer/rnnt_multi_blank/rnnt.py
 espnet2/asr/transducer/rnnt_multi_blank/rnnt_multi_blank.py
 espnet2/asr/transducer/rnnt_multi_blank/utils/__init__.py
 espnet2/asr/transducer/rnnt_multi_blank/utils/global_constants.py
 espnet2/asr/transducer/rnnt_multi_blank/utils/rnnt_helper.py
@@ -320,44 +325,72 @@
 espnet2/asr/transducer/rnnt_multi_blank/utils/cuda_utils/reduce.py
 espnet2/asr_transducer/__init__.py
 espnet2/asr_transducer/activation.py
 espnet2/asr_transducer/beam_search_transducer.py
 espnet2/asr_transducer/error_calculator.py
 espnet2/asr_transducer/espnet_transducer_model.py
 espnet2/asr_transducer/joint_network.py
+espnet2/asr_transducer/normalization.py
 espnet2/asr_transducer/utils.py
 espnet2/asr_transducer/decoder/__init__.py
 espnet2/asr_transducer/decoder/abs_decoder.py
+espnet2/asr_transducer/decoder/mega_decoder.py
 espnet2/asr_transducer/decoder/rnn_decoder.py
+espnet2/asr_transducer/decoder/rwkv_decoder.py
 espnet2/asr_transducer/decoder/stateless_decoder.py
+espnet2/asr_transducer/decoder/blocks/__init__.py
+espnet2/asr_transducer/decoder/blocks/mega.py
+espnet2/asr_transducer/decoder/blocks/rwkv.py
+espnet2/asr_transducer/decoder/modules/__init__.py
+espnet2/asr_transducer/decoder/modules/mega/__init__.py
+espnet2/asr_transducer/decoder/modules/mega/feed_forward.py
+espnet2/asr_transducer/decoder/modules/mega/multi_head_damped_ema.py
+espnet2/asr_transducer/decoder/modules/mega/positional_bias.py
+espnet2/asr_transducer/decoder/modules/rwkv/__init__.py
+espnet2/asr_transducer/decoder/modules/rwkv/attention.py
+espnet2/asr_transducer/decoder/modules/rwkv/feed_forward.py
 espnet2/asr_transducer/encoder/__init__.py
 espnet2/asr_transducer/encoder/building.py
 espnet2/asr_transducer/encoder/encoder.py
 espnet2/asr_transducer/encoder/validation.py
 espnet2/asr_transducer/encoder/blocks/__init__.py
 espnet2/asr_transducer/encoder/blocks/branchformer.py
 espnet2/asr_transducer/encoder/blocks/conformer.py
 espnet2/asr_transducer/encoder/blocks/conv1d.py
 espnet2/asr_transducer/encoder/blocks/conv_input.py
+espnet2/asr_transducer/encoder/blocks/ebranchformer.py
 espnet2/asr_transducer/encoder/modules/__init__.py
 espnet2/asr_transducer/encoder/modules/attention.py
 espnet2/asr_transducer/encoder/modules/convolution.py
 espnet2/asr_transducer/encoder/modules/multi_blocks.py
-espnet2/asr_transducer/encoder/modules/normalization.py
 espnet2/asr_transducer/encoder/modules/positional_encoding.py
+espnet2/asr_transducer/frontend/__init__.py
+espnet2/asr_transducer/frontend/online_audio_processor.py
+espnet2/asvspoof/__init__.py
+espnet2/asvspoof/espnet_model.py
+espnet2/asvspoof/decoder/__init__.py
+espnet2/asvspoof/decoder/abs_decoder.py
+espnet2/asvspoof/decoder/linear_decoder.py
+espnet2/asvspoof/loss/__init__.py
+espnet2/asvspoof/loss/abs_loss.py
+espnet2/asvspoof/loss/am_softmax_loss.py
+espnet2/asvspoof/loss/binary_loss.py
+espnet2/asvspoof/loss/oc_softmax_loss.py
 espnet2/bin/__init__.py
 espnet2/bin/aggregate_stats_dirs.py
 espnet2/bin/asr_align.py
 espnet2/bin/asr_inference.py
 espnet2/bin/asr_inference_k2.py
 espnet2/bin/asr_inference_maskctc.py
 espnet2/bin/asr_inference_streaming.py
 espnet2/bin/asr_train.py
 espnet2/bin/asr_transducer_inference.py
 espnet2/bin/asr_transducer_train.py
+espnet2/bin/asvspoof_inference.py
+espnet2/bin/asvspoof_train.py
 espnet2/bin/diar_inference.py
 espnet2/bin/diar_train.py
 espnet2/bin/enh_inference.py
 espnet2/bin/enh_inference_streaming.py
 espnet2/bin/enh_s2t_train.py
 espnet2/bin/enh_scoring.py
 espnet2/bin/enh_train.py
@@ -372,14 +405,15 @@
 espnet2/bin/lm_inference.py
 espnet2/bin/lm_train.py
 espnet2/bin/mt_inference.py
 espnet2/bin/mt_train.py
 espnet2/bin/pack.py
 espnet2/bin/slu_inference.py
 espnet2/bin/slu_train.py
+espnet2/bin/spk_train.py
 espnet2/bin/split_scps.py
 espnet2/bin/st_inference.py
 espnet2/bin/st_inference_streaming.py
 espnet2/bin/st_train.py
 espnet2/bin/svs_inference.py
 espnet2/bin/svs_train.py
 espnet2/bin/tokenize_text.py
@@ -430,14 +464,15 @@
 espnet2/enh/layers/beamformer_th.py
 espnet2/enh/layers/complex_utils.py
 espnet2/enh/layers/complexnn.py
 espnet2/enh/layers/conv_utils.py
 espnet2/enh/layers/dc_crn.py
 espnet2/enh/layers/dnn_beamformer.py
 espnet2/enh/layers/dnn_wpe.py
+espnet2/enh/layers/dnsmos.py
 espnet2/enh/layers/dpmulcat.py
 espnet2/enh/layers/dprnn.py
 espnet2/enh/layers/dptnet.py
 espnet2/enh/layers/fasnet.py
 espnet2/enh/layers/ifasnet.py
 espnet2/enh/layers/mask_estimator.py
 espnet2/enh/layers/skim.py
@@ -486,24 +521,34 @@
 espnet2/fileio/sound_scp.py
 espnet2/fileio/vad_scp.py
 espnet2/fst/__init__.py
 espnet2/fst/lm_rescore.py
 espnet2/gan_svs/__init__.py
 espnet2/gan_svs/abs_gan_svs.py
 espnet2/gan_svs/espnet_model.py
+espnet2/gan_svs/avocodo/__init__.py
+espnet2/gan_svs/avocodo/avocodo.py
 espnet2/gan_svs/joint/__init__.py
 espnet2/gan_svs/joint/joint_score2wav.py
+espnet2/gan_svs/uhifigan/__init__.py
+espnet2/gan_svs/uhifigan/sine_generator.py
+espnet2/gan_svs/uhifigan/uhifigan.py
+espnet2/gan_svs/utils/__init__.py
+espnet2/gan_svs/utils/expand_f0.py
+espnet2/gan_svs/visinger2/__init__.py
+espnet2/gan_svs/visinger2/ddsp.py
+espnet2/gan_svs/visinger2/visinger2_vocoder.py
 espnet2/gan_svs/vits/__init__.py
 espnet2/gan_svs/vits/duration_predictor.py
-espnet2/gan_svs/vits/frame_prior_net.py
 espnet2/gan_svs/vits/generator.py
 espnet2/gan_svs/vits/length_regulator.py
 espnet2/gan_svs/vits/modules.py
 espnet2/gan_svs/vits/phoneme_predictor.py
 espnet2/gan_svs/vits/pitch_predictor.py
+espnet2/gan_svs/vits/prior_decoder.py
 espnet2/gan_svs/vits/text_encoder.py
 espnet2/gan_svs/vits/vits.py
 espnet2/gan_tts/__init__.py
 espnet2/gan_tts/abs_gan_tts.py
 espnet2/gan_tts/espnet_model.py
 espnet2/gan_tts/hifigan/__init__.py
 espnet2/gan_tts/hifigan/hifigan.py
@@ -586,61 +631,85 @@
 espnet2/samplers/folded_batch_sampler.py
 espnet2/samplers/length_batch_sampler.py
 espnet2/samplers/num_elements_batch_sampler.py
 espnet2/samplers/sorted_batch_sampler.py
 espnet2/samplers/unsorted_batch_sampler.py
 espnet2/schedulers/__init__.py
 espnet2/schedulers/abs_scheduler.py
+espnet2/schedulers/cosine_anneal_warmup_restart.py
 espnet2/schedulers/noam_lr.py
 espnet2/schedulers/warmup_lr.py
+espnet2/schedulers/warmup_reducelronplateau.py
 espnet2/schedulers/warmup_step_lr.py
 espnet2/slu/__init__.py
 espnet2/slu/espnet_model.py
 espnet2/slu/postdecoder/__init__.py
 espnet2/slu/postdecoder/abs_postdecoder.py
 espnet2/slu/postdecoder/hugging_face_transformers_postdecoder.py
 espnet2/slu/postencoder/__init__.py
 espnet2/slu/postencoder/conformer_postencoder.py
 espnet2/slu/postencoder/transformer_postencoder.py
+espnet2/spk/__init__.py
+espnet2/spk/espnet_model.py
+espnet2/spk/encoder/__init__.py
+espnet2/spk/encoder/rawnet3_encoder.py
+espnet2/spk/layers/RawNetBasicBlock.py
+espnet2/spk/layers/__init__.py
+espnet2/spk/loss/__init__.py
+espnet2/spk/loss/aamsoftmax.py
+espnet2/spk/loss/abs_loss.py
+espnet2/spk/pooling/__init__.py
+espnet2/spk/pooling/abs_pooling.py
+espnet2/spk/pooling/chn_attn_stat_pooling.py
+espnet2/spk/projector/__init__.py
+espnet2/spk/projector/abs_projector.py
+espnet2/spk/projector/rawnet3_projector.py
 espnet2/st/__init__.py
 espnet2/st/espnet_model.py
 espnet2/svs/__init__.py
 espnet2/svs/abs_svs.py
 espnet2/svs/espnet_model.py
 espnet2/svs/feats_extract/__init__.py
 espnet2/svs/feats_extract/score_feats_extract.py
 espnet2/svs/naive_rnn/__init__.py
 espnet2/svs/naive_rnn/naive_rnn.py
 espnet2/svs/naive_rnn/naive_rnn_dp.py
+espnet2/svs/singing_tacotron/__init__.py
+espnet2/svs/singing_tacotron/decoder.py
+espnet2/svs/singing_tacotron/encoder.py
+espnet2/svs/singing_tacotron/singing_tacotron.py
 espnet2/svs/xiaoice/XiaoiceSing.py
 espnet2/svs/xiaoice/__init__.py
 espnet2/svs/xiaoice/loss.py
 espnet2/tasks/__init__.py
 espnet2/tasks/abs_task.py
 espnet2/tasks/asr.py
 espnet2/tasks/asr_transducer.py
+espnet2/tasks/asvspoof.py
 espnet2/tasks/diar.py
 espnet2/tasks/enh.py
 espnet2/tasks/enh_s2t.py
 espnet2/tasks/enh_tse.py
 espnet2/tasks/gan_svs.py
 espnet2/tasks/gan_tts.py
 espnet2/tasks/hubert.py
 espnet2/tasks/lm.py
 espnet2/tasks/mt.py
 espnet2/tasks/slu.py
+espnet2/tasks/spk.py
 espnet2/tasks/st.py
 espnet2/tasks/svs.py
 espnet2/tasks/tts.py
 espnet2/tasks/uasr.py
 espnet2/text/__init__.py
 espnet2/text/abs_tokenizer.py
 espnet2/text/build_tokenizer.py
 espnet2/text/char_tokenizer.py
 espnet2/text/cleaner.py
+espnet2/text/hugging_face_token_id_converter.py
 espnet2/text/hugging_face_tokenizer.py
 espnet2/text/korean_cleaner.py
 espnet2/text/phoneme_tokenizer.py
 espnet2/text/sentencepiece_tokenizer.py
 espnet2/text/token_id_converter.py
 espnet2/text/whisper_token_id_converter.py
 espnet2/text/whisper_tokenizer.py
@@ -663,14 +732,15 @@
 espnet2/train/collate_fn.py
 espnet2/train/dataset.py
 espnet2/train/distributed_utils.py
 espnet2/train/gan_trainer.py
 espnet2/train/iterable_dataset.py
 espnet2/train/preprocessor.py
 espnet2/train/reporter.py
+espnet2/train/spk_trainer.py
 espnet2/train/trainer.py
 espnet2/train/uasr_trainer.py
 espnet2/tts/__init__.py
 espnet2/tts/abs_tts.py
 espnet2/tts/espnet_model.py
 espnet2/tts/fastspeech/__init__.py
 espnet2/tts/fastspeech/fastspeech.py
@@ -681,14 +751,16 @@
 espnet2/tts/feats_extract/__init__.py
 espnet2/tts/feats_extract/abs_feats_extract.py
 espnet2/tts/feats_extract/dio.py
 espnet2/tts/feats_extract/energy.py
 espnet2/tts/feats_extract/linear_spectrogram.py
 espnet2/tts/feats_extract/log_mel_fbank.py
 espnet2/tts/feats_extract/log_spectrogram.py
+espnet2/tts/feats_extract/yin.py
+espnet2/tts/feats_extract/ying.py
 espnet2/tts/gst/__init__.py
 espnet2/tts/gst/style_encoder.py
 espnet2/tts/prodiff/__init__.py
 espnet2/tts/prodiff/denoiser.py
 espnet2/tts/prodiff/loss.py
 espnet2/tts/prodiff/prodiff.py
 espnet2/tts/tacotron2/__init__.py
@@ -716,27 +788,29 @@
 espnet2/uasr/segmenter/__init__.py
 espnet2/uasr/segmenter/abs_segmenter.py
 espnet2/uasr/segmenter/join_segmenter.py
 espnet2/uasr/segmenter/random_segmenter.py
 espnet2/utils/__init__.py
 espnet2/utils/build_dataclass.py
 espnet2/utils/config_argparse.py
+espnet2/utils/eer.py
 espnet2/utils/get_default_kwargs.py
 espnet2/utils/griffin_lim.py
 espnet2/utils/kwargs2args.py
 espnet2/utils/nested_dict_action.py
 espnet2/utils/sized_dict.py
 espnet2/utils/types.py
 espnet2/utils/yaml_no_alias_safe_dump.py
 test/test_asr_init.py
 test/test_asr_interface.py
 test/test_asr_quantize.py
 test/test_batch_beam_search.py
 test/test_beam_search.py
 test/test_beam_search_timesync.py
+test/test_beam_search_timesync_streaming.py
 test/test_cli.py
 test/test_custom_transducer.py
 test/test_distributed_launch.py
 test/test_e2e_asr.py
 test/test_e2e_asr_conformer.py
 test/test_e2e_asr_maskctc.py
 test/test_e2e_asr_mulenc.py
@@ -754,14 +828,15 @@
 test/test_e2e_vc_tacotron2.py
 test/test_e2e_vc_transformer.py
 test/test_initialization.py
 test/test_io_voxforge.py
 test/test_lm.py
 test/test_loss.py
 test/test_multi_spkrs.py
+test/test_nets_utils.py
 test/test_ngram.py
 test/test_optimizer.py
 test/test_positional_encoding.py
 test/test_recog.py
 test/test_scheduler.py
 test/test_sentencepiece.py
 test/test_tensorboard.py
```

### Comparing `espnet-202304/espnet.egg-info/requires.txt` & `espnet-202308/espnet.egg-info/requires.txt`

 * *Files 6% similar despite different names*

```diff
@@ -10,44 +10,46 @@
 PyYAML>=5.1.2
 soundfile>=0.10.2
 h5py>=2.10.0
 kaldiio>=2.18.0
 torch>=1.3.0
 torch_complex
 nltk>=3.4.5
-numpy
+numpy<1.24
 protobuf<=3.20.1
 hydra-core
 opt-einsum
 sentencepiece==0.1.97
 ctc-segmentation>=1.6.6
-pyworld>=0.2.10
+pyworld>=0.3.4
 pypinyin<=0.44.0
 espnet_tts_frontend
 ci_sdr
 pytorch_wpe
 fast-bss-eval==0.1.3
+asteroid_filterbanks==0.4.0
 editdistance
 importlib-metadata<5.0
 
 [all]
 torchaudio
 torch_optimizer
 fairscale
 transformers
 gtn==0.0.0
+evaluate
 matplotlib
-pillow>=6.1.0
+pillow==9.5.0
 editdistance==0.5.2
 wandb
 tensorboard>=1.14
 espnet_model_zoo
 gdown
 resampy
-pysptk>=0.1.17
+pysptk>=0.2.1
 morfessor
 youtube_dl
 nnmnkwii
 museval>=0.2.1
 pystoi>=0.2.2
 mir-eval>=0.6
 fastdtw
@@ -66,15 +68,15 @@
 nbsphinx>=0.4.2
 sphinx-markdown-tables>=0.0.12
 
 [recipe]
 espnet_model_zoo
 gdown
 resampy
-pysptk>=0.1.17
+pysptk>=0.2.1
 morfessor
 youtube_dl
 nnmnkwii
 museval>=0.2.1
 pystoi>=0.2.2
 mir-eval>=0.6
 fastdtw
@@ -92,20 +94,19 @@
 mock>=2.0.0
 pycodestyle
 jsondiff<2.0.0,>=1.2.0
 flake8>=3.7.8
 flake8-docstrings>=1.3.1
 black
 isort
-music21
 matplotlib
-pillow>=6.1.0
+pillow==9.5.0
 editdistance==0.5.2
 wandb
 tensorboard>=1.14
 
 [train]
 matplotlib
-pillow>=6.1.0
+pillow==9.5.0
 editdistance==0.5.2
 wandb
 tensorboard>=1.14
```

### Comparing `espnet-202304/espnet2/asr/ctc.py` & `espnet-202308/espnet2/enh/layers/adapt_layers.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,127 +1,128 @@
-import torch
-import torch.nn.functional as F
-from typeguard import check_argument_types
-
-
-class CTC(torch.nn.Module):
-    """CTC module.
-
-    Args:
-        odim: dimension of outputs
-        encoder_output_size: number of encoder projection units
-        dropout_rate: dropout rate (0.0 ~ 1.0)
-        ctc_type: builtin or gtnctc
-        reduce: reduce the CTC loss into a scalar
-        ignore_nan_grad: Same as zero_infinity (keeping for backward compatiblity)
-        zero_infinity:  Whether to zero infinite losses and the associated gradients.
-    """
-
-    def __init__(
-        self,
-        odim: int,
-        encoder_output_size: int,
-        dropout_rate: float = 0.0,
-        ctc_type: str = "builtin",
-        reduce: bool = True,
-        ignore_nan_grad: bool = None,
-        zero_infinity: bool = True,
-    ):
-        assert check_argument_types()
-        super().__init__()
-        eprojs = encoder_output_size
-        self.dropout_rate = dropout_rate
-        self.ctc_lo = torch.nn.Linear(eprojs, odim)
-        self.ctc_type = ctc_type
-        if ignore_nan_grad is not None:
-            zero_infinity = ignore_nan_grad
-
-        if self.ctc_type == "builtin":
-            self.ctc_loss = torch.nn.CTCLoss(
-                reduction="none", zero_infinity=zero_infinity
-            )
+# noqa E501: Ported from https://github.com/BUTSpeechFIT/speakerbeam/blob/main/src/models/adapt_layers.py
+# Copyright (c) 2021 Brno University of Technology
+# Copyright (c) 2021 Nippon Telegraph and Telephone corporation (NTT).
+# All rights reserved
+# By Katerina Zmolikova, August 2021.
 
-        elif self.ctc_type == "gtnctc":
-            from espnet.nets.pytorch_backend.gtn_ctc import GTNCTCLossFunction
+from functools import partial
 
-            self.ctc_loss = GTNCTCLossFunction.apply
-        else:
-            raise ValueError(f'ctc_type must be "builtin" or "gtnctc": {self.ctc_type}')
+import torch
+import torch.nn as nn
 
-        self.reduce = reduce
 
-    def loss_fn(self, th_pred, th_target, th_ilen, th_olen) -> torch.Tensor:
-        if self.ctc_type == "builtin":
-            th_pred = th_pred.log_softmax(2)
-            loss = self.ctc_loss(th_pred, th_target, th_ilen, th_olen)
-            size = th_pred.size(1)
-
-            if self.reduce:
-                # Batch-size average
-                loss = loss.sum() / size
-            else:
-                loss = loss / size
-            return loss
+def make_adapt_layer(type, indim, enrolldim, ninputs=1):
+    adapt_class = adaptation_layer_types.get(type)
+    return adapt_class(indim, enrolldim, ninputs)
+
+
+def into_tuple(x):
+    """Transforms tensor/list/tuple into tuple."""
+    if isinstance(x, list):
+        return tuple(x)
+    elif isinstance(x, torch.Tensor):
+        return (x,)
+    elif isinstance(x, tuple):
+        return x
+    else:
+        raise ValueError("x should be tensor, list of tuple")
+
+
+def into_orig_type(x, orig_type):
+    """Inverts into_tuple function."""
+    if orig_type is tuple:
+        return x
+    if orig_type is list:
+        return list(x)
+    if orig_type is torch.Tensor:
+        return x[0]
+    else:
+        assert False
 
-        elif self.ctc_type == "gtnctc":
-            log_probs = torch.nn.functional.log_softmax(th_pred, dim=2)
-            return self.ctc_loss(log_probs, th_target, th_ilen, 0, "none")
 
-        else:
-            raise NotImplementedError
+class ConcatAdaptLayer(nn.Module):
+    def __init__(self, indim, enrolldim, ninputs=1):
+        super().__init__()
+        self.ninputs = ninputs
+        self.transform = nn.ModuleList(
+            [nn.Linear(indim + enrolldim, indim) for _ in range(ninputs)]
+        )
 
-    def forward(self, hs_pad, hlens, ys_pad, ys_lens):
-        """Calculate CTC loss.
+    def forward(self, main, enroll):
+        """ConcatAdaptLayer forward.
 
         Args:
-            hs_pad: batch of padded hidden state sequences (B, Tmax, D)
-            hlens: batch of lengths of hidden state sequences (B)
-            ys_pad: batch of padded character id sequence tensor (B, Lmax)
-            ys_lens: batch of lengths of character sequence (B)
+            main: tensor or tuple or list
+                  activations in the main neural network, which are adapted
+                  tuple/list may be useful when we want to apply the adaptation
+                    to both normal and skip connection at once
+            enroll: tensor or tuple or list
+                    embedding extracted from enrollment
+                    tuple/list may be useful when we want to apply the adaptation
+                      to both normal and skip connection at once
         """
-        # hs_pad: (B, L, NProj) -> ys_hat: (B, L, Nvocab)
-        ys_hat = self.ctc_lo(F.dropout(hs_pad, p=self.dropout_rate))
+        assert type(main) == type(enroll)
+        orig_type = type(main)
+        main, enroll = into_tuple(main), into_tuple(enroll)
+        assert len(main) == len(enroll) == self.ninputs
+
+        out = []
+        for transform, main0, enroll0 in zip(self.transform, main, enroll):
+            out.append(
+                transform(
+                    torch.cat(
+                        (main0, enroll0[:, :, None].expand(main0.shape)), dim=1
+                    ).permute(0, 2, 1)
+                ).permute(0, 2, 1)
+            )
+        return into_orig_type(tuple(out), orig_type)
 
-        if self.ctc_type == "gtnctc":
-            # gtn expects list form for ys
-            ys_true = [y[y != -1] for y in ys_pad]  # parse padded ys
-        else:
-            # ys_hat: (B, L, D) -> (L, B, D)
-            ys_hat = ys_hat.transpose(0, 1)
-            # (B, L) -> (BxL,)
-            ys_true = torch.cat([ys_pad[i, :l] for i, l in enumerate(ys_lens)])
 
-        loss = self.loss_fn(ys_hat, ys_true, hlens, ys_lens).to(
-            device=hs_pad.device, dtype=hs_pad.dtype
-        )
+class MulAddAdaptLayer(nn.Module):
+    def __init__(self, indim, enrolldim, ninputs=1, do_addition=True):
+        super().__init__()
+        self.ninputs = ninputs
+        self.do_addition = do_addition
 
-        return loss
+        if do_addition:
+            assert enrolldim == 2 * indim, (enrolldim, indim)
+        else:
+            assert enrolldim == indim, (enrolldim, indim)
 
-    def softmax(self, hs_pad):
-        """softmax of frame activations
+    def forward(self, main, enroll):
+        """MulAddAdaptLayer Forward.
 
         Args:
-            Tensor hs_pad: 3d tensor (B, Tmax, eprojs)
-        Returns:
-            torch.Tensor: softmax applied 3d tensor (B, Tmax, odim)
+            main: tensor or tuple or list
+                  activations in the main neural network, which are adapted
+                  tuple/list may be useful when we want to apply the adaptation
+                    to both normal and skip connection at once
+            enroll: tensor or tuple or list
+                    embedding extracted from enrollment
+                    tuple/list may be useful when we want to apply the adaptation
+                      to both normal and skip connection at once
         """
-        return F.softmax(self.ctc_lo(hs_pad), dim=2)
-
-    def log_softmax(self, hs_pad):
-        """log_softmax of frame activations
+        assert type(main) == type(enroll)
+        orig_type = type(main)
+        main, enroll = into_tuple(main), into_tuple(enroll)
+        assert len(main) == len(enroll) == self.ninputs, (
+            len(main),
+            len(enroll),
+            self.ninputs,
+        )
 
-        Args:
-            Tensor hs_pad: 3d tensor (B, Tmax, eprojs)
-        Returns:
-            torch.Tensor: log softmax applied 3d tensor (B, Tmax, odim)
-        """
-        return F.log_softmax(self.ctc_lo(hs_pad), dim=2)
+        out = []
+        for main0, enroll0 in zip(main, enroll):
+            if self.do_addition:
+                enroll0_mul, enroll0_add = torch.chunk(enroll0, 2, dim=1)
+                out.append(enroll0_mul[:, :, None] * main0 + enroll0_add[:, :, None])
+            else:
+                out.append(enroll0[:, :, None] * main0)
+        return into_orig_type(tuple(out), orig_type)
 
-    def argmax(self, hs_pad):
-        """argmax of frame activations
 
-        Args:
-            torch.Tensor hs_pad: 3d tensor (B, Tmax, eprojs)
-        Returns:
-            torch.Tensor: argmax applied 2d tensor (B, Tmax)
-        """
-        return torch.argmax(self.ctc_lo(hs_pad), dim=2)
+# aliases for possible adaptation layer types
+adaptation_layer_types = {
+    "concat": ConcatAdaptLayer,
+    "muladd": MulAddAdaptLayer,
+    "mul": partial(MulAddAdaptLayer, do_addition=False),
+}
```

### Comparing `espnet-202304/espnet2/asr/decoder/hugging_face_transformers_decoder.py` & `espnet-202308/espnet2/asr/encoder/hugging_face_transformers_encoder.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,112 +1,95 @@
 #!/usr/bin/env python3
-#  2022, University of Stuttgart;  Pavel Denisov
+#  2021, University of Stuttgart;  Pavel Denisov
 #  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)
 
-"""Hugging Face Transformers Decoder."""
+"""Hugging Face Transformers PostEncoder."""
 
 import copy
 import logging
 from typing import Tuple
 
 import torch
 from typeguard import check_argument_types
 
-from espnet2.asr.decoder.abs_decoder import AbsDecoder
+from espnet2.asr.encoder.abs_encoder import AbsEncoder
 from espnet.nets.pytorch_backend.nets_utils import make_pad_mask
 
 try:
-    from transformers import AutoModelForSeq2SeqLM
+    from transformers import AutoModel
 
     is_transformers_available = True
 except ImportError:
     is_transformers_available = False
 
 
-class HuggingFaceTransformersDecoder(AbsDecoder):
-    """Hugging Face Transformers Decoder.
-
-    Args:
-        encoder_output_size: dimension of encoder attention
-        model_name_or_path: Hugging Face Transformers model name
-    """
+class HuggingFaceTransformersEncoder(AbsEncoder):
+    """Hugging Face Transformers PostEncoder."""
 
     def __init__(
         self,
-        vocab_size: int,
-        encoder_output_size: int,
+        input_size: int,
         model_name_or_path: str,
+        lang_token_id: int = -1,
     ):
+        """Initialize the module."""
         assert check_argument_types()
         super().__init__()
 
         if not is_transformers_available:
             raise ImportError(
                 "`transformers` is not available. Please install it via `pip install"
                 " transformers` or `cd /path/to/espnet/tools && . ./activate_python.sh"
                 " && ./installers/install_transformers.sh`."
             )
 
-        model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)
+        model = AutoModel.from_pretrained(model_name_or_path)
 
-        if hasattr(model, "model"):
-            self.decoder = model.model.decoder
+        if hasattr(model, "encoder"):
+            self.transformer = model.encoder
         else:
-            self.decoder = model.decoder
-
-        self.lm_head = model.lm_head
-        self.model_name_or_path = model_name_or_path
+            self.transformer = model
 
-        self.decoder_pretrained_params = copy.deepcopy(self.decoder.state_dict())
-        self.lm_head_pretrained_params = copy.deepcopy(self.lm_head.state_dict())
+        self.pretrained_params = copy.deepcopy(self.transformer.state_dict())
 
-        if encoder_output_size != self.decoder.config.hidden_size:
-            self.linear_in = torch.nn.Linear(
-                encoder_output_size, self.decoder.config.hidden_size
-            )
-        else:
-            self.linear_in = torch.nn.Identity()
+        self.lang_token_id = lang_token_id
 
     def forward(
-        self,
-        hs_pad: torch.Tensor,
-        hlens: torch.Tensor,
-        ys_in_pad: torch.Tensor,
-        ys_in_lens: torch.Tensor,
+        self, input: torch.Tensor, input_lengths: torch.Tensor
     ) -> Tuple[torch.Tensor, torch.Tensor]:
-        """Forward decoder.
+        """Forward."""
 
-        Args:
-            hs_pad: encoded memory, float32  (batch, maxlen_in, feat)
-            hlens: (batch)
-            ys_in_pad: input tensor (batch, maxlen_out, #mels)
-            ys_in_lens: (batch)
-        Returns:
-            (tuple): tuple containing:
-
-            x: decoded token score before softmax (batch, maxlen_out, token)
-                if use_output_layer is True,
-            olens: (batch, )
-        """
         args = {"return_dict": True}
 
-        if self.decoder.__class__.__name__ == "MBartDecoder":
-            ys_in_pad[:, 0] = 2
-
-        args["input_ids"] = ys_in_pad
-        mask = (~make_pad_mask(ys_in_lens)).to(ys_in_pad.device).float()
-        args["attention_mask"] = mask
+        if self.lang_token_id != -1:
+            input = torch.cat(
+                (
+                    torch.tensor(
+                        [self.lang_token_id] * input.shape[0], device=input.device
+                    ).unsqueeze(1),
+                    input,
+                ),
+                dim=-1,
+            )
+            input_lengths = input_lengths + 1
 
-        args["encoder_hidden_states"] = self.linear_in(hs_pad)
-        hs_mask = (~make_pad_mask(hlens)).to(hs_pad.device).float()
-        args["encoder_attention_mask"] = hs_mask
+        args["input_ids"] = input
 
-        x = self.decoder(**args).last_hidden_state
-        x = self.lm_head(x)
+        mask = (~make_pad_mask(input_lengths)).to(input.device).float()
+        args["attention_mask"] = mask
+        output = self.transformer(**args).last_hidden_state
 
-        olens = mask.sum(1).to(torch.int)
-        return x, olens
+        return output, input_lengths
 
     def reload_pretrained_parameters(self):
-        self.decoder.load_state_dict(self.decoder_pretrained_params)
-        self.lm_head.load_state_dict(self.lm_head_pretrained_params)
+        self.transformer.load_state_dict(self.pretrained_params)
         logging.info("Pretrained Transformers model parameters reloaded!")
+
+    def output_size(self) -> int:
+        """Get the output size."""
+        return self.transformer.config.hidden_size
+
+
+def _extend_attention_mask(mask: torch.Tensor) -> torch.Tensor:
+    mask = mask[:, None, None, :]
+    mask = (1.0 - mask) * -10000.0
+    return mask
```

### Comparing `espnet-202304/espnet2/asr/decoder/mlm_decoder.py` & `espnet-202308/espnet2/asr/decoder/mlm_decoder.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/decoder/rnn_decoder.py` & `espnet-202308/espnet2/asr/decoder/rnn_decoder.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/decoder/s4_decoder.py` & `espnet-202308/espnet2/asr/decoder/s4_decoder.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/decoder/transducer_decoder.py` & `espnet-202308/espnet2/asr/decoder/transducer_decoder.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/decoder/whisper_decoder.py` & `espnet-202308/espnet2/asr/decoder/whisper_decoder.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/encoder/branchformer_encoder.py` & `espnet-202308/espnet2/asr/encoder/branchformer_encoder.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/encoder/conformer_encoder.py` & `espnet-202308/espnet2/asr/encoder/conformer_encoder.py`

 * *Files 0% similar despite different names*

```diff
@@ -86,15 +86,15 @@
         output_size: int = 256,
         attention_heads: int = 4,
         linear_units: int = 2048,
         num_blocks: int = 6,
         dropout_rate: float = 0.1,
         positional_dropout_rate: float = 0.1,
         attention_dropout_rate: float = 0.0,
-        input_layer: str = "conv2d",
+        input_layer: Optional[str] = "conv2d",
         normalize_before: bool = True,
         concat_after: bool = False,
         positionwise_layer_type: str = "linear",
         positionwise_conv_kernel_size: int = 3,
         macaron_style: bool = False,
         rel_pos_type: str = "legacy",
         pos_enc_layer_type: str = "rel_pos",
```

### Comparing `espnet-202304/espnet2/asr/encoder/contextual_block_conformer_encoder.py` & `espnet-202308/espnet2/asr/encoder/contextual_block_conformer_encoder.py`

 * *Files 1% similar despite different names*

```diff
@@ -497,15 +497,16 @@
             xs_chunk = self.pos_enc(xs_pad).unsqueeze(1)
             xs_pad, _, _, _, _, _, _ = self.encoders(
                 xs_chunk, None, True, None, None, True
             )
             xs_pad = xs_pad.squeeze(0)
             if self.normalize_before:
                 xs_pad = self.after_norm(xs_pad)
-            return xs_pad, None, None
+            return xs_pad, xs_pad.new_zeros(bsize), None
+            # return xs_pad, None, None
 
         # start block processing
         xs_chunk = xs_pad.new_zeros(
             bsize, block_num, self.block_size + 2, xs_pad.size(-1)
         )
 
         for i in range(block_num):
@@ -583,8 +584,13 @@
                 "buffer_before_downsampling": buffer_before_downsampling,
                 "ilens_buffer": ilens_buffer,
                 "buffer_after_downsampling": buffer_after_downsampling,
                 "n_processed_blocks": n_processed_blocks + block_num,
                 "past_encoder_ctx": past_encoder_ctx,
             }
 
-        return ys_pad, None, next_states
+        return (
+            ys_pad,
+            torch.tensor([y_length], dtype=xs_pad.dtype, device=ys_pad.device),
+            next_states,
+        )
+        # return ys_pad, None, next_states
```

### Comparing `espnet-202304/espnet2/asr/encoder/contextual_block_transformer_encoder.py` & `espnet-202308/espnet2/asr/encoder/contextual_block_transformer_encoder.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/encoder/e_branchformer_encoder.py` & `espnet-202308/espnet2/asr/encoder/e_branchformer_encoder.py`

 * *Files 2% similar despite different names*

```diff
@@ -6,15 +6,15 @@
     Kwangyoun Kim, Felix Wu, Yifan Peng, Jing Pan,
     Prashant Sridhar, Kyu J. Han, Shinji Watanabe,
     "E-Branchformer: Branchformer with Enhanced merging
     for speech recognition," in SLT 2022.
 """
 
 import logging
-from typing import Optional, Tuple
+from typing import List, Optional, Tuple
 
 import torch
 from typeguard import check_argument_types
 
 from espnet2.asr.ctc import CTC
 from espnet2.asr.encoder.abs_encoder import AbsEncoder
 from espnet2.asr.layers.cgmlp import ConvolutionalGatingMLP
@@ -33,14 +33,16 @@
 )
 from espnet.nets.pytorch_backend.transformer.layer_norm import LayerNorm
 from espnet.nets.pytorch_backend.transformer.positionwise_feed_forward import (
     PositionwiseFeedForward,
 )
 from espnet.nets.pytorch_backend.transformer.repeat import repeat
 from espnet.nets.pytorch_backend.transformer.subsampling import (
+    Conv1dSubsampling2,
+    Conv1dSubsampling3,
     Conv2dSubsampling,
     Conv2dSubsampling1,
     Conv2dSubsampling2,
     Conv2dSubsampling6,
     Conv2dSubsampling8,
     TooShortUttError,
     check_short_utt,
@@ -245,14 +247,28 @@
         if input_layer == "linear":
             self.embed = torch.nn.Sequential(
                 torch.nn.Linear(input_size, output_size),
                 torch.nn.LayerNorm(output_size),
                 torch.nn.Dropout(dropout_rate),
                 pos_enc_class(output_size, positional_dropout_rate, max_pos_emb_len),
             )
+        elif input_layer == "conv1d2":
+            self.embed = Conv1dSubsampling2(
+                input_size,
+                output_size,
+                dropout_rate,
+                pos_enc_class(output_size, positional_dropout_rate, max_pos_emb_len),
+            )
+        elif input_layer == "conv1d3":
+            self.embed = Conv1dSubsampling3(
+                input_size,
+                output_size,
+                dropout_rate,
+                pos_enc_class(output_size, positional_dropout_rate, max_pos_emb_len),
+            )
         elif input_layer == "conv2d":
             self.embed = Conv2dSubsampling(
                 input_size,
                 output_size,
                 dropout_rate,
                 pos_enc_class(output_size, positional_dropout_rate, max_pos_emb_len),
             )
@@ -292,15 +308,17 @@
         elif isinstance(input_layer, torch.nn.Module):
             self.embed = torch.nn.Sequential(
                 input_layer,
                 pos_enc_class(output_size, positional_dropout_rate, max_pos_emb_len),
             )
         elif input_layer is None:
             if input_size == output_size:
-                self.embed = None
+                self.embed = torch.nn.Sequential(
+                    pos_enc_class(output_size, positional_dropout_rate, max_pos_emb_len)
+                )
             else:
                 self.embed = torch.nn.Linear(input_size, output_size)
         else:
             raise ValueError("unknown input_layer: " + input_layer)
 
         activation = get_activation(ffn_activation_type)
         if positionwise_layer_type == "linear":
@@ -414,14 +432,16 @@
             torch.Tensor: Not to be used now.
         """
 
         masks = (~make_pad_mask(ilens)[:, None, :]).to(xs_pad.device)
 
         if (
             isinstance(self.embed, Conv2dSubsampling)
+            or isinstance(self.embed, Conv1dSubsampling2)
+            or isinstance(self.embed, Conv1dSubsampling3)
             or isinstance(self.embed, Conv2dSubsampling1)
             or isinstance(self.embed, Conv2dSubsampling2)
             or isinstance(self.embed, Conv2dSubsampling6)
             or isinstance(self.embed, Conv2dSubsampling8)
         ):
             short_status, limit_size = check_short_utt(self.embed, xs_pad.size(1))
             if short_status:
```

### Comparing `espnet-202304/espnet2/asr/encoder/hubert_encoder.py` & `espnet-202308/espnet2/asr/encoder/hubert_encoder.py`

 * *Files 2% similar despite different names*

```diff
@@ -29,15 +29,15 @@
     """Torch Audio Hubert encoder module.
 
     Args:
         extractor_mode: Operation mode of feature extractor.
             Valid values are "group_norm" or "layer_norm".
         extractor_conv_layer_config: Configuration of convolution layers in feature
             extractor. List of convolution configuration,
-            i.e. [(output_channel, kernel_size, stride), ...]
+            i.e. [[output_channel, kernel_size, stride], ...]
         extractor_conv_bias: Whether to include bias term to each convolution
             operation.
         encoder_embed_dim: The dimension of embedding in encoder.
         encoder_projection_dropout: The dropout probability applied after the input
             feature is projected to "encoder_embed_dim".
         encoder_pos_conv_kernel: Kernel size of convolutional positional embeddings.
         encoder_pos_conv_groups: Number of groups of convolutional positional
@@ -85,22 +85,22 @@
         https://pytorch.org/audio/stable/generated/torchaudio.models.hubert_pretrain_model.html#torchaudio.models.hubert_pretrain_model
     """
 
     def __init__(
         self,
         input_size: int = None,
         extractor_mode: str = "group_norm",
-        extractor_conv_layer_config: Optional[List[Tuple[int, int, int]]] = [
-            (512, 10, 5),
-            (512, 3, 2),
-            (512, 3, 2),
-            (512, 3, 2),
-            (512, 3, 2),
-            (512, 2, 2),
-            (512, 2, 2),
+        extractor_conv_layer_config: Optional[List[List[int]]] = [
+            [512, 10, 5],
+            [512, 3, 2],
+            [512, 3, 2],
+            [512, 3, 2],
+            [512, 3, 2],
+            [512, 2, 2],
+            [512, 2, 2],
         ],
         extractor_conv_bias: bool = False,
         encoder_embed_dim: int = 768,
         encoder_projection_dropout: float = 0.1,
         encoder_pos_conv_kernel: int = 128,
         encoder_pos_conv_groups: int = 16,
         encoder_num_layers: int = 12,
```

### Comparing `espnet-202304/espnet2/asr/encoder/longformer_encoder.py` & `espnet-202308/espnet2/asr/encoder/longformer_encoder.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/encoder/rnn_encoder.py` & `espnet-202308/espnet2/asr/encoder/rnn_encoder.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/encoder/transformer_encoder.py` & `espnet-202308/espnet2/asr/encoder/transformer_encoder.py`

 * *Files 5% similar despite different names*

```diff
@@ -20,14 +20,15 @@
     MultiLayeredConv1d,
 )
 from espnet.nets.pytorch_backend.transformer.positionwise_feed_forward import (
     PositionwiseFeedForward,
 )
 from espnet.nets.pytorch_backend.transformer.repeat import repeat
 from espnet.nets.pytorch_backend.transformer.subsampling import (
+    Conv1dSubsampling2,
     Conv2dSubsampling,
     Conv2dSubsampling1,
     Conv2dSubsampling2,
     Conv2dSubsampling6,
     Conv2dSubsampling8,
     TooShortUttError,
     check_short_utt,
@@ -87,14 +88,21 @@
             self.embed = torch.nn.Sequential(
                 torch.nn.Linear(input_size, output_size),
                 torch.nn.LayerNorm(output_size),
                 torch.nn.Dropout(dropout_rate),
                 torch.nn.ReLU(),
                 pos_enc_class(output_size, positional_dropout_rate),
             )
+        elif input_layer == "conv1d2":
+            self.embed = Conv1dSubsampling2(
+                input_size,
+                output_size,
+                dropout_rate,
+                pos_enc_class(output_size, positional_dropout_rate),
+            )
         elif input_layer == "conv2d":
             self.embed = Conv2dSubsampling(input_size, output_size, dropout_rate)
         elif input_layer == "conv2d1":
             self.embed = Conv2dSubsampling1(input_size, output_size, dropout_rate)
         elif input_layer == "conv2d2":
             self.embed = Conv2dSubsampling2(input_size, output_size, dropout_rate)
         elif input_layer == "conv2d6":
@@ -182,14 +190,15 @@
         """
         masks = (~make_pad_mask(ilens)[:, None, :]).to(xs_pad.device)
 
         if self.embed is None:
             xs_pad = xs_pad
         elif (
             isinstance(self.embed, Conv2dSubsampling)
+            or isinstance(self.embed, Conv1dSubsampling2)
             or isinstance(self.embed, Conv2dSubsampling1)
             or isinstance(self.embed, Conv2dSubsampling2)
             or isinstance(self.embed, Conv2dSubsampling6)
             or isinstance(self.embed, Conv2dSubsampling8)
         ):
             short_status, limit_size = check_short_utt(self.embed, xs_pad.size(1))
             if short_status:
```

### Comparing `espnet-202304/espnet2/asr/encoder/transformer_encoder_multispkr.py` & `espnet-202308/espnet2/asr/encoder/transformer_encoder_multispkr.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/encoder/vgg_rnn_encoder.py` & `espnet-202308/espnet2/asr/encoder/vgg_rnn_encoder.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/encoder/wav2vec2_encoder.py` & `espnet-202308/espnet2/asr/encoder/wav2vec2_encoder.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/encoder/whisper_encoder.py` & `espnet-202308/espnet2/asr/encoder/whisper_encoder.py`

 * *Files 3% similar despite different names*

```diff
@@ -171,12 +171,14 @@
     ) -> Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]:
         if self.do_pad_trim:
             xs_pad = self.pad_or_trim(xs_pad, self.pad_samples)
 
         feats, feats_lens = self.log_mel_spectrogram(xs_pad, ilens)
 
         if self.specaug is not None and self.encoders.training:
+            feats = torch.transpose(feats, 1, 2)
             feats, feats_lens = self.specaug(feats, feats_lens)
+            feats = torch.transpose(feats, 1, 2)
 
         xs_pad, olens = self.whisper_encode(feats, feats_lens)
 
         return xs_pad, olens, None
```

### Comparing `espnet-202304/espnet2/asr/espnet_model.py` & `espnet-202308/espnet2/asr/espnet_model.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/frontend/default.py` & `espnet-202308/espnet2/asr/frontend/default.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/frontend/fused.py` & `espnet-202308/espnet2/asr/frontend/fused.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/frontend/s3prl.py` & `espnet-202308/espnet2/asr/frontend/s3prl.py`

 * *Files 8% similar despite different names*

```diff
@@ -46,22 +46,18 @@
         assert frontend_conf.get("upstream", None) in S3PRLUpstream.available_names()
         upstream = S3PRLUpstream(
             frontend_conf.get("upstream"),
             path_or_url=frontend_conf.get("path_or_url", None),
             normalize=frontend_conf.get("normalize", False),
             extra_conf=frontend_conf.get("extra_conf", None),
         )
+        if getattr(upstream.upstream, "model", None):
+            if getattr(upstream.upstream.model, "feature_grad_mult", None) is not None:
+                upstream.upstream.model.feature_grad_mult = 1.0
         upstream.eval()
-        if getattr(
-            upstream.upstream, "model", None
-        ) is not None and upstream.upstream.model.__class__.__name__ in [
-            "Wav2Vec2Model",
-            "HubertModel",
-        ]:
-            upstream.upstream.model.encoder.layerdrop = 0.0
 
         if layer != -1:
             layer_selections = [layer]
             assert (
                 not multilayer_feature
             ), "multilayer feature will be deactivated, when specific layer used"
         else:
```

### Comparing `espnet-202304/espnet2/asr/frontend/whisper.py` & `espnet-202308/espnet2/asr/frontend/whisper.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/frontend/windowing.py` & `espnet-202308/espnet2/asr/frontend/windowing.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/layers/cgmlp.py` & `espnet-202308/espnet2/asr/layers/cgmlp.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/layers/fastformer.py` & `espnet-202308/espnet2/asr/layers/fastformer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/maskctc_model.py` & `espnet-202308/espnet2/asr/maskctc_model.py`

 * *Files 2% similar despite different names*

```diff
@@ -298,15 +298,15 @@
         cnt = 0
         for i, y in enumerate(y_hat.tolist()):
             probs_hat.append(-1)
             while cnt < ctc_ids.shape[1] and y == ctc_ids[0][cnt]:
                 if probs_hat[i] < ctc_probs[0][cnt]:
                     probs_hat[i] = ctc_probs[0][cnt].item()
                 cnt += 1
-        probs_hat = torch.from_numpy(numpy.array(probs_hat))
+        probs_hat = torch.from_numpy(numpy.array(probs_hat)).to(enc_out.device)
 
         # mask ctc outputs based on ctc probabilities
         p_thres = self.threshold_probability
         mask_idx = torch.nonzero(probs_hat[y_idx] < p_thres).squeeze(-1)
         confident_idx = torch.nonzero(probs_hat[y_idx] >= p_thres).squeeze(-1)
         mask_num = len(mask_idx)
```

### Comparing `espnet-202304/espnet2/asr/pit_espnet_model.py` & `espnet-202308/espnet2/asr/pit_espnet_model.py`

 * *Files 2% similar despite different names*

```diff
@@ -87,14 +87,15 @@
             ) / len(permutation)
 
         losses = torch.stack(
             [pair_loss(p) for p in all_permutations], dim=1
         )  # (batch_size, num_perm)
 
         min_losses, min_ids = torch.min(losses, dim=1)
+        min_ids = min_ids.cpu()  # because all_permutations is a cpu tensor.
         opt_perm = all_permutations[min_ids]  # (batch_size, num_ref)
 
         # Permute the inf and inf_lens according to the optimal perm
         return min_losses.mean(), opt_perm
 
     @classmethod
     def permutate(self, perm, *args):
```

### Comparing `espnet-202304/espnet2/asr/postencoder/hugging_face_transformers_postencoder.py` & `espnet-202308/espnet2/asr/postencoder/hugging_face_transformers_postencoder.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/preencoder/linear.py` & `espnet-202308/espnet2/asr/preencoder/linear.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/preencoder/sinc.py` & `espnet-202308/espnet2/asr/preencoder/sinc.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/specaug/specaug.py` & `espnet-202308/espnet2/asr/specaug/specaug.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/state_spaces/attention.py` & `espnet-202308/espnet2/asr/state_spaces/attention.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/state_spaces/base.py` & `espnet-202308/espnet2/asr/state_spaces/base.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/state_spaces/block.py` & `espnet-202308/espnet2/asr/state_spaces/block.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/state_spaces/cauchy.py` & `espnet-202308/espnet2/asr/state_spaces/cauchy.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/state_spaces/components.py` & `espnet-202308/espnet2/asr/state_spaces/components.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/state_spaces/ff.py` & `espnet-202308/espnet2/asr/state_spaces/ff.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/state_spaces/model.py` & `espnet-202308/espnet2/asr/state_spaces/model.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/state_spaces/pool.py` & `espnet-202308/espnet2/asr/state_spaces/pool.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/state_spaces/residual.py` & `espnet-202308/espnet2/asr/state_spaces/residual.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/state_spaces/s4.py` & `espnet-202308/espnet2/asr/state_spaces/s4.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/state_spaces/utils.py` & `espnet-202308/espnet2/asr/state_spaces/utils.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/transducer/beam_search_transducer.py` & `espnet-202308/espnet2/asr/transducer/beam_search_transducer.py`

 * *Files 2% similar despite different names*

```diff
@@ -56,14 +56,15 @@
         nstep: int = 1,
         prefix_alpha: int = 1,
         expansion_gamma: int = 2.3,
         expansion_beta: int = 2,
         multi_blank_durations: List[int] = [],
         multi_blank_indices: List[int] = [],
         score_norm: bool = True,
+        score_norm_during: bool = False,
         nbest: int = 1,
         token_list: Optional[List[str]] = None,
     ):
         """Initialize Transducer search module.
 
         Args:
             decoder: Decoder module.
@@ -78,14 +79,16 @@
             prefix_alpha: Maximum prefix length in prefix search. (NSC/mAES)
             expansion_beta:
               Number of additional candidates for expanded hypotheses selection. (mAES)
             expansion_gamma: Allowed logp difference for prune-by-value method. (mAES)
             multi_blank_durations: The duration of each blank token. (MBG)
             multi_blank_indices: The index of each blank token in token_list. (MBG)
             score_norm: Normalize final scores by length. ("default")
+            score_norm_during:
+              Normalize scores by length during search. (default, TSD, ALSD)
             nbest: Number of final hypothesis.
 
         """
         self.decoder = decoder
         self.joint_network = joint_network
 
         self.beam_size = beam_size
@@ -153,14 +156,15 @@
         self.lm = lm
         self.lm_weight = lm_weight
 
         if self.use_lm and self.beam_size == 1:
             logging.warning("LM is provided but not used, since this is greedy search.")
 
         self.score_norm = score_norm
+        self.score_norm_during = score_norm_during
         self.nbest = nbest
 
     def __call__(
         self, enc_out: torch.Tensor
     ) -> Union[List[Hypothesis], List[ExtendedHypothesis]]:
         """Perform beam search.
 
@@ -301,15 +305,18 @@
                             + f", score: {round(float(hyp.score), 2)}"
                             for hyp in sorted(hyps, key=lambda x: x.score, reverse=True)
                         ]
                     )
                 )
 
             while True:
-                max_hyp = max(hyps, key=lambda x: x.score)
+                if self.score_norm_during:
+                    max_hyp = max(hyps, key=lambda x: x.score / len(x.yseq))
+                else:
+                    max_hyp = max(hyps, key=lambda x: x.score)
                 hyps.remove(max_hyp)
 
                 dec_out, state, lm_tokens = self.decoder.score(max_hyp, cache)
 
                 logp = torch.log_softmax(
                     self.joint_network(enc_out_t, dec_out),
                     dim=-1,
@@ -352,15 +359,20 @@
                             score=score,
                             yseq=max_hyp.yseq[:] + [int(k + 1)],
                             dec_state=state,
                             lm_state=lm_state,
                         )
                     )
 
-                hyps_max = float(max(hyps, key=lambda x: x.score).score)
+                if self.score_norm_during:
+                    hyps_max = float(
+                        max(hyps, key=lambda x: x.score / len(x.yseq)).score
+                    )
+                else:
+                    hyps_max = float(max(hyps, key=lambda x: x.score).score)
                 kept_most_prob = sorted(
                     [hyp for hyp in kept_hyps if hyp.score > hyps_max],
                     key=lambda x: x.score,
                 )
                 if len(kept_most_prob) >= beam:
                     kept_hyps = kept_most_prob
                     break
@@ -453,17 +465,25 @@
 
                             if self.use_lm:
                                 new_hyp.score += self.lm_weight * beam_lm_scores[i, k]
                                 new_hyp.lm_state = beam_lm_states[i]
 
                             D.append(new_hyp)
 
-                C = sorted(D, key=lambda x: x.score, reverse=True)[:beam]
+                if self.score_norm_during:
+                    C = sorted(D, key=lambda x: x.score / len(x.yseq), reverse=True)[
+                        :beam
+                    ]
+                else:
+                    C = sorted(D, key=lambda x: x.score, reverse=True)[:beam]
 
-            B = sorted(A, key=lambda x: x.score, reverse=True)[:beam]
+            if self.score_norm_during:
+                B = sorted(A, key=lambda x: x.score / len(x.yseq), reverse=True)[:beam]
+            else:
+                B = sorted(A, key=lambda x: x.score, reverse=True)[:beam]
 
         return self.sort_nbest(B)
 
     def align_length_sync_decoding(self, enc_out: torch.Tensor) -> List[Hypothesis]:
         """Alignment-length synchronous beam search implementation.
 
         Based on https://ieeexplore.ieee.org/document/9053040
@@ -556,15 +576,20 @@
 
                         if self.use_lm:
                             new_hyp.score += self.lm_weight * beam_lm_scores[i, k]
                             new_hyp.lm_state = beam_lm_states[i]
 
                         A.append(new_hyp)
 
-                B = sorted(A, key=lambda x: x.score, reverse=True)[:beam]
+                if self.score_norm_during:
+                    B = sorted(A, key=lambda x: x.score / len(x.yseq), reverse=True)[
+                        :beam
+                    ]
+                else:
+                    B = sorted(A, key=lambda x: x.score, reverse=True)[:beam]
                 B = recombine_hyps(B)
 
         if final:
             return self.sort_nbest(final)
         else:
             return B
```

### Comparing `espnet-202304/espnet2/asr/transducer/error_calculator.py` & `espnet-202308/espnet2/asr/transducer/error_calculator.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/transducer/rnnt_multi_blank/__init__.py` & `espnet-202308/espnet2/asr/transducer/rnnt_multi_blank/__init__.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/transducer/rnnt_multi_blank/rnnt.py` & `espnet-202308/espnet2/asr/transducer/rnnt_multi_blank/rnnt.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/transducer/rnnt_multi_blank/rnnt_multi_blank.py` & `espnet-202308/espnet2/asr/transducer/rnnt_multi_blank/rnnt_multi_blank.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/transducer/rnnt_multi_blank/utils/__init__.py` & `espnet-202308/espnet2/asr/transducer/rnnt_multi_blank/utils/__init__.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/transducer/rnnt_multi_blank/utils/cpu_utils/__init__.py` & `espnet-202308/espnet2/asr/transducer/rnnt_multi_blank/utils/cpu_utils/__init__.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/transducer/rnnt_multi_blank/utils/cpu_utils/cpu_rnnt.py` & `espnet-202308/espnet2/asr/transducer/rnnt_multi_blank/utils/cpu_utils/cpu_rnnt.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/transducer/rnnt_multi_blank/utils/cuda_utils/__init__.py` & `espnet-202308/espnet2/asr/transducer/rnnt_multi_blank/utils/cuda_utils/__init__.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/transducer/rnnt_multi_blank/utils/cuda_utils/gpu_rnnt.py` & `espnet-202308/espnet2/asr/transducer/rnnt_multi_blank/utils/cuda_utils/gpu_rnnt.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/transducer/rnnt_multi_blank/utils/cuda_utils/gpu_rnnt_kernel.py` & `espnet-202308/espnet2/asr/transducer/rnnt_multi_blank/utils/cuda_utils/gpu_rnnt_kernel.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/transducer/rnnt_multi_blank/utils/cuda_utils/reduce.py` & `espnet-202308/espnet2/asr/transducer/rnnt_multi_blank/utils/cuda_utils/reduce.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/transducer/rnnt_multi_blank/utils/global_constants.py` & `espnet-202308/espnet2/asr/transducer/rnnt_multi_blank/utils/global_constants.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr/transducer/rnnt_multi_blank/utils/rnnt_helper.py` & `espnet-202308/espnet2/asr/transducer/rnnt_multi_blank/utils/rnnt_helper.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/asr_transducer/activation.py` & `espnet-202308/espnet2/asr_transducer/activation.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-"""Activation functions for Transducer."""
+"""Activation functions for Transducer models."""
 
 import torch
 from packaging.version import parse as V
 
 
 def get_activation(
     activation_type: str,
```

### Comparing `espnet-202304/espnet2/asr_transducer/beam_search_transducer.py` & `espnet-202308/espnet2/asr_transducer/beam_search_transducer.py`

 * *Files 2% similar despite different names*

```diff
@@ -13,16 +13,15 @@
 @dataclass
 class Hypothesis:
     """Default hypothesis definition for Transducer search algorithms.
 
     Args:
         score: Total log-probability.
         yseq: Label sequence as integer ID sequence.
-        dec_state: RNNDecoder or StatelessDecoder state.
-                     ((N, 1, D_dec), (N, 1, D_dec) or None) or None
+        dec_state: RNN/MEGA Decoder state (None if Stateless).
         lm_state: RNNLM state. ((N, D_lm), (N, D_lm)) or None
 
     """
 
     score: float
     yseq: List[int]
     dec_state: Optional[Tuple[torch.Tensor, Optional[torch.Tensor]]] = None
@@ -47,15 +46,15 @@
 class BeamSearchTransducer:
     """Beam search implementation for Transducer.
 
     Args:
         decoder: Decoder module.
         joint_network: Joint network module.
         beam_size: Size of the beam.
-        lm: LM class.
+        lm: LM module.
         lm_weight: LM weight for soft fusion.
         search_type: Search algorithm to use during inference.
         max_sym_exp: Number of maximum symbol expansions at each time step. (TSD)
         u_max: Maximum expected target sequence length. (ALSD)
         nstep: Number of maximum expansion steps at each time step. (mAES)
         expansion_gamma: Allowed logp difference for prune-by-value method. (mAES)
         expansion_beta:
@@ -142,15 +141,15 @@
 
             self.lm = lm
             self.lm_weight = lm_weight
 
         self.score_norm = score_norm
         self.nbest = nbest
 
-        self.reset_inference_cache()
+        self.reset_cache()
 
     def __call__(
         self,
         enc_out: torch.Tensor,
         is_final: bool = True,
     ) -> List[Hypothesis]:
         """Perform beam search.
@@ -164,24 +163,24 @@
 
         """
         self.decoder.set_device(enc_out.device)
 
         hyps = self.search_algorithm(enc_out)
 
         if is_final:
-            self.reset_inference_cache()
+            self.reset_cache()
 
             return self.sort_nbest(hyps)
 
         self.search_cache = hyps
 
         return hyps
 
-    def reset_inference_cache(self) -> None:
-        """Reset cache for decoder scoring and streaming."""
+    def reset_cache(self) -> None:
+        """Reset cache for streaming decoding."""
         self.decoder.score_cache = {}
         self.search_cache = None
 
     def sort_nbest(self, hyps: List[Hypothesis]) -> List[Hypothesis]:
         """Sort in-place hypotheses by score or score given sequence length.
 
         Args:
@@ -308,22 +307,15 @@
             hyps = kept_hyps
             kept_hyps = []
 
             while True:
                 max_hyp = max(hyps, key=lambda x: x.score)
                 hyps.remove(max_hyp)
 
-                label = torch.full(
-                    (1, 1),
-                    max_hyp.yseq[-1],
-                    dtype=torch.long,
-                    device=self.decoder.device,
-                )
                 dec_out, state = self.decoder.score(
-                    label,
                     max_hyp.yseq,
                     max_hyp.dec_state,
                 )
 
                 logp = torch.log_softmax(
                     self.joint_network(enc_out[t : t + 1, :], dec_out),
                     dim=-1,
@@ -366,14 +358,15 @@
                     )
 
                 hyps_max = float(max(hyps, key=lambda x: x.score).score)
                 kept_most_prob = sorted(
                     [hyp for hyp in kept_hyps if hyp.score > hyps_max],
                     key=lambda x: x.score,
                 )
+
                 if len(kept_most_prob) >= self.beam_size:
                     kept_hyps = kept_most_prob
                     break
 
         return kept_hyps
 
     def align_length_sync_decoding(
@@ -401,14 +394,15 @@
             B[0].lm_state = self.lm.zero_state()
 
         for i in range(t_max + u_max):
             A = []
 
             B_ = []
             B_enc_out = []
+
             for hyp in B:
                 u = len(hyp.yseq) - 1
                 t = i - u
 
                 if t > (t_max - 1):
                     continue
```

### Comparing `espnet-202304/espnet2/asr_transducer/decoder/rnn_decoder.py` & `espnet-202308/espnet2/asr_transducer/decoder/rnn_decoder.py`

 * *Files 5% similar despite different names*

```diff
@@ -64,37 +64,30 @@
 
         self.output_size = hidden_size
         self.vocab_size = vocab_size
 
         self.device = next(self.parameters()).device
         self.score_cache = {}
 
-    def forward(
-        self,
-        labels: torch.Tensor,
-        states: Optional[Tuple[torch.Tensor, Optional[torch.Tensor]]] = None,
-    ) -> torch.Tensor:
+    def forward(self, labels: torch.Tensor) -> torch.Tensor:
         """Encode source label sequences.
 
         Args:
             labels: Label ID sequences. (B, L)
-            states: Decoder hidden states.
-                      ((N, B, D_dec), (N, B, D_dec) or None) or None
 
         Returns:
-            dec_out: Decoder output sequences. (B, U, D_dec)
+            out: Decoder output sequences. (B, U, D_dec)
 
         """
-        if states is None:
-            states = self.init_state(labels.size(0))
+        states = self.init_state(labels.size(0))
 
-        dec_embed = self.dropout_embed(self.embed(labels))
-        dec_out, states = self.rnn_forward(dec_embed, states)
+        embed = self.dropout_embed(self.embed(labels))
+        out, _ = self.rnn_forward(embed, states)
 
-        return dec_out
+        return out
 
     def rnn_forward(
         self,
         x: torch.Tensor,
         state: Tuple[torch.Tensor, Optional[torch.Tensor]],
     ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, Optional[torch.Tensor]]]:
         """Encode source label sequences.
@@ -124,65 +117,72 @@
 
             x = self.dropout_rnn[layer](x)
 
         return x, (h_next, c_next)
 
     def score(
         self,
-        label: torch.Tensor,
         label_sequence: List[int],
-        dec_state: Tuple[torch.Tensor, Optional[torch.Tensor]],
+        states: Tuple[torch.Tensor, Optional[torch.Tensor]],
     ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, Optional[torch.Tensor]]]:
         """One-step forward hypothesis.
 
         Args:
-            label: Previous label. (1, 1)
             label_sequence: Current label sequence.
-            dec_state: Previous decoder hidden states.
-                         ((N, 1, D_dec), (N, 1, D_dec) or None)
+            states: Decoder hidden states.
+                      ((N, 1, D_dec), (N, 1, D_dec) or None)
 
         Returns:
-            dec_out: Decoder output sequence. (1, D_dec)
-            dec_state: Decoder hidden states.
-                         ((N, 1, D_dec), (N, 1, D_dec) or None)
+            out: Decoder output sequence. (1, D_dec)
+            states: Decoder hidden states.
+                      ((N, 1, D_dec), (N, 1, D_dec) or None)
 
         """
         str_labels = "_".join(map(str, label_sequence))
 
         if str_labels in self.score_cache:
-            dec_out, dec_state = self.score_cache[str_labels]
+            out, states = self.score_cache[str_labels]
         else:
-            dec_embed = self.embed(label)
-            dec_out, dec_state = self.rnn_forward(dec_embed, dec_state)
+            label = torch.full(
+                (1, 1),
+                label_sequence[-1],
+                dtype=torch.long,
+                device=self.device,
+            )
 
-            self.score_cache[str_labels] = (dec_out, dec_state)
+            embed = self.embed(label)
+            out, states = self.rnn_forward(embed, states)
 
-        return dec_out[0], dec_state
+            self.score_cache[str_labels] = (out, states)
+
+        return out[0], states
 
     def batch_score(
         self,
         hyps: List[Hypothesis],
     ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, Optional[torch.Tensor]]]:
         """One-step forward hypotheses.
 
         Args:
             hyps: Hypotheses.
 
         Returns:
-            dec_out: Decoder output sequences. (B, D_dec)
+            out: Decoder output sequences. (B, D_dec)
             states: Decoder hidden states. ((N, B, D_dec), (N, B, D_dec) or None)
 
         """
-        labels = torch.LongTensor([[h.yseq[-1]] for h in hyps], device=self.device)
-        dec_embed = self.embed(labels)
+        labels = torch.tensor(
+            [[h.yseq[-1]] for h in hyps], dtype=torch.long, device=self.device
+        )
+        embed = self.embed(labels)
 
         states = self.create_batch_states([h.dec_state for h in hyps])
-        dec_out, states = self.rnn_forward(dec_embed, states)
+        out, states = self.rnn_forward(embed, states)
 
-        return dec_out.squeeze(1), states
+        return out.squeeze(1), states
 
     def set_device(self, device: torch.device) -> None:
         """Set GPU device to use.
 
         Args:
             device: Device ID.
 
@@ -241,15 +241,16 @@
     def create_batch_states(
         self,
         new_states: List[Tuple[torch.Tensor, Optional[torch.Tensor]]],
     ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
         """Create decoder hidden states.
 
         Args:
-            new_states: Decoder hidden states. [N x ((1, D_dec), (1, D_dec) or None)]
+            new_states: Decoder hidden states.
+                            [B x ((N, 1, D_dec), (N, 1, D_dec) or None)]
 
         Returns:
             states: Decoder hidden states. ((N, B, D_dec), (N, B, D_dec) or None)
 
         """
         return (
             torch.cat([s[0] for s in new_states], dim=1),
```

### Comparing `espnet-202304/espnet2/asr_transducer/decoder/stateless_decoder.py` & `espnet-202308/espnet2/asr_transducer/decoder/stateless_decoder.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 """Stateless decoder definition for Transducer models."""
 
-from typing import List, Optional, Tuple
+from typing import Any, List, Optional, Tuple
 
 import torch
 from typeguard import check_argument_types
 
 from espnet2.asr_transducer.beam_search_transducer import Hypothesis
 from espnet2.asr_transducer.decoder.abs_decoder import AbsDecoder
 
@@ -40,77 +40,81 @@
 
         self.device = next(self.parameters()).device
         self.score_cache = {}
 
     def forward(
         self,
         labels: torch.Tensor,
-        states: Optional[Tuple[torch.Tensor, Optional[torch.Tensor]]] = None,
+        states: Optional[Any] = None,
     ) -> torch.Tensor:
         """Encode source label sequences.
 
         Args:
             labels: Label ID sequences. (B, L)
             states: Decoder hidden states. None
 
         Returns:
-            dec_embed: Decoder output sequences. (B, U, D_emb)
+            embed: Decoder output sequences. (B, U, D_emb)
 
         """
-        dec_embed = self.embed_dropout_rate(self.embed(labels))
+        embed = self.embed_dropout_rate(self.embed(labels))
 
-        return dec_embed
+        return embed
 
     def score(
         self,
-        label: torch.Tensor,
         label_sequence: List[int],
-        state: None,
+        states: Optional[Any] = None,
     ) -> Tuple[torch.Tensor, None]:
         """One-step forward hypothesis.
 
         Args:
-            label: Previous label. (1, 1)
             label_sequence: Current label sequence.
-            state: Previous decoder hidden states. None
+            states: Decoder hidden states. None
 
         Returns:
-            dec_out: Decoder output sequence. (1, D_emb)
+            : Decoder output sequence. (1, D_emb)
             state: Decoder hidden states. None
 
         """
         str_labels = "_".join(map(str, label_sequence))
 
         if str_labels in self.score_cache:
-            dec_embed = self.score_cache[str_labels]
+            embed = self.score_cache[str_labels]
         else:
-            dec_embed = self.embed(label)
+            label = torch.full(
+                (1, 1),
+                label_sequence[-1],
+                dtype=torch.long,
+                device=self.device,
+            )
 
-            self.score_cache[str_labels] = dec_embed
+            embed = self.embed(label)
 
-        return dec_embed[0], None
+            self.score_cache[str_labels] = embed
 
-    def batch_score(
-        self,
-        hyps: List[Hypothesis],
-    ) -> Tuple[torch.Tensor, None]:
+        return embed[0], None
+
+    def batch_score(self, hyps: List[Hypothesis]) -> Tuple[torch.Tensor, None]:
         """One-step forward hypotheses.
 
         Args:
             hyps: Hypotheses.
 
         Returns:
-            dec_out: Decoder output sequences. (B, D_dec)
+            out: Decoder output sequences. (B, D_dec)
             states: Decoder hidden states. None
 
         """
-        labels = torch.LongTensor([[h.yseq[-1]] for h in hyps], device=self.device)
-        dec_embed = self.embed(labels)
+        labels = torch.tensor(
+            [[h.yseq[-1]] for h in hyps], dtype=torch.long, device=self.device
+        )
+        embed = self.embed(labels)
 
-        return dec_embed.squeeze(1), None
+        return embed.squeeze(1), None
 
     def set_device(self, device: torch.device) -> None:
         """Set GPU device to use.
 
         Args:
             device: Device ID.
 
@@ -137,7 +141,22 @@
             idx: State ID to extract.
 
         Returns:
             : Decoder hidden state for given ID. None
 
         """
         return None
+
+    def create_batch_states(
+        self,
+        new_states: List[Optional[torch.Tensor]],
+    ) -> None:
+        """Create decoder hidden states.
+
+        Args:
+            new_states: Decoder hidden states. [N x None]
+
+        Returns:
+            states: Decoder hidden states. None
+
+        """
+        return None
```

### Comparing `espnet-202304/espnet2/asr_transducer/encoder/blocks/branchformer.py` & `espnet-202308/espnet2/asr_transducer/encoder/blocks/branchformer.py`

 * *Files 9% similar despite different names*

```diff
@@ -54,15 +54,16 @@
         self.linear_size = linear_size
         self.cache = None
 
     def reset_streaming_cache(self, left_context: int, device: torch.device) -> None:
         """Initialize/Reset self-attention and convolution modules cache for streaming.
 
         Args:
-            left_context: Number of left frames during chunk-by-chunk inference.
+            left_context: Number of previous frames the attention module can see
+                          in current chunk.
             device: Device to use for cache tensor.
 
         """
         self.cache = [
             torch.zeros(
                 (1, left_context, self.block_size),
                 device=device,
@@ -106,15 +107,15 @@
         x1 = self.dropout(
             self.self_att(x1, x1, x1, pos_enc, mask=mask, chunk_mask=chunk_mask)
         )
 
         x2 = self.norm_mlp(x2)
 
         x2 = self.channel_proj1(x2)
-        x2, _ = self.conv_mod(x2)
+        x2, _ = self.conv_mod(x2, mask)
         x2 = self.channel_proj2(x2)
 
         x2 = self.dropout(x2)
 
         x = x + self.dropout(self.merge_proj(torch.cat([x1, x2], dim=-1)))
 
         x = self.norm_final(x)
@@ -123,24 +124,23 @@
 
     def chunk_forward(
         self,
         x: torch.Tensor,
         pos_enc: torch.Tensor,
         mask: torch.Tensor,
         left_context: int = 0,
-        right_context: int = 0,
     ) -> Tuple[torch.Tensor, torch.Tensor]:
         """Encode chunk of input sequence.
 
         Args:
             x: Branchformer input sequences. (B, T, D_block)
             pos_enc: Positional embedding sequences. (B, 2 * (T - 1), D_block)
             mask: Source mask. (B, T_2)
-            left_context: Number of frames in left context.
-            right_context: Number of frames in right context.
+            left_context: Number of previous frames the attention module can see
+                          in current chunk.
 
         Returns:
             x: Branchformer output sequences. (B, T, D_block)
             pos_enc: Positional embedding sequences. (B, 2 * (T - 1), D_block)
 
         """
         x1 = x
@@ -150,29 +150,23 @@
 
         if left_context > 0:
             key = torch.cat([self.cache[0], x1], dim=1)
         else:
             key = x1
         val = key
 
-        if right_context > 0:
-            att_cache = key[:, -(left_context + right_context) : -right_context, :]
-        else:
-            att_cache = key[:, -left_context:, :]
-
+        att_cache = key[:, -left_context:, :]
         x1 = self.self_att(x1, key, val, pos_enc, mask=mask, left_context=left_context)
 
         x2 = self.norm_mlp(x2)
-        x2 = self.channel_proj1(x2)
-
-        x2, conv_cache = self.conv_mod(
-            x2, cache=self.cache[1], right_context=right_context
-        )
 
+        x2 = self.channel_proj1(x2)
+        x2, conv_cache = self.conv_mod(x2, cache=self.cache[1])
         x2 = self.channel_proj2(x2)
 
         x = x + self.merge_proj(torch.cat([x1, x2], dim=-1))
 
         x = self.norm_final(x)
+
         self.cache = [att_cache, conv_cache]
 
         return x, pos_enc
```

### Comparing `espnet-202304/espnet2/asr_transducer/encoder/blocks/conformer.py` & `espnet-202308/espnet2/asr_transducer/encoder/blocks/conformer.py`

 * *Files 5% similar despite different names*

```diff
@@ -54,15 +54,16 @@
         self.block_size = block_size
         self.cache = None
 
     def reset_streaming_cache(self, left_context: int, device: torch.device) -> None:
         """Initialize/Reset self-attention and convolution modules cache for streaming.
 
         Args:
-            left_context: Number of left frames during chunk-by-chunk inference.
+            left_context: Number of previous frames the attention module can see
+                          in current chunk.
             device: Device to use for cache tensor.
 
         """
         self.cache = [
             torch.zeros(
                 (1, left_context, self.block_size),
                 device=device,
@@ -103,19 +104,18 @@
         x = self.norm_macaron(x)
         x = residual + self.feed_forward_scale * self.dropout(
             self.feed_forward_macaron(x)
         )
 
         residual = x
         x = self.norm_self_att(x)
-        x_q = x
 
         x = residual + self.dropout(
             self.self_att(
-                x_q,
+                x,
                 x,
                 x,
                 pos_enc,
                 mask,
                 chunk_mask=chunk_mask,
             )
         )
@@ -137,67 +137,61 @@
 
     def chunk_forward(
         self,
         x: torch.Tensor,
         pos_enc: torch.Tensor,
         mask: torch.Tensor,
         left_context: int = 0,
-        right_context: int = 0,
     ) -> Tuple[torch.Tensor, torch.Tensor]:
         """Encode chunk of input sequence.
 
         Args:
             x: Conformer input sequences. (B, T, D_block)
             pos_enc: Positional embedding sequences. (B, 2 * (T - 1), D_block)
             mask: Source mask. (B, T_2)
-            left_context: Number of frames in left context.
-            right_context: Number of frames in right context.
+            left_context: Number of previous frames the attention module can see
+                          in current chunk.
 
         Returns:
             x: Conformer output sequences. (B, T, D_block)
             pos_enc: Positional embedding sequences. (B, 2 * (T - 1), D_block)
 
         """
         residual = x
 
         x = self.norm_macaron(x)
         x = residual + self.feed_forward_scale * self.feed_forward_macaron(x)
 
         residual = x
         x = self.norm_self_att(x)
+
         if left_context > 0:
             key = torch.cat([self.cache[0], x], dim=1)
         else:
             key = x
-        val = key
-
-        if right_context > 0:
-            att_cache = key[:, -(left_context + right_context) : -right_context, :]
-        else:
-            att_cache = key[:, -left_context:, :]
+        att_cache = key[:, -left_context:, :]
 
         x = residual + self.self_att(
             x,
             key,
-            val,
+            key,
             pos_enc,
             mask,
             left_context=left_context,
         )
 
         residual = x
 
         x = self.norm_conv(x)
-        x, conv_cache = self.conv_mod(
-            x, cache=self.cache[1], right_context=right_context
-        )
-
+        x, conv_cache = self.conv_mod(x, cache=self.cache[1])
         x = residual + x
+
         residual = x
 
         x = self.norm_feed_forward(x)
         x = residual + self.feed_forward_scale * self.feed_forward(x)
 
         x = self.norm_final(x)
+
         self.cache = [att_cache, conv_cache]
 
         return x, pos_enc
```

### Comparing `espnet-202304/espnet2/asr_transducer/encoder/blocks/conv1d.py` & `espnet-202308/espnet2/asr_transducer/encoder/blocks/conv1d.py`

 * *Files 8% similar despite different names*

```diff
@@ -80,15 +80,16 @@
 
         self.cache = None
 
     def reset_streaming_cache(self, left_context: int, device: torch.device) -> None:
         """Initialize/Reset Conv1d cache for streaming.
 
         Args:
-            left_context: Number of left frames during chunk-by-chunk inference.
+            left_context: Number of previous frames the attention module can see
+                          in current chunk (not used here).
             device: Device to use for cache tensor.
 
         """
         self.cache = torch.zeros(
             (1, self.input_size, self.kernel_size - 1), device=device
         )
 
@@ -138,36 +139,31 @@
 
     def chunk_forward(
         self,
         x: torch.Tensor,
         pos_enc: torch.Tensor,
         mask: torch.Tensor,
         left_context: int = 0,
-        right_context: int = 0,
     ) -> Tuple[torch.Tensor, torch.Tensor]:
         """Encode chunk of input sequence.
 
         Args:
             x: Conv1d input sequences. (B, T, D_in)
             pos_enc: Positional embedding sequences. (B, 2 * (T - 1), D_in)
             mask: Source mask. (B, T)
-            left_context: Number of frames in left context.
-            right_context: Number of frames in right context.
+            left_context: Number of previous frames the attention module can see
+                          in current chunk (not used here).
 
         Returns:
             x: Conv1d output sequences. (B, T, D_out)
             pos_enc: Positional embedding sequences. (B, 2 * (T - 1), D_out)
 
         """
         x = torch.cat([self.cache, x.transpose(1, 2)], dim=2)
-
-        if right_context > 0:
-            self.cache = x[:, :, -(self.lorder + right_context) : -right_context]
-        else:
-            self.cache = x[:, :, -self.lorder :]
+        self.cache = x[:, :, -self.lorder :]
 
         x = self.conv(x)
 
         if self.batch_norm:
             x = self.bn(x)
 
         x = self.dropout(x)
```

### Comparing `espnet-202304/espnet2/asr_transducer/encoder/blocks/conv_input.py` & `espnet-202308/espnet2/asr_transducer/encoder/blocks/conv_input.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 """ConvInput block for Transducer encoder."""
 
 from typing import Optional, Tuple, Union
 
 import torch
 
-from espnet2.asr_transducer.utils import sub_factor_to_params
+from espnet2.asr_transducer.utils import get_convinput_module_parameters
 
 
 class ConvInput(torch.nn.Module):
     """ConvInput module definition.
 
     Args:
         input_size: Input size.
@@ -27,66 +27,65 @@
         vgg_like: bool = True,
         output_size: Optional[int] = None,
     ) -> None:
         """Construct a ConvInput object."""
         super().__init__()
 
         self.subsampling_factor = subsampling_factor
+        self.vgg_like = vgg_like
 
         if vgg_like:
             conv_size1, conv_size2 = conv_size
 
-            kernel_1 = int(subsampling_factor / 2)
+            self.maxpool_kernel1, output_proj = get_convinput_module_parameters(
+                input_size, conv_size2, subsampling_factor, is_vgg=True
+            )
 
             self.conv = torch.nn.Sequential(
                 torch.nn.Conv2d(1, conv_size1, 3, stride=1, padding=1),
                 torch.nn.ReLU(),
-                torch.nn.Conv2d(conv_size1, conv_size1, 3, stride=1, padding=1),
+                torch.nn.Conv2d(conv_size1, conv_size1, 3, stride=1, padding=0),
                 torch.nn.ReLU(),
-                torch.nn.MaxPool2d((kernel_1, 2)),
+                torch.nn.MaxPool2d(
+                    self.maxpool_kernel1, stride=2, padding=0, ceil_mode=True
+                ),
                 torch.nn.Conv2d(conv_size1, conv_size2, 3, stride=1, padding=1),
                 torch.nn.ReLU(),
-                torch.nn.Conv2d(conv_size2, conv_size2, 3, stride=1, padding=1),
+                torch.nn.Conv2d(conv_size2, conv_size2, 3, stride=1, padding=0),
                 torch.nn.ReLU(),
-                torch.nn.MaxPool2d((2, 2)),
+                torch.nn.MaxPool2d(2, stride=2, padding=0, ceil_mode=True),
             )
-
-            output_proj = conv_size2 * ((input_size // 2) // 2)
-
-            self.stride_1 = kernel_1
         else:
-            kernel_2, stride_2, conv_2_output_size = sub_factor_to_params(
-                subsampling_factor,
-                input_size,
+            (
+                self.conv_kernel2,
+                self.conv_stride2,
+            ), output_proj = get_convinput_module_parameters(
+                input_size, conv_size, subsampling_factor, is_vgg=False
             )
 
             self.conv = torch.nn.Sequential(
                 torch.nn.Conv2d(1, conv_size, 3, 2),
                 torch.nn.ReLU(),
-                torch.nn.Conv2d(conv_size, conv_size, kernel_2, stride_2),
+                torch.nn.Conv2d(
+                    conv_size, conv_size, self.conv_kernel2, self.conv_stride2
+                ),
                 torch.nn.ReLU(),
             )
 
-            output_proj = conv_size * conv_2_output_size
-
-            self.kernel_2 = kernel_2
-            self.stride_2 = stride_2
-
-        self.vgg_like = vgg_like
         self.min_frame_length = 7 if subsampling_factor < 6 else 11
 
         if output_size is not None:
             self.output = torch.nn.Linear(output_proj, output_size)
             self.output_size = output_size
         else:
             self.output = None
             self.output_size = output_proj
 
     def forward(
-        self, x: torch.Tensor, mask: Optional[torch.Tensor]
+        self, x: torch.Tensor, mask: Optional[torch.Tensor] = None
     ) -> Tuple[torch.Tensor, torch.Tensor]:
         """Encode input sequences.
 
         Args:
             x: ConvInput input sequences. (B, T, D_feats)
             mask: Mask of input sequences. (B, 1, T)
 
@@ -103,22 +102,7 @@
         if self.output is not None:
             x = self.output(x)
 
         if mask is not None:
             mask = mask[:, : x.size(1)]
 
         return x, mask
-
-    def get_size_before_subsampling(self, size: int) -> int:
-        """Return the original size before subsampling for a given size.
-
-        Args:
-            size: Number of frames after subsampling.
-
-        Returns:
-            : Number of frames before subsampling.
-
-        """
-        if self.vgg_like:
-            return ((size * 2) * self.stride_1) + 1
-
-        return ((size + 2) * 2) + (self.kernel_2 - 1) * self.stride_2
```

### Comparing `espnet-202304/espnet2/asr_transducer/encoder/building.py` & `espnet-202308/espnet2/asr_transducer/encoder/building.py`

 * *Files 13% similar despite different names*

```diff
@@ -3,26 +3,28 @@
 from typing import Any, Dict, List, Optional, Union
 
 from espnet2.asr_transducer.activation import get_activation
 from espnet2.asr_transducer.encoder.blocks.branchformer import Branchformer
 from espnet2.asr_transducer.encoder.blocks.conformer import Conformer
 from espnet2.asr_transducer.encoder.blocks.conv1d import Conv1d
 from espnet2.asr_transducer.encoder.blocks.conv_input import ConvInput
+from espnet2.asr_transducer.encoder.blocks.ebranchformer import EBranchformer
 from espnet2.asr_transducer.encoder.modules.attention import (  # noqa: H301
     RelPositionMultiHeadedAttention,
 )
 from espnet2.asr_transducer.encoder.modules.convolution import (  # noqa: H301
     ConformerConvolution,
     ConvolutionalSpatialGatingUnit,
+    DepthwiseConvolution,
 )
 from espnet2.asr_transducer.encoder.modules.multi_blocks import MultiBlocks
-from espnet2.asr_transducer.encoder.modules.normalization import get_normalization
 from espnet2.asr_transducer.encoder.modules.positional_encoding import (  # noqa: H301
     RelPositionalEncoding,
 )
+from espnet2.asr_transducer.normalization import get_normalization
 from espnet.nets.pytorch_backend.transformer.positionwise_feed_forward import (
     PositionwiseFeedForward,
 )
 
 
 def build_main_parameters(
     pos_wise_act_type: str = "swish",
@@ -30,38 +32,41 @@
     pos_enc_dropout_rate: float = 0.0,
     pos_enc_max_len: int = 5000,
     simplified_att_score: bool = False,
     norm_type: str = "layer_norm",
     conv_mod_norm_type: str = "layer_norm",
     after_norm_eps: Optional[float] = None,
     after_norm_partial: Optional[float] = None,
+    blockdrop_rate: float = 0.0,
     dynamic_chunk_training: bool = False,
     short_chunk_threshold: float = 0.75,
     short_chunk_size: int = 25,
-    left_chunk_size: int = 0,
+    num_left_chunks: int = 0,
     **activation_parameters,
 ) -> Dict[str, Any]:
     """Build encoder main parameters.
 
     Args:
-        pos_wise_act_type: Conformer position-wise feed-forward activation type.
-        conv_mod_act_type: Conformer convolution module activation type.
+        pos_wise_act_type: X-former position-wise feed-forward activation type.
+        conv_mod_act_type: X-former convolution module activation type.
         pos_enc_dropout_rate: Positional encoding dropout rate.
         pos_enc_max_len: Positional encoding maximum length.
         simplified_att_score: Whether to use simplified attention score computation.
         norm_type: X-former normalization module type.
         conv_mod_norm_type: Conformer convolution module normalization type.
         after_norm_eps: Epsilon value for the final normalization.
         after_norm_partial: Value for the final normalization with RMSNorm.
+        blockdrop_rate: Probability threshold of dropping out each encoder block.
         dynamic_chunk_training: Whether to use dynamic chunk training.
         short_chunk_threshold: Threshold for dynamic chunk selection.
         short_chunk_size: Minimum number of frames during dynamic chunk training.
-        left_chunk_size: Number of frames in left context.
-        **activations_parameters: Parameters of the activation functions.
-                                    (See espnet2/asr_transducer/activation.py)
+        num_left_chunks: Number of left chunks the attention module can see.
+                         (null or negative value means full context)
+        **activation_parameters: Parameters of the activation functions.
+                                 (See espnet2/asr_transducer/activation.py)
 
     Returns:
         : Main encoder parameters
 
     """
     main_params = {}
 
@@ -82,18 +87,20 @@
     main_params["conv_mod_norm_type"] = conv_mod_norm_type
 
     (
         main_params["after_norm_class"],
         main_params["after_norm_args"],
     ) = get_normalization(norm_type, eps=after_norm_eps, partial=after_norm_partial)
 
+    main_params["blockdrop_rate"] = blockdrop_rate
+
     main_params["dynamic_chunk_training"] = dynamic_chunk_training
     main_params["short_chunk_threshold"] = max(0, short_chunk_threshold)
     main_params["short_chunk_size"] = max(0, short_chunk_size)
-    main_params["left_chunk_size"] = max(0, left_chunk_size)
+    main_params["num_left_chunks"] = max(0, num_left_chunks)
 
     return main_params
 
 
 def build_positional_encoding(
     block_size: int, configuration: Dict[str, Any]
 ) -> RelPositionalEncoding:
@@ -136,15 +143,15 @@
         output_size=configuration["output_size"],
     )
 
 
 def build_branchformer_block(
     configuration: List[Dict[str, Any]],
     main_params: Dict[str, Any],
-) -> Conformer:
+) -> Branchformer:
     """Build Branchformer block.
 
     Args:
         configuration: Branchformer block configuration.
         main_params: Encoder main parameters.
 
     Returns:
@@ -281,14 +288,90 @@
         relu=configuration.get("relu", True),
         batch_norm=configuration.get("batch_norm", False),
         causal=causal,
         dropout_rate=configuration.get("dropout_rate", 0.0),
     )
 
 
+def build_ebranchformer_block(
+    configuration: List[Dict[str, Any]],
+    main_params: Dict[str, Any],
+) -> EBranchformer:
+    """Build E-Branchformer block.
+
+    Args:
+        configuration: E-Branchformer block configuration.
+        main_params: Encoder main parameters.
+
+    Returns:
+        : E-Branchformer block function.
+
+    """
+    hidden_size = configuration["hidden_size"]
+    linear_size = configuration["linear_size"]
+
+    dropout_rate = configuration.get("dropout_rate", 0.0)
+
+    pos_wise_args = (
+        hidden_size,
+        linear_size,
+        configuration.get("pos_wise_dropout_rate", 0.0),
+        main_params["pos_wise_act"],
+    )
+
+    conv_mod_norm_class, conv_mod_norm_args = get_normalization(
+        main_params["conv_mod_norm_type"],
+        eps=configuration.get("conv_mod_norm_eps"),
+        partial=configuration.get("conv_mod_norm_partial"),
+    )
+
+    conv_mod_args = (
+        linear_size,
+        configuration["conv_mod_kernel_size"],
+        conv_mod_norm_class,
+        conv_mod_norm_args,
+        dropout_rate,
+        main_params["dynamic_chunk_training"],
+    )
+
+    mult_att_args = (
+        configuration.get("heads", 4),
+        hidden_size,
+        configuration.get("att_dropout_rate", 0.0),
+        main_params["simplified_att_score"],
+    )
+
+    depthwise_conv_args = (
+        hidden_size,
+        configuration.get(
+            "depth_conv_kernel_size", configuration["conv_mod_kernel_size"]
+        ),
+        main_params["dynamic_chunk_training"],
+    )
+
+    norm_class, norm_args = get_normalization(
+        main_params["norm_type"],
+        eps=configuration.get("norm_eps"),
+        partial=configuration.get("norm_partial"),
+    )
+
+    return lambda: EBranchformer(
+        hidden_size,
+        linear_size,
+        RelPositionMultiHeadedAttention(*mult_att_args),
+        PositionwiseFeedForward(*pos_wise_args),
+        PositionwiseFeedForward(*pos_wise_args),
+        ConvolutionalSpatialGatingUnit(*conv_mod_args),
+        DepthwiseConvolution(*depthwise_conv_args),
+        norm_class=norm_class,
+        norm_args=norm_args,
+        dropout_rate=dropout_rate,
+    )
+
+
 def build_body_blocks(
     configuration: List[Dict[str, Any]],
     main_params: Dict[str, Any],
     output_size: int,
 ) -> MultiBlocks:
     """Build encoder body blocks.
 
@@ -317,18 +400,21 @@
 
         if block_type == "branchformer":
             module = build_branchformer_block(c, main_params)
         elif block_type == "conformer":
             module = build_conformer_block(c, main_params)
         elif block_type == "conv1d":
             module = build_conv1d_block(c, main_params["dynamic_chunk_training"])
+        elif block_type == "ebranchformer":
+            module = build_ebranchformer_block(c, main_params)
         else:
             raise NotImplementedError
 
         fn_modules.append(module)
 
     return MultiBlocks(
         [fn() for fn in fn_modules],
         output_size,
         norm_class=main_params["after_norm_class"],
         norm_args=main_params["after_norm_args"],
+        blockdrop_rate=main_params["blockdrop_rate"],
     )
```

### Comparing `espnet-202304/espnet2/asr_transducer/encoder/encoder.py` & `espnet-202308/espnet2/asr_transducer/encoder/encoder.py`

 * *Files 12% similar despite different names*

```diff
@@ -53,36 +53,22 @@
         self.encoders = build_body_blocks(body_conf, main_params, output_size)
 
         self.output_size = output_size
 
         self.dynamic_chunk_training = main_params["dynamic_chunk_training"]
         self.short_chunk_threshold = main_params["short_chunk_threshold"]
         self.short_chunk_size = main_params["short_chunk_size"]
-        self.left_chunk_size = main_params["left_chunk_size"]
+        self.num_left_chunks = main_params["num_left_chunks"]
 
-    def get_encoder_input_raw_size(self, size: int, hop_length: int) -> int:
-        """Return the corresponding number of sample for a given chunk size, in frames.
-
-        Where size is the number of features frames after applying subsampling.
-
-        Args:
-            size: Number of frames after subsampling.
-            hop_length: Frontend's hop length
-
-        Returns:
-            : Number of raw samples
-
-        """
-        return self.embed.get_size_before_subsampling(size) * hop_length
-
-    def reset_streaming_cache(self, left_context: int, device: torch.device) -> None:
-        """Initialize/Reset encoder streaming cache.
+    def reset_cache(self, left_context: int, device: torch.device) -> None:
+        """Initialize/Reset encoder cache for streaming.
 
         Args:
-            left_context: Number of frames in left context.
+            left_context: Number of previous frames (AFTER subsampling) the attention
+                          module can see in current chunk.
             device: Device ID.
 
         """
         return self.encoders.reset_streaming_cache(left_context, device)
 
     def forward(
         self,
@@ -125,15 +111,15 @@
                 chunk_size = max_len
             else:
                 chunk_size = (chunk_size % self.short_chunk_size) + 1
 
             chunk_mask = make_chunk_mask(
                 x.size(1),
                 chunk_size,
-                left_chunk_size=self.left_chunk_size,
+                num_left_chunks=self.num_left_chunks,
                 device=x.device,
             )
         else:
             chunk_mask = None
 
         x = self.encoders(
             x,
@@ -146,48 +132,45 @@
 
     def chunk_forward(
         self,
         x: torch.Tensor,
         x_len: torch.Tensor,
         processed_frames: torch.tensor,
         left_context: int = 32,
-        right_context: int = 0,
     ) -> torch.Tensor:
         """Encode input sequences as chunks.
 
         Args:
             x: Encoder input features. (1, T_in, F)
             x_len: Encoder input features lengths. (1,)
             processed_frames: Number of frames already seen.
-            left_context: Number of frames in left context.
-            right_context: Number of frames in right context.
+            left_context: Number of previous frames (AFTER subsampling) the attention
+                          module can see in current chunk.
 
         Returns:
            x: Encoder outputs. (B, T_out, D_enc)
 
         """
         mask = make_source_mask(x_len)
         x, mask = self.embed(x, mask)
 
-        if left_context > 0:
-            processed_mask = (
-                torch.arange(left_context, device=x.device)
-                .view(1, left_context)
-                .flip(1)
-            )
-            processed_mask = processed_mask >= processed_frames
-            mask = torch.cat([processed_mask, mask], dim=1)
+        x = x[:, 1:-1, :]
+        mask = mask[:, 1:-1]
 
         pos_enc = self.pos_enc(x, left_context=left_context)
 
+        processed_mask = (
+            torch.arange(left_context, device=x.device).view(1, left_context).flip(1)
+        )
+
+        processed_mask = processed_mask >= processed_frames
+
+        mask = torch.cat([processed_mask, mask], dim=1)
+
         x = self.encoders.chunk_forward(
             x,
             pos_enc,
             mask,
             left_context=left_context,
-            right_context=right_context,
         )
 
-        if right_context > 0:
-            x = x[:, 0:-right_context, :]
-
         return x
```

### Comparing `espnet-202304/espnet2/asr_transducer/encoder/modules/attention.py` & `espnet-202308/espnet2/asr_transducer/encoder/modules/attention.py`

 * *Files 7% similar despite different names*

```diff
@@ -58,15 +58,16 @@
         self.attn = None
 
     def rel_shift(self, x: torch.Tensor, left_context: int = 0) -> torch.Tensor:
         """Compute relative positional encoding.
 
         Args:
             x: Input sequence. (B, H, T_1, 2 * T_1 - 1)
-            left_context: Number of frames in left context.
+            left_context: Number of previous frames to use for current chunk
+                          attention computation.
 
         Returns:
             x: Output sequence. (B, H, T_1, T_2)
 
         """
         batch_size, n_heads, time1, n = x.shape
         time2 = time1 + left_context
@@ -90,15 +91,16 @@
 
         Reference: https://github.com/k2-fsa/icefall/pull/458
 
         Args:
             query: Transformed query tensor. (B, H, T_1, d_k)
             key: Transformed key tensor. (B, H, T_2, d_k)
             pos_enc: Positional embedding tensor. (B, 2 * T_1 - 1, size)
-            left_context: Number of frames in left context.
+            left_context: Number of previous frames to use for current chunk
+                          attention computation.
 
         Returns:
             : Attention score. (B, H, T_1, T_2)
 
         """
         pos_enc = self.linear_pos(pos_enc)
 
@@ -120,15 +122,16 @@
     ) -> torch.Tensor:
         """Attention score computation.
 
         Args:
             query: Transformed query tensor. (B, H, T_1, d_k)
             key: Transformed key tensor. (B, H, T_2, d_k)
             pos_enc: Positional embedding tensor. (B, 2 * T_1 - 1, size)
-            left_context: Number of frames in left context.
+            left_context: Number of previous frames to use for current chunk
+                          attention computation.
 
         Returns:
             : Attention score. (B, H, T_1, T_2)
 
         """
         p = self.linear_pos(pos_enc).view(pos_enc.size(0), -1, self.num_heads, self.d_k)
 
@@ -233,15 +236,16 @@
         Args:
             query: Query tensor. (B, T_1, size)
             key: Key tensor. (B, T_2, size)
             value: Value tensor. (B, T_2, size)
             pos_enc: Positional embedding tensor. (B, 2 * T_1 - 1, size)
             mask: Source mask. (B, T_2)
             chunk_mask: Chunk mask. (T_1, T_1)
-            left_context: Number of frames in left context.
+            left_context: Number of previous frames to use for current chunk
+                          attention computation.
 
         Returns:
             : Output tensor. (B, T_1, H * d_k)
 
         """
         q, k, v = self.forward_qkv(query, key, value)
```

### Comparing `espnet-202304/espnet2/asr_transducer/encoder/modules/convolution.py` & `espnet-202308/espnet2/asr_transducer/encoder/modules/convolution.py`

 * *Files 18% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 
 class ConformerConvolution(torch.nn.Module):
     """ConformerConvolution module definition.
 
     Args:
         channels: The number of channels.
         kernel_size: Size of the convolving kernel.
-        activation: Type of activation function.
+        activation: Activation function.
         norm_args: Normalization module arguments.
         causal: Whether to use causal convolution (set to True if streaming).
 
     """
 
     def __init__(
         self,
@@ -65,46 +65,41 @@
         )
 
         self.activation = activation
 
     def forward(
         self,
         x: torch.Tensor,
-        cache: Optional[torch.Tensor] = None,
         mask: Optional[torch.Tensor] = None,
-        right_context: int = 0,
+        cache: Optional[torch.Tensor] = None,
     ) -> Tuple[torch.Tensor, torch.Tensor]:
         """Compute convolution module.
 
         Args:
             x: ConformerConvolution input sequences. (B, T, D_hidden)
-            cache: ConformerConvolution input cache. (1, conv_kernel, D_hidden)
-            right_context: Number of frames in right context.
+            mask: Source mask. (B, T_2)
+            cache: ConformerConvolution input cache. (1, D_hidden, conv_kernel)
 
         Returns:
-            x: ConformerConvolution output sequences. (B, T, D_hidden)
-            cache: ConformerConvolution output cache. (1, conv_kernel, D_hidden)
+            x: ConformerConvolution output sequences. (B, ?, D_hidden)
+            cache: ConformerConvolution output cache. (1, D_hidden, conv_kernel)
 
         """
         x = self.pointwise_conv1(x.transpose(1, 2))
         x = torch.nn.functional.glu(x, dim=1)
 
         if mask is not None:
             x.masked_fill_(mask.unsqueeze(1).expand_as(x), 0.0)
 
         if self.lorder > 0:
             if cache is None:
                 x = torch.nn.functional.pad(x, (self.lorder, 0), "constant", 0.0)
             else:
                 x = torch.cat([cache, x], dim=2)
-
-                if right_context > 0:
-                    cache = x[:, :, -(self.lorder + right_context) : -right_context]
-                else:
-                    cache = x[:, :, -self.lorder :]
+                cache = x[..., -self.lorder :]
 
         x = self.depthwise_conv(x)
         x = self.activation(self.norm(x))
 
         x = self.pointwise_conv2(x).transpose(1, 2)
 
         return x, cache
@@ -159,42 +154,114 @@
         self.activation = torch.nn.Identity()
 
         self.dropout = torch.nn.Dropout(dropout_rate)
 
     def forward(
         self,
         x: torch.Tensor,
+        mask: Optional[torch.Tensor] = None,
         cache: Optional[torch.Tensor] = None,
-        right_context: int = 0,
     ) -> Tuple[torch.Tensor, torch.Tensor]:
         """Compute convolution module.
 
         Args:
             x: ConvolutionalSpatialGatingUnit input sequences. (B, T, D_hidden)
+            mask: Source mask. (B, T_2)
             cache: ConvolutionalSpationGatingUnit input cache.
-                   (1, conv_kernel, D_hidden)
-            right_context: Number of frames in right context.
+                   (1, D_hidden, conv_kernel)
 
         Returns:
-            x: ConvolutionalSpatialGatingUnit output sequences. (B, T, D_hidden // 2)
+            x: ConvolutionalSpatialGatingUnit output sequences. (B, ?, D_hidden)
 
         """
         x_r, x_g = x.chunk(2, dim=-1)
 
         x_g = self.norm(x_g).transpose(1, 2)
 
+        if mask is not None:
+            x_g.masked_fill_(mask.unsqueeze(1).expand_as(x_g), 0.0)
+
         if self.lorder > 0:
             if cache is None:
                 x_g = torch.nn.functional.pad(x_g, (self.lorder, 0), "constant", 0.0)
             else:
                 x_g = torch.cat([cache, x_g], dim=2)
-
-                if right_context > 0:
-                    cache = x_g[:, :, -(self.lorder + right_context) : -right_context]
-                else:
-                    cache = x_g[:, :, -self.lorder :]
+                cache = x_g[..., -self.lorder :]
 
         x_g = self.conv(x_g).transpose(1, 2)
 
         x = self.dropout(x_r * self.activation(x_g))
 
         return x, cache
+
+
+class DepthwiseConvolution(torch.nn.Module):
+    """Depth-wise Convolution module definition.
+
+    Args:
+        size: Initial size to determine the number of channels.
+        kernel_size: Size of the convolving kernel.
+        causal: Whether to use causal convolution (set to True if streaming).
+
+    """
+
+    def __init__(
+        self,
+        size: int,
+        kernel_size: int,
+        causal: bool = False,
+    ) -> None:
+        """Construct a DepthwiseConvolution object."""
+        super().__init__()
+
+        channels = size + size
+
+        self.kernel_size = kernel_size
+
+        if causal:
+            self.lorder = kernel_size - 1
+            padding = 0
+        else:
+            self.lorder = 0
+            padding = (kernel_size - 1) // 2
+
+        self.conv = torch.nn.Conv1d(
+            channels,
+            channels,
+            kernel_size,
+            stride=1,
+            padding=padding,
+            groups=channels,
+        )
+
+    def forward(
+        self,
+        x: torch.Tensor,
+        mask: Optional[torch.Tensor] = None,
+        cache: Optional[torch.Tensor] = None,
+    ) -> Tuple[torch.Tensor, torch.Tensor]:
+        """Compute convolution module.
+
+        Args:
+            x: DepthwiseConvolution input sequences. (B, T, D_hidden)
+            mask: Source mask. (B, T_2)
+            cache: DepthwiseConvolution input cache. (1, conv_kernel, D_hidden)
+
+        Returns:
+            x: DepthwiseConvolution output sequences. (B, ?, D_hidden)
+
+        """
+        x = x.transpose(1, 2)
+
+        if mask is not None:
+            x.masked_fill_(mask.unsqueeze(1).expand_as(x), 0.0)
+
+        if self.lorder > 0:
+            if cache is None:
+                x = torch.nn.functional.pad(x, (self.lorder, 0), "constant", 0.0)
+            else:
+                x = torch.cat([cache, x], dim=2)
+                cache = x[..., -self.lorder :]
+
+        x = self.conv(x).transpose(1, 2)
+
+        return x, cache
```

### Comparing `espnet-202304/espnet2/asr_transducer/encoder/modules/multi_blocks.py` & `espnet-202308/espnet2/asr_transducer/encoder/modules/multi_blocks.py`

 * *Files 17% similar despite different names*

```diff
@@ -9,42 +9,47 @@
     """MultiBlocks definition.
 
     Args:
         block_list: Individual blocks of the encoder architecture.
         output_size: Architecture output size.
         norm_class: Normalization module class.
         norm_args: Normalization module arguments.
+        blockdrop_rate: Probability threshold of dropping out each block.
 
     """
 
     def __init__(
         self,
         block_list: List[torch.nn.Module],
         output_size: int,
         norm_class: torch.nn.Module = torch.nn.LayerNorm,
         norm_args: Optional[Dict] = None,
+        blockdrop_rate: int = 0.0,
     ) -> None:
         """Construct a MultiBlocks object."""
         super().__init__()
 
         self.blocks = torch.nn.ModuleList(block_list)
         self.norm_blocks = norm_class(output_size, **norm_args)
 
-        self.num_blocks = len(block_list)
+        self.blockdrop_rate = blockdrop_rate
+        self.blockdrop_decay = 1.0 / len(self.blocks)
+        self.keep_probs = torch.ones(len(self.blocks))
 
     def reset_streaming_cache(self, left_context: int, device: torch.device) -> None:
         """Initialize/Reset encoder streaming cache.
 
         Args:
-            left_context: Number of left frames during chunk-by-chunk inference.
+            left_context: Number of previous frames the attention module can see
+                          in current chunk (used by Conformer and Branchformer block).
             device: Device to use for cache tensor.
 
         """
-        for idx in range(self.num_blocks):
-            self.blocks[idx].reset_streaming_cache(left_context, device)
+        for block in self.blocks:
+            block.reset_streaming_cache(left_context, device)
 
     def forward(
         self,
         x: torch.Tensor,
         pos_enc: torch.Tensor,
         mask: torch.Tensor,
         chunk_mask: Optional[torch.Tensor] = None,
@@ -57,47 +62,51 @@
             mask: Source mask. (B, T)
             chunk_mask: Chunk mask. (T_2, T_2)
 
         Returns:
             x: Output sequences. (B, T, D_block_N)
 
         """
-        for block_index, block in enumerate(self.blocks):
-            x, mask, pos_enc = block(x, pos_enc, mask, chunk_mask=chunk_mask)
+        self.keep_probs[:-1].uniform_()
+
+        for idx, block in enumerate(self.blocks):
+            if not self.training or (
+                self.keep_probs[idx]
+                >= (self.blockdrop_rate * (self.blockdrop_decay * idx))
+            ):
+                x, mask, pos_enc = block(x, pos_enc, mask, chunk_mask=chunk_mask)
 
         x = self.norm_blocks(x)
 
         return x
 
     def chunk_forward(
         self,
         x: torch.Tensor,
         pos_enc: torch.Tensor,
         mask: torch.Tensor,
         left_context: int = 0,
-        right_context: int = 0,
     ) -> torch.Tensor:
         """Forward each block of the encoder architecture.
 
         Args:
             x: MultiBlocks input sequences. (B, T, D_block_1)
             pos_enc: Positional embedding sequences. (B, 2 * (T - 1), D_att)
             mask: Source mask. (B, T_2)
-            left_context: Number of frames in left context.
-            right_context: Number of frames in right context.
+            left_context: Number of previous frames the attention module can see
+                          in current chunk (used by Conformer and Branchformer block).
 
         Returns:
             x: MultiBlocks output sequences. (B, T, D_block_N)
 
         """
-        for block_idx, block in enumerate(self.blocks):
+        for block in self.blocks:
             x, pos_enc = block.chunk_forward(
                 x,
                 pos_enc,
                 mask,
                 left_context=left_context,
-                right_context=right_context,
             )
 
         x = self.norm_blocks(x)
 
         return x
```

### Comparing `espnet-202304/espnet2/asr_transducer/encoder/modules/normalization.py` & `espnet-202308/espnet2/asr_transducer/normalization.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-"""Normalization modules for X-former blocks."""
+"""Normalization modules for Transducer."""
 
 from typing import Dict, Optional, Tuple
 
 import torch
 
 
 def get_normalization(
```

### Comparing `espnet-202304/espnet2/asr_transducer/encoder/modules/positional_encoding.py` & `espnet-202308/espnet2/asr_transducer/encoder/modules/positional_encoding.py`

 * *Files 15% similar despite different names*

```diff
@@ -32,15 +32,16 @@
         self._register_load_state_dict_pre_hook(_pre_hook)
 
     def extend_pe(self, x: torch.Tensor, left_context: int = 0) -> None:
         """Reset positional encoding.
 
         Args:
             x: Input sequences. (B, T, ?)
-            left_context: Number of frames in left context.
+            left_context: Number of previous frames the attention module can see
+                          in current chunk.
 
         """
         time1 = x.size(1) + left_context
 
         if self.pe is not None:
             if self.pe.size(1) >= time1 * 2 - 1:
                 if self.pe.dtype != x.dtype or self.pe.device != x.device:
@@ -69,15 +70,16 @@
         )
 
     def forward(self, x: torch.Tensor, left_context: int = 0) -> torch.Tensor:
         """Compute positional encoding.
 
         Args:
             x: Input sequences. (B, T, ?)
-            left_context: Number of frames in left context.
+            left_context: Number of previous frames the attention module can see
+                          in current chunk.
 
         Returns:
             pos_enc: Positional embedding sequences. (B, 2 * (T - 1), ?)
 
         """
         self.extend_pe(x, left_context=left_context)
```

### Comparing `espnet-202304/espnet2/asr_transducer/encoder/validation.py` & `espnet-202308/espnet2/asr_transducer/encoder/validation.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 """Set of methods to validate encoder architecture."""
 
 from typing import Any, Dict, List, Tuple
 
-from espnet2.asr_transducer.utils import sub_factor_to_params
+from espnet2.asr_transducer.utils import get_convinput_module_parameters
 
 
 def validate_block_arguments(
     configuration: Dict[str, Any],
     block_id: int,
     previous_block_output: int,
 ) -> Tuple[int, int]:
@@ -25,15 +25,15 @@
     block_type = configuration.get("block_type")
 
     if block_type is None:
         raise ValueError(
             "Block %d in encoder doesn't have a type assigned. " % block_id
         )
 
-    if block_type in ["branchformer", "conformer"]:
+    if block_type in ["branchformer", "conformer", "ebranchformer"]:
         if configuration.get("linear_size") is None:
             raise ValueError(
                 "Missing 'linear_size' argument for X-former block (ID: %d)" % block_id
             )
 
         if configuration.get("conv_mod_kernel_size") is None:
             raise ValueError(
@@ -76,56 +76,57 @@
 
     Return:
         output_size: Encoder input block output size.
 
     """
     vgg_like = configuration.get("vgg_like", False)
     next_block_type = body_first_conf.get("block_type")
-    allowed_next_block_type = ["branchformer", "conformer", "conv1d"]
+    allowed_next_block_type = ["branchformer", "conformer", "conv1d", "ebranchformer"]
 
     if next_block_type is None or (next_block_type not in allowed_next_block_type):
         return -1
 
     if configuration.get("subsampling_factor") is None:
         configuration["subsampling_factor"] = 4
     sub_factor = configuration["subsampling_factor"]
 
     if vgg_like:
         conv_size = configuration.get("conv_size", (64, 128))
 
         if isinstance(conv_size, int):
             conv_size = (conv_size, conv_size)
 
-        assert sub_factor in {
-            4,
-            6,
-        }, "Subsampling factor for the VGG2L block should be either 4 or 6."
+        if sub_factor not in [4, 6]:
+            raise ValueError(
+                "VGG2L input module only support subsampling factor of 4 and 6."
+            )
     else:
         conv_size = configuration.get("conv_size", None)
 
         if isinstance(conv_size, tuple):
             conv_size = conv_size[0]
 
+        if sub_factor not in [2, 4, 6]:
+            raise ValueError(
+                "Conv2D input module only support subsampling factor of 2, 4 and 6."
+            )
+
     if next_block_type == "conv1d":
         if vgg_like:
-            output_size = conv_size[1] * ((input_size // 2) // 2)
+            _, output_size = get_convinput_module_parameters(
+                input_size, conv_size[1], sub_factor, is_vgg=True
+            )
         else:
             if conv_size is None:
                 conv_size = body_first_conf.get("output_size", 64)
 
-            _, _, conv_osize = sub_factor_to_params(sub_factor, input_size)
-            assert (
-                conv_osize > 0
-            ), "Conv2D output size is <1 with input size %d and subsampling %d" % (
-                input_size,
-                sub_factor,
+            _, output_size = get_convinput_module_parameters(
+                input_size, conv_size, sub_factor, is_vgg=False
             )
 
-            output_size = conv_osize * conv_size
-
         configuration["output_size"] = None
     else:
         output_size = body_first_conf.get("hidden_size")
 
         if conv_size is None:
             conv_size = output_size
```

### Comparing `espnet-202304/espnet2/asr_transducer/espnet_transducer_model.py` & `espnet-202308/espnet2/diar/espnet_model.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,300 +1,260 @@
-"""ESPnet2 ASR Transducer model."""
+# Copyright 2021 Jiatong Shi
+#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)
 
-import logging
 from contextlib import contextmanager
-from typing import Dict, List, Optional, Tuple, Union
+from itertools import permutations
+from typing import Dict, Optional, Tuple
 
+import numpy as np
 import torch
+import torch.nn.functional as F
 from packaging.version import parse as V
 from typeguard import check_argument_types
 
+from espnet2.asr.encoder.abs_encoder import AbsEncoder
 from espnet2.asr.frontend.abs_frontend import AbsFrontend
 from espnet2.asr.specaug.abs_specaug import AbsSpecAug
-from espnet2.asr_transducer.decoder.abs_decoder import AbsDecoder
-from espnet2.asr_transducer.encoder.encoder import Encoder
-from espnet2.asr_transducer.joint_network import JointNetwork
-from espnet2.asr_transducer.utils import get_transducer_task_io
+from espnet2.diar.attractor.abs_attractor import AbsAttractor
+from espnet2.diar.decoder.abs_decoder import AbsDecoder
 from espnet2.layers.abs_normalize import AbsNormalize
 from espnet2.torch_utils.device_funcs import force_gatherable
 from espnet2.train.abs_espnet_model import AbsESPnetModel
+from espnet.nets.pytorch_backend.nets_utils import to_device
 
 if V(torch.__version__) >= V("1.6.0"):
     from torch.cuda.amp import autocast
 else:
-
+    # Nothing to do if torch<1.6.0
     @contextmanager
     def autocast(enabled=True):
         yield
 
 
-class ESPnetASRTransducerModel(AbsESPnetModel):
-    """ESPnet2ASRTransducerModel module definition.
-
-    Args:
-        vocab_size: Size of complete vocabulary (w/ SOS/EOS and blank included).
-        token_list: List of tokens in vocabulary (minus reserved tokens).
-        frontend: Frontend module.
-        specaug: SpecAugment module.
-        normalize: Normalization module.
-        encoder: Encoder module.
-        decoder: Decoder module.
-        joint_network: Joint Network module.
-        transducer_weight: Weight of the Transducer loss.
-        fastemit_lambda: FastEmit lambda value.
-        auxiliary_ctc_weight: Weight of auxiliary CTC loss.
-        auxiliary_ctc_dropout_rate: Dropout rate for auxiliary CTC loss inputs.
-        auxiliary_lm_loss_weight: Weight of auxiliary LM loss.
-        auxiliary_lm_loss_smoothing: Smoothing rate for LM loss' label smoothing.
-        ignore_id: Initial padding ID.
-        sym_space: Space symbol.
-        sym_blank: Blank Symbol.
-        report_cer: Whether to report Character Error Rate during validation.
-        report_wer: Whether to report Word Error Rate during validation.
-        extract_feats_in_collect_stats: Whether to use extract_feats stats collection.
+class ESPnetDiarizationModel(AbsESPnetModel):
+    """Speaker Diarization model
 
+    If "attractor" is "None", SA-EEND will be used.
+    Else if "attractor" is not "None", EEND-EDA will be used.
+    For the details about SA-EEND and EEND-EDA, refer to the following papers:
+    SA-EEND: https://arxiv.org/pdf/1909.06247.pdf
+    EEND-EDA: https://arxiv.org/pdf/2005.09921.pdf, https://arxiv.org/pdf/2106.10654.pdf
     """
 
     def __init__(
         self,
-        vocab_size: int,
-        token_list: Union[Tuple[str, ...], List[str]],
         frontend: Optional[AbsFrontend],
         specaug: Optional[AbsSpecAug],
         normalize: Optional[AbsNormalize],
-        encoder: Encoder,
+        label_aggregator: torch.nn.Module,
+        encoder: AbsEncoder,
         decoder: AbsDecoder,
-        joint_network: JointNetwork,
-        transducer_weight: float = 1.0,
-        fastemit_lambda: float = 0.0,
-        auxiliary_ctc_weight: float = 0.0,
-        auxiliary_ctc_dropout_rate: float = 0.0,
-        auxiliary_lm_loss_weight: float = 0.0,
-        auxiliary_lm_loss_smoothing: float = 0.05,
-        ignore_id: int = -1,
-        sym_space: str = "<space>",
-        sym_blank: str = "<blank>",
-        report_cer: bool = False,
-        report_wer: bool = False,
-        extract_feats_in_collect_stats: bool = True,
-    ) -> None:
-        """Construct an ESPnetASRTransducerModel object."""
-        super().__init__()
-
+        attractor: Optional[AbsAttractor],
+        diar_weight: float = 1.0,
+        attractor_weight: float = 1.0,
+    ):
         assert check_argument_types()
 
-        # The following labels ID are reserved:
-        #    - 0: Blank symbol.
-        #    - 1: Unknown symbol.
-        #    - vocab_size - 1: SOS/EOS symbol.
-        self.vocab_size = vocab_size
-        self.ignore_id = ignore_id
-        self.token_list = token_list.copy()
-
-        self.sym_space = sym_space
-        self.sym_blank = sym_blank
+        super().__init__()
 
+        self.encoder = encoder
+        self.normalize = normalize
         self.frontend = frontend
         self.specaug = specaug
-        self.normalize = normalize
-
-        self.encoder = encoder
+        self.label_aggregator = label_aggregator
+        self.diar_weight = diar_weight
+        self.attractor_weight = attractor_weight
+        self.attractor = attractor
         self.decoder = decoder
-        self.joint_network = joint_network
-
-        self.criterion_transducer = None
-        self.error_calculator = None
-
-        self.use_auxiliary_ctc = auxiliary_ctc_weight > 0
-        self.use_auxiliary_lm_loss = auxiliary_lm_loss_weight > 0
 
-        if self.use_auxiliary_ctc:
-            self.ctc_lin = torch.nn.Linear(encoder.output_size, vocab_size)
-            self.ctc_dropout_rate = auxiliary_ctc_dropout_rate
-
-        if self.use_auxiliary_lm_loss:
-            self.lm_lin = torch.nn.Linear(decoder.output_size, vocab_size)
-
-            eps = auxiliary_lm_loss_smoothing / (vocab_size - 1)
-
-            self.lm_loss_smooth_neg = eps
-            self.lm_loss_smooth_pos = (1 - auxiliary_lm_loss_smoothing) + eps
-
-        self.transducer_weight = transducer_weight
-        self.fastemit_lambda = fastemit_lambda
-
-        self.auxiliary_ctc_weight = auxiliary_ctc_weight
-        self.auxiliary_lm_loss_weight = auxiliary_lm_loss_weight
-
-        self.report_cer = report_cer
-        self.report_wer = report_wer
-
-        self.extract_feats_in_collect_stats = extract_feats_in_collect_stats
+        if self.attractor is not None:
+            self.decoder = None
+        elif self.decoder is not None:
+            self.num_spk = decoder.num_spk
+        else:
+            raise NotImplementedError
 
     def forward(
         self,
         speech: torch.Tensor,
-        speech_lengths: torch.Tensor,
-        text: torch.Tensor,
-        text_lengths: torch.Tensor,
+        speech_lengths: torch.Tensor = None,
+        spk_labels: torch.Tensor = None,
+        spk_labels_lengths: torch.Tensor = None,
         **kwargs,
     ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor]:
-        """Forward architecture and compute loss(es).
+        """Frontend + Encoder + Decoder + Calc loss
 
         Args:
-            speech: Speech sequences. (B, S)
-            speech_lengths: Speech sequences lengths. (B,)
-            text: Label ID sequences. (B, L)
-            text_lengths: Label ID sequences lengths. (B,)
-            kwargs: Contains "utts_id".
-
-        Return:
-            loss: Main loss value.
-            stats: Task statistics.
-            weight: Task weights.
-
+            speech: (Batch, samples)
+            speech_lengths: (Batch,) default None for chunk interator,
+                                     because the chunk-iterator does not
+                                     have the speech_lengths returned.
+                                     see in
+                                     espnet2/iterators/chunk_iter_factory.py
+            spk_labels: (Batch, )
+            kwargs: "utt_id" is among the input.
         """
-        assert text_lengths.dim() == 1, text_lengths.shape
-        assert (
-            speech.shape[0]
-            == speech_lengths.shape[0]
-            == text.shape[0]
-            == text_lengths.shape[0]
-        ), (speech.shape, speech_lengths.shape, text.shape, text_lengths.shape)
-
+        assert speech.shape[0] == spk_labels.shape[0], (speech.shape, spk_labels.shape)
         batch_size = speech.shape[0]
-        text = text[:, : text_lengths.max()]
 
         # 1. Encoder
-        encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)
-
-        # 2. Transducer-related I/O preparation
-        decoder_in, target, t_len, u_len = get_transducer_task_io(
-            text,
-            encoder_out_lens,
-            ignore_id=self.ignore_id,
+        # Use bottleneck_feats if exist. Only for "enh + diar" task.
+        bottleneck_feats = kwargs.get("bottleneck_feats", None)
+        bottleneck_feats_lengths = kwargs.get("bottleneck_feats_lengths", None)
+        encoder_out, encoder_out_lens = self.encode(
+            speech, speech_lengths, bottleneck_feats, bottleneck_feats_lengths
         )
 
-        # 3. Decoder
-        self.decoder.set_device(encoder_out.device)
-        decoder_out = self.decoder(decoder_in)
-
-        # 4. Joint Network
-        joint_out = self.joint_network(
-            encoder_out.unsqueeze(2), decoder_out.unsqueeze(1)
-        )
-
-        # 5. Losses
-        loss_trans, cer_trans, wer_trans = self._calc_transducer_loss(
-            encoder_out,
-            joint_out,
-            target,
-            t_len,
-            u_len,
+        if self.attractor is None:
+            # 2a. Decoder (baiscally a predction layer after encoder_out)
+            pred = self.decoder(encoder_out, encoder_out_lens)
+        else:
+            # 2b. Encoder Decoder Attractors
+            # Shuffle the chronological order of encoder_out, then calculate attractor
+            encoder_out_shuffled = encoder_out.clone()
+            for i in range(len(encoder_out_lens)):
+                encoder_out_shuffled[i, : encoder_out_lens[i], :] = encoder_out[
+                    i, torch.randperm(encoder_out_lens[i]), :
+                ]
+            attractor, att_prob = self.attractor(
+                encoder_out_shuffled,
+                encoder_out_lens,
+                to_device(
+                    self,
+                    torch.zeros(
+                        encoder_out.size(0), spk_labels.size(2) + 1, encoder_out.size(2)
+                    ),
+                ),
+            )
+            # Remove the final attractor which does not correspond to a speaker
+            # Then multiply the attractors and encoder_out
+            pred = torch.bmm(encoder_out, attractor[:, :-1, :].permute(0, 2, 1))
+        # 3. Aggregate time-domain labels
+        spk_labels, spk_labels_lengths = self.label_aggregator(
+            spk_labels, spk_labels_lengths
         )
 
-        loss_ctc, loss_lm = 0.0, 0.0
-
-        if self.use_auxiliary_ctc:
-            loss_ctc = self._calc_ctc_loss(
-                encoder_out,
-                target,
-                t_len,
-                u_len,
+        # If encoder uses conv* as input_layer (i.e., subsampling),
+        # the sequence length of 'pred' might be slighly less than the
+        # length of 'spk_labels'. Here we force them to be equal.
+        length_diff_tolerance = 2
+        length_diff = spk_labels.shape[1] - pred.shape[1]
+        if length_diff > 0 and length_diff <= length_diff_tolerance:
+            spk_labels = spk_labels[:, 0 : pred.shape[1], :]
+
+        if self.attractor is None:
+            loss_pit, loss_att = None, None
+            loss, perm_idx, perm_list, label_perm = self.pit_loss(
+                pred, spk_labels, encoder_out_lens
             )
-
-        if self.use_auxiliary_lm_loss:
-            loss_lm = self._calc_lm_loss(decoder_out, target)
-
-        loss = (
-            self.transducer_weight * loss_trans
-            + self.auxiliary_ctc_weight * loss_ctc
-            + self.auxiliary_lm_loss_weight * loss_lm
-        )
+        else:
+            loss_pit, perm_idx, perm_list, label_perm = self.pit_loss(
+                pred, spk_labels, encoder_out_lens
+            )
+            loss_att = self.attractor_loss(att_prob, spk_labels)
+            loss = self.diar_weight * loss_pit + self.attractor_weight * loss_att
+        (
+            correct,
+            num_frames,
+            speech_scored,
+            speech_miss,
+            speech_falarm,
+            speaker_scored,
+            speaker_miss,
+            speaker_falarm,
+            speaker_error,
+        ) = self.calc_diarization_error(pred, label_perm, encoder_out_lens)
+
+        if speech_scored > 0 and num_frames > 0:
+            sad_mr, sad_fr, mi, fa, cf, acc, der = (
+                speech_miss / speech_scored,
+                speech_falarm / speech_scored,
+                speaker_miss / speaker_scored,
+                speaker_falarm / speaker_scored,
+                speaker_error / speaker_scored,
+                correct / num_frames,
+                (speaker_miss + speaker_falarm + speaker_error) / speaker_scored,
+            )
+        else:
+            sad_mr, sad_fr, mi, fa, cf, acc, der = 0, 0, 0, 0, 0, 0, 0
 
         stats = dict(
             loss=loss.detach(),
-            loss_transducer=loss_trans.detach(),
-            loss_aux_ctc=loss_ctc.detach() if loss_ctc > 0.0 else None,
-            loss_aux_lm=loss_lm.detach() if loss_lm > 0.0 else None,
-            cer_transducer=cer_trans,
-            wer_transducer=wer_trans,
+            loss_att=loss_att.detach() if loss_att is not None else None,
+            loss_pit=loss_pit.detach() if loss_pit is not None else None,
+            sad_mr=sad_mr,
+            sad_fr=sad_fr,
+            mi=mi,
+            fa=fa,
+            cf=cf,
+            acc=acc,
+            der=der,
         )
 
-        # force_gatherable: to-device and to-tensor if scalar for DataParallel
         loss, stats, weight = force_gatherable((loss, stats, batch_size), loss.device)
-
         return loss, stats, weight
 
     def collect_feats(
         self,
         speech: torch.Tensor,
         speech_lengths: torch.Tensor,
-        text: torch.Tensor,
-        text_lengths: torch.Tensor,
+        spk_labels: torch.Tensor = None,
+        spk_labels_lengths: torch.Tensor = None,
         **kwargs,
     ) -> Dict[str, torch.Tensor]:
-        """Collect features sequences and features lengths sequences.
-
-        Args:
-            speech: Speech sequences. (B, S)
-            speech_lengths: Speech sequences lengths. (B,)
-            text: Label ID sequences. (B, L)
-            text_lengths: Label ID sequences lengths. (B,)
-            kwargs: Contains "utts_id".
-
-        Return:
-            {}: "feats": Features sequences. (B, T, D_feats),
-                "feats_lengths": Features sequences lengths. (B,)
-
-        """
-        if self.extract_feats_in_collect_stats:
-            feats, feats_lengths = self._extract_feats(speech, speech_lengths)
-        else:
-            # Generate dummy stats if extract_feats_in_collect_stats is False
-            logging.warning(
-                "Generating dummy stats for feats and feats_lengths, "
-                "because encoder_conf.extract_feats_in_collect_stats is "
-                f"{self.extract_feats_in_collect_stats}"
-            )
-
-            feats, feats_lengths = speech, speech_lengths
-
+        feats, feats_lengths = self._extract_feats(speech, speech_lengths)
         return {"feats": feats, "feats_lengths": feats_lengths}
 
     def encode(
         self,
         speech: torch.Tensor,
         speech_lengths: torch.Tensor,
+        bottleneck_feats: torch.Tensor,
+        bottleneck_feats_lengths: torch.Tensor,
     ) -> Tuple[torch.Tensor, torch.Tensor]:
-        """Encoder speech sequences.
+        """Frontend + Encoder
 
         Args:
-            speech: Speech sequences. (B, S)
-            speech_lengths: Speech sequences lengths. (B,)
-
-        Return:
-            encoder_out: Encoder outputs. (B, T, D_enc)
-            encoder_out_lens: Encoder outputs lengths. (B,)
-
+            speech: (Batch, Length, ...)
+            speech_lengths: (Batch,)
+            bottleneck_feats: (Batch, Length, ...): used for enh + diar
         """
         with autocast(False):
             # 1. Extract feats
             feats, feats_lengths = self._extract_feats(speech, speech_lengths)
 
             # 2. Data augmentation
             if self.specaug is not None and self.training:
                 feats, feats_lengths = self.specaug(feats, feats_lengths)
 
             # 3. Normalization for feature: e.g. Global-CMVN, Utterance-CMVN
             if self.normalize is not None:
                 feats, feats_lengths = self.normalize(feats, feats_lengths)
 
-        # 4. Forward encoder
-        encoder_out, encoder_out_lens = self.encoder(feats, feats_lengths)
+            # 4. Forward encoder
+            # feats: (Batch, Length, Dim)
+            # -> encoder_out: (Batch, Length2, Dim)
+            if bottleneck_feats is None:
+                encoder_out, encoder_out_lens, _ = self.encoder(feats, feats_lengths)
+            elif self.frontend is None:
+                # use only bottleneck feature
+                encoder_out, encoder_out_lens, _ = self.encoder(
+                    bottleneck_feats, bottleneck_feats_lengths
+                )
+            else:
+                # use both frontend and bottleneck feats
+                # interpolate (copy) feats frames
+                # to match the length with bottleneck_feats
+                feats = F.interpolate(
+                    feats.transpose(1, 2), size=bottleneck_feats.shape[1]
+                ).transpose(1, 2)
+                # concatenate frontend LMF feature and bottleneck feature
+                encoder_out, encoder_out_lens, _ = self.encoder(
+                    torch.cat((bottleneck_feats, feats), 2), bottleneck_feats_lengths
+                )
 
         assert encoder_out.size(0) == speech.size(0), (
             encoder_out.size(),
             speech.size(0),
         )
         assert encoder_out.size(1) <= encoder_out_lens.max(), (
             encoder_out.size(),
@@ -302,170 +262,120 @@
         )
 
         return encoder_out, encoder_out_lens
 
     def _extract_feats(
         self, speech: torch.Tensor, speech_lengths: torch.Tensor
     ) -> Tuple[torch.Tensor, torch.Tensor]:
-        """Extract features sequences and features sequences lengths.
-
-        Args:
-            speech: Speech sequences. (B, S)
-            speech_lengths: Speech sequences lengths. (B,)
-
-        Return:
-            feats: Features sequences. (B, T, D_feats)
-            feats_lengths: Features sequences lengths. (B,)
+        batch_size = speech.shape[0]
+        speech_lengths = (
+            speech_lengths
+            if speech_lengths is not None
+            else torch.ones(batch_size).int() * speech.shape[1]
+        )
 
-        """
         assert speech_lengths.dim() == 1, speech_lengths.shape
 
         # for data-parallel
         speech = speech[:, : speech_lengths.max()]
 
         if self.frontend is not None:
+            # Frontend
+            #  e.g. STFT and Feature extract
+            #       data_loader may send time-domain signal in this case
+            # speech (Batch, NSamples) -> feats: (Batch, NFrames, Dim)
             feats, feats_lengths = self.frontend(speech, speech_lengths)
         else:
+            # No frontend and no feature extract
             feats, feats_lengths = speech, speech_lengths
-
         return feats, feats_lengths
 
-    def _calc_transducer_loss(
-        self,
-        encoder_out: torch.Tensor,
-        joint_out: torch.Tensor,
-        target: torch.Tensor,
-        t_len: torch.Tensor,
-        u_len: torch.Tensor,
-    ) -> Tuple[torch.Tensor, Optional[float], Optional[float]]:
-        """Compute Transducer loss.
-
-        Args:
-            encoder_out: Encoder output sequences. (B, T, D_enc)
-            joint_out: Joint Network output sequences (B, T, U, D_joint)
-            target: Target label ID sequences. (B, L)
-            t_len: Encoder output sequences lengths. (B,)
-            u_len: Target label ID sequences lengths. (B,)
-
-        Return:
-            loss_transducer: Transducer loss value.
-            cer_transducer: Character error rate for Transducer.
-            wer_transducer: Word Error Rate for Transducer.
-
-        """
-        if self.criterion_transducer is None:
-            try:
-                from warprnnt_pytorch import RNNTLoss
-
-                self.criterion_transducer = RNNTLoss(
-                    reduction="mean",
-                    fastemit_lambda=self.fastemit_lambda,
-                )
-            except ImportError:
-                logging.error(
-                    "warp-rnnt was not installed."
-                    "Please consult the installation documentation."
-                )
-                exit(1)
-
-        loss_transducer = self.criterion_transducer(
-            joint_out,
-            target,
-            t_len,
-            u_len,
+    def pit_loss_single_permute(self, pred, label, length):
+        bce_loss = torch.nn.BCEWithLogitsLoss(reduction="none")
+        mask = self.create_length_mask(length, label.size(1), label.size(2))
+        loss = bce_loss(pred, label)
+        loss = loss * mask
+        loss = torch.sum(torch.mean(loss, dim=2), dim=1)
+        loss = torch.unsqueeze(loss, dim=1)
+        return loss
+
+    def pit_loss(self, pred, label, lengths):
+        # Note (jiatong): Credit to https://github.com/hitachi-speech/EEND
+        num_output = label.size(2)
+        permute_list = [np.array(p) for p in permutations(range(num_output))]
+        loss_list = []
+        for p in permute_list:
+            label_perm = label[:, :, p]
+            loss_perm = self.pit_loss_single_permute(pred, label_perm, lengths)
+            loss_list.append(loss_perm)
+        loss = torch.cat(loss_list, dim=1)
+        min_loss, min_idx = torch.min(loss, dim=1)
+        loss = torch.sum(min_loss) / torch.sum(lengths.float())
+        batch_size = len(min_idx)
+        label_list = []
+        for i in range(batch_size):
+            label_list.append(label[i, :, permute_list[min_idx[i]]].data.cpu().numpy())
+        label_permute = torch.from_numpy(np.array(label_list)).float()
+        return loss, min_idx, permute_list, label_permute
+
+    def create_length_mask(self, length, max_len, num_output):
+        batch_size = len(length)
+        mask = torch.zeros(batch_size, max_len, num_output)
+        for i in range(batch_size):
+            mask[i, : length[i], :] = 1
+        mask = to_device(self, mask)
+        return mask
+
+    def attractor_loss(self, att_prob, label):
+        batch_size = len(label)
+        bce_loss = torch.nn.BCEWithLogitsLoss(reduction="none")
+        # create attractor label [1, 1, ..., 1, 0]
+        # att_label: (Batch, num_spk + 1, 1)
+        att_label = to_device(self, torch.zeros(batch_size, label.size(2) + 1, 1))
+        att_label[:, : label.size(2), :] = 1
+        loss = bce_loss(att_prob, att_label)
+        loss = torch.mean(torch.mean(loss, dim=1))
+        return loss
+
+    @staticmethod
+    def calc_diarization_error(pred, label, length):
+        # Note (jiatong): Credit to https://github.com/hitachi-speech/EEND
+
+        (batch_size, max_len, num_output) = label.size()
+        # mask the padding part
+        mask = np.zeros((batch_size, max_len, num_output))
+        for i in range(batch_size):
+            mask[i, : length[i], :] = 1
+
+        # pred and label have the shape (batch_size, max_len, num_output)
+        label_np = label.data.cpu().numpy().astype(int)
+        pred_np = (pred.data.cpu().numpy() > 0).astype(int)
+        label_np = label_np * mask
+        pred_np = pred_np * mask
+        length = length.data.cpu().numpy()
+
+        # compute speech activity detection error
+        n_ref = np.sum(label_np, axis=2)
+        n_sys = np.sum(pred_np, axis=2)
+        speech_scored = float(np.sum(n_ref > 0))
+        speech_miss = float(np.sum(np.logical_and(n_ref > 0, n_sys == 0)))
+        speech_falarm = float(np.sum(np.logical_and(n_ref == 0, n_sys > 0)))
+
+        # compute speaker diarization error
+        speaker_scored = float(np.sum(n_ref))
+        speaker_miss = float(np.sum(np.maximum(n_ref - n_sys, 0)))
+        speaker_falarm = float(np.sum(np.maximum(n_sys - n_ref, 0)))
+        n_map = np.sum(np.logical_and(label_np == 1, pred_np == 1), axis=2)
+        speaker_error = float(np.sum(np.minimum(n_ref, n_sys) - n_map))
+        correct = float(1.0 * np.sum((label_np == pred_np) * mask) / num_output)
+        num_frames = np.sum(length)
+        return (
+            correct,
+            num_frames,
+            speech_scored,
+            speech_miss,
+            speech_falarm,
+            speaker_scored,
+            speaker_miss,
+            speaker_falarm,
+            speaker_error,
         )
-
-        if not self.training and (self.report_cer or self.report_wer):
-            if self.error_calculator is None:
-                from espnet2.asr_transducer.error_calculator import ErrorCalculator
-
-                self.error_calculator = ErrorCalculator(
-                    self.decoder,
-                    self.joint_network,
-                    self.token_list,
-                    self.sym_space,
-                    self.sym_blank,
-                    report_cer=self.report_cer,
-                    report_wer=self.report_wer,
-                )
-
-            cer_transducer, wer_transducer = self.error_calculator(
-                encoder_out, target, t_len
-            )
-
-            return loss_transducer, cer_transducer, wer_transducer
-
-        return loss_transducer, None, None
-
-    def _calc_ctc_loss(
-        self,
-        encoder_out: torch.Tensor,
-        target: torch.Tensor,
-        t_len: torch.Tensor,
-        u_len: torch.Tensor,
-    ) -> torch.Tensor:
-        """Compute CTC loss.
-
-        Args:
-            encoder_out: Encoder output sequences. (B, T, D_enc)
-            target: Target label ID sequences. (B, L)
-            t_len: Encoder output sequences lengths. (B,)
-            u_len: Target label ID sequences lengths. (B,)
-
-        Return:
-            loss_ctc: CTC loss value.
-
-        """
-        ctc_in = self.ctc_lin(
-            torch.nn.functional.dropout(encoder_out, p=self.ctc_dropout_rate)
-        )
-        ctc_in = torch.log_softmax(ctc_in.transpose(0, 1), dim=-1)
-
-        target_mask = target != 0
-        ctc_target = target[target_mask].cpu()
-
-        with torch.backends.cudnn.flags(deterministic=True):
-            loss_ctc = torch.nn.functional.ctc_loss(
-                ctc_in,
-                ctc_target,
-                t_len,
-                u_len,
-                zero_infinity=True,
-                reduction="sum",
-            )
-        loss_ctc /= target.size(0)
-
-        return loss_ctc
-
-    def _calc_lm_loss(
-        self,
-        decoder_out: torch.Tensor,
-        target: torch.Tensor,
-    ) -> torch.Tensor:
-        """Compute LM loss (i.e.: Cross-entropy with smoothing).
-
-        Args:
-            decoder_out: Decoder output sequences. (B, U, D_dec)
-            target: Target label ID sequences. (B, L)
-
-        Return:
-            loss_lm: LM loss value.
-
-        """
-        batch_size = decoder_out.size(0)
-
-        logp = torch.log_softmax(
-            self.lm_lin(decoder_out[:, :-1, :]).view(-1, self.vocab_size),
-            dim=1,
-        )
-        target = target.view(-1).type(torch.int64)
-        ignore = (target == 0).unsqueeze(1)
-
-        with torch.no_grad():
-            true_dist = logp.clone().fill_(self.lm_loss_smooth_neg)
-
-            true_dist.scatter_(1, target.unsqueeze(1), self.lm_loss_smooth_pos)
-
-        loss_lm = torch.nn.functional.kl_div(logp, true_dist, reduction="none")
-        loss_lm = loss_lm.masked_fill(ignore, 0).sum() / batch_size
-
-        return loss_lm
```

### Comparing `espnet-202304/espnet2/asr_transducer/joint_network.py` & `espnet-202308/espnet2/asr_transducer/joint_network.py`

 * *Files 18% similar despite different names*

```diff
@@ -7,53 +7,65 @@
 
 class JointNetwork(torch.nn.Module):
     """Transducer joint network module.
 
     Args:
         output_size: Output size.
         encoder_size: Encoder output size.
-        decoder_size: Decoder output size..
+        decoder_size: Decoder output size.
         joint_space_size: Joint space size.
         joint_act_type: Type of activation for joint network.
         **activation_parameters: Parameters for the activation function.
 
     """
 
     def __init__(
         self,
         output_size: int,
         encoder_size: int,
         decoder_size: int,
         joint_space_size: int = 256,
         joint_activation_type: str = "tanh",
+        lin_dec_bias: bool = True,
         **activation_parameters,
     ) -> None:
         """Construct a JointNetwork object."""
         super().__init__()
 
         self.lin_enc = torch.nn.Linear(encoder_size, joint_space_size)
-        self.lin_dec = torch.nn.Linear(decoder_size, joint_space_size, bias=False)
+        self.lin_dec = torch.nn.Linear(
+            decoder_size, joint_space_size, bias=lin_dec_bias
+        )
 
         self.lin_out = torch.nn.Linear(joint_space_size, output_size)
 
         self.joint_activation = get_activation(
             joint_activation_type, **activation_parameters
         )
 
     def forward(
         self,
         enc_out: torch.Tensor,
         dec_out: torch.Tensor,
+        no_projection: bool = False,
     ) -> torch.Tensor:
         """Joint computation of encoder and decoder hidden state sequences.
 
         Args:
-            enc_out: Expanded encoder output state sequences (B, T, 1, D_enc)
-            dec_out: Expanded decoder output state sequences (B, 1, U, D_dec)
+            enc_out: Expanded encoder output state sequences.
+                         (B, T, s_range, D_enc) or (B, T, 1, D_enc)
+            dec_out: Expanded decoder output state sequences.
+                         (B, T, s_range, D_dec) or (B, 1, U, D_dec)
 
         Returns:
-            joint_out: Joint output state sequences. (B, T, U, D_out)
+            joint_out: Joint output state sequences.
+                           (B, T, U, D_out) or (B, T, s_range, D_out)
 
         """
-        joint_out = self.joint_activation(self.lin_enc(enc_out) + self.lin_dec(dec_out))
+        if no_projection:
+            joint_out = self.joint_activation(enc_out + dec_out)
+        else:
+            joint_out = self.joint_activation(
+                self.lin_enc(enc_out) + self.lin_dec(dec_out)
+            )
 
         return self.lin_out(joint_out)
```

### Comparing `espnet-202304/espnet2/bin/aggregate_stats_dirs.py` & `espnet-202308/espnet2/bin/aggregate_stats_dirs.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/bin/asr_align.py` & `espnet-202308/espnet2/bin/asr_align.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/bin/asr_inference.py` & `espnet-202308/espnet2/bin/asr_inference.py`

 * *Files 3% similar despite different names*

```diff
@@ -8,44 +8,50 @@
 from typing import Any, Dict, List, Optional, Sequence, Tuple, Union
 
 import numpy as np
 import torch
 import torch.quantization
 from typeguard import check_argument_types, check_return_type
 
+from espnet2.asr.decoder.hugging_face_transformers_decoder import (
+    get_hugging_face_model_lm_head,
+    get_hugging_face_model_network,
+)
 from espnet2.asr.decoder.s4_decoder import S4Decoder
 from espnet2.asr.transducer.beam_search_transducer import BeamSearchTransducer
 from espnet2.asr.transducer.beam_search_transducer import (
     ExtendedHypothesis as ExtTransHypothesis,
 )
 from espnet2.asr.transducer.beam_search_transducer import Hypothesis as TransHypothesis
 from espnet2.fileio.datadir_writer import DatadirWriter
 from espnet2.tasks.asr import ASRTask
 from espnet2.tasks.enh_s2t import EnhS2TTask
 from espnet2.tasks.lm import LMTask
 from espnet2.text.build_tokenizer import build_tokenizer
+from espnet2.text.hugging_face_token_id_converter import HuggingFaceTokenIDConverter
 from espnet2.text.token_id_converter import TokenIDConverter
 from espnet2.text.whisper_token_id_converter import OpenAIWhisperTokenIDConverter
 from espnet2.torch_utils.device_funcs import to_device
 from espnet2.torch_utils.set_all_random_seed import set_all_random_seed
 from espnet2.utils import config_argparse
+from espnet2.utils.nested_dict_action import NestedDictAction
 from espnet2.utils.types import str2bool, str2triple_str, str_or_none
 from espnet.nets.batch_beam_search import BatchBeamSearch
 from espnet.nets.batch_beam_search_online_sim import BatchBeamSearchOnlineSim
 from espnet.nets.beam_search import BeamSearch, Hypothesis
 from espnet.nets.beam_search_timesync import BeamSearchTimeSync
 from espnet.nets.pytorch_backend.transformer.add_sos_eos import add_sos_eos
 from espnet.nets.pytorch_backend.transformer.subsampling import TooShortUttError
 from espnet.nets.scorer_interface import BatchScorerInterface
 from espnet.nets.scorers.ctc import CTCPrefixScorer
 from espnet.nets.scorers.length_bonus import LengthBonus
 from espnet.utils.cli_utils import get_commandline_args
 
 try:
-    from transformers import AutoModelForSeq2SeqLM
+    from transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM
     from transformers.file_utils import ModelOutput
 
     is_transformers_available = True
 except ImportError:
     is_transformers_available = False
 
 # Alias for typing
@@ -96,15 +102,15 @@
         streaming: bool = False,
         enh_s2t_task: bool = False,
         quantize_asr_model: bool = False,
         quantize_lm: bool = False,
         quantize_modules: List[str] = ["Linear"],
         quantize_dtype: str = "qint8",
         hugging_face_decoder: bool = False,
-        hugging_face_decoder_max_length: int = 256,
+        hugging_face_decoder_conf: Dict[str, Any] = {},
         time_sync: bool = False,
         multi_asr: bool = False,
     ):
         assert check_argument_types()
 
         task = ASRTask if not enh_s2t_task else EnhS2TTask
 
@@ -223,35 +229,63 @@
                 raise ImportError(
                     "`transformers` is not available."
                     " Please install it via `pip install transformers`"
                     " or `cd /path/to/espnet/tools && . ./activate_python.sh"
                     " && ./installers/install_transformers.sh`."
                 )
 
-            hugging_face_model = AutoModelForSeq2SeqLM.from_pretrained(
-                decoder.model_name_or_path
-            )
+            if decoder.causal_lm:
+                hugging_face_model = AutoModelForCausalLM.from_pretrained(
+                    decoder.model_name_or_path
+                )
 
-            hugging_face_model.lm_head.load_state_dict(decoder.lm_head.state_dict())
+                hugging_face_model.resize_token_embeddings(decoder.lm_head.out_features)
 
-            if hasattr(hugging_face_model, "model"):
-                hugging_face_model.model.decoder.load_state_dict(
-                    decoder.decoder.state_dict()
-                )
-                del hugging_face_model.model.encoder
+                transformer = get_hugging_face_model_network(hugging_face_model)
+                transformer.load_state_dict(decoder.decoder.state_dict())
+
+                lm_head = get_hugging_face_model_lm_head(hugging_face_model)
+                lm_head.load_state_dict(decoder.lm_head.state_dict())
             else:
-                hugging_face_model.decoder.load_state_dict(decoder.decoder.state_dict())
-                del hugging_face_model.encoder
+                hugging_face_model = AutoModelForSeq2SeqLM.from_pretrained(
+                    decoder.model_name_or_path
+                )
+
+                hugging_face_model.lm_head.load_state_dict(decoder.lm_head.state_dict())
+
+                if hasattr(hugging_face_model, "model"):
+                    hugging_face_model.model.decoder.load_state_dict(
+                        decoder.decoder.state_dict()
+                    )
+                    del hugging_face_model.model.encoder
+                else:
+                    hugging_face_model.decoder.load_state_dict(
+                        decoder.decoder.state_dict()
+                    )
+                    del hugging_face_model.encoder
 
             del asr_model.decoder.lm_head
             del asr_model.decoder.decoder
 
             hugging_face_linear_in = decoder.linear_in
             hugging_face_model.to(device=device).eval()
 
+            if "num_beams" not in hugging_face_decoder_conf:
+                hugging_face_decoder_conf[
+                    "num_beams"
+                ] = hugging_face_model.config.num_beams
+
+            if (
+                hugging_face_model.config.pad_token_id is None
+                and "pad_token_id" not in hugging_face_decoder_conf
+            ):
+                hugging_face_decoder_conf[
+                    "pad_token_id"
+                ] = hugging_face_model.config.eos_token_id
+
             beam_search = None
             beam_search_transducer = None
         else:
             beam_search_transducer = None
             hugging_face_model = None
             hugging_face_linear_in = None
 
@@ -340,15 +374,17 @@
             if bpemodel is not None:
                 tokenizer = build_tokenizer(token_type=token_type, bpemodel=bpemodel)
             else:
                 tokenizer = None
         else:
             tokenizer = build_tokenizer(token_type=token_type)
 
-        if bpemodel not in ["whisper_en", "whisper_multilingual"]:
+        if token_type == "hugging_face":
+            converter = HuggingFaceTokenIDConverter(model_name_or_path=bpemodel)
+        elif bpemodel not in ["whisper_en", "whisper_multilingual"]:
             converter = TokenIDConverter(token_list=token_list)
         else:
             converter = OpenAIWhisperTokenIDConverter(model_type=bpemodel)
             beam_search.set_hyp_primer(
                 list(converter.tokenizer.sot_sequence_including_notimestamps)
             )
         logging.info(f"Text tokenizer: {tokenizer}")
@@ -357,16 +393,15 @@
         self.asr_train_args = asr_train_args
         self.converter = converter
         self.tokenizer = tokenizer
         self.beam_search = beam_search
         self.beam_search_transducer = beam_search_transducer
         self.hugging_face_model = hugging_face_model
         self.hugging_face_linear_in = hugging_face_linear_in
-        self.hugging_face_beam_size = beam_size
-        self.hugging_face_decoder_max_length = hugging_face_decoder_max_length
+        self.hugging_face_decoder_conf = hugging_face_decoder_conf
         self.maxlenratio = maxlenratio
         self.minlenratio = minlenratio
         self.device = device
         self.dtype = dtype
         self.nbest = nbest
         self.enh_s2t_task = enh_s2t_task
         self.multi_asr = multi_asr
@@ -476,30 +511,56 @@
             logging.info(
                 f"normalized log probability: {best.score / len(best.yseq):.2f}"
             )
             logging.info(
                 "best hypo: " + "".join(self.converter.ids2tokens(best.yseq[1:])) + "\n"
             )
         elif self.hugging_face_model:
-            decoder_start_token_id = (
-                self.hugging_face_model.config.decoder_start_token_id
-            )
-            yseq = self.hugging_face_model.generate(
-                encoder_outputs=ModelOutput(
-                    last_hidden_state=self.hugging_face_linear_in(enc).unsqueeze(0)
-                ),
-                use_cache=True,
-                decoder_start_token_id=decoder_start_token_id,
-                num_beams=self.hugging_face_beam_size,
-                max_length=self.hugging_face_decoder_max_length,
-            )
+            num_beams = self.hugging_face_decoder_conf["num_beams"]
+            enc = self.hugging_face_linear_in(enc).unsqueeze(0)
+            if self.asr_model.decoder.causal_lm:
+                forward_args, _ = self.asr_model.decoder.add_prefix_postfix(
+                    enc,
+                    torch.tensor([enc.shape[1]]).to(enc.device),
+                    torch.ones([1, 1], dtype=int, device=enc.device),
+                    torch.ones([1], dtype=int, device=enc.device),
+                )
+
+                # input_ids are ignored if we provide inputs_embeds,
+                # but input_ids are still required, so we make fake ones
+                input_ids = torch.ones(
+                    [1, forward_args["inputs_embeds"].shape[1]],
+                    dtype=int,
+                    device=enc.device,
+                )
+
+                yseq = self.hugging_face_model.generate(
+                    input_ids.repeat(num_beams, 1),
+                    inputs_embeds=forward_args["inputs_embeds"].repeat(num_beams, 1, 1),
+                    attention_mask=input_ids.repeat(num_beams, 1),
+                    **self.hugging_face_decoder_conf,
+                )
+
+                yseq = yseq[:, input_ids.shape[1] - 1 :]
+            else:
+                decoder_start_token_id = (
+                    self.hugging_face_model.config.decoder_start_token_id
+                )
+                yseq = self.hugging_face_model.generate(
+                    encoder_outputs=ModelOutput(last_hidden_state=enc),
+                    decoder_start_token_id=decoder_start_token_id,
+                    **self.hugging_face_decoder_conf,
+                )
+
             nbest_hyps = [Hypothesis(yseq=yseq[0])]
             logging.info(
                 "best hypo: "
-                + "".join(self.converter.ids2tokens(nbest_hyps[0].yseq[1:]))
+                + self.tokenizer.tokens2text(
+                    self.converter.ids2tokens(nbest_hyps[0].yseq[1:])
+                )
                 + "\n"
             )
         else:
             if hasattr(self.beam_search.nn_dict, "decoder"):
                 if isinstance(self.beam_search.nn_dict.decoder, S4Decoder):
                     # Setup: required for S4 autoregressive generation
                     for module in self.beam_search.nn_dict.decoder.modules():
@@ -600,15 +661,15 @@
     streaming: bool,
     enh_s2t_task: bool,
     quantize_asr_model: bool,
     quantize_lm: bool,
     quantize_modules: List[str],
     quantize_dtype: str,
     hugging_face_decoder: bool,
-    hugging_face_decoder_max_length: int,
+    hugging_face_decoder_conf: Dict[str, Any],
     time_sync: bool,
     multi_asr: bool,
 ):
     assert check_argument_types()
     if batch_size > 1:
         raise NotImplementedError("batch decoding is not implemented")
     if word_lm_train_config is not None:
@@ -653,15 +714,15 @@
         enh_s2t_task=enh_s2t_task,
         multi_asr=multi_asr,
         quantize_asr_model=quantize_asr_model,
         quantize_lm=quantize_lm,
         quantize_modules=quantize_modules,
         quantize_dtype=quantize_dtype,
         hugging_face_decoder=hugging_face_decoder,
-        hugging_face_decoder_max_length=hugging_face_decoder_max_length,
+        hugging_face_decoder_conf=hugging_face_decoder_conf,
         time_sync=time_sync,
     )
     speech2text = Speech2Text.from_pretrained(
         model_tag=model_tag,
         **speech2text_kwargs,
     )
 
@@ -917,15 +978,20 @@
         default=0.5,
         help="CTC weight in joint decoding",
     )
     group.add_argument("--lm_weight", type=float, default=1.0, help="RNNLM weight")
     group.add_argument("--ngram_weight", type=float, default=0.9, help="ngram weight")
     group.add_argument("--streaming", type=str2bool, default=False)
     group.add_argument("--hugging_face_decoder", type=str2bool, default=False)
-    group.add_argument("--hugging_face_decoder_max_length", type=int, default=256)
+    group.add_argument(
+        "--hugging_face_decoder_conf",
+        type=NestedDictAction,
+        default=dict(),
+        help="Custom kwargs for the HF .generate()",
+    )
 
     group.add_argument(
         "--transducer_conf",
         default=None,
         help="The keyword arguments for transducer beam search.",
     )
```

### Comparing `espnet-202304/espnet2/bin/asr_inference_k2.py` & `espnet-202308/espnet2/bin/asr_inference_k2.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/bin/asr_inference_maskctc.py` & `espnet-202308/espnet2/bin/asr_inference_maskctc.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/bin/asr_inference_streaming.py` & `espnet-202308/espnet2/bin/asr_inference_streaming.py`

 * *Files 0% similar despite different names*

```diff
@@ -218,27 +218,26 @@
                 next_states = {"waveform_buffer": speech.clone()}
                 return feats, feats_lengths, next_states
 
         if is_final:
             speech_to_process = speech
             waveform_buffer = None
         else:
-            n_frames = (
-                speech.size(0) - (self.win_length - self.hop_length)
-            ) // self.hop_length
-            n_residual = (
-                speech.size(0) - (self.win_length - self.hop_length)
-            ) % self.hop_length
-            speech_to_process = speech.narrow(
-                0, 0, (self.win_length - self.hop_length) + n_frames * self.hop_length
-            )
+            n_frames = speech.size(0) // self.hop_length
+            n_residual = speech.size(0) % self.hop_length
+            speech_to_process = speech.narrow(0, 0, n_frames * self.hop_length)
             waveform_buffer = speech.narrow(
                 0,
-                speech.size(0) - (self.win_length - self.hop_length) - n_residual,
-                (self.win_length - self.hop_length) + n_residual,
+                speech.size(0)
+                - (math.ceil(math.ceil(self.win_length / self.hop_length) / 2) * 2 - 1)
+                * self.hop_length
+                - n_residual,
+                (math.ceil(math.ceil(self.win_length / self.hop_length) / 2) * 2 - 1)
+                * self.hop_length
+                + n_residual,
             ).clone()
 
         # data: (Nsamples,) -> (1, Nsamples)
         speech_to_process = speech_to_process.unsqueeze(0).to(
             getattr(torch, self.dtype)
         )
         lengths = speech_to_process.new_full(
```

### Comparing `espnet-202304/espnet2/bin/asr_transducer_inference.py` & `espnet-202308/espnet2/bin/asr_transducer_inference.py`

 * *Files 19% similar despite different names*

```diff
@@ -2,35 +2,34 @@
 
 """ Inference class definition for Transducer models."""
 
 from __future__ import annotations
 
 import argparse
 import logging
-import math
 import sys
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Sequence, Tuple, Union
 
 import numpy as np
 import torch
 from packaging.version import parse as V
 from typeguard import check_argument_types, check_return_type
 
 from espnet2.asr_transducer.beam_search_transducer import (
     BeamSearchTransducer,
     Hypothesis,
 )
+from espnet2.asr_transducer.frontend.online_audio_processor import OnlineAudioProcessor
 from espnet2.asr_transducer.utils import TooShortUttError
 from espnet2.fileio.datadir_writer import DatadirWriter
 from espnet2.tasks.asr_transducer import ASRTransducerTask
 from espnet2.tasks.lm import LMTask
 from espnet2.text.build_tokenizer import build_tokenizer
 from espnet2.text.token_id_converter import TokenIDConverter
-from espnet2.torch_utils.device_funcs import to_device
 from espnet2.torch_utils.set_all_random_seed import set_all_random_seed
 from espnet2.utils import config_argparse
 from espnet2.utils.types import str2bool, str2triple_str, str_or_none
 from espnet.utils.cli_utils import get_commandline_args
 
 
 class Speech2Text:
@@ -49,18 +48,17 @@
         dtype: Data type.
         lm_weight: Language model weight.
         quantize_asr_model: Whether to apply dynamic quantization to ASR model.
         quantize_modules: List of module names to apply dynamic quantization on.
         quantize_dtype: Dynamic quantization data type.
         nbest: Number of final hypothesis.
         streaming: Whether to perform chunk-by-chunk inference.
-        chunk_size: Number of frames in chunk AFTER subsampling.
-        left_context: Number of frames in left context AFTER subsampling.
-        right_context: Number of frames in right context AFTER subsampling.
-        display_partial_hypotheses: Whether to display partial hypotheses.
+        decoding_window: Size of the decoding window (in milliseconds).
+        left_context: Number of previous frames the attention module can see
+                      in current chunk (used by Conformer and Branchformer block).
 
     """
 
     def __init__(
         self,
         asr_train_config: Union[Path, str] = None,
         asr_model_file: Union[Path, str] = None,
@@ -74,18 +72,16 @@
         dtype: str = "float32",
         lm_weight: float = 1.0,
         quantize_asr_model: bool = False,
         quantize_modules: List[str] = None,
         quantize_dtype: str = "qint8",
         nbest: int = 1,
         streaming: bool = False,
-        chunk_size: int = 16,
+        decoding_window: int = 640,
         left_context: int = 32,
-        right_context: int = 0,
-        display_partial_hypotheses: bool = False,
     ) -> None:
         """Construct a Speech2Text object."""
         super().__init__()
 
         assert check_argument_types()
 
         asr_model, asr_train_args = ASRTransducerTask.build_model_from_file(
@@ -113,14 +109,30 @@
 
             asr_model = torch.quantization.quantize_dynamic(
                 asr_model, q_config, dtype=q_dtype
             ).eval()
         else:
             asr_model.to(dtype=getattr(torch, dtype)).eval()
 
+        if hasattr(asr_model.decoder, "rescale_every") and (
+            asr_model.decoder.rescale_every > 0
+        ):
+            rescale_every = asr_model.decoder.rescale_every
+
+            with torch.no_grad():
+                for block_id, block in enumerate(asr_model.decoder.rwkv_blocks):
+                    block.att.proj_output.weight.div_(
+                        2 ** int(block_id // rescale_every)
+                    )
+                    block.ffn.proj_value.weight.div_(
+                        2 ** int(block_id // rescale_every)
+                    )
+
+            asr_model.decoder.rescaled_layers = True
+
         if lm_train_config is not None:
             lm, lm_train_args = LMTask.build_model_from_file(
                 lm_train_config, lm_file, device
             )
             lm_scorer = lm.lm
         else:
             lm_scorer = None
@@ -139,222 +151,102 @@
             **beam_search_config,
         )
 
         token_list = asr_model.token_list
 
         if token_type is None:
             token_type = asr_train_args.token_type
+
         if bpemodel is None:
             bpemodel = asr_train_args.bpemodel
 
-        if token_type is None:
-            tokenizer = None
-        elif token_type == "bpe":
+        if token_type == "bpe":
             if bpemodel is not None:
                 tokenizer = build_tokenizer(token_type=token_type, bpemodel=bpemodel)
             else:
                 tokenizer = None
         else:
             tokenizer = build_tokenizer(token_type=token_type)
         converter = TokenIDConverter(token_list=token_list)
-        logging.info(f"Text tokenizer: {tokenizer}")
 
         self.asr_model = asr_model
         self.asr_train_args = asr_train_args
         self.device = device
         self.dtype = dtype
         self.nbest = nbest
 
         self.converter = converter
         self.tokenizer = tokenizer
 
         self.beam_search = beam_search
-        self.streaming = streaming
-        self.chunk_size = max(chunk_size, 0)
-        self.left_context = max(left_context, 0)
-        self.right_context = max(right_context, 0)
-
-        if not streaming or chunk_size == 0:
-            self.streaming = False
-            self.asr_model.encoder.dynamic_chunk_training = False
-
-        self.n_fft = asr_train_args.frontend_conf.get("n_fft", 512)
-        self.hop_length = asr_train_args.frontend_conf.get("hop_length", 128)
 
-        if asr_train_args.frontend_conf.get("win_length", None) is not None:
-            self.frontend_window_size = asr_train_args.frontend_conf["win_length"]
-        else:
-            self.frontend_window_size = self.n_fft
-
-        self.window_size = self.chunk_size + self.right_context
-        self._raw_ctx = self.asr_model.encoder.get_encoder_input_raw_size(
-            self.window_size, self.hop_length
-        )
+        self.streaming = streaming and decoding_window >= 0
+        self.asr_model.encoder.dynamic_chunk_training = False
+        self.left_context = max(left_context, 0)
 
-        self.last_chunk_length = (
-            self.asr_model.encoder.embed.min_frame_length + self.right_context + 1
-        ) * self.hop_length
+        if streaming:
+            self.audio_processor = OnlineAudioProcessor(
+                asr_model._extract_feats,
+                asr_model.normalize,
+                decoding_window,
+                asr_model.encoder.embed.subsampling_factor,
+                asr_train_args.frontend_conf,
+                device,
+            )
 
-        self.reset_inference_cache()
+            self.reset_streaming_cache()
 
-    def reset_inference_cache(self) -> None:
+    def reset_streaming_cache(self) -> None:
         """Reset Speech2Text parameters."""
-        self.frontend_cache = None
 
-        self.asr_model.encoder.reset_streaming_cache(
-            self.left_context, device=self.device
-        )
-        self.beam_search.reset_inference_cache()
+        self.asr_model.encoder.reset_cache(self.left_context, device=self.device)
+        self.beam_search.reset_cache()
+        self.audio_processor.reset_cache()
 
         self.num_processed_frames = torch.tensor([[0]], device=self.device)
 
-    def apply_frontend(
-        self, speech: torch.Tensor, is_final: bool = False
-    ) -> Tuple[torch.Tensor, torch.Tensor]:
-        """Forward frontend.
-
-        Args:
-            speech: Speech data. (S)
-            is_final: Whether speech corresponds to the final (or only) chunk of data.
-
-        Returns:
-            feats: Features sequence. (1, T_in, F)
-            feats_lengths: Features sequence length. (1, T_in, F)
-
-        """
-        if self.frontend_cache is not None:
-            speech = torch.cat([self.frontend_cache["waveform_buffer"], speech], dim=0)
-
-        if is_final:
-            if self.streaming and speech.size(0) < self.last_chunk_length:
-                pad = torch.zeros(
-                    self.last_chunk_length - speech.size(0), dtype=speech.dtype
-                )
-                speech = torch.cat([speech, pad], dim=0)
-
-            speech_to_process = speech
-            waveform_buffer = None
-        else:
-            n_frames = (
-                speech.size(0) - (self.frontend_window_size - self.hop_length)
-            ) // self.hop_length
-
-            n_residual = (
-                speech.size(0) - (self.frontend_window_size - self.hop_length)
-            ) % self.hop_length
-
-            speech_to_process = speech.narrow(
-                0,
-                0,
-                (self.frontend_window_size - self.hop_length)
-                + n_frames * self.hop_length,
-            )
-
-            waveform_buffer = speech.narrow(
-                0,
-                speech.size(0)
-                - (self.frontend_window_size - self.hop_length)
-                - n_residual,
-                (self.frontend_window_size - self.hop_length) + n_residual,
-            ).clone()
-
-        speech_to_process = speech_to_process.unsqueeze(0).to(
-            getattr(torch, self.dtype)
-        )
-        lengths = speech_to_process.new_full(
-            [1], dtype=torch.long, fill_value=speech_to_process.size(1)
-        )
-        batch = {"speech": speech_to_process, "speech_lengths": lengths}
-        batch = to_device(batch, device=self.device)
-
-        feats, feats_lengths = self.asr_model._extract_feats(**batch)
-        if self.asr_model.normalize is not None:
-            feats, feats_lengths = self.asr_model.normalize(feats, feats_lengths)
-
-        if is_final:
-            if self.frontend_cache is None:
-                pass
-            else:
-                feats = feats.narrow(
-                    1,
-                    math.ceil(
-                        math.ceil(self.frontend_window_size / self.hop_length) / 2
-                    ),
-                    feats.size(1)
-                    - math.ceil(
-                        math.ceil(self.frontend_window_size / self.hop_length) / 2
-                    ),
-                )
-        else:
-            if self.frontend_cache is None:
-                feats = feats.narrow(
-                    1,
-                    0,
-                    feats.size(1)
-                    - math.ceil(
-                        math.ceil(self.frontend_window_size / self.hop_length) / 2
-                    ),
-                )
-            else:
-                feats = feats.narrow(
-                    1,
-                    math.ceil(
-                        math.ceil(self.frontend_window_size / self.hop_length) / 2
-                    ),
-                    feats.size(1)
-                    - 2
-                    * math.ceil(
-                        math.ceil(self.frontend_window_size / self.hop_length) / 2
-                    ),
-                )
-
-        feats_lengths = feats.new_full([1], dtype=torch.long, fill_value=feats.size(1))
-
-        if is_final:
-            self.frontend_cache = None
-        else:
-            self.frontend_cache = {"waveform_buffer": waveform_buffer}
-
-        return feats, feats_lengths
-
     @torch.no_grad()
     def streaming_decode(
         self,
         speech: Union[torch.Tensor, np.ndarray],
-        is_final: bool = True,
+        is_final: bool = False,
     ) -> List[Hypothesis]:
         """Speech2Text streaming call.
 
         Args:
             speech: Chunk of speech data. (S)
             is_final: Whether speech corresponds to the final chunk of data.
 
         Returns:
             nbest_hypothesis: N-best hypothesis.
 
         """
+        nbest_hyps = []
+
         if isinstance(speech, np.ndarray):
             speech = torch.tensor(speech)
 
-        feats, feats_length = self.apply_frontend(speech, is_final=is_final)
+        speech = speech.to(device=self.device)
+
+        feats, feats_length = self.audio_processor.compute_features(
+            speech.to(getattr(torch, self.dtype)), is_final
+        )
 
         enc_out = self.asr_model.encoder.chunk_forward(
             feats,
             feats_length,
             self.num_processed_frames,
             left_context=self.left_context,
-            right_context=self.right_context,
         )
+        self.num_processed_frames += enc_out.size(1)
 
         nbest_hyps = self.beam_search(enc_out[0], is_final=is_final)
 
-        self.num_processed_frames += self.chunk_size
-
         if is_final:
-            self.reset_inference_cache()
+            self.reset_streaming_cache()
 
         return nbest_hyps
 
     @torch.no_grad()
     def __call__(self, speech: Union[torch.Tensor, np.ndarray]) -> List[Hypothesis]:
         """Speech2Text call.
 
@@ -366,15 +258,26 @@
 
         """
         assert check_argument_types()
 
         if isinstance(speech, np.ndarray):
             speech = torch.tensor(speech)
 
-        feats, feats_length = self.apply_frontend(speech)
+        speech = speech.unsqueeze(0).to(
+            dtype=getattr(torch, self.dtype), device=self.device
+        )
+        lengths = speech.new_full(
+            [1], dtype=torch.long, fill_value=speech.size(1), device=self.device
+        )
+
+        feats, feats_length = self.asr_model._extract_feats(speech, lengths)
+
+        if self.asr_model.normalize is not None:
+            feats, feats_length = self.asr_model.normalize(feats, feats_length)
+
         enc_out, _ = self.asr_model.encoder(feats, feats_length)
 
         nbest_hyps = self.beam_search(enc_out[0])
 
         return nbest_hyps
 
     def hypotheses_to_results(self, nbest_hyps: List[Hypothesis]) -> List[Any]:
@@ -455,19 +358,18 @@
     token_type: Optional[str],
     bpemodel: Optional[str],
     key_file: Optional[str],
     allow_variable_data_keys: bool,
     quantize_asr_model: Optional[bool],
     quantize_modules: Optional[List[str]],
     quantize_dtype: Optional[str],
-    streaming: Optional[bool],
-    chunk_size: Optional[int],
-    left_context: Optional[int],
-    right_context: Optional[int],
-    display_partial_hypotheses: bool,
+    streaming: bool,
+    decoding_window: int,
+    left_context: int,
+    display_hypotheses: bool,
 ) -> None:
     """Transducer model inference.
 
     Args:
         output_dir: Output directory path.
         batch_size: Batch decoding size.
         dtype: Data type.
@@ -489,18 +391,18 @@
         bpemodel: BPE model path.
         key_file: File key.
         allow_variable_data_keys: Whether to allow variable data keys.
         quantize_asr_model: Whether to apply dynamic quantization to ASR model.
         quantize_modules: List of module names to apply dynamic quantization on.
         quantize_dtype: Dynamic quantization data type.
         streaming: Whether to perform chunk-by-chunk inference.
-        chunk_size: Number of frames in chunk AFTER subsampling.
-        left_context: Number of frames in left context AFTER subsampling.
-        right_context: Number of frames in right context AFTER subsampling.
-        display_partial_hypotheses: Whether to display partial hypotheses.
+        decoding_window: Audio length (in milliseconds) to process during decoding.
+        left_context: Number of previous frames the attention module can see
+                      in current chunk (used by Conformer and Branchformer block).
+        display_hypotheses: Whether to display (partial and full) hypotheses.
 
     """
     assert check_argument_types()
 
     if batch_size > 1:
         raise NotImplementedError("batch decoding is not implemented")
     if ngpu > 1:
@@ -533,23 +435,25 @@
         beam_size=beam_size,
         lm_weight=lm_weight,
         nbest=nbest,
         quantize_asr_model=quantize_asr_model,
         quantize_modules=quantize_modules,
         quantize_dtype=quantize_dtype,
         streaming=streaming,
-        chunk_size=chunk_size,
+        decoding_window=decoding_window,
         left_context=left_context,
-        right_context=right_context,
     )
     speech2text = Speech2Text.from_pretrained(
         model_tag=model_tag,
         **speech2text_kwargs,
     )
 
+    if speech2text.streaming:
+        decoding_samples = speech2text.audio_processor.decoding_samples
+
     # 3. Build data-iterator
     loader = ASRTransducerTask.build_streaming_iterator(
         data_path_and_name_and_type,
         dtype=dtype,
         batch_size=batch_size,
         key_file=key_file,
         num_workers=num_workers,
@@ -574,31 +478,47 @@
             batch = {k: v[0] for k, v in batch.items() if not k.endswith("_lengths")}
             assert len(batch.keys()) == 1
 
             try:
                 if speech2text.streaming:
                     speech = batch["speech"]
 
-                    _steps = len(speech) // speech2text._raw_ctx
-                    _end = 0
-
-                    for i in range(_steps):
-                        _end = (i + 1) * speech2text._raw_ctx
+                    decoding_steps = len(speech) // decoding_samples
 
-                        speech2text.streaming_decode(
-                            speech[i * speech2text._raw_ctx : _end], is_final=False
-                        )
+                    for i in range(0, decoding_steps + 1, 1):
+                        _start = i * decoding_samples
 
-                    final_hyps = speech2text.streaming_decode(
-                        speech[_end : len(speech)], is_final=True
-                    )
+                        if i == decoding_steps:
+                            final_hyps = speech2text.streaming_decode(
+                                speech[i * decoding_samples : len(speech)],
+                                is_final=True,
+                            )
+                        else:
+                            part_hyps = speech2text.streaming_decode(
+                                speech[
+                                    (i * decoding_samples) : _start + decoding_samples
+                                ],
+                                is_final=False,
+                            )
+
+                            if display_hypotheses:
+                                _result = speech2text.hypotheses_to_results(part_hyps)
+                                _length = (i + 1) * decoding_window
+
+                                logging.info(
+                                    f"Current best hypothesis (0-{_length}ms): "
+                                    f"{keys}: {_result[0][0]}"
+                                )
                 else:
                     final_hyps = speech2text(**batch)
 
                 results = speech2text.hypotheses_to_results(final_hyps)
+
+                if display_hypotheses:
+                    logging.info(f"Final best hypothesis: {keys}: {results[0][0]}")
             except TooShortUttError as e:
                 logging.warning(f"Utterance {keys} {e}")
                 hyp = Hypothesis(score=0.0, yseq=[], dec_state=None)
                 results = [[" ", ["<space>"], [2], hyp]] * nbest
 
             key = keys[0]
             for n, (text, token, token_int, hyp) in zip(range(1, nbest + 1), results):
@@ -749,36 +669,32 @@
     parser.add_argument(
         "--streaming",
         type=bool,
         default=False,
         help="Whether to perform chunk-by-chunk inference.",
     )
     parser.add_argument(
-        "--chunk_size",
+        "--decoding_window",
         type=int,
-        default=16,
-        help="Number of frames in chunk AFTER subsampling.",
+        default=640,
+        help="Audio length (in milliseconds) to process during decoding.",
     )
     parser.add_argument(
         "--left_context",
         type=int,
         default=32,
-        help="Number of frames in left context of the chunk AFTER subsampling.",
-    )
-    parser.add_argument(
-        "--right_context",
-        type=int,
-        default=0,
-        help="Number of frames in right context of the chunk AFTER subsampling.",
+        help="""Number of previous frames (AFTER subsamplingà the attention module
+        can see in current chunk (used by Conformer and Branchformer block).""",
     )
     parser.add_argument(
-        "--display_partial_hypotheses",
+        "--display_hypotheses",
         type=bool,
         default=False,
-        help="Whether to display partial hypotheses during chunk-by-chunk inference.",
+        help="""Whether to display hypotheses during inference. If streaming=True,
+        partial hypotheses will also be shown.""",
     )
 
     return parser
 
 
 def main(cmd=None):
     print(get_commandline_args(), file=sys.stderr)
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `espnet-202304/espnet2/bin/asr_transducer_train.py` & `espnet-202308/espnet2/bin/asr_transducer_train.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/bin/diar_inference.py` & `espnet-202308/espnet2/bin/diar_inference.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/bin/enh_inference.py` & `espnet-202308/espnet2/bin/enh_inference.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/bin/enh_inference_streaming.py` & `espnet-202308/espnet2/bin/enh_inference_streaming.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/bin/enh_tse_inference.py` & `espnet-202308/espnet2/bin/enh_tse_inference.py`

 * *Files 0% similar despite different names*

```diff
@@ -218,15 +218,15 @@
         aux_lengths = [
             aux.new_full([batch_size], dtype=torch.long, fill_value=aux.size(1))
             for aux in enroll_ref
         ]
 
         # a. To device
         speech_mix = to_device(speech_mix, device=self.device)
-        lengths = to_device(lengths, device=self.device)
+        enroll_ref = to_device(enroll_ref, device=self.device)
         if self.enh_model.share_encoder:
             feats_aux, flens_aux = zip(
                 *[
                     self.enh_model.encoder(enroll_ref[spk], aux_lengths[spk])
                     for spk in range(len(enroll_ref))
                 ]
             )
```

### Comparing `espnet-202304/espnet2/bin/enh_tse_train.py` & `espnet-202308/espnet2/bin/enh_tse_train.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/bin/hugging_face_export_vocabulary.py` & `espnet-202308/espnet2/bin/hugging_face_export_vocabulary.py`

 * *Files 6% similar despite different names*

```diff
@@ -44,15 +44,16 @@
         fout = p.open("w", encoding="utf-8")
 
     tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)
     words = ["" for _ in range(tokenizer.vocab_size)]
     vocab = tokenizer.get_vocab()
 
     for w in vocab:
-        words[vocab[w]] = w
+        if vocab[w] < tokenizer.vocab_size:  # pythia tokenizer
+            words[vocab[w]] = w
 
     # Parse the values of --add_symbol
     for symbol_and_id in add_symbol:
         # e.g symbol="<blank>:0"
         try:
             symbol, idx = symbol_and_id.split(":")
             idx = int(idx)
```

### Comparing `espnet-202304/espnet2/bin/launch.py` & `espnet-202308/espnet2/bin/launch.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/bin/lm_calc_perplexity.py` & `espnet-202308/espnet2/bin/lm_calc_perplexity.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/bin/lm_inference.py` & `espnet-202308/espnet2/bin/lm_inference.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/bin/mt_inference.py` & `espnet-202308/espnet2/bin/mt_inference.py`

 * *Files 3% similar despite different names*

```diff
@@ -18,14 +18,15 @@
 from espnet2.torch_utils.set_all_random_seed import set_all_random_seed
 from espnet2.utils import config_argparse
 from espnet2.utils.types import str2bool, str2triple_str, str_or_none
 from espnet.nets.batch_beam_search import BatchBeamSearch
 from espnet.nets.beam_search import BeamSearch, Hypothesis
 from espnet.nets.pytorch_backend.transformer.subsampling import TooShortUttError
 from espnet.nets.scorer_interface import BatchScorerInterface
+from espnet.nets.scorers.ctc import CTCPrefixScorer
 from espnet.nets.scorers.length_bonus import LengthBonus
 from espnet.utils.cli_utils import get_commandline_args
 
 
 class Text2Text:
     """Text2Text class
 
@@ -48,14 +49,15 @@
         bpemodel: str = None,
         device: str = "cpu",
         maxlenratio: float = 0.0,
         minlenratio: float = 0.0,
         batch_size: int = 1,
         dtype: str = "float32",
         beam_size: int = 20,
+        ctc_weight: float = 0.5,
         lm_weight: float = 1.0,
         ngram_weight: float = 0.9,
         penalty: float = 0.0,
         nbest: int = 1,
     ):
         assert check_argument_types()
 
@@ -63,17 +65,23 @@
         scorers = {}
         mt_model, mt_train_args = MTTask.build_model_from_file(
             mt_train_config, mt_model_file, device
         )
         mt_model.to(dtype=getattr(torch, dtype)).eval()
 
         decoder = mt_model.decoder
+        ctc = (
+            CTCPrefixScorer(ctc=mt_model.ctc, eos=mt_model.eos)
+            if ctc_weight != 0.0
+            else None
+        )
         token_list = mt_model.token_list
         scorers.update(
             decoder=decoder,
+            ctc=ctc,
             length_bonus=LengthBonus(len(token_list)),
         )
 
         # 2. Build Language model
         if lm_train_config is not None:
             lm, lm_train_args = LMTask.build_model_from_file(
                 lm_train_config, lm_file, device
@@ -91,29 +99,31 @@
 
                 ngram = NgramPartScorer(ngram_file, token_list)
         else:
             ngram = None
         scorers["ngram"] = ngram
 
         # 4. Build BeamSearch object
+
         weights = dict(
-            decoder=1.0,
+            decoder=1.0 - ctc_weight,
+            ctc=ctc_weight,
             lm=lm_weight,
             ngram=ngram_weight,
             length_bonus=penalty,
         )
         beam_search = BeamSearch(
             beam_size=beam_size,
             weights=weights,
             scorers=scorers,
             sos=mt_model.sos,
             eos=mt_model.eos,
             vocab_size=len(token_list),
             token_list=token_list,
-            pre_beam_score_key="full",
+            pre_beam_score_key=None if ctc_weight == 1.0 else "full",
         )
         # TODO(karita): make all scorers batchfied
         if batch_size == 1:
             non_batch = [
                 k
                 for k, v in beam_search.full_scorers.items()
                 if not isinstance(v, BatchScorerInterface)
@@ -187,14 +197,17 @@
         batch = {"src_text": src_text, "src_text_lengths": lengths}
 
         # a. To device
         batch = to_device(batch, device=self.device)
 
         # b. Forward Encoder
         enc, _ = self.mt_model.encode(**batch)
+        # self-condition case
+        if isinstance(enc, tuple):
+            enc = enc[0]
         assert len(enc) == 1, len(enc)
 
         # c. Passed the encoder result and the beam search
         nbest_hyps = self.beam_search(
             x=enc[0], maxlenratio=self.maxlenratio, minlenratio=self.minlenratio
         )
         nbest_hyps = nbest_hyps[: self.nbest]
@@ -260,14 +273,15 @@
     maxlenratio: float,
     minlenratio: float,
     batch_size: int,
     dtype: str,
     beam_size: int,
     ngpu: int,
     seed: int,
+    ctc_weight: float,
     lm_weight: float,
     ngram_weight: float,
     penalty: float,
     nbest: int,
     num_workers: int,
     log_level: Union[int, str],
     data_path_and_name_and_type: Sequence[Tuple[str, str, str]],
@@ -315,14 +329,15 @@
         token_type=token_type,
         bpemodel=bpemodel,
         device=device,
         maxlenratio=maxlenratio,
         minlenratio=minlenratio,
         dtype=dtype,
         beam_size=beam_size,
+        ctc_weight=ctc_weight,
         lm_weight=lm_weight,
         ngram_weight=ngram_weight,
         penalty=penalty,
         nbest=nbest,
     )
     text2text = Text2Text.from_pretrained(
         model_tag=model_tag,
@@ -488,14 +503,20 @@
     )
     group.add_argument(
         "--minlenratio",
         type=float,
         default=0.0,
         help="Input length ratio to obtain min output length",
     )
+    group.add_argument(
+        "--ctc_weight",
+        type=float,
+        default=0.0,
+        help="CTC weight in joint decoding",
+    )
     group.add_argument("--lm_weight", type=float, default=1.0, help="RNNLM weight")
     group.add_argument("--ngram_weight", type=float, default=0.9, help="ngram weight")
 
     group = parser.add_argument_group("Text converter related")
     group.add_argument(
         "--token_type",
         type=str_or_none,
```

### Comparing `espnet-202304/espnet2/bin/pack.py` & `espnet-202308/espnet2/bin/pack.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/bin/slu_inference.py` & `espnet-202308/espnet2/bin/slu_inference.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/bin/split_scps.py` & `espnet-202308/espnet2/bin/split_scps.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/bin/st_inference.py` & `espnet-202308/espnet2/bin/uasr_inference.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,105 +1,121 @@
 #!/usr/bin/env python3
 import argparse
 import logging
 import sys
+from distutils.version import LooseVersion
 from pathlib import Path
 from typing import Any, List, Optional, Sequence, Tuple, Union
 
 import numpy as np
 import torch
+import torch.quantization
 from typeguard import check_argument_types, check_return_type
 
 from espnet2.fileio.datadir_writer import DatadirWriter
-from espnet2.tasks.enh_s2t import EnhS2TTask
 from espnet2.tasks.lm import LMTask
-from espnet2.tasks.st import STTask
+from espnet2.tasks.uasr import UASRTask
 from espnet2.text.build_tokenizer import build_tokenizer
 from espnet2.text.token_id_converter import TokenIDConverter
 from espnet2.torch_utils.device_funcs import to_device
 from espnet2.torch_utils.set_all_random_seed import set_all_random_seed
 from espnet2.utils import config_argparse
 from espnet2.utils.types import str2bool, str2triple_str, str_or_none
 from espnet.nets.batch_beam_search import BatchBeamSearch
 from espnet.nets.beam_search import BeamSearch, Hypothesis
 from espnet.nets.pytorch_backend.transformer.subsampling import TooShortUttError
 from espnet.nets.scorer_interface import BatchScorerInterface
-from espnet.nets.scorers.length_bonus import LengthBonus
+from espnet.nets.scorers.uasr import UASRPrefixScorer
+
+# from espnet.nets.scorers.uasr import UASRPrefixScorer
 from espnet.utils.cli_utils import get_commandline_args
 
 
 class Speech2Text:
-    """Speech2Text class
+    """Speech2Text class for unsupervised ASR
 
     Examples:
         >>> import soundfile
-        >>> speech2text = Speech2Text("st_config.yml", "st.pth")
+        >>> speech2text = Speech2Text("uasr_config.yml", "uasr.pth")
         >>> audio, rate = soundfile.read("speech.wav")
         >>> speech2text(audio)
-        [(text, token, token_int, hypothesis object), ...]
+        [(text, token, token_int, hypothesis_object), ...]
 
     """
 
     def __init__(
         self,
-        st_train_config: Union[Path, str] = None,
-        st_model_file: Union[Path, str] = None,
+        uasr_train_config: Union[Path, str] = None,
+        uasr_model_file: Union[Path, str] = None,
         lm_train_config: Union[Path, str] = None,
         lm_file: Union[Path, str] = None,
         ngram_scorer: str = "full",
         ngram_file: Union[Path, str] = None,
         token_type: str = None,
         bpemodel: str = None,
         device: str = "cpu",
-        maxlenratio: float = 0.0,
-        minlenratio: float = 0.0,
         batch_size: int = 1,
         dtype: str = "float32",
         beam_size: int = 20,
         lm_weight: float = 1.0,
         ngram_weight: float = 0.9,
-        penalty: float = 0.0,
         nbest: int = 1,
-        enh_s2t_task: bool = False,
+        quantize_uasr_model: bool = False,
+        quantize_lm: bool = False,
+        quantize_modules: List[str] = ["Linear"],
+        quantize_dtype: str = "qint8",
     ):
         assert check_argument_types()
 
-        task = STTask if not enh_s2t_task else EnhS2TTask
+        if quantize_uasr_model or quantize_lm:
+            if quantize_dtype == "float16" and torch.__version__ < LooseVersion(
+                "1.5.0"
+            ):
+                raise ValueError(
+                    "float16 dtype for dynamic quantization is not supported with "
+                    "torch version < 1.5.0. Switch to qint8 dtype instead."
+                )
+
+        quantize_modules = set([getattr(torch.nn, q) for q in quantize_modules])
+        quantize_dtype = getattr(torch, quantize_dtype)
 
-        # 1. Build ST model
+        # 1. Build UASR model
         scorers = {}
-        st_model, st_train_args = task.build_model_from_file(
-            st_train_config, st_model_file, device
+        uasr_model, uasr_train_args = UASRTask.build_model_from_file(
+            uasr_train_config, uasr_model_file, device
         )
-        if enh_s2t_task:
-            st_model.inherite_attributes(
-                inherite_s2t_attrs=[
-                    "ctc",
-                    "decoder",
-                    "eos",
-                    "joint_network",
-                    "sos",
-                    "token_list",
-                    "use_transducer_decoder",
-                ]
+        # TODO(Jiatong): change to not used pre-extracted features for inference
+        uasr_model.use_collected_training_feats = True
+        uasr_model.to(dtype=getattr(torch, dtype)).eval()
+
+        if quantize_uasr_model:
+            logging.info("Use quantized uasr model for decoding.")
+
+            uasr_model = torch.quantization.quantize_dynamic(
+                uasr_model, qconfig_spec=quantize_modules, dtype=quantize_dtype
             )
-        st_model.to(dtype=getattr(torch, dtype)).eval()
 
-        decoder = st_model.decoder
-        token_list = st_model.token_list
-        scorers.update(
-            decoder=decoder,
-            length_bonus=LengthBonus(len(token_list)),
-        )
+        decoder = UASRPrefixScorer(eos=uasr_model.eos)
+        token_list = uasr_model.token_list
+        scorers.update(decoder=decoder)
+        logging.info(f"beam search token list: {token_list}")
 
         # 2. Build Language model
         if lm_train_config is not None:
             lm, lm_train_args = LMTask.build_model_from_file(
                 lm_train_config, lm_file, device
             )
+
+            if quantize_lm:
+                logging.info("Use quantized lm for decoding.")
+
+                lm = torch.quantization.quantize_dynamic(
+                    lm, qconfig_spec=quantize_modules, dtype=quantize_dtype
+                )
+
             scorers["lm"] = lm.lm
 
         # 3. Build ngram model
         if ngram_file is not None:
             if ngram_scorer == "full":
                 from espnet.nets.scorers.ngram import NgramFullScorer
 
@@ -113,26 +129,27 @@
         scorers["ngram"] = ngram
 
         # 4. Build BeamSearch object
         weights = dict(
             decoder=1.0,
             lm=lm_weight,
             ngram=ngram_weight,
-            length_bonus=penalty,
         )
+
         beam_search = BeamSearch(
             beam_size=beam_size,
             weights=weights,
             scorers=scorers,
-            sos=st_model.sos,
-            eos=st_model.eos,
+            sos=uasr_model.sos,
+            eos=uasr_model.eos,
             vocab_size=len(token_list),
             token_list=token_list,
-            pre_beam_score_key="full",
+            pre_beam_score_key=None,  # NOTE(jiatong): for frame-decoding
         )
+
         # TODO(karita): make all scorers batchfied
         if batch_size == 1:
             non_batch = [
                 k
                 for k, v in beam_search.full_scorers.items()
                 if not isinstance(v, BatchScorerInterface)
             ]
@@ -140,54 +157,56 @@
                 beam_search.__class__ = BatchBeamSearch
                 logging.info("BatchBeamSearch implementation is selected.")
             else:
                 logging.warning(
                     f"As non-batch scorers {non_batch} are found, "
                     f"fall back to non-batch implementation."
                 )
+
         beam_search.to(device=device, dtype=getattr(torch, dtype)).eval()
         for scorer in scorers.values():
             if isinstance(scorer, torch.nn.Module):
                 scorer.to(device=device, dtype=getattr(torch, dtype)).eval()
         logging.info(f"Beam_search: {beam_search}")
         logging.info(f"Decoding device={device}, dtype={dtype}")
 
-        # 4. [Optional] Build Text converter: e.g. bpe-sym -> Text
+        # 5. [Optional] Build Text converter: e.g. bpe-sym -> Text
         if token_type is None:
-            token_type = st_train_args.token_type
+            token_type = uasr_train_args.token_type
         if bpemodel is None:
-            bpemodel = st_train_args.bpemodel
+            bpemodel = uasr_train_args.bpemodel
+
+        # delete
+        token_type = "word"
 
         if token_type is None:
             tokenizer = None
         elif token_type == "bpe":
             if bpemodel is not None:
                 tokenizer = build_tokenizer(token_type=token_type, bpemodel=bpemodel)
             else:
                 tokenizer = None
         else:
             tokenizer = build_tokenizer(token_type=token_type)
         converter = TokenIDConverter(token_list=token_list)
         logging.info(f"Text tokenizer: {tokenizer}")
 
-        self.st_model = st_model
-        self.st_train_args = st_train_args
+        self.uasr_model = uasr_model
+        self.uasr_train_args = uasr_train_args
         self.converter = converter
         self.tokenizer = tokenizer
         self.beam_search = beam_search
-        self.maxlenratio = maxlenratio
-        self.minlenratio = minlenratio
         self.device = device
         self.dtype = dtype
         self.nbest = nbest
 
     @torch.no_grad()
     def __call__(
         self, speech: Union[torch.Tensor, np.ndarray]
-    ) -> List[Tuple[Optional[str], List[str], List[int], Hypothesis]]:
+    ) -> List[Tuple[Optional[str], List[str], List[int], Union[Hypothesis]]]:
         """Inference
 
         Args:
             data: Input speech data
         Returns:
             text, token, token_int, hyp
 
@@ -203,33 +222,37 @@
         # lengths: (1,)
         lengths = speech.new_full([1], dtype=torch.long, fill_value=speech.size(1))
         batch = {"speech": speech, "speech_lengths": lengths}
 
         # a. To device
         batch = to_device(batch, device=self.device)
 
-        # b. Forward Encoder
-        enc, _ = self.st_model.encode(**batch)
-        assert len(enc) == 1, len(enc)
-
-        # c. Passed the encoder result and the beam search
-        nbest_hyps = self.beam_search(
-            x=enc[0], maxlenratio=self.maxlenratio, minlenratio=self.minlenratio
+        # b. Forward encoder
+        generated_sample, generated_sample_padding_mask = self.uasr_model.inference(
+            **batch
         )
+        assert len(generated_sample) == 1, len(generated_sample)
+
+        # TODO(jiatong): add beamsearch
+        nbest_hyps = self.beam_search(x=generated_sample[0], maxlenratio=1.0)
+
         nbest_hyps = nbest_hyps[: self.nbest]
 
         results = []
         for hyp in nbest_hyps:
             assert isinstance(hyp, Hypothesis), type(hyp)
 
             # remove sos/eos and get results
-            token_int = hyp.yseq[1:-1].tolist()
+            if isinstance(hyp.yseq, list):
+                token_int = hyp.yseq[1:-1]
+            else:
+                token_int = hyp.yseq[1:-1].tolist()
 
             # remove blank symbol id, which is assumed to be 0
-            token_int = list(filter(lambda x: x != 0, token_int))
+            token_int = list(filter(lambda x: x >= 4, token_int))
 
             # Change integer-ids to tokens
             token = self.converter.ids2tokens(token_int)
 
             if self.tokenizer is not None:
                 text = self.tokenizer.tokens2text(token)
             else:
@@ -245,14 +268,15 @@
         **kwargs: Optional[Any],
     ):
         """Build Speech2Text instance from the pretrained model.
 
         Args:
             model_tag (Optional[str]): Model tag of the pretrained models.
                 Currently, the tags of espnet_model_zoo are supported.
+
         Returns:
             Speech2Text: Speech2Text instance.
 
         """
         if model_tag is not None:
             try:
                 from espnet_model_zoo.downloader import ModelDownloader
@@ -267,41 +291,41 @@
             kwargs.update(**d.download_and_unpack(model_tag))
 
         return Speech2Text(**kwargs)
 
 
 def inference(
     output_dir: str,
-    maxlenratio: float,
-    minlenratio: float,
     batch_size: int,
     dtype: str,
     beam_size: int,
     ngpu: int,
     seed: int,
     lm_weight: float,
     ngram_weight: float,
-    penalty: float,
     nbest: int,
     num_workers: int,
     log_level: Union[int, str],
     data_path_and_name_and_type: Sequence[Tuple[str, str, str]],
     key_file: Optional[str],
-    st_train_config: Optional[str],
-    st_model_file: Optional[str],
+    uasr_train_config: Optional[str],
+    uasr_model_file: Optional[str],
     lm_train_config: Optional[str],
     lm_file: Optional[str],
     word_lm_train_config: Optional[str],
     word_lm_file: Optional[str],
     ngram_file: Optional[str],
     model_tag: Optional[str],
     token_type: Optional[str],
     bpemodel: Optional[str],
     allow_variable_data_keys: bool,
-    enh_s2t_task: bool,
+    quantize_uasr_model: bool,
+    quantize_lm: bool,
+    quantize_modules: List[str],
+    quantize_dtype: str,
 ):
     assert check_argument_types()
     if batch_size > 1:
         raise NotImplementedError("batch decoding is not implemented")
     if word_lm_train_config is not None:
         raise NotImplementedError("Word LM is not implemented")
     if ngpu > 1:
@@ -318,46 +342,46 @@
         device = "cpu"
 
     # 1. Set random-seed
     set_all_random_seed(seed)
 
     # 2. Build speech2text
     speech2text_kwargs = dict(
-        st_train_config=st_train_config,
-        st_model_file=st_model_file,
+        uasr_train_config=uasr_train_config,
+        uasr_model_file=uasr_model_file,
         lm_train_config=lm_train_config,
         lm_file=lm_file,
         ngram_file=ngram_file,
         token_type=token_type,
         bpemodel=bpemodel,
         device=device,
-        maxlenratio=maxlenratio,
-        minlenratio=minlenratio,
         dtype=dtype,
         beam_size=beam_size,
         lm_weight=lm_weight,
         ngram_weight=ngram_weight,
-        penalty=penalty,
         nbest=nbest,
-        enh_s2t_task=enh_s2t_task,
+        quantize_uasr_model=quantize_uasr_model,
+        quantize_lm=quantize_lm,
+        quantize_modules=quantize_modules,
+        quantize_dtype=quantize_dtype,
     )
     speech2text = Speech2Text.from_pretrained(
         model_tag=model_tag,
         **speech2text_kwargs,
     )
 
     # 3. Build data-iterator
-    loader = STTask.build_streaming_iterator(
+    loader = UASRTask.build_streaming_iterator(
         data_path_and_name_and_type,
         dtype=dtype,
         batch_size=batch_size,
         key_file=key_file,
         num_workers=num_workers,
-        preprocess_fn=STTask.build_preprocess_fn(speech2text.st_train_args, False),
-        collate_fn=STTask.build_collate_fn(speech2text.st_train_args, False),
+        preprocess_fn=UASRTask.build_preprocess_fn(speech2text.uasr_train_args, False),
+        collate_fn=UASRTask.build_collate_fn(speech2text.uasr_train_args, False),
         allow_variable_data_keys=allow_variable_data_keys,
         inference=True,
     )
 
     # 7 .Start for-loop
     # FIXME(kamo): The output format should be discussed about
     with DatadirWriter(output_dir) as writer:
@@ -385,19 +409,21 @@
                 # Write the result to each file
                 ibest_writer["token"][key] = " ".join(token)
                 ibest_writer["token_int"][key] = " ".join(map(str, token_int))
                 ibest_writer["score"][key] = str(hyp.score)
 
                 if text is not None:
                     ibest_writer["text"][key] = text
+                    logging.info("key: {} text: {}".format(key, text))
+                    logging.info("key: {} token_int: {}\n".format(key, token_int))
 
 
 def get_parser():
     parser = config_argparse.ArgumentParser(
-        description="ST Decoding",
+        description="UASR Decoding",
         formatter_class=argparse.ArgumentDefaultsHelpFormatter,
     )
 
     # Note(kamo): Use '_' instead of '-' as separator.
     # '-' is confusing if written in yaml.
     parser.add_argument(
         "--log_level",
@@ -436,22 +462,22 @@
         action="append",
     )
     group.add_argument("--key_file", type=str_or_none)
     group.add_argument("--allow_variable_data_keys", type=str2bool, default=False)
 
     group = parser.add_argument_group("The model configuration related")
     group.add_argument(
-        "--st_train_config",
+        "--uasr_train_config",
         type=str,
-        help="ST training configuration",
+        help="uasr training configuration",
     )
     group.add_argument(
-        "--st_model_file",
+        "--uasr_model_file",
         type=str,
-        help="ST model parameter file",
+        help="uasr model parameter file",
     )
     group.add_argument(
         "--lm_train_config",
         type=str,
         help="LM training configuration",
     )
     group.add_argument(
@@ -476,58 +502,65 @@
     )
     group.add_argument(
         "--model_tag",
         type=str,
         help="Pretrained model tag. If specify this option, *_train_config and "
         "*_file will be overwritten",
     )
+
+    group = parser.add_argument_group("Quantization related")
+    group.add_argument(
+        "--quantize_uasr_model",
+        type=str2bool,
+        default=False,
+        help="Apply dynamic quantization to uasr model.",
+    )
     group.add_argument(
-        "--enh_s2t_task",
+        "--quantize_lm",
         type=str2bool,
         default=False,
-        help="enhancement and asr joint model",
+        help="Apply dynamic quantization to LM.",
+    )
+    group.add_argument(
+        "--quantize_modules",
+        type=str,
+        nargs="*",
+        default=["Linear"],
+        help="""List of modules to be dynamically quantized.
+        E.g.: --quantize_modules=[Linear,LSTM,GRU].
+        Each specified module should be an attribute of 'torch.nn', e.g.:
+        torch.nn.Linear, torch.nn.LSTM, torch.nn.GRU, ...""",
+    )
+    group.add_argument(
+        "--quantize_dtype",
+        type=str,
+        default="qint8",
+        choices=["float16", "qint8"],
+        help="Dtype for dynamic quantization.",
     )
 
     group = parser.add_argument_group("Beam-search related")
     group.add_argument(
         "--batch_size",
         type=int,
         default=1,
         help="The batch size for inference",
     )
     group.add_argument("--nbest", type=int, default=1, help="Output N-best hypotheses")
     group.add_argument("--beam_size", type=int, default=20, help="Beam size")
-    group.add_argument("--penalty", type=float, default=0.0, help="Insertion penalty")
-    group.add_argument(
-        "--maxlenratio",
-        type=float,
-        default=0.0,
-        help="Input length ratio to obtain max output length. "
-        "If maxlenratio=0.0 (default), it uses a end-detect "
-        "function "
-        "to automatically find maximum hypothesis lengths."
-        "If maxlenratio<0.0, its absolute value is interpreted"
-        "as a constant max output length",
-    )
-    group.add_argument(
-        "--minlenratio",
-        type=float,
-        default=0.0,
-        help="Input length ratio to obtain min output length",
-    )
     group.add_argument("--lm_weight", type=float, default=1.0, help="RNNLM weight")
     group.add_argument("--ngram_weight", type=float, default=0.9, help="ngram weight")
 
     group = parser.add_argument_group("Text converter related")
     group.add_argument(
         "--token_type",
         type=str_or_none,
         default=None,
         choices=["char", "bpe", None],
-        help="The token type for ST model. "
+        help="The token type for uasr model. "
         "If not given, refers from the training args",
     )
     group.add_argument(
         "--bpemodel",
         type=str_or_none,
         default=None,
         help="The model path of sentencepiece. "
```

### Comparing `espnet-202304/espnet2/bin/st_inference_streaming.py` & `espnet-202308/espnet2/bin/st_inference_streaming.py`

 * *Files 16% similar despite different names*

```diff
@@ -12,30 +12,39 @@
 
 from espnet2.asr.encoder.contextual_block_conformer_encoder import (  # noqa: H301
     ContextualBlockConformerEncoder,
 )
 from espnet2.asr.encoder.contextual_block_transformer_encoder import (  # noqa: H301
     ContextualBlockTransformerEncoder,
 )
+from espnet2.asr.frontend.s3prl import S3prlFrontend
 from espnet2.fileio.datadir_writer import DatadirWriter
 from espnet2.tasks.lm import LMTask
 from espnet2.tasks.st import STTask
 from espnet2.text.build_tokenizer import build_tokenizer
 from espnet2.text.token_id_converter import TokenIDConverter
 from espnet2.torch_utils.device_funcs import to_device
 from espnet2.torch_utils.set_all_random_seed import set_all_random_seed
 from espnet2.utils import config_argparse
 from espnet2.utils.types import str2bool, str2triple_str, str_or_none
 from espnet.nets.batch_beam_search_online import BatchBeamSearchOnline
 from espnet.nets.beam_search import Hypothesis
 from espnet.nets.pytorch_backend.transformer.subsampling import TooShortUttError
 from espnet.nets.scorer_interface import BatchScorerInterface
+from espnet.nets.scorers.ctc import CTCPrefixScorer
 from espnet.nets.scorers.length_bonus import LengthBonus
 from espnet.utils.cli_utils import get_commandline_args
 
+try:
+    from transformers import AutoModelForSeq2SeqLM
+
+    is_transformers_available = True
+except ImportError:
+    is_transformers_available = False
+
 
 class Speech2TextStreaming:
     """Speech2TextStreaming class
 
     Details in "Streaming Transformer ASR with Blockwise Synchronous Beam Search"
     (https://arxiv.org/abs/2006.14941)
 
@@ -58,88 +67,185 @@
         bpemodel: str = None,
         device: str = "cpu",
         maxlenratio: float = 0.0,
         minlenratio: float = 0.0,
         batch_size: int = 1,
         dtype: str = "float32",
         beam_size: int = 20,
+        ctc_weight: float = 0.0,
         lm_weight: float = 1.0,
         penalty: float = 0.0,
         nbest: int = 1,
         disable_repetition_detection=False,
         decoder_text_length_limit=0,
         encoded_feat_length_limit=0,
+        time_sync: bool = False,
+        incremental_decode: bool = False,
+        blank_penalty: float = 1.0,
+        hold_n: int = 0,
+        transducer_conf: dict = None,
+        hugging_face_decoder: bool = False,
     ):
         assert check_argument_types()
 
         # 1. Build ST model
         scorers = {}
         st_model, st_train_args = STTask.build_model_from_file(
             st_train_config, st_model_file, device
         )
         st_model.to(dtype=getattr(torch, dtype)).eval()
 
-        assert isinstance(
+        if isinstance(
             st_model.encoder, ContextualBlockTransformerEncoder
-        ) or isinstance(st_model.encoder, ContextualBlockConformerEncoder)
+        ) or isinstance(st_model.encoder, ContextualBlockConformerEncoder):
+            if isinstance(st_model.frontend, S3prlFrontend):
+                raise NotImplementedError(
+                    "S3prlFrontend not supported with blockwise encoder"
+                )
+            if st_model.hier_encoder is not None:
+                raise NotImplementedError(
+                    "hierarchical encoder not supported with blockwise encoder"
+                )
+            block_size = st_train_args.encoder_conf["block_size"]
+        else:
+            block_size = 0  # recompute encoder with every new chunk
 
         decoder = st_model.decoder
+        if hasattr(st_model, "st_ctc"):
+            ctc = CTCPrefixScorer(ctc=st_model.st_ctc, eos=st_model.eos)
+        else:
+            ctc = None
+
         token_list = st_model.token_list
         scorers.update(
             decoder=decoder,
+            ctc=ctc,
             length_bonus=LengthBonus(len(token_list)),
         )
 
         # 2. Build Language model
         if lm_train_config is not None:
             lm, lm_train_args = LMTask.build_model_from_file(
                 lm_train_config, lm_file, device
             )
             scorers["lm"] = lm.lm
 
         # 3. Build BeamSearch object
         weights = dict(
-            decoder=1.0,
+            decoder=1.0 - ctc_weight,
+            ctc=ctc_weight,
             lm=lm_weight,
             length_bonus=penalty,
+            blank_penalty=blank_penalty,
         )
 
-        assert "encoder_conf" in st_train_args
-        assert "look_ahead" in st_train_args.encoder_conf
-        assert "hop_size" in st_train_args.encoder_conf
-        assert "block_size" in st_train_args.encoder_conf
-        # look_ahead = st_train_args.encoder_conf['look_ahead']
-        # hop_size   = st_train_args.encoder_conf['hop_size']
-        # block_size = st_train_args.encoder_conf['block_size']
+        # assert "encoder_conf" in st_train_args
+        # assert "look_ahead" in st_train_args.encoder_conf
+        # assert "hop_size" in st_train_args.encoder_conf
+        # assert "block_size" in st_train_args.encoder_conf
 
         assert batch_size == 1
 
-        beam_search = BatchBeamSearchOnline(
-            beam_size=beam_size,
-            weights=weights,
-            scorers=scorers,
-            sos=st_model.sos,
-            eos=st_model.eos,
-            vocab_size=len(token_list),
-            token_list=token_list,
-            pre_beam_score_key="full",
-            disable_repetition_detection=disable_repetition_detection,
-            decoder_text_length_limit=decoder_text_length_limit,
-            encoded_feat_length_limit=encoded_feat_length_limit,
-        )
+        if (
+            decoder.__class__.__name__ == "HuggingFaceTransformersDecoder"
+            and hugging_face_decoder
+        ):
+            if not is_transformers_available:
+                raise ImportError(
+                    "`transformers` is not available."
+                    " Please install it via `pip install transformers`"
+                    " or `cd /path/to/espnet/tools && . ./activate_python.sh"
+                    " && ./installers/install_transformers.sh`."
+                )
+
+            hugging_face_model = AutoModelForSeq2SeqLM.from_pretrained(
+                decoder.model_name_or_path
+            )
+
+            hugging_face_model.lm_head.load_state_dict(decoder.lm_head.state_dict())
+
+            if hasattr(hugging_face_model, "model"):
+                hugging_face_model.model.decoder.load_state_dict(
+                    decoder.decoder.state_dict()
+                )
+                del hugging_face_model.model.encoder
+            else:
+                hugging_face_model.decoder.load_state_dict(decoder.decoder.state_dict())
+                del hugging_face_model.encoder
+
+            # del st_model.decoder.lm_head
+            # del st_model.decoder.decoder
 
-        non_batch = [
-            k
-            for k, v in beam_search.full_scorers.items()
-            if not isinstance(v, BatchScorerInterface)
-        ]
-        assert len(non_batch) == 0
+            hugging_face_linear_in = decoder.linear_in
+            hugging_face_model.to(device=device).eval()
+
+            # hacky way to use .score()
+            st_model.decoder.hf_generate = hugging_face_model
+            weights = dict(
+                decoder=1.0 - ctc_weight,
+                ctc=ctc_weight,
+                lm=lm_weight,
+                length_bonus=penalty,
+            )
+            beam_search = BatchBeamSearchOnline(
+                beam_size=beam_size,
+                weights=weights,
+                scorers=scorers,
+                sos=hugging_face_model.config.decoder_start_token_id,
+                eos=hugging_face_model.config.eos_token_id,
+                vocab_size=len(token_list),
+                token_list=token_list,
+                pre_beam_score_key="full",
+                disable_repetition_detection=disable_repetition_detection,
+                decoder_text_length_limit=decoder_text_length_limit,
+                encoded_feat_length_limit=encoded_feat_length_limit,
+                incremental_decode=incremental_decode,
+                time_sync=time_sync,
+                block_size=block_size,
+                ctc=st_model.st_ctc if hasattr(st_model, "st_ctc") else None,
+                hold_n=hold_n,
+            )
+            self.hugging_face_model = hugging_face_model
+            self.hugging_face_linear_in = hugging_face_linear_in
+
+        else:
+            beam_search = BatchBeamSearchOnline(
+                beam_size=beam_size,
+                weights=weights,
+                scorers=scorers,
+                sos=st_model.sos,
+                eos=st_model.eos,
+                vocab_size=len(token_list),
+                token_list=token_list,
+                pre_beam_score_key="full",
+                disable_repetition_detection=disable_repetition_detection,
+                decoder_text_length_limit=decoder_text_length_limit,
+                encoded_feat_length_limit=encoded_feat_length_limit,
+                incremental_decode=incremental_decode,
+                time_sync=time_sync,
+                ctc=st_model.st_ctc if hasattr(st_model, "st_ctc") else None,
+                hold_n=hold_n,
+                transducer_conf=transducer_conf,
+                joint_network=st_model.st_joint_network
+                if hasattr(st_model, "st_joint_network")
+                else None,
+            )
+            self.hugging_face_model = None
+            self.hugging_face_linear_in = None
 
-        # TODO(karita): make all scorers batchfied
-        logging.info("BatchBeamSearchOnline implementation is selected.")
+        if transducer_conf is None:
+            non_batch = [
+                k
+                for k, v in beam_search.full_scorers.items()
+                if not isinstance(v, BatchScorerInterface)
+            ]
+            assert len(non_batch) == 0
+
+            # TODO(karita): make all scorers batchfied
+            logging.info("BatchBeamSearchOnline implementation is selected.")
 
         beam_search.to(device=device, dtype=getattr(torch, dtype)).eval()
         for scorer in scorers.values():
             if isinstance(scorer, torch.nn.Module):
                 scorer.to(device=device, dtype=getattr(torch, dtype)).eval()
         logging.info(f"Beam_search: {beam_search}")
         logging.info(f"Decoding device={device}, dtype={dtype}")
@@ -148,15 +254,15 @@
         if token_type is None:
             token_type = st_train_args.token_type
         if bpemodel is None:
             bpemodel = st_train_args.bpemodel
 
         if token_type is None:
             tokenizer = None
-        elif token_type == "bpe":
+        elif token_type == "bpe" or token_type == "hugging_face":
             if bpemodel is not None:
                 tokenizer = build_tokenizer(token_type=token_type, bpemodel=bpemodel)
             else:
                 tokenizer = None
         else:
             tokenizer = build_tokenizer(token_type=token_type)
         converter = TokenIDConverter(token_list=token_list)
@@ -189,14 +295,15 @@
             self.win_length = self.n_fft
 
         self.reset()
 
     def reset(self):
         self.frontend_states = None
         self.encoder_states = None
+        self.hier_encoder_states = None
         self.beam_search.reset()
 
     def apply_frontend(
         self, speech: torch.Tensor, prev_states=None, is_final: bool = False
     ):
         if prev_states is not None:
             buf = prev_states["waveform_buffer"]
@@ -287,24 +394,34 @@
         """
         assert check_argument_types()
 
         # Input as audio signal
         if isinstance(speech, np.ndarray):
             speech = torch.tensor(speech)
 
-        feats, feats_lengths, self.frontend_states = self.apply_frontend(
-            speech, self.frontend_states, is_final=is_final
-        )
-        enc, _, self.encoder_states = self.st_model.encoder(
-            feats,
-            feats_lengths,
-            self.encoder_states,
-            is_final=is_final,
-            infer_mode=True,
-        )
+        if isinstance(
+            self.st_model.encoder, ContextualBlockTransformerEncoder
+        ) or isinstance(self.st_model.encoder, ContextualBlockConformerEncoder):
+            feats, feats_lengths, self.frontend_states = self.apply_frontend(
+                speech, self.frontend_states, is_final=is_final
+            )
+            enc, _, self.encoder_states = self.st_model.encoder(
+                feats,
+                feats_lengths,
+                self.encoder_states,
+                is_final=is_final,
+                infer_mode=True,
+            )
+        else:
+            speech = speech.unsqueeze(0).to(getattr(torch, self.dtype))
+            lengths = speech.new_full([1], dtype=torch.long, fill_value=speech.size(1))
+            batch = {"speech": speech, "speech_lengths": lengths}
+            batch = to_device(batch, device=self.device)
+            enc, enc_lengths = self.st_model.encode(**batch)
+
         nbest_hyps = self.beam_search(
             x=enc[0],
             maxlenratio=self.maxlenratio,
             minlenratio=self.minlenratio,
             is_final=is_final,
         )
 
@@ -343,14 +460,15 @@
     maxlenratio: float,
     minlenratio: float,
     batch_size: int,
     dtype: str,
     beam_size: int,
     ngpu: int,
     seed: int,
+    ctc_weight: float,
     lm_weight: float,
     penalty: float,
     nbest: int,
     num_workers: int,
     log_level: Union[int, str],
     data_path_and_name_and_type: Sequence[Tuple[str, str, str]],
     key_file: Optional[str],
@@ -363,14 +481,20 @@
     token_type: Optional[str],
     bpemodel: Optional[str],
     allow_variable_data_keys: bool,
     sim_chunk_length: int,
     disable_repetition_detection: bool,
     encoded_feat_length_limit: int,
     decoder_text_length_limit: int,
+    time_sync: bool,
+    incremental_decode: bool,
+    blank_penalty: float,
+    hold_n: int,
+    transducer_conf: Optional[dict],
+    hugging_face_decoder: bool,
 ):
     assert check_argument_types()
     if batch_size > 1:
         raise NotImplementedError("batch decoding is not implemented")
     if word_lm_train_config is not None:
         raise NotImplementedError("Word LM is not implemented")
     if ngpu > 1:
@@ -398,20 +522,27 @@
         token_type=token_type,
         bpemodel=bpemodel,
         device=device,
         maxlenratio=maxlenratio,
         minlenratio=minlenratio,
         dtype=dtype,
         beam_size=beam_size,
+        ctc_weight=ctc_weight,
         lm_weight=lm_weight,
         penalty=penalty,
         nbest=nbest,
         disable_repetition_detection=disable_repetition_detection,
         decoder_text_length_limit=decoder_text_length_limit,
         encoded_feat_length_limit=encoded_feat_length_limit,
+        time_sync=time_sync,
+        incremental_decode=incremental_decode,
+        blank_penalty=blank_penalty,
+        hold_n=hold_n,
+        transducer_conf=transducer_conf,
+        hugging_face_decoder=hugging_face_decoder,
     )
 
     # 3. Build data-iterator
     loader = STTask.build_streaming_iterator(
         data_path_and_name_and_type,
         dtype=dtype,
         batch_size=batch_size,
@@ -437,25 +568,40 @@
             try:
                 if sim_chunk_length == 0:
                     # N-best list of (text, token, token_int, hyp_object)
                     results = speech2text(**batch)
                 else:
                     speech = batch["speech"]
                     if (len(speech) // sim_chunk_length) > 1:
-                        for i in range(len(speech) // sim_chunk_length):
-                            speech2text(
-                                speech=speech[
-                                    i * sim_chunk_length : (i + 1) * sim_chunk_length
-                                ],
-                                is_final=False,
+                        # recompute with incrementally longer input
+                        if isinstance(speech2text.st_model.frontend, S3prlFrontend):
+                            for i in range(len(speech) // sim_chunk_length):
+                                speech2text(
+                                    speech=speech[: (i + 1) * sim_chunk_length],
+                                    is_final=False,
+                                )
+                            results = speech2text(
+                                speech[: len(speech)],
+                                is_final=True,
+                            )
+                        # non recompute
+                        else:
+                            for i in range(len(speech) // sim_chunk_length):
+                                speech2text(
+                                    speech=speech[
+                                        i
+                                        * sim_chunk_length : (i + 1)
+                                        * sim_chunk_length
+                                    ],
+                                    is_final=False,
+                                )
+                            results = speech2text(
+                                speech[(i + 1) * sim_chunk_length : len(speech)],
+                                is_final=True,
                             )
-                        results = speech2text(
-                            speech[(i + 1) * sim_chunk_length : len(speech)],
-                            is_final=True,
-                        )
                     else:
                         results = speech2text(**batch)
 
             except TooShortUttError as e:
                 logging.warning(f"Utterance {keys} {e}")
                 hyp = Hypothesis(score=0.0, scores={}, states={}, yseq=[])
                 results = [[" ", ["<space>"], [2], hyp]] * nbest
@@ -558,14 +704,15 @@
     )
     group.add_argument(
         "--minlenratio",
         type=float,
         default=0.0,
         help="Input length ratio to obtain min output length",
     )
+    group.add_argument("--ctc_weight", type=float, default=0.0, help="CTC weight")
     group.add_argument("--lm_weight", type=float, default=1.0, help="RNNLM weight")
     group.add_argument("--disable_repetition_detection", type=str2bool, default=False)
 
     group.add_argument(
         "--encoded_feat_length_limit",
         type=int,
         default=0,
@@ -590,14 +737,44 @@
     group.add_argument(
         "--bpemodel",
         type=str_or_none,
         default=None,
         help="The model path of sentencepiece. "
         "If not given, refers from the training args",
     )
+    group.add_argument(
+        "--time_sync",
+        type=str2bool,
+        default=False,
+        help="Time synchronous beam search.",
+    )
+    group.add_argument(
+        "--incremental_decode",
+        type=str2bool,
+        default=False,
+        help="Time synchronous beam search.",
+    )
+    group.add_argument(
+        "--blank_penalty",
+        type=float,
+        default=1.0,
+        help="Time synchronous beam search.",
+    )
+    group.add_argument(
+        "--hold_n",
+        type=int,
+        default=0,
+        help="Time synchronous beam search.",
+    )
+    group.add_argument(
+        "--transducer_conf",
+        default=None,
+        help="The keyword arguments for transducer beam search.",
+    )
+    group.add_argument("--hugging_face_decoder", type=str2bool, default=False)
 
     return parser
 
 
 def main(cmd=None):
     print(get_commandline_args(), file=sys.stderr)
     parser = get_parser()
```

### Comparing `espnet-202304/espnet2/bin/svs_inference.py` & `espnet-202308/espnet2/bin/svs_inference.py`

 * *Files 3% similar despite different names*

```diff
@@ -13,23 +13,23 @@
 import numpy as np
 import soundfile as sf
 import torch
 from typeguard import check_argument_types
 
 from espnet2.fileio.npy_scp import NpyScpWriter
 from espnet2.gan_svs.vits import VITS
+from espnet2.svs.singing_tacotron.singing_tacotron import singing_tacotron
 from espnet2.tasks.svs import SVSTask
 from espnet2.torch_utils.device_funcs import to_device
 from espnet2.torch_utils.set_all_random_seed import set_all_random_seed
+from espnet2.tts.utils import DurationCalculator
 from espnet2.utils import config_argparse
 from espnet2.utils.types import str2bool, str2triple_str, str_or_none
 from espnet.utils.cli_utils import get_commandline_args
 
-# from espnet2.tts.utils import DurationCalculator
-
 
 class SingingGenerate:
     """SingingGenerate class
 
     Examples:
         >>> import soundfile
         >>> svs = SingingGenerate("config.yml", "model.pth")
@@ -37,40 +37,49 @@
         >>> soundfile.write("out.wav", wav.numpy(), svs.fs, "PCM_16")
     """
 
     def __init__(
         self,
         train_config: Optional[Union[Path, str]],
         model_file: Optional[Union[Path, str]] = None,
+        threshold: float = 0.5,
+        minlenratio: float = 0.0,
+        maxlenratio: float = 10.0,
         use_teacher_forcing: bool = False,
+        use_att_constraint: bool = False,
+        use_dynamic_filter: bool = False,
+        backward_window: int = 2,
+        forward_window: int = 4,
+        speed_control_alpha: float = 1.0,
         noise_scale: float = 0.667,
         noise_scale_dur: float = 0.8,
         vocoder_config: Union[Path, str] = None,
         vocoder_checkpoint: Union[Path, str] = None,
         dtype: str = "float32",
         device: str = "cpu",
         seed: int = 777,
         always_fix_seed: bool = False,
         prefer_normalized_feats: bool = False,
     ):
+        """Initialize SingingGenerate module."""
         assert check_argument_types()
 
         # setup model
         model, train_args = SVSTask.build_model_from_file(
             train_config, model_file, device
         )
         model.to(dtype=getattr(torch, dtype)).eval()
         self.device = device
         self.dtype = dtype
         self.train_args = train_args
         self.model = model
         self.svs = model.svs
         self.normalize = model.normalize
         self.feats_extract = model.feats_extract
-        # self.duration_calculator = DurationCalculator() # TODO(Yuning)
+        self.duration_calculator = DurationCalculator()
         self.preprocess_fn = SVSTask.build_preprocess_fn(train_args, False)
         self.use_teacher_forcing = use_teacher_forcing
         self.seed = seed
         self.always_fix_seed = always_fix_seed
         self.vocoder = None
         self.prefer_normalized_feats = prefer_normalized_feats
         if vocoder_checkpoint is not None:
@@ -91,14 +100,24 @@
         decode_conf = {}
         decode_conf.update({"use_teacher_forcing": use_teacher_forcing})
         if isinstance(self.svs, VITS):
             decode_conf.update(
                 noise_scale=noise_scale,
                 noise_scale_dur=noise_scale_dur,
             )
+        if isinstance(self.svs, singing_tacotron):
+            decode_conf.update(
+                threshold=threshold,
+                maxlenratio=maxlenratio,
+                minlenratio=minlenratio,
+                use_att_constraint=use_att_constraint,
+                use_dynamic_filter=use_dynamic_filter,
+                forward_window=forward_window,
+                backward_window=backward_window,
+            )
         self.decode_conf = decode_conf
 
     @torch.no_grad()
     def __call__(
         self,
         text: Union[Dict[str, Tuple], torch.Tensor, np.ndarray],
         singing: Union[torch.Tensor, np.ndarray] = None,
@@ -138,14 +157,16 @@
             duration_syb = data["duration_syb"]
             phn_cnt = data["phn_cnt"]
             slur = data["slur"]
             batch = dict(text=data["label"])
         else:
             batch = dict(text=text)
 
+        if singing is not None:
+            batch.update(singing=singing)
         if label is not None:
             batch.update(label=label)
         if midi is not None:
             batch.update(midi=midi)
         if duration_phn is not None:
             batch.update(duration_phn=duration_phn)
         if duration_ruled_phn is not None:
@@ -170,17 +191,17 @@
 
         cfg = self.decode_conf
         if decode_conf is not None:
             cfg = self.decode_conf.copy()
             cfg.update(decode_conf)
         output_dict = self.model.inference(**batch, **cfg)
 
-        if output_dict.get("att_ws") is not None:
-            output_dict.update(duration=None, focus_rate=None)
-            # duration, focus_rate = self.duration_calculator(att_ws)
+        if output_dict.get("att_w") is not None:
+            duration, focus_rate = self.duration_calculator(output_dict["att_w"])
+            output_dict.update(duration=duration, focus_rate=focus_rate)
         else:
             output_dict.update(duration=None, focus_rate=None)
 
         # apply vocoder (mel-to-wav)
         if self.vocoder is not None:
             if (
                 self.prefer_normalized_feats
@@ -300,14 +321,15 @@
     model_file: Optional[str],
     use_teacher_forcing: bool,
     noise_scale: float,
     noise_scale_dur: float,
     allow_variable_data_keys: bool,
     vocoder_config: Optional[str] = None,
     vocoder_checkpoint: Optional[str] = None,
+    vocoder_tag: Optional[str] = None,
 ):
     """Perform SVS model decoding."""
     assert check_argument_types()
     if batch_size > 1:
         raise NotImplementedError("batch decoding is not implemented")
     if ngpu > 1:
         raise NotImplementedError("only single GPU decoding is supported")
```

### Comparing `espnet-202304/espnet2/bin/tokenize_text.py` & `espnet-202308/espnet2/bin/tokenize_text.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/bin/tts_inference.py` & `espnet-202308/espnet2/bin/tts_inference.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/bin/uasr_extract_feature.py` & `espnet-202308/espnet2/bin/uasr_extract_feature.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/bin/uasr_inference.py` & `espnet-202308/espnet2/bin/uasr_inference_k2.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,277 +1,290 @@
 #!/usr/bin/env python3
 import argparse
+import datetime
 import logging
 import sys
-from distutils.version import LooseVersion
 from pathlib import Path
 from typing import Any, List, Optional, Sequence, Tuple, Union
 
 import numpy as np
 import torch
-import torch.quantization
+import yaml
 from typeguard import check_argument_types, check_return_type
 
 from espnet2.fileio.datadir_writer import DatadirWriter
 from espnet2.tasks.lm import LMTask
 from espnet2.tasks.uasr import UASRTask
 from espnet2.text.build_tokenizer import build_tokenizer
 from espnet2.text.token_id_converter import TokenIDConverter
 from espnet2.torch_utils.device_funcs import to_device
 from espnet2.torch_utils.set_all_random_seed import set_all_random_seed
 from espnet2.utils import config_argparse
 from espnet2.utils.types import str2bool, str2triple_str, str_or_none
-from espnet.nets.batch_beam_search import BatchBeamSearch
-from espnet.nets.beam_search import BeamSearch, Hypothesis
 from espnet.nets.pytorch_backend.transformer.subsampling import TooShortUttError
-from espnet.nets.scorer_interface import BatchScorerInterface
-from espnet.nets.scorers.uasr import UASRPrefixScorer
-
-# from espnet.nets.scorers.uasr import UASRPrefixScorer
 from espnet.utils.cli_utils import get_commandline_args
 
+try:
+    import k2  # for CI import
+    from icefall.decode import get_lattice, one_best_decoding
+    from icefall.utils import get_texts
+except ImportError or ModuleNotFoundError:
+    k2 = None
+
+
+def indices_to_split_size(indices, total_elements: int = None):
+    """convert indices to split_size
+
+    During decoding, the api torch.tensor_split should be used.
+    However, torch.tensor_split is only available with pytorch >= 1.8.0.
+    So torch.split is used to pass ci with pytorch < 1.8.0.
+    This fuction is used to prepare input for torch.split.
+    """
+    if indices[0] != 0:
+        indices = [0] + indices
+
+    split_size = [indices[i] - indices[i - 1] for i in range(1, len(indices))]
+    if total_elements is not None and sum(split_size) != total_elements:
+        split_size.append(total_elements - sum(split_size))
+    return split_size
 
-class Speech2Text:
-    """Speech2Text class for unsupervised ASR
+
+class k2Speech2Text:
+    """Speech2Text class
 
     Examples:
         >>> import soundfile
-        >>> speech2text = Speech2Text("uasr_config.yml", "uasr.pth")
+        >>> speech2text = k2Speech2Text("uasr_config.yml", "uasr.pth")
         >>> audio, rate = soundfile.read("speech.wav")
-        >>> speech2text(audio)
-        [(text, token, token_int, hypothesis_object), ...]
+        >>> speech = np.expand_dims(audio, 0) # shape: [batch_size, speech_length]
+        >>> speech_lengths = np.array([audio.shape[0]]) # shape: [batch_size]
+        >>> batch = {"speech": speech, "speech_lengths", speech_lengths}
+        >>> speech2text(batch)
+        [(text, token, token_int, score), ...]
 
     """
 
     def __init__(
         self,
-        uasr_train_config: Union[Path, str] = None,
+        uasr_train_config: Union[Path, str],
+        decoding_graph: str,
         uasr_model_file: Union[Path, str] = None,
         lm_train_config: Union[Path, str] = None,
         lm_file: Union[Path, str] = None,
-        ngram_scorer: str = "full",
-        ngram_file: Union[Path, str] = None,
         token_type: str = None,
         bpemodel: str = None,
         device: str = "cpu",
+        maxlenratio: float = 0.0,
+        minlenratio: float = 0.0,
         batch_size: int = 1,
         dtype: str = "float32",
-        beam_size: int = 20,
+        beam_size: int = 8,
+        ctc_weight: float = 0.5,
         lm_weight: float = 1.0,
-        ngram_weight: float = 0.9,
+        penalty: float = 0.0,
         nbest: int = 1,
-        quantize_uasr_model: bool = False,
-        quantize_lm: bool = False,
-        quantize_modules: List[str] = ["Linear"],
-        quantize_dtype: str = "qint8",
+        streaming: bool = False,
+        search_beam_size: int = 20,
+        output_beam_size: int = 20,
+        min_active_states: int = 14000,
+        max_active_states: int = 56000,
+        blank_bias: float = 0.0,
+        lattice_weight: float = 1.0,
+        is_ctc_decoding: bool = True,
+        lang_dir: Optional[str] = None,
+        token_list_file: Optional[str] = None,
+        use_fgram_rescoring: bool = False,
+        use_nbest_rescoring: bool = False,
+        am_weight: float = 0.5,
+        decoder_weight: float = 0.5,
+        nnlm_weight: float = 1.0,
+        num_paths: int = 1000,
+        nbest_batch_size: int = 500,
+        nll_batch_size: int = 100,
     ):
         assert check_argument_types()
 
-        if quantize_uasr_model or quantize_lm:
-            if quantize_dtype == "float16" and torch.__version__ < LooseVersion(
-                "1.5.0"
-            ):
-                raise ValueError(
-                    "float16 dtype for dynamic quantization is not supported with "
-                    "torch version < 1.5.0. Switch to qint8 dtype instead."
-                )
-
-        quantize_modules = set([getattr(torch.nn, q) for q in quantize_modules])
-        quantize_dtype = getattr(torch, quantize_dtype)
-
         # 1. Build UASR model
-        scorers = {}
         uasr_model, uasr_train_args = UASRTask.build_model_from_file(
             uasr_train_config, uasr_model_file, device
         )
-        # TODO(Jiatong): change to not used pre-extracted features for inference
         uasr_model.use_collected_training_feats = True
         uasr_model.to(dtype=getattr(torch, dtype)).eval()
 
-        if quantize_uasr_model:
-            logging.info("Use quantized uasr model for decoding.")
-
-            uasr_model = torch.quantization.quantize_dynamic(
-                uasr_model, qconfig_spec=quantize_modules, dtype=quantize_dtype
-            )
-
-        decoder = UASRPrefixScorer(eos=uasr_model.eos)
-        token_list = uasr_model.token_list
-        scorers.update(decoder=decoder)
-        logging.info(f"beam search token list: {token_list}")
+        if token_list_file is not None:
+            token_list = []
+            with open(token_list_file, "r") as tlf:
+                for line in tlf.readlines():
+                    token, _ = line.split()
+                    assert token not in token_list
+                    token_list.append(token)
+        else:
+            token_list = uasr_model.token_list
 
         # 2. Build Language model
         if lm_train_config is not None:
             lm, lm_train_args = LMTask.build_model_from_file(
                 lm_train_config, lm_file, device
             )
+            self.lm = lm
 
-            if quantize_lm:
-                logging.info("Use quantized lm for decoding.")
-
-                lm = torch.quantization.quantize_dynamic(
-                    lm, qconfig_spec=quantize_modules, dtype=quantize_dtype
-                )
-
-            scorers["lm"] = lm.lm
-
-        # 3. Build ngram model
-        if ngram_file is not None:
-            if ngram_scorer == "full":
-                from espnet.nets.scorers.ngram import NgramFullScorer
-
-                ngram = NgramFullScorer(ngram_file, token_list)
-            else:
-                from espnet.nets.scorers.ngram import NgramPartScorer
+        self.is_ctc_decoding = is_ctc_decoding
+        self.use_fgram_rescoring = use_fgram_rescoring
+        self.use_nbest_rescoring = use_nbest_rescoring
+
+        # load decoding graph
+        self.decoding_graph = k2.Fsa.from_dict(torch.load(decoding_graph))
+        self.decoding_graph = self.decoding_graph.to(device)
 
-                ngram = NgramPartScorer(ngram_file, token_list)
-        else:
-            ngram = None
-        scorers["ngram"] = ngram
-
-        # 4. Build BeamSearch object
-        weights = dict(
-            decoder=1.0,
-            lm=lm_weight,
-            ngram=ngram_weight,
-        )
-
-        beam_search = BeamSearch(
-            beam_size=beam_size,
-            weights=weights,
-            scorers=scorers,
-            sos=uasr_model.sos,
-            eos=uasr_model.eos,
-            vocab_size=len(token_list),
-            token_list=token_list,
-            pre_beam_score_key=None,  # NOTE(jiatong): for frame-decoding
-        )
-
-        # TODO(karita): make all scorers batchfied
-        if batch_size == 1:
-            non_batch = [
-                k
-                for k, v in beam_search.full_scorers.items()
-                if not isinstance(v, BatchScorerInterface)
-            ]
-            if len(non_batch) == 0:
-                beam_search.__class__ = BatchBeamSearch
-                logging.info("BatchBeamSearch implementation is selected.")
-            else:
-                logging.warning(
-                    f"As non-batch scorers {non_batch} are found, "
-                    f"fall back to non-batch implementation."
-                )
-
-        beam_search.to(device=device, dtype=getattr(torch, dtype)).eval()
-        for scorer in scorers.values():
-            if isinstance(scorer, torch.nn.Module):
-                scorer.to(device=device, dtype=getattr(torch, dtype)).eval()
-        logging.info(f"Beam_search: {beam_search}")
-        logging.info(f"Decoding device={device}, dtype={dtype}")
-
-        # 5. [Optional] Build Text converter: e.g. bpe-sym -> Text
-        if token_type is None:
-            token_type = uasr_train_args.token_type
-        if bpemodel is None:
-            bpemodel = uasr_train_args.bpemodel
-
-        # delete
-        token_type = "word"
-
-        if token_type is None:
-            tokenizer = None
-        elif token_type == "bpe":
-            if bpemodel is not None:
-                tokenizer = build_tokenizer(token_type=token_type, bpemodel=bpemodel)
-            else:
-                tokenizer = None
-        else:
-            tokenizer = build_tokenizer(token_type=token_type)
+        assert token_type is not None
+        tokenizer = build_tokenizer(token_type=token_type)
         converter = TokenIDConverter(token_list=token_list)
         logging.info(f"Text tokenizer: {tokenizer}")
+        logging.info(f"Running on : {device}")
 
         self.uasr_model = uasr_model
         self.uasr_train_args = uasr_train_args
         self.converter = converter
         self.tokenizer = tokenizer
-        self.beam_search = beam_search
         self.device = device
         self.dtype = dtype
-        self.nbest = nbest
+        self.search_beam_size = search_beam_size
+        self.output_beam_size = output_beam_size
+        self.min_active_states = min_active_states
+        self.max_active_states = max_active_states
+        self.blank_bias = blank_bias
+        self.lattice_weight = lattice_weight
+        self.am_weight = am_weight
+        self.decoder_weight = decoder_weight
+        self.nnlm_weight = nnlm_weight
+        self.num_paths = num_paths
+        self.nbest_batch_size = nbest_batch_size
+        self.nll_batch_size = nll_batch_size
+        self.uasr_model_ignore_id = 0
 
     @torch.no_grad()
     def __call__(
         self, speech: Union[torch.Tensor, np.ndarray]
-    ) -> List[Tuple[Optional[str], List[str], List[int], Union[Hypothesis]]]:
+    ) -> List[Tuple[Optional[str], List[str], List[int], float]]:
         """Inference
 
         Args:
-            data: Input speech data
+            batch: Input speech data and corresponding lengths
         Returns:
             text, token, token_int, hyp
 
         """
         assert check_argument_types()
 
-        # Input as audio signal
         if isinstance(speech, np.ndarray):
             speech = torch.tensor(speech)
 
         # data: (Nsamples,) -> (1, Nsamples)
         speech = speech.unsqueeze(0).to(getattr(torch, self.dtype))
         # lengths: (1,)
         lengths = speech.new_full([1], dtype=torch.long, fill_value=speech.size(1))
         batch = {"speech": speech, "speech_lengths": lengths}
 
         # a. To device
         batch = to_device(batch, device=self.device)
 
-        # b. Forward encoder
-        generated_sample, generated_sample_padding_mask = self.uasr_model.inference(
-            **batch
+        # b. Forward Encoder
+        # enc: [N, T, C]
+        generated_sample, _ = self.uasr_model.inference(**batch)
+
+        # nnet_output: [N, T, C]
+        logp_encoder_output = torch.nn.functional.log_softmax(generated_sample, dim=-1)
+
+        # It maybe useful to tune blank_bias.
+        # The valid range of blank_bias is [-inf, 0]
+        #        logp_encoder_output[:, :, 4] += 0
+
+        batch_size, time_length, _ = generated_sample.shape
+        assert batch_size == 1
+        sequence_idx = 0
+        start_frame = 0
+        num_frames = time_length
+        supervision_segments = torch.Tensor([[sequence_idx, start_frame, num_frames]])
+        supervision_segments = supervision_segments.to(torch.int32)
+
+        # An introduction to DenseFsaVec:
+        # https://k2-fsa.github.io/k2/core_concepts/index.html#dense-fsa-vector
+        # It could be viewed as a fsa-type lopg_encoder_output,
+        # whose weight on the arcs are initialized with logp_encoder_output.
+        # The goal of converting tensor-type to fsa-type is using
+        # fsa related functions in k2. e.g. k2.intersect_dense_pruned below
+
+        # The term "intersect" is similar to "compose" in k2.
+        # The differences is are:
+        # for "compose" functions, the composition involves
+        # mathcing output label of a.fsa and input label of b.fsa
+        # while for "intersect" functions, the composition involves
+        # matching input label of a.fsa and input label of b.fsa
+        # Actually, in compose functions, b.fsa is inverted and then
+        # a.fsa and inv_b.fsa are intersected together.
+        # For difference between compose and interset:
+        # https://github.com/k2-fsa/k2/blob/master/k2/python/k2/fsa_algo.py#L308
+        # For definition of k2.intersect_dense_pruned:
+        # https://github.com/k2-fsa/k2/blob/master/k2/python/k2/autograd.py#L648
+
+        lattices = get_lattice(
+            nnet_output=logp_encoder_output,
+            decoding_graph=self.decoding_graph,
+            supervision_segments=supervision_segments,
+            search_beam=self.search_beam_size,
+            output_beam=self.output_beam_size,
+            min_active_states=self.min_active_states,
+            max_active_states=self.max_active_states,
         )
-        assert len(generated_sample) == 1, len(generated_sample)
-
-        # TODO(jiatong): add beamsearch
-        nbest_hyps = self.beam_search(x=generated_sample[0], maxlenratio=1.0)
 
-        nbest_hyps = nbest_hyps[: self.nbest]
+        # lattices.scores is the sum of decode_graph.scores(a.k.a. lm weight) and
+        # dense_fsa_vec.scores(a.k.a. am weight) on related arcs.
+        # For ctc decoding graph, lattices.scores only store am weight
+        # since the decoder_graph only define the ctc topology and
+        # has no lm weight on its arcs.
+        # While for 3-gram decoding, whose graph is converted from language models,
+        # lattice.scores contains both am weights and lm weights
+        #
+        # It maybe useful to tune lattice.scores
+        # The valid range of lattice_weight is [0, inf)
+        # The lattice_weight will affect the search of k2.random_paths
+        lattices.scores *= self.lattice_weight
 
         results = []
-        for hyp in nbest_hyps:
-            assert isinstance(hyp, Hypothesis), type(hyp)
-
-            # remove sos/eos and get results
-            if isinstance(hyp.yseq, list):
-                token_int = hyp.yseq[1:-1]
-            else:
-                token_int = hyp.yseq[1:-1].tolist()
-
-            # remove blank symbol id, which is assumed to be 0
-            token_int = list(filter(lambda x: x >= 4, token_int))
+        # TODO(Dongji): add nbest_rescoring
+        if self.use_nbest_rescoring:
+            raise ValueError("Currently nbest rescoring is not supported")
+        else:
+            best_paths = one_best_decoding(lattices, use_double_scores=True)
+            scores = best_paths.get_tot_scores(
+                use_double_scores=True, log_semiring=False
+            ).tolist()
+            hyps = get_texts(best_paths)
+
+        assert len(scores) == len(hyps)
+
+        for token_int, score in zip(hyps, scores):
+            # For decoding methods nbest_rescoring and ctc_decoding
+            # hyps stores token_index, which is lattice.labels.
 
-            # Change integer-ids to tokens
+            # convert token_id to text with self.tokenizer
             token = self.converter.ids2tokens(token_int)
-
-            if self.tokenizer is not None:
-                text = self.tokenizer.tokens2text(token)
-            else:
-                text = None
-            results.append((text, token, token_int, hyp))
+            assert self.tokenizer is not None
+            text = self.tokenizer.tokens2text(token)
+            results.append((text, token, token_int, score))
 
         assert check_return_type(results)
         return results
 
     @staticmethod
     def from_pretrained(
         model_tag: Optional[str] = None,
         **kwargs: Optional[Any],
     ):
-        """Build Speech2Text instance from the pretrained model.
+        """Build k2Speech2Text instance from the pretrained model.
 
         Args:
             model_tag (Optional[str]): Model tag of the pretrained models.
                 Currently, the tags of espnet_model_zoo are supported.
 
         Returns:
             Speech2Text: Speech2Text instance.
@@ -286,52 +299,56 @@
                     "`espnet_model_zoo` is not installed. "
                     "Please install via `pip install -U espnet_model_zoo`."
                 )
                 raise
             d = ModelDownloader()
             kwargs.update(**d.download_and_unpack(model_tag))
 
-        return Speech2Text(**kwargs)
+        return k2Speech2Text(**kwargs)
 
 
 def inference(
     output_dir: str,
+    decoding_graph: str,
+    maxlenratio: float,
+    minlenratio: float,
     batch_size: int,
     dtype: str,
     beam_size: int,
     ngpu: int,
     seed: int,
+    ctc_weight: float,
     lm_weight: float,
-    ngram_weight: float,
+    penalty: float,
     nbest: int,
     num_workers: int,
     log_level: Union[int, str],
     data_path_and_name_and_type: Sequence[Tuple[str, str, str]],
     key_file: Optional[str],
     uasr_train_config: Optional[str],
     uasr_model_file: Optional[str],
     lm_train_config: Optional[str],
     lm_file: Optional[str],
     word_lm_train_config: Optional[str],
     word_lm_file: Optional[str],
-    ngram_file: Optional[str],
     model_tag: Optional[str],
     token_type: Optional[str],
+    word_token_list: Optional[str],
     bpemodel: Optional[str],
     allow_variable_data_keys: bool,
-    quantize_uasr_model: bool,
-    quantize_lm: bool,
-    quantize_modules: List[str],
-    quantize_dtype: str,
+    streaming: bool,
+    is_ctc_decoding: bool,
+    use_nbest_rescoring: bool,
+    num_paths: int,
+    nbest_batch_size: int,
+    nll_batch_size: int,
+    k2_config: Optional[str],
 ):
+    assert is_ctc_decoding, "Currently, only ctc_decoding graph is supported."
     assert check_argument_types()
-    if batch_size > 1:
-        raise NotImplementedError("batch decoding is not implemented")
-    if word_lm_train_config is not None:
-        raise NotImplementedError("Word LM is not implemented")
     if ngpu > 1:
         raise NotImplementedError("only single GPU decoding is supported")
 
     logging.basicConfig(
         level=log_level,
         format="%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s",
     )
@@ -339,36 +356,46 @@
     if ngpu >= 1:
         device = "cuda"
     else:
         device = "cpu"
 
     # 1. Set random-seed
     set_all_random_seed(seed)
+    with open(k2_config) as k2_config_file:
+        dict_k2_config = yaml.safe_load(k2_config_file)
 
     # 2. Build speech2text
     speech2text_kwargs = dict(
         uasr_train_config=uasr_train_config,
         uasr_model_file=uasr_model_file,
+        decoding_graph=decoding_graph,
         lm_train_config=lm_train_config,
         lm_file=lm_file,
-        ngram_file=ngram_file,
         token_type=token_type,
+        token_list_file=word_token_list,
         bpemodel=bpemodel,
         device=device,
+        maxlenratio=maxlenratio,
+        minlenratio=minlenratio,
         dtype=dtype,
         beam_size=beam_size,
+        ctc_weight=ctc_weight,
         lm_weight=lm_weight,
-        ngram_weight=ngram_weight,
+        penalty=penalty,
         nbest=nbest,
-        quantize_uasr_model=quantize_uasr_model,
-        quantize_lm=quantize_lm,
-        quantize_modules=quantize_modules,
-        quantize_dtype=quantize_dtype,
+        streaming=streaming,
+        is_ctc_decoding=is_ctc_decoding,
+        use_nbest_rescoring=use_nbest_rescoring,
+        num_paths=num_paths,
+        nbest_batch_size=nbest_batch_size,
+        nll_batch_size=nll_batch_size,
     )
-    speech2text = Speech2Text.from_pretrained(
+
+    speech2text_kwargs = dict(**speech2text_kwargs, **dict_k2_config)
+    speech2text = k2Speech2Text.from_pretrained(
         model_tag=model_tag,
         **speech2text_kwargs,
     )
 
     # 3. Build data-iterator
     loader = UASRTask.build_streaming_iterator(
         data_path_and_name_and_type,
@@ -378,47 +405,45 @@
         num_workers=num_workers,
         preprocess_fn=UASRTask.build_preprocess_fn(speech2text.uasr_train_args, False),
         collate_fn=UASRTask.build_collate_fn(speech2text.uasr_train_args, False),
         allow_variable_data_keys=allow_variable_data_keys,
         inference=True,
     )
 
-    # 7 .Start for-loop
-    # FIXME(kamo): The output format should be discussed about
     with DatadirWriter(output_dir) as writer:
-        for keys, batch in loader:
+        start_decoding_time = datetime.datetime.now()
+        for batch_idx, (keys, batch) in enumerate(loader):
+            if batch_idx % 10 == 0:
+                logging.info(f"Processing {batch_idx} batch")
             assert isinstance(batch, dict), type(batch)
             assert all(isinstance(s, str) for s in keys), keys
             _bs = len(next(iter(batch.values())))
             assert len(keys) == _bs, f"{len(keys)} != {_bs}"
             batch = {k: v[0] for k, v in batch.items() if not k.endswith("_lengths")}
 
-            # N-best list of (text, token, token_int, hyp_object)
+            # 1-best list of (text, token, token_int)
             try:
                 results = speech2text(**batch)
             except TooShortUttError as e:
                 logging.warning(f"Utterance {keys} {e}")
-                hyp = Hypothesis(score=0.0, scores={}, states={}, yseq=[])
-                results = [[" ", ["<space>"], [2], hyp]] * nbest
-
-            # Only supporting batch_size==1
-            key = keys[0]
-            for n, (text, token, token_int, hyp) in zip(range(1, nbest + 1), results):
-                # Create a directory: outdir/{n}best_recog
-                ibest_writer = writer[f"{n}best_recog"]
 
+            for key_idx, (text, token, token_int, score) in enumerate(results):
+                key = keys[key_idx]
+                best_writer = writer["1best_recog"]
                 # Write the result to each file
-                ibest_writer["token"][key] = " ".join(token)
-                ibest_writer["token_int"][key] = " ".join(map(str, token_int))
-                ibest_writer["score"][key] = str(hyp.score)
+                best_writer["token"][key] = " ".join(token)
+                best_writer["token_int"][key] = " ".join(map(str, token_int))
+                best_writer["score"][key] = str(score)
 
                 if text is not None:
-                    ibest_writer["text"][key] = text
-                    logging.info("key: {} text: {}".format(key, text))
-                    logging.info("key: {} token_int: {}\n".format(key, token_int))
+                    best_writer["text"][key] = text
+
+        end_decoding_time = datetime.datetime.now()
+        decoding_duration = end_decoding_time - start_decoding_time
+        logging.info(f"Decoding duration is {decoding_duration.seconds} seconds")
 
 
 def get_parser():
     parser = config_argparse.ArgumentParser(
         description="UASR Decoding",
         formatter_class=argparse.ArgumentDefaultsHelpFormatter,
     )
@@ -464,20 +489,20 @@
     group.add_argument("--key_file", type=str_or_none)
     group.add_argument("--allow_variable_data_keys", type=str2bool, default=False)
 
     group = parser.add_argument_group("The model configuration related")
     group.add_argument(
         "--uasr_train_config",
         type=str,
-        help="uasr training configuration",
+        help="UASR training configuration",
     )
     group.add_argument(
         "--uasr_model_file",
         type=str,
-        help="uasr model parameter file",
+        help="UASR model parameter file",
     )
     group.add_argument(
         "--lm_train_config",
         type=str,
         help="LM training configuration",
     )
     group.add_argument(
@@ -492,89 +517,108 @@
     )
     group.add_argument(
         "--word_lm_file",
         type=str,
         help="Word LM parameter file",
     )
     group.add_argument(
-        "--ngram_file",
-        type=str,
-        help="N-gram parameter file",
-    )
-    group.add_argument(
         "--model_tag",
         type=str,
         help="Pretrained model tag. If specify this option, *_train_config and "
         "*_file will be overwritten",
     )
 
-    group = parser.add_argument_group("Quantization related")
-    group.add_argument(
-        "--quantize_uasr_model",
-        type=str2bool,
-        default=False,
-        help="Apply dynamic quantization to uasr model.",
-    )
-    group.add_argument(
-        "--quantize_lm",
-        type=str2bool,
-        default=False,
-        help="Apply dynamic quantization to LM.",
-    )
-    group.add_argument(
-        "--quantize_modules",
-        type=str,
-        nargs="*",
-        default=["Linear"],
-        help="""List of modules to be dynamically quantized.
-        E.g.: --quantize_modules=[Linear,LSTM,GRU].
-        Each specified module should be an attribute of 'torch.nn', e.g.:
-        torch.nn.Linear, torch.nn.LSTM, torch.nn.GRU, ...""",
-    )
-    group.add_argument(
-        "--quantize_dtype",
-        type=str,
-        default="qint8",
-        choices=["float16", "qint8"],
-        help="Dtype for dynamic quantization.",
-    )
-
     group = parser.add_argument_group("Beam-search related")
     group.add_argument(
         "--batch_size",
         type=int,
         default=1,
         help="The batch size for inference",
     )
     group.add_argument("--nbest", type=int, default=1, help="Output N-best hypotheses")
     group.add_argument("--beam_size", type=int, default=20, help="Beam size")
+    group.add_argument("--penalty", type=float, default=0.0, help="Insertion penalty")
+    group.add_argument(
+        "--maxlenratio",
+        type=float,
+        default=0.0,
+        help="Input length ratio to obtain max output length. "
+        "If maxlenratio=0.0 (default), it uses a end-detect "
+        "function "
+        "to automatically find maximum hypothesis lengths",
+    )
+    group.add_argument(
+        "--minlenratio",
+        type=float,
+        default=0.0,
+        help="Input length ratio to obtain min output length",
+    )
+    group.add_argument(
+        "--ctc_weight",
+        type=float,
+        default=0.5,
+        help="CTC weight in joint decoding",
+    )
     group.add_argument("--lm_weight", type=float, default=1.0, help="RNNLM weight")
-    group.add_argument("--ngram_weight", type=float, default=0.9, help="ngram weight")
+    group.add_argument("--streaming", type=str2bool, default=False)
 
     group = parser.add_argument_group("Text converter related")
     group.add_argument(
         "--token_type",
         type=str_or_none,
         default=None,
-        choices=["char", "bpe", None],
-        help="The token type for uasr model. "
+        choices=["phn", "word"],
+        help="The token type for UASR model. "
         "If not given, refers from the training args",
     )
     group.add_argument(
         "--bpemodel",
         type=str_or_none,
         default=None,
         help="The model path of sentencepiece. "
         "If not given, refers from the training args",
     )
+    group.add_argument(
+        "--is_ctc_decoding",
+        type=str2bool,
+        default=True,
+        help="Use ctc topology as decoding graph",
+    )
+    group.add_argument("--use_nbest_rescoring", type=str2bool, default=False)
+    group.add_argument(
+        "--num_paths",
+        type=int,
+        default=1000,
+        help="The third argument for k2.random_paths",
+    )
+    group.add_argument(
+        "--nbest_batch_size",
+        type=int,
+        default=500,
+        help="batchify nbest list when computing am/lm scores to avoid OOM",
+    )
+    group.add_argument(
+        "--nll_batch_size",
+        type=int,
+        default=100,
+        help="batch_size when computing nll during nbest rescoring",
+    )
+    group.add_argument("--decoding_graph", type=str, help="decoding graph")
+    group.add_argument(
+        "--word_token_list", type=str_or_none, default=None, help="output token list"
+    )
+    group.add_argument("--k2_config", type=str, help="Config file for decoding with k2")
 
     return parser
 
 
 def main(cmd=None):
+    assert (
+        k2 is not None
+    ), "k2/icefall is not installed, please follow 'tools/installers' to install"
     print(get_commandline_args(), file=sys.stderr)
     parser = get_parser()
     args = parser.parse_args(cmd)
     kwargs = vars(args)
     kwargs.pop("config", None)
     inference(**kwargs)
```

### Comparing `espnet-202304/espnet2/bin/whisper_export_vocabulary.py` & `espnet-202308/espnet2/bin/whisper_export_vocabulary.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/diar/abs_diar.py` & `espnet-202308/espnet2/diar/abs_diar.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/diar/attractor/rnn_attractor.py` & `espnet-202308/espnet2/diar/attractor/rnn_attractor.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/diar/decoder/linear_decoder.py` & `espnet-202308/espnet2/diar/decoder/linear_decoder.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/diar/espnet_model.py` & `espnet-202308/espnet2/hubert/espnet_model.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,282 +1,434 @@
-# Copyright 2021 Jiatong Shi
-#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
 
+# Thanks to Abdelrahman Mohamed and Wei-Ning Hsu's help in this implementation,
+# Their origial Hubert work is in:
+#     Paper: https://arxiv.org/pdf/2106.07447.pdf
+#     Code in Fairseq: https://github.com/pytorch/fairseq/tree/master/examples/hubert
+
+import logging
 from contextlib import contextmanager
-from itertools import permutations
-from typing import Dict, Optional, Tuple
+from typing import Dict, List, Optional, Tuple, Union
 
-import numpy as np
 import torch
-import torch.nn.functional as F
 from packaging.version import parse as V
 from typeguard import check_argument_types
 
 from espnet2.asr.encoder.abs_encoder import AbsEncoder
 from espnet2.asr.frontend.abs_frontend import AbsFrontend
+from espnet2.asr.preencoder.abs_preencoder import AbsPreEncoder
 from espnet2.asr.specaug.abs_specaug import AbsSpecAug
-from espnet2.diar.attractor.abs_attractor import AbsAttractor
-from espnet2.diar.decoder.abs_decoder import AbsDecoder
+from espnet2.hubert.hubert_loss import HubertPretrainLoss
 from espnet2.layers.abs_normalize import AbsNormalize
 from espnet2.torch_utils.device_funcs import force_gatherable
 from espnet2.train.abs_espnet_model import AbsESPnetModel
-from espnet.nets.pytorch_backend.nets_utils import to_device
+from espnet.nets.e2e_asr_common import ErrorCalculator
 
 if V(torch.__version__) >= V("1.6.0"):
     from torch.cuda.amp import autocast
 else:
     # Nothing to do if torch<1.6.0
     @contextmanager
     def autocast(enabled=True):
         yield
 
 
-class ESPnetDiarizationModel(AbsESPnetModel):
-    """Speaker Diarization model
-
-    If "attractor" is "None", SA-EEND will be used.
-    Else if "attractor" is not "None", EEND-EDA will be used.
-    For the details about SA-EEND and EEND-EDA, refer to the following papers:
-    SA-EEND: https://arxiv.org/pdf/1909.06247.pdf
-    EEND-EDA: https://arxiv.org/pdf/2005.09921.pdf, https://arxiv.org/pdf/2106.10654.pdf
-    """
+class TorchAudioHubertPretrainModel(AbsESPnetModel):
+    """TorchAudio Hubert Pretrain model"""
 
     def __init__(
         self,
+        vocab_size: int,
+        token_list: Union[Tuple[str, ...], List[str]],
         frontend: Optional[AbsFrontend],
         specaug: Optional[AbsSpecAug],
         normalize: Optional[AbsNormalize],
-        label_aggregator: torch.nn.Module,
+        preencoder: Optional[AbsPreEncoder],
         encoder: AbsEncoder,
-        decoder: AbsDecoder,
-        attractor: Optional[AbsAttractor],
-        diar_weight: float = 1.0,
-        attractor_weight: float = 1.0,
+        ignore_id: int = -1,
     ):
         assert check_argument_types()
 
         super().__init__()
+        self.vocab_size = vocab_size
+        self.ignore_id = ignore_id
+        self.token_list = token_list.copy()
 
-        self.encoder = encoder
-        self.normalize = normalize
         self.frontend = frontend
         self.specaug = specaug
-        self.label_aggregator = label_aggregator
-        self.diar_weight = diar_weight
-        self.attractor_weight = attractor_weight
-        self.attractor = attractor
-        self.decoder = decoder
-
-        if self.attractor is not None:
-            self.decoder = None
-        elif self.decoder is not None:
-            self.num_spk = decoder.num_spk
-        else:
-            raise NotImplementedError
+        self.normalize = normalize
+        self.preencoder = preencoder
+        self.encoder = encoder
+        self.error_calculator = None
+
+        self.nan_loss_count = 0.0
 
     def forward(
         self,
         speech: torch.Tensor,
-        speech_lengths: torch.Tensor = None,
-        spk_labels: torch.Tensor = None,
-        spk_labels_lengths: torch.Tensor = None,
+        speech_lengths: torch.Tensor,
+        text: torch.Tensor,
+        text_lengths: torch.Tensor,
         **kwargs,
     ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor]:
-        """Frontend + Encoder + Decoder + Calc loss
+        """Frontend + Encoder + Calc loss
 
         Args:
-            speech: (Batch, samples)
-            speech_lengths: (Batch,) default None for chunk interator,
-                                     because the chunk-iterator does not
-                                     have the speech_lengths returned.
-                                     see in
-                                     espnet2/iterators/chunk_iter_factory.py
-            spk_labels: (Batch, )
+            speech: (Batch, Length, ...)
+            speech_lengths: (Batch, )
+            text: (Batch, Length)
+            text_lengths: (Batch,)
             kwargs: "utt_id" is among the input.
         """
-        assert speech.shape[0] == spk_labels.shape[0], (speech.shape, spk_labels.shape)
+        assert text_lengths.dim() == 1, text_lengths.shape
+        # Check that batch_size is unified
+        assert (
+            speech.shape[0]
+            == speech_lengths.shape[0]
+            == text.shape[0]
+            == text_lengths.shape[0]
+        ), (speech.shape, speech_lengths.shape, text.shape, text_lengths.shape)
         batch_size = speech.shape[0]
 
+        # for data-parallel
+        text = text[:, : text_lengths.max()]
+
         # 1. Encoder
-        # Use bottleneck_feats if exist. Only for "enh + diar" task.
-        bottleneck_feats = kwargs.get("bottleneck_feats", None)
-        bottleneck_feats_lengths = kwargs.get("bottleneck_feats_lengths", None)
-        encoder_out, encoder_out_lens = self.encode(
-            speech, speech_lengths, bottleneck_feats, bottleneck_feats_lengths
+        logit_m, logit_u, feature_penalty = self.encode(
+            speech, speech_lengths, text, text_lengths
+        )
+
+        # 2a. Hubert criterion
+        loss = self._calc_hubert_loss(
+            logit_m,
+            logit_u,
+            feature_penalty,
         )
 
-        if self.attractor is None:
-            # 2a. Decoder (baiscally a predction layer after encoder_out)
-            pred = self.decoder(encoder_out, encoder_out_lens)
+        if not torch.isinf(loss) and not torch.isnan(loss):
+            pass
+            # logging.warning(f"loss, {loss.item() / logit_m.size(0)}")
         else:
-            # 2b. Encoder Decoder Attractors
-            # Shuffle the chronological order of encoder_out, then calculate attractor
-            encoder_out_shuffled = encoder_out.clone()
-            for i in range(len(encoder_out_lens)):
-                encoder_out_shuffled[i, : encoder_out_lens[i], :] = encoder_out[
-                    i, torch.randperm(encoder_out_lens[i]), :
-                ]
-            attractor, att_prob = self.attractor(
-                encoder_out_shuffled,
-                encoder_out_lens,
-                to_device(
-                    self,
-                    torch.zeros(
-                        encoder_out.size(0), spk_labels.size(2) + 1, encoder_out.size(2)
-                    ),
-                ),
-            )
-            # Remove the final attractor which does not correspond to a speaker
-            # Then multiply the attractors and encoder_out
-            pred = torch.bmm(encoder_out, attractor[:, :-1, :].permute(0, 2, 1))
-        # 3. Aggregate time-domain labels
-        spk_labels, spk_labels_lengths = self.label_aggregator(
-            spk_labels, spk_labels_lengths
+            self.nan_loss_count += 1
+            logging.warning(f"nan_loss_count, {self.nan_loss_count}")
+
+        # log accuracies of masked and unmasked frames
+        correct_m, count_m = self._compute_correct(logit_m)
+        correct_u, count_u = self._compute_correct(logit_u)
+
+        stats = dict(
+            loss=loss.detach(),
+            correct_m=correct_m,
+            count_m=count_m,
+            acc_m=correct_m / count_m,
+            correct_u=correct_u,
+            count_u=count_u,
+            acc_u=correct_u / count_u,
         )
 
-        # If encoder uses conv* as input_layer (i.e., subsampling),
-        # the sequence length of 'pred' might be slighly less than the
-        # length of 'spk_labels'. Here we force them to be equal.
-        length_diff_tolerance = 2
-        length_diff = spk_labels.shape[1] - pred.shape[1]
-        if length_diff > 0 and length_diff <= length_diff_tolerance:
-            spk_labels = spk_labels[:, 0 : pred.shape[1], :]
-
-        if self.attractor is None:
-            loss_pit, loss_att = None, None
-            loss, perm_idx, perm_list, label_perm = self.pit_loss(
-                pred, spk_labels, encoder_out_lens
-            )
+        # force_gatherable: to-device and to-tensor if scalar for DataParallel
+        loss, stats, weight = force_gatherable((loss, stats, batch_size), loss.device)
+        return loss, stats, weight
+
+    def collect_feats(
+        self,
+        speech: torch.Tensor,
+        speech_lengths: torch.Tensor,
+        text: torch.Tensor,
+        text_lengths: torch.Tensor,
+        **kwargs,
+    ) -> Dict[str, torch.Tensor]:
+        feats, feats_lengths = self._extract_feats(speech, speech_lengths)
+        return {"feats": feats, "feats_lengths": feats_lengths}
+
+    def encode(
+        self,
+        speech: torch.Tensor,
+        speech_lengths: torch.Tensor,
+        y_pad: torch.Tensor,
+        y_pad_length: torch.Tensor,
+    ) -> Tuple[torch.Tensor, torch.Tensor]:
+        """Frontend + Encoder. Note that this method is used by asr_inference.py
+
+        Args:
+            speech: (Batch, Length, ...)
+            speech_lengths: (Batch, )
+            y_pad: (Batch, Length, ...)
+            y_pad_length: (Batch, )
+        """
+        with autocast(False):
+            # 1. Extract feats
+            feats, feats_lengths = self._extract_feats(speech, speech_lengths)
+
+            # 2. Data augmentation
+            if self.specaug is not None and self.training:
+                feats, feats_lengths = self.specaug(feats, feats_lengths)
+
+            # 3. Normalization for feature: e.g. Global-CMVN, Utterance-CMVN
+            if self.normalize is not None:
+                feats, feats_lengths = self.normalize(feats, feats_lengths)
+
+        # Pre-encoder, e.g. used for raw input data
+        if self.preencoder is not None:
+            feats, feats_lengths = self.preencoder(feats, feats_lengths)
+
+        # 4. Forward encoder
+        # feats: (Batch, Length, Dim)
+        # -> encoder_out: (Batch, Length2, Dim2)
+        encoder_out = self.encoder(feats, feats_lengths, y_pad, y_pad_length)
+
+        return encoder_out
+
+    def _extract_feats(
+        self, speech: torch.Tensor, speech_lengths: torch.Tensor
+    ) -> Tuple[torch.Tensor, torch.Tensor]:
+        assert speech_lengths.dim() == 1, speech_lengths.shape
+
+        # for data-parallel
+        speech = speech[:, : speech_lengths.max()]
+
+        if self.frontend is not None:
+            # Frontend
+            #  e.g. STFT and Feature extract
+            #       data_loader may send time-domain signal in this case
+            # speech (Batch, NSamples) -> feats: (Batch, NFrames, Dim)
+            feats, feats_lengths = self.frontend(speech, speech_lengths)
+        else:
+            # No frontend and no feature extract
+            feats, feats_lengths = speech, speech_lengths
+        return feats, feats_lengths
+
+    def _compute_correct(
+        self,
+        logits,
+    ):
+        if logits.numel() == 0:
+            corr, count = 0, 0
         else:
-            loss_pit, perm_idx, perm_list, label_perm = self.pit_loss(
-                pred, spk_labels, encoder_out_lens
+            assert logits.dim() > 1, logits.shape
+            max = logits.argmax(-1) == 0
+            min = logits.argmin(-1) == 0
+            both = max & min
+            corr = max.long().sum().item() - both.long().sum().item()
+            count = max.numel()
+        return corr, count
+
+    def _calc_hubert_loss(
+        self,
+        logit_m: Optional[torch.Tensor],
+        logit_u: Optional[torch.Tensor],
+        feature_penalty: torch.Tensor,
+        masked_weight: float = 1.0,
+        unmasked_weight: float = 0.0,
+        feature_weight: float = 10.0,
+        reduction: str = "sum",
+    ) -> torch.Tensor:
+        """Compute the cross-entropy loss on HuBERT masked and non-masked logits.
+
+        Args:
+            logit_m (Tensor or None): The masked logit Tensor of dimension
+                `(masked_frames, final_dim)`.
+            logit_u (Tensor or None): The non-masked logit Tensor of dimension
+                `(unmasked_frames, final_dim)`.
+            feature_penalty (Tensor): The feature mean value for additional penalty
+                loss.
+            masked_weight (float, optional): The weight for masked cross-entropy loss
+                (Default: ``1.0``).
+            unmasked_weight (float, optional): The weight for non-masked cross-entropy
+                loss (Default: ``0.0``).
+            feature_weight (float, optional): The weight for feature penalty loss
+                (Default: ``10.0``).
+            reduction (str, optional): The reduction method for cross-entropy loss
+                (Default: ``"sum"``).
+        Ref:
+            torchaudio: examples/hubert/loss/hubert_loss.py
+        """
+        loss = feature_penalty * feature_weight * logit_m.shape[0]
+        if logit_m is not None:
+            target_m = torch.zeros(
+                logit_m.shape[0], dtype=torch.long, device=logit_m.device
+            )
+            loss_m = torch.nn.functional.cross_entropy(
+                logit_m, target_m, reduction=reduction
             )
-            loss_att = self.attractor_loss(att_prob, spk_labels)
-            loss = self.diar_weight * loss_pit + self.attractor_weight * loss_att
-        (
-            correct,
-            num_frames,
-            speech_scored,
-            speech_miss,
-            speech_falarm,
-            speaker_scored,
-            speaker_miss,
-            speaker_falarm,
-            speaker_error,
-        ) = self.calc_diarization_error(pred, label_perm, encoder_out_lens)
-
-        if speech_scored > 0 and num_frames > 0:
-            sad_mr, sad_fr, mi, fa, cf, acc, der = (
-                speech_miss / speech_scored,
-                speech_falarm / speech_scored,
-                speaker_miss / speaker_scored,
-                speaker_falarm / speaker_scored,
-                speaker_error / speaker_scored,
-                correct / num_frames,
-                (speaker_miss + speaker_falarm + speaker_error) / speaker_scored,
+            loss += loss_m * masked_weight
+        if logit_u is not None:
+            target_u = torch.zeros(
+                logit_u.shape[0], dtype=torch.long, device=logit_m.device
+            )
+            loss_u = torch.nn.functional.cross_entropy(
+                logit_u, target_u, reduction=reduction
+            )
+            loss += loss_u * unmasked_weight
+        return loss
+
+
+class HubertPretrainModel(AbsESPnetModel):
+    """Hubert Pretrain model"""
+
+    def __init__(
+        self,
+        vocab_size: int,
+        token_list: Union[Tuple[str, ...], List[str]],
+        frontend: Optional[AbsFrontend],
+        specaug: Optional[AbsSpecAug],
+        normalize: Optional[AbsNormalize],
+        preencoder: Optional[AbsPreEncoder],
+        encoder: AbsEncoder,
+        ignore_id: int = -1,
+        lsm_weight: float = 0.0,
+        length_normalized_loss: bool = False,
+        report_cer: bool = False,
+        report_wer: bool = False,
+        sym_space: str = "<space>",
+        sym_blank: str = "<blank>",
+        pred_masked_weight: float = 1.0,
+        pred_nomask_weight: float = 0.0,
+        loss_weights: float = 0.0,
+    ):
+        assert check_argument_types()
+
+        super().__init__()
+        # note that eos is the same as sos (equivalent ID)
+        self.sos = vocab_size - 1
+        self.eos = vocab_size - 1
+        self.vocab_size = vocab_size
+        self.ignore_id = ignore_id
+        self.token_list = token_list.copy()
+
+        self.frontend = frontend
+        self.specaug = specaug
+        self.normalize = normalize
+        self.preencoder = preencoder
+        self.encoder = encoder
+        self.criterion_hubert = HubertPretrainLoss(
+            pred_masked_weight,
+            pred_nomask_weight,
+            loss_weights,
+        )
+        self.pred_masked_weight = pred_masked_weight
+        self.pred_nomask_weight = pred_nomask_weight
+        self.loss_weights = loss_weights
+
+        if report_cer or report_wer:
+            self.error_calculator = ErrorCalculator(
+                token_list, sym_space, sym_blank, report_cer, report_wer
             )
         else:
-            sad_mr, sad_fr, mi, fa, cf, acc, der = 0, 0, 0, 0, 0, 0, 0
+            self.error_calculator = None
+
+    def forward(
+        self,
+        speech: torch.Tensor,
+        speech_lengths: torch.Tensor,
+        text: torch.Tensor,
+        text_lengths: torch.Tensor,
+        **kwargs,
+    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor]:
+        """Frontend + Encoder + Calc loss
+
+        Args:
+            speech: (Batch, Length, ...)
+            speech_lengths: (Batch, )
+            text: (Batch, Length)
+            text_lengths: (Batch,)
+            kwargs: "utt_id" is among the input.
+        """
+        assert text_lengths.dim() == 1, text_lengths.shape
+        # Check that batch_size is unified
+        assert (
+            speech.shape[0]
+            == speech_lengths.shape[0]
+            == text.shape[0]
+            == text_lengths.shape[0]
+        ), (speech.shape, speech_lengths.shape, text.shape, text_lengths.shape)
+        batch_size = speech.shape[0]
+
+        # for data-parallel
+        text = text[:, : text_lengths.max()]
+
+        # 1. Encoder
+        encoder_out = self.encode(speech, speech_lengths, text, text_lengths)
+
+        # 2a. Hubert criterion
+        loss, acc_mask, acc_unmask = self._calc_hubert_loss(
+            encoder_out,
+        )
 
         stats = dict(
             loss=loss.detach(),
-            loss_att=loss_att.detach() if loss_att is not None else None,
-            loss_pit=loss_pit.detach() if loss_pit is not None else None,
-            sad_mr=sad_mr,
-            sad_fr=sad_fr,
-            mi=mi,
-            fa=fa,
-            cf=cf,
-            acc=acc,
-            der=der,
+            acc_mask=acc_mask,
+            acc_unmask=acc_unmask,
+            acc=acc_mask,
         )
 
+        # force_gatherable: to-device and to-tensor if scalar for DataParallel
         loss, stats, weight = force_gatherable((loss, stats, batch_size), loss.device)
         return loss, stats, weight
 
     def collect_feats(
         self,
         speech: torch.Tensor,
         speech_lengths: torch.Tensor,
-        spk_labels: torch.Tensor = None,
-        spk_labels_lengths: torch.Tensor = None,
+        text: torch.Tensor,
+        text_lengths: torch.Tensor,
         **kwargs,
     ) -> Dict[str, torch.Tensor]:
         feats, feats_lengths = self._extract_feats(speech, speech_lengths)
         return {"feats": feats, "feats_lengths": feats_lengths}
 
     def encode(
         self,
         speech: torch.Tensor,
         speech_lengths: torch.Tensor,
-        bottleneck_feats: torch.Tensor,
-        bottleneck_feats_lengths: torch.Tensor,
+        y_pad: torch.Tensor,
+        y_pad_length: torch.Tensor,
     ) -> Tuple[torch.Tensor, torch.Tensor]:
-        """Frontend + Encoder
+        """Frontend + Encoder. Note that this method is used by asr_inference.py
 
         Args:
             speech: (Batch, Length, ...)
-            speech_lengths: (Batch,)
-            bottleneck_feats: (Batch, Length, ...): used for enh + diar
+            speech_lengths: (Batch, )
+            y_pad: (Batch, Length, ...)
+            y_pad_length: (Batch, )
         """
         with autocast(False):
             # 1. Extract feats
             feats, feats_lengths = self._extract_feats(speech, speech_lengths)
 
             # 2. Data augmentation
             if self.specaug is not None and self.training:
                 feats, feats_lengths = self.specaug(feats, feats_lengths)
 
             # 3. Normalization for feature: e.g. Global-CMVN, Utterance-CMVN
             if self.normalize is not None:
                 feats, feats_lengths = self.normalize(feats, feats_lengths)
 
-            # 4. Forward encoder
-            # feats: (Batch, Length, Dim)
-            # -> encoder_out: (Batch, Length2, Dim)
-            if bottleneck_feats is None:
-                encoder_out, encoder_out_lens, _ = self.encoder(feats, feats_lengths)
-            elif self.frontend is None:
-                # use only bottleneck feature
-                encoder_out, encoder_out_lens, _ = self.encoder(
-                    bottleneck_feats, bottleneck_feats_lengths
-                )
-            else:
-                # use both frontend and bottleneck feats
-                # interpolate (copy) feats frames
-                # to match the length with bottleneck_feats
-                feats = F.interpolate(
-                    feats.transpose(1, 2), size=bottleneck_feats.shape[1]
-                ).transpose(1, 2)
-                # concatenate frontend LMF feature and bottleneck feature
-                encoder_out, encoder_out_lens, _ = self.encoder(
-                    torch.cat((bottleneck_feats, feats), 2), bottleneck_feats_lengths
-                )
-
-        assert encoder_out.size(0) == speech.size(0), (
-            encoder_out.size(),
-            speech.size(0),
-        )
-        assert encoder_out.size(1) <= encoder_out_lens.max(), (
-            encoder_out.size(),
-            encoder_out_lens.max(),
-        )
+        # Pre-encoder, e.g. used for raw input data
+        if self.preencoder is not None:
+            feats, feats_lengths = self.preencoder(feats, feats_lengths)
+
+        # 4. Forward encoder
+        # feats: (Batch, Length, Dim)
+        # -> encoder_out: (Batch, Length2, Dim2)
+        encoder_out = self.encoder(feats, feats_lengths, y_pad, y_pad_length)
+
+        if hasattr(self.encoder, "encoder"):
+            logp_m_list = self.encoder.encoder.get_logits(encoder_out, True)
+            assert self.pred_masked_weight == 0 or len(logp_m_list) > 0
+
+            logp_u_list = self.encoder.encoder.get_logits(encoder_out, False)
+            assert self.pred_nomask_weight == 0 or len(logp_u_list) > 0
 
-        return encoder_out, encoder_out_lens
+        return encoder_out
 
     def _extract_feats(
         self, speech: torch.Tensor, speech_lengths: torch.Tensor
     ) -> Tuple[torch.Tensor, torch.Tensor]:
-        batch_size = speech.shape[0]
-        speech_lengths = (
-            speech_lengths
-            if speech_lengths is not None
-            else torch.ones(batch_size).int() * speech.shape[1]
-        )
-
         assert speech_lengths.dim() == 1, speech_lengths.shape
 
         # for data-parallel
         speech = speech[:, : speech_lengths.max()]
 
         if self.frontend is not None:
             # Frontend
@@ -285,97 +437,47 @@
             # speech (Batch, NSamples) -> feats: (Batch, NFrames, Dim)
             feats, feats_lengths = self.frontend(speech, speech_lengths)
         else:
             # No frontend and no feature extract
             feats, feats_lengths = speech, speech_lengths
         return feats, feats_lengths
 
-    def pit_loss_single_permute(self, pred, label, length):
-        bce_loss = torch.nn.BCEWithLogitsLoss(reduction="none")
-        mask = self.create_length_mask(length, label.size(1), label.size(2))
-        loss = bce_loss(pred, label)
-        loss = loss * mask
-        loss = torch.sum(torch.mean(loss, dim=2), dim=1)
-        loss = torch.unsqueeze(loss, dim=1)
-        return loss
-
-    def pit_loss(self, pred, label, lengths):
-        # Note (jiatong): Credit to https://github.com/hitachi-speech/EEND
-        num_output = label.size(2)
-        permute_list = [np.array(p) for p in permutations(range(num_output))]
-        loss_list = []
-        for p in permute_list:
-            label_perm = label[:, :, p]
-            loss_perm = self.pit_loss_single_permute(pred, label_perm, lengths)
-            loss_list.append(loss_perm)
-        loss = torch.cat(loss_list, dim=1)
-        min_loss, min_idx = torch.min(loss, dim=1)
-        loss = torch.sum(min_loss) / torch.sum(lengths.float())
-        batch_size = len(min_idx)
-        label_list = []
-        for i in range(batch_size):
-            label_list.append(label[i, :, permute_list[min_idx[i]]].data.cpu().numpy())
-        label_permute = torch.from_numpy(np.array(label_list)).float()
-        return loss, min_idx, permute_list, label_permute
-
-    def create_length_mask(self, length, max_len, num_output):
-        batch_size = len(length)
-        mask = torch.zeros(batch_size, max_len, num_output)
-        for i in range(batch_size):
-            mask[i, : length[i], :] = 1
-        mask = to_device(self, mask)
-        return mask
-
-    def attractor_loss(self, att_prob, label):
-        batch_size = len(label)
-        bce_loss = torch.nn.BCEWithLogitsLoss(reduction="none")
-        # create attractor label [1, 1, ..., 1, 0]
-        # att_label: (Batch, num_spk + 1, 1)
-        att_label = to_device(self, torch.zeros(batch_size, label.size(2) + 1, 1))
-        att_label[:, : label.size(2), :] = 1
-        loss = bce_loss(att_prob, att_label)
-        loss = torch.mean(torch.mean(loss, dim=1))
-        return loss
+    def compute_correct(
+        self,
+        logits,
+    ):
+        if logits.numel() == 0:
+            corr, count = 0, 0
+        else:
+            assert logits.dim() > 1, logits.shape
+            max = logits.argmax(-1) == 0
+            min = logits.argmin(-1) == 0
+            both = max & min
+            corr = max.long().sum().item() - both.long().sum().item()
+            count = max.numel()
+        return corr, count
 
-    @staticmethod
-    def calc_diarization_error(pred, label, length):
-        # Note (jiatong): Credit to https://github.com/hitachi-speech/EEND
-
-        (batch_size, max_len, num_output) = label.size()
-        # mask the padding part
-        mask = np.zeros((batch_size, max_len, num_output))
-        for i in range(batch_size):
-            mask[i, : length[i], :] = 1
-
-        # pred and label have the shape (batch_size, max_len, num_output)
-        label_np = label.data.cpu().numpy().astype(int)
-        pred_np = (pred.data.cpu().numpy() > 0).astype(int)
-        label_np = label_np * mask
-        pred_np = pred_np * mask
-        length = length.data.cpu().numpy()
-
-        # compute speech activity detection error
-        n_ref = np.sum(label_np, axis=2)
-        n_sys = np.sum(pred_np, axis=2)
-        speech_scored = float(np.sum(n_ref > 0))
-        speech_miss = float(np.sum(np.logical_and(n_ref > 0, n_sys == 0)))
-        speech_falarm = float(np.sum(np.logical_and(n_ref == 0, n_sys > 0)))
-
-        # compute speaker diarization error
-        speaker_scored = float(np.sum(n_ref))
-        speaker_miss = float(np.sum(np.maximum(n_ref - n_sys, 0)))
-        speaker_falarm = float(np.sum(np.maximum(n_sys - n_ref, 0)))
-        n_map = np.sum(np.logical_and(label_np == 1, pred_np == 1), axis=2)
-        speaker_error = float(np.sum(np.minimum(n_ref, n_sys) - n_map))
-        correct = float(1.0 * np.sum((label_np == pred_np) * mask) / num_output)
-        num_frames = np.sum(length)
-        return (
-            correct,
-            num_frames,
-            speech_scored,
-            speech_miss,
-            speech_falarm,
-            speaker_scored,
-            speaker_miss,
-            speaker_falarm,
-            speaker_error,
+    def _calc_hubert_loss(
+        self,
+        encoder_out: Dict[str, torch.Tensor],
+    ):
+        # 1. Compute hubert loss
+        loss, logp_m_list, logp_u_list = self.criterion_hubert(
+            self.encoder.encoder, encoder_out
         )
+
+        corr_masked, count_masked = 0, 0
+        corr_unmask, count_unmask = 0, 0
+        with torch.no_grad():
+            for i, logp_m in enumerate(logp_m_list):
+                corr_m, count_m = self.compute_correct(logp_m)
+                corr_masked += corr_m
+                count_masked += count_m
+            for i, logp_u in enumerate(logp_u_list):
+                corr_u, count_u = self.compute_correct(logp_u)
+                corr_unmask += corr_u
+                count_unmask += count_u
+
+        acc_m = corr_masked / (count_masked + 1e-10)
+        acc_u = corr_unmask / (count_unmask + 1e-10)
+
+        return loss, acc_m, acc_u
```

### Comparing `espnet-202304/espnet2/diar/label_processor.py` & `espnet-202308/espnet2/diar/label_processor.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/diar/layers/multi_mask.py` & `espnet-202308/espnet2/diar/layers/multi_mask.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/diar/layers/tcn_nomask.py` & `espnet-202308/espnet2/diar/layers/tcn_nomask.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/diar/separator/tcn_separator_nomask.py` & `espnet-202308/espnet2/diar/separator/tcn_separator_nomask.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/abs_enh.py` & `espnet-202308/espnet2/enh/abs_enh.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/decoder/abs_decoder.py` & `espnet-202308/espnet2/enh/decoder/abs_decoder.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/decoder/conv_decoder.py` & `espnet-202308/espnet2/enh/decoder/conv_decoder.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/decoder/stft_decoder.py` & `espnet-202308/espnet2/enh/decoder/stft_decoder.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/encoder/abs_encoder.py` & `espnet-202308/espnet2/enh/encoder/abs_encoder.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/encoder/conv_encoder.py` & `espnet-202308/espnet2/enh/encoder/conv_encoder.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/encoder/stft_encoder.py` & `espnet-202308/espnet2/enh/encoder/stft_encoder.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/espnet_enh_s2t_model.py` & `espnet-202308/espnet2/enh/espnet_enh_s2t_model.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/espnet_model.py` & `espnet-202308/espnet2/enh/espnet_model.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,12 @@
 """Enhancement model module."""
+import contextlib
 from typing import Dict, List, Optional, OrderedDict, Tuple
 
+import numpy as np
 import torch
 from packaging.version import parse as V
 from typeguard import check_argument_types
 
 from espnet2.diar.layers.abs_mask import AbsMask
 from espnet2.enh.decoder.abs_decoder import AbsDecoder
 from espnet2.enh.encoder.abs_encoder import AbsEncoder
@@ -30,14 +32,15 @@
         separator: AbsSeparator,
         decoder: AbsDecoder,
         mask_module: Optional[AbsMask],
         loss_wrappers: List[AbsLossWrapper],
         stft_consistency: bool = False,
         loss_type: str = "mask_mse",
         mask_type: Optional[str] = None,
+        extract_feats_in_collect_stats: bool = False,
     ):
         assert check_argument_types()
 
         super().__init__()
 
         self.encoder = encoder
         self.separator = separator
@@ -64,14 +67,18 @@
         self.stft_consistency = stft_consistency
 
         # for multi-channel signal
         self.ref_channel = getattr(self.separator, "ref_channel", None)
         if self.ref_channel is None:
             self.ref_channel = 0
 
+        # Used in espnet2/tasks/abs_task.py for determining whether or not to do
+        # collect_feats during collect stats (stage 5).
+        self.extract_feats_in_collect_stats = extract_feats_in_collect_stats
+
     def forward(
         self,
         speech_mix: torch.Tensor,
         speech_mix_lengths: torch.Tensor = None,
         **kwargs,
     ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor]:
         """Frontend + Encoder + Decoder + Calc loss
@@ -279,21 +286,23 @@
                 ]
                 if len(signal_pre) == 0:
                     signal_pre = None
             else:
                 signal_ref = speech_ref
                 signal_pre = speech_pre
 
+            zero_weight = loss_wrapper.weight == 0.0
             if isinstance(criterion, TimeDomainLoss):
                 assert signal_pre is not None
                 sref, spre = self._align_ref_pre_channels(
                     signal_ref, signal_pre, ch_dim=2, force_1ch=True
                 )
                 # for the time domain criterions
-                l, s, o = loss_wrapper(sref, spre, {**others, **o})
+                with torch.no_grad() if zero_weight else contextlib.ExitStack():
+                    l, s, o = loss_wrapper(sref, spre, {**others, **o})
             elif isinstance(criterion, FrequencyDomainLoss):
                 sref, spre = self._align_ref_pre_channels(
                     signal_ref, signal_pre, ch_dim=2, force_1ch=False
                 )
                 # for the time-frequency domain criterions
                 if criterion.compute_on_mask:
                     # compute loss on masks
@@ -337,15 +346,16 @@
                         tf_pre = [
                             [self.encoder(sp, speech_lengths)[0] for sp in ps]
                             for ps in spre
                         ]
                     else:
                         tf_pre = [self.encoder(sp, speech_lengths)[0] for sp in spre]
 
-                l, s, o = loss_wrapper(tf_ref, tf_pre, {**others, **o})
+                with torch.no_grad() if zero_weight else contextlib.ExitStack():
+                    l, s, o = loss_wrapper(tf_ref, tf_pre, {**others, **o})
             else:
                 raise NotImplementedError("Unsupported loss type: %s" % str(criterion))
 
             loss += l * loss_wrapper.weight
             stats.update(s)
 
             if perm is None and "perm" in o:
```

### Comparing `espnet-202304/espnet2/enh/espnet_model_tse.py` & `espnet-202308/espnet2/enh/espnet_model_tse.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Enhancement model module."""
+import contextlib
 from typing import Dict, List, OrderedDict, Tuple
 
 import torch
 from typeguard import check_argument_types
 
 from espnet2.enh.decoder.abs_decoder import AbsDecoder
 from espnet2.enh.encoder.abs_encoder import AbsEncoder
@@ -23,14 +24,15 @@
         self,
         encoder: AbsEncoder,
         extractor: AbsExtractor,
         decoder: AbsDecoder,
         loss_wrappers: List[AbsLossWrapper],
         num_spk: int = 1,
         share_encoder: bool = True,
+        extract_feats_in_collect_stats: bool = False,
     ):
         assert check_argument_types()
 
         super().__init__()
 
         self.encoder = encoder
         self.extractor = extractor
@@ -48,14 +50,18 @@
                 raise ValueError("is_noise_loss=True is not supported")
             elif getattr(w.criterion, "is_dereverb_loss", False):
                 raise ValueError("is_dereverb_loss=True is not supported")
 
         # for multi-channel signal
         self.ref_channel = getattr(self.extractor, "ref_channel", -1)
 
+        # Used in espnet2/tasks/abs_task.py for determining whether or not to do
+        # collect_feats during collect stats (stage 5).
+        self.extract_feats_in_collect_stats = extract_feats_in_collect_stats
+
     def forward(
         self,
         speech_mix: torch.Tensor,
         speech_mix_lengths: torch.Tensor = None,
         **kwargs,
     ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor]:
         """Frontend + Encoder + Decoder + Calc loss
@@ -205,21 +211,23 @@
         o = {}
         perm = None
         for loss_wrapper in self.loss_wrappers:
             criterion = loss_wrapper.criterion
             if getattr(criterion, "only_for_test", False) and self.training:
                 continue
 
+            zero_weight = loss_wrapper.weight == 0.0
             if isinstance(criterion, TimeDomainLoss):
                 assert speech_pre is not None
                 sref, spre = self._align_ref_pre_channels(
                     speech_ref, speech_pre, ch_dim=2, force_1ch=True
                 )
                 # for the time domain criterions
-                l, s, o = loss_wrapper(sref, spre, {**others, **o})
+                with torch.no_grad() if zero_weight else contextlib.ExitStack():
+                    l, s, o = loss_wrapper(sref, spre, {**others, **o})
             elif isinstance(criterion, FrequencyDomainLoss):
                 sref, spre = self._align_ref_pre_channels(
                     speech_ref, speech_pre, ch_dim=2, force_1ch=False
                 )
                 # for the time-frequency domain criterions
                 if criterion.compute_on_mask:
                     # compute loss on masks
@@ -233,15 +241,16 @@
                         others,
                     )
                 else:
                     # compute on spectrum
                     tf_ref = [self.encoder(sr, speech_lengths)[0] for sr in sref]
                     tf_pre = [self.encoder(sp, speech_lengths)[0] for sp in spre]
 
-                l, s, o = loss_wrapper(tf_ref, tf_pre, {**others, **o})
+                with torch.no_grad() if zero_weight else contextlib.ExitStack():
+                    l, s, o = loss_wrapper(tf_ref, tf_pre, {**others, **o})
             else:
                 raise NotImplementedError("Unsupported loss type: %s" % str(criterion))
 
             loss += l * loss_wrapper.weight
             stats.update(s)
 
             if perm is None and "perm" in o:
```

### Comparing `espnet-202304/espnet2/enh/extractor/td_speakerbeam_extractor.py` & `espnet-202308/espnet2/enh/extractor/td_speakerbeam_extractor.py`

 * *Files 19% similar despite different names*

```diff
@@ -18,76 +18,98 @@
         stack: int = 3,
         bottleneck_dim: int = 128,
         hidden_dim: int = 512,
         skip_dim: int = 128,
         kernel: int = 3,
         causal: bool = False,
         norm_type: str = "gLN",
+        pre_nonlinear: str = "prelu",
         nonlinear: str = "relu",
         # enrollment related arguments
         i_adapt_layer: int = 7,
         adapt_layer_type: str = "mul",
         adapt_enroll_dim: int = 128,
+        use_spk_emb: bool = False,
+        spk_emb_dim: int = 256,
     ):
         """Time-Domain SpeakerBeam Extractor.
 
         Args:
             input_dim: input feature dimension
             layer: int, number of layers in each stack
             stack: int, number of stacks
             bottleneck_dim: bottleneck dimension
             hidden_dim: number of convolution channel
             skip_dim: int, number of skip connection channels
             kernel: int, kernel size.
             causal: bool, defalut False.
             norm_type: str, choose from 'BN', 'gLN', 'cLN'
+            pre_nonlinear: the nonlinear function right before mask estimation
+                       select from 'prelu', 'relu', 'tanh', 'sigmoid', 'linear'
             nonlinear: the nonlinear function for mask estimation,
-                       select from 'relu', 'tanh', 'sigmoid'
+                       select from 'relu', 'tanh', 'sigmoid', 'linear'
             i_adapt_layer: int, index of adaptation layer
             adapt_layer_type: str, type of adaptation layer
                 see espnet2.enh.layers.adapt_layers for options
             adapt_enroll_dim: int, dimensionality of the speaker embedding
+            use_spk_emb: bool, whether to use speaker embeddings as enrollment
+            spk_emb_dim: int, dimension of input speaker embeddings
+                         only used when `use_spk_emb` is True
         """
         super().__init__()
 
+        if pre_nonlinear not in ("sigmoid", "prelu", "relu", "tanh", "linear"):
+            raise ValueError("Not supporting pre_nonlinear={}".format(pre_nonlinear))
         if nonlinear not in ("sigmoid", "relu", "tanh", "linear"):
             raise ValueError("Not supporting nonlinear={}".format(nonlinear))
 
         self.tcn = TemporalConvNetInformed(
             N=input_dim,
             B=bottleneck_dim,
             H=hidden_dim,
             P=kernel,
             X=layer,
             R=stack,
             Sc=skip_dim,
             out_channel=None,
             norm_type=norm_type,
             causal=causal,
+            pre_mask_nonlinear=pre_nonlinear,
             mask_nonlinear=nonlinear,
             i_adapt_layer=i_adapt_layer,
             adapt_layer_type=adapt_layer_type,
             adapt_enroll_dim=adapt_enroll_dim,
         )
 
         # Auxiliary network
-        self.auxiliary_net = TemporalConvNet(
-            N=input_dim,
-            B=bottleneck_dim,
-            H=hidden_dim,
-            P=kernel,
-            X=layer,
-            R=1,
-            C=1,
-            Sc=skip_dim,
-            out_channel=adapt_enroll_dim if skip_dim is None else adapt_enroll_dim * 2,
-            norm_type=norm_type,
-            causal=False,
-            mask_nonlinear="linear",
-        )
+        self.use_spk_emb = use_spk_emb
+        if use_spk_emb:
+            self.auxiliary_net = torch.nn.Conv1d(
+                spk_emb_dim,
+                adapt_enroll_dim if skip_dim is None else adapt_enroll_dim * 2,
+                1,
+            )
+        else:
+            self.auxiliary_net = TemporalConvNet(
+                N=input_dim,
+                B=bottleneck_dim,
+                H=hidden_dim,
+                P=kernel,
+                X=layer,
+                R=1,
+                C=1,
+                Sc=skip_dim,
+                out_channel=adapt_enroll_dim
+                if skip_dim is None
+                else adapt_enroll_dim * 2,
+                norm_type=norm_type,
+                causal=False,
+                pre_mask_nonlinear=pre_nonlinear,
+                mask_nonlinear="linear",
+            )
 
     def forward(
         self,
         input: Union[torch.Tensor, ComplexTensor],
         ilens: torch.Tensor,
         input_aux: torch.Tensor,
         ilens_aux: torch.Tensor,
@@ -95,15 +117,15 @@
     ) -> Tuple[List[Union[torch.Tensor, ComplexTensor]], torch.Tensor, OrderedDict]:
         """TD-SpeakerBeam Forward.
 
         Args:
             input (torch.Tensor or ComplexTensor): Encoded feature [B, T, N]
             ilens (torch.Tensor): input lengths [Batch]
             input_aux (torch.Tensor or ComplexTensor): Encoded auxiliary feature
-                for the target speaker [B, T, N]
+                for the target speaker [B, T, N] or [B, N]
             ilens_aux (torch.Tensor): input lengths of auxiliary input for the
                 target speaker [Batch]
             suffix_tag (str): suffix to append to the keys in `others`
 
         Returns:
             masked (List[Union(torch.Tensor, ComplexTensor)]): [(B, T, N), ...]
             ilens (torch.Tensor): (B,)
@@ -114,18 +136,29 @@
         """  # noqa: E501
         # if complex spectrum
         feature = abs(input) if is_complex(input) else input
         aux_feature = abs(input_aux) if is_complex(input_aux) else input_aux
         B, L, N = feature.shape
 
         feature = feature.transpose(1, 2)  # B, N, L
-        aux_feature = aux_feature.transpose(1, 2)  # B, N, L'
+        # NOTE(wangyou): When `self.use_spk_emb` is True, `aux_feature` is assumed to be
+        # a speaker embedding; otherwise, it is assumed to be an enrollment audio.
+        if self.use_spk_emb:
+            # B, N, L'=1
+            if aux_feature.dim() == 2:
+                aux_feature = aux_feature.unsqueeze(-1)
+            elif aux_feature.size(-2) == 1:
+                assert aux_feature.dim() == 3, aux_feature.shape
+                aux_feature = aux_feature.transpose(1, 2)
+        else:
+            aux_feature = aux_feature.transpose(1, 2)  # B, N, L'
 
         enroll_emb = self.auxiliary_net(aux_feature).squeeze(1)  # B, N', L'
-        enroll_emb.masked_fill_(make_pad_mask(ilens_aux, enroll_emb, -1), 0.0)
+        if not self.use_spk_emb:
+            enroll_emb.masked_fill_(make_pad_mask(ilens_aux, enroll_emb, -1), 0.0)
         enroll_emb = enroll_emb.mean(dim=-1)  # B, N'
 
         mask = self.tcn(feature, enroll_emb)  # B, N, L
         mask = mask.transpose(-1, -2)  # B, L, N
 
         masked = input * mask
```

### Comparing `espnet-202304/espnet2/enh/layers/beamformer.py` & `espnet-202308/espnet2/enh/layers/beamformer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/layers/beamformer_th.py` & `espnet-202308/espnet2/enh/layers/beamformer_th.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/layers/complex_utils.py` & `espnet-202308/espnet2/enh/layers/complex_utils.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/layers/complexnn.py` & `espnet-202308/espnet2/enh/layers/complexnn.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/layers/conv_utils.py` & `espnet-202308/espnet2/enh/layers/conv_utils.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/layers/dc_crn.py` & `espnet-202308/espnet2/enh/layers/dc_crn.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/layers/dnn_beamformer.py` & `espnet-202308/espnet2/enh/layers/dnn_beamformer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/layers/dnn_wpe.py` & `espnet-202308/espnet2/enh/layers/dnn_wpe.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/layers/dpmulcat.py` & `espnet-202308/espnet2/enh/layers/dpmulcat.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/layers/dprnn.py` & `espnet-202308/espnet2/enh/layers/dprnn.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/layers/dptnet.py` & `espnet-202308/espnet2/enh/layers/dptnet.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/layers/fasnet.py` & `espnet-202308/espnet2/enh/layers/fasnet.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/layers/ifasnet.py` & `espnet-202308/espnet2/enh/layers/ifasnet.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/layers/mask_estimator.py` & `espnet-202308/espnet2/enh/layers/mask_estimator.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/layers/skim.py` & `espnet-202308/espnet2/enh/layers/skim.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/layers/tcn.py` & `espnet-202308/espnet2/enh/layers/tcn.py`

 * *Files 8% similar despite different names*

```diff
@@ -27,14 +27,15 @@
         X,
         R,
         C,
         Sc=None,
         out_channel=None,
         norm_type="gLN",
         causal=False,
+        pre_mask_nonlinear="linear",
         mask_nonlinear="relu",
     ):
         """Basic Module of tasnet.
 
         Args:
             N: Number of filters in autoencoder
             B: Number of channels in bottleneck 1 * 1-conv block
@@ -44,14 +45,15 @@
             R: Number of repeats
             C: Number of speakers
             Sc: Number of channels in skip-connection paths' 1x1-conv blocks
             out_channel: Number of output channels
                 if it is None, `N` will be used instead.
             norm_type: BN, gLN, cLN
             causal: causal or non-causal
+            pre_mask_nonlinear: the non-linear function before masknet
             mask_nonlinear: use which non-linear function to generate mask
         """
         super().__init__()
         # Hyper-parameter
         self.C = C
         self.mask_nonlinear = mask_nonlinear
         self.skip_connection = Sc is not None
@@ -90,31 +92,42 @@
                     )
                 ]
             repeats += [nn.Sequential(*blocks)]
         temporal_conv_net = nn.Sequential(*repeats)
         # [M, B, K] -> [M, C*N, K]
         mask_conv1x1 = nn.Conv1d(B, C * self.out_channel, 1, bias=False)
         # Put together (for compatibility with older versions)
-        self.network = nn.Sequential(
-            layer_norm, bottleneck_conv1x1, temporal_conv_net, mask_conv1x1
-        )
+        if pre_mask_nonlinear == "linear":
+            self.network = nn.Sequential(
+                layer_norm, bottleneck_conv1x1, temporal_conv_net, mask_conv1x1
+            )
+        else:
+            activ = {
+                "prelu": nn.PReLU(),
+                "relu": nn.ReLU(),
+                "tanh": nn.Tanh(),
+                "sigmoid": nn.Sigmoid(),
+            }[pre_mask_nonlinear]
+            self.network = nn.Sequential(
+                layer_norm, bottleneck_conv1x1, temporal_conv_net, activ, mask_conv1x1
+            )
 
     def forward(self, mixture_w):
         """Keep this API same with TasNet.
 
         Args:
             mixture_w: [M, N, K], M is batch size
 
         Returns:
             est_mask: [M, C, N, K]
         """
         M, N, K = mixture_w.size()
         bottleneck = self.network[:2]
         tcns = self.network[2]
-        masknet = self.network[3]
+        masknet = self.network[3:]
         output = bottleneck(mixture_w)
         skip_conn = 0.0
         for block in tcns:
             for layer in block:
                 tcn_out = layer(output)
                 if self.skip_connection:
                     residual, skip = tcn_out
@@ -154,14 +167,15 @@
         P,
         X,
         R,
         Sc=None,
         out_channel=None,
         norm_type="gLN",
         causal=False,
+        pre_mask_nonlinear="prelu",
         mask_nonlinear="relu",
         i_adapt_layer: int = 7,
         adapt_layer_type: str = "mul",
         adapt_enroll_dim: int = 128,
         **adapt_layer_kwargs
     ):
         """Basic Module of TasNet with adaptation layers.
@@ -174,14 +188,15 @@
             X: Number of convolutional blocks in each repeat
             R: Number of repeats
             Sc: Number of channels in skip-connection paths' 1x1-conv blocks
             out_channel: Number of output channels
                 if it is None, `N` will be used instead.
             norm_type: BN, gLN, cLN
             causal: causal or non-causal
+            pre_mask_nonlinear: the non-linear function before masknet
             mask_nonlinear: use which non-linear function to generate mask
             i_adapt_layer: int, index of the adaptation layer
             adapt_layer_type: str, type of adaptation layer
                 see espnet2.enh.layers.adapt_layers for options
             adapt_enroll_dim: int, dimensionality of the speaker embedding
         """
         super().__init__(
@@ -192,14 +207,15 @@
             X,
             R,
             1,
             Sc=Sc,
             out_channel=out_channel,
             norm_type=norm_type,
             causal=causal,
+            pre_mask_nonlinear=pre_mask_nonlinear,
             mask_nonlinear=mask_nonlinear,
         )
         self.i_adapt_layer = i_adapt_layer
         self.adapt_enroll_dim = adapt_enroll_dim
         self.adapt_layer_type = adapt_layer_type
         self.adapt_layer = make_adapt_layer(
             adapt_layer_type,
@@ -220,15 +236,15 @@
         Returns:
             est_mask: [M, N, K]
         """
         M, N, K = mixture_w.size()
 
         bottleneck = self.network[:2]
         tcns = self.network[2]
-        masknet = self.network[3]
+        masknet = self.network[3:]
         output = bottleneck(mixture_w)
         skip_conn = 0.0
         for i, block in enumerate(tcns):
             for j, layer in enumerate(block):
                 idx = i * len(block) + j
                 is_adapt_layer = idx == self.i_adapt_layer
                 tcn_out = layer(output)
```

### Comparing `espnet-202304/espnet2/enh/layers/tcndenseunet.py` & `espnet-202308/espnet2/enh/layers/tcndenseunet.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/layers/wpe.py` & `espnet-202308/espnet2/enh/layers/wpe.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/loss/criterions/abs_loss.py` & `espnet-202308/espnet2/enh/loss/criterions/abs_loss.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/loss/criterions/tf_domain.py` & `espnet-202308/espnet2/enh/loss/criterions/tf_domain.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/loss/criterions/time_domain.py` & `espnet-202308/espnet2/enh/loss/criterions/time_domain.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/loss/wrappers/abs_wrapper.py` & `espnet-202308/espnet2/enh/loss/wrappers/abs_wrapper.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/loss/wrappers/dpcl_solver.py` & `espnet-202308/espnet2/enh/loss/wrappers/dpcl_solver.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/loss/wrappers/fixed_order.py` & `espnet-202308/espnet2/enh/loss/wrappers/fixed_order.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/loss/wrappers/mixit_solver.py` & `espnet-202308/espnet2/enh/loss/wrappers/mixit_solver.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/loss/wrappers/multilayer_pit_solver.py` & `espnet-202308/espnet2/enh/loss/wrappers/multilayer_pit_solver.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/loss/wrappers/pit_solver.py` & `espnet-202308/espnet2/enh/loss/wrappers/pit_solver.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/separator/abs_separator.py` & `espnet-202308/espnet2/enh/separator/abs_separator.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/separator/asteroid_models.py` & `espnet-202308/espnet2/enh/separator/asteroid_models.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/separator/conformer_separator.py` & `espnet-202308/espnet2/enh/separator/conformer_separator.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/separator/dan_separator.py` & `espnet-202308/espnet2/enh/separator/dan_separator.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/separator/dc_crn_separator.py` & `espnet-202308/espnet2/enh/separator/dc_crn_separator.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/separator/dccrn_separator.py` & `espnet-202308/espnet2/enh/separator/dccrn_separator.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/separator/dpcl_e2e_separator.py` & `espnet-202308/espnet2/enh/separator/dpcl_e2e_separator.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/separator/dpcl_separator.py` & `espnet-202308/espnet2/enh/separator/dpcl_separator.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/separator/dprnn_separator.py` & `espnet-202308/espnet2/enh/separator/dprnn_separator.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/separator/dptnet_separator.py` & `espnet-202308/espnet2/enh/separator/dptnet_separator.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/separator/fasnet_separator.py` & `espnet-202308/espnet2/enh/separator/fasnet_separator.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/separator/ineube_separator.py` & `espnet-202308/espnet2/enh/separator/ineube_separator.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/separator/neural_beamformer.py` & `espnet-202308/espnet2/enh/separator/neural_beamformer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/separator/rnn_separator.py` & `espnet-202308/espnet2/enh/separator/skim_separator.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,64 +1,77 @@
 from collections import OrderedDict
 from typing import Dict, List, Optional, Tuple, Union
 
 import torch
-from packaging.version import parse as V
 from torch_complex.tensor import ComplexTensor
 
 from espnet2.enh.layers.complex_utils import is_complex
+from espnet2.enh.layers.skim import SkiM
 from espnet2.enh.separator.abs_separator import AbsSeparator
-from espnet.nets.pytorch_backend.rnn.encoders import RNN
 
-is_torch_1_9_plus = V(torch.__version__) >= V("1.9.0")
 
+class SkiMSeparator(AbsSeparator):
+    """Skipping Memory (SkiM) Separator
+
+    Args:
+        input_dim: input feature dimension
+        causal: bool, whether the system is causal.
+        num_spk: number of target speakers.
+        nonlinear: the nonlinear function for mask estimation,
+            select from 'relu', 'tanh', 'sigmoid'
+        layer: int, number of SkiM blocks. Default is 3.
+        unit: int, dimension of the hidden state.
+        segment_size: segmentation size for splitting long features
+        dropout: float, dropout ratio. Default is 0.
+        mem_type: 'hc', 'h', 'c', 'id' or None.
+            It controls whether the hidden (or cell) state of
+            SegLSTM will be processed by MemLSTM.
+            In 'id' mode, both the hidden and cell states
+            will be identically returned.
+            When mem_type is None, the MemLSTM will be removed.
+        seg_overlap: Bool, whether the segmentation will reserve 50%
+            overlap for adjacent segments. Default is False.
+    """
 
-class RNNSeparator(AbsSeparator):
     def __init__(
         self,
         input_dim: int,
-        rnn_type: str = "blstm",
+        causal: bool = True,
         num_spk: int = 2,
         predict_noise: bool = False,
-        nonlinear: str = "sigmoid",
+        nonlinear: str = "relu",
         layer: int = 3,
         unit: int = 512,
+        segment_size: int = 20,
         dropout: float = 0.0,
+        mem_type: str = "hc",
+        seg_overlap: bool = False,
     ):
-        """RNN Separator
-
-        Args:
-            input_dim: input feature dimension
-            rnn_type: string, select from 'blstm', 'lstm' etc.
-            bidirectional: bool, whether the inter-chunk RNN layers are bidirectional.
-            num_spk: number of speakers
-            predict_noise: whether to output the estimated noise signal
-            nonlinear: the nonlinear function for mask estimation,
-                       select from 'relu', 'tanh', 'sigmoid'
-            layer: int, number of stacked RNN layers. Default is 3.
-            unit: int, dimension of the hidden state.
-            dropout: float, dropout ratio. Default is 0.
-        """
         super().__init__()
 
         self._num_spk = num_spk
         self.predict_noise = predict_noise
 
-        self.rnn = RNN(
-            idim=input_dim,
-            elayers=layer,
-            cdim=unit,
-            hdim=unit,
-            dropout=dropout,
-            typ=rnn_type,
-        )
+        self.segment_size = segment_size
 
-        num_outputs = self.num_spk + 1 if self.predict_noise else self.num_spk
-        self.linear = torch.nn.ModuleList(
-            [torch.nn.Linear(unit, input_dim) for _ in range(num_outputs)]
+        if mem_type not in ("hc", "h", "c", "id", None):
+            raise ValueError("Not supporting mem_type={}".format(mem_type))
+
+        self.num_outputs = self.num_spk + 1 if self.predict_noise else self.num_spk
+        self.skim = SkiM(
+            input_size=input_dim,
+            hidden_size=unit,
+            output_size=input_dim * self.num_outputs,
+            dropout=dropout,
+            num_blocks=layer,
+            bidirectional=(not causal),
+            norm_type="cLN" if causal else "gLN",
+            segment_size=segment_size,
+            seg_overlap=seg_overlap,
+            mem_type=mem_type,
         )
 
         if nonlinear not in ("sigmoid", "relu", "tanh"):
             raise ValueError("Not supporting nonlinear={}".format(nonlinear))
 
         self.nonlinear = {
             "sigmoid": torch.nn.Sigmoid(),
@@ -93,65 +106,54 @@
 
         # if complex spectrum,
         if is_complex(input):
             feature = abs(input)
         else:
             feature = input
 
-        x, ilens, _ = self.rnn(feature, ilens)
-
-        masks = []
+        B, T, N = feature.shape
 
-        for linear in self.linear:
-            y = linear(x)
-            y = self.nonlinear(y)
-            masks.append(y)
+        processed = self.skim(feature)  # B,T, N
 
+        processed = processed.view(B, T, N, self.num_outputs)
+        masks = self.nonlinear(processed).unbind(dim=3)
         if self.predict_noise:
             *masks, mask_noise = masks
 
         masked = [input * m for m in masks]
 
         others = OrderedDict(
             zip(["mask_spk{}".format(i + 1) for i in range(len(masks))], masks)
         )
         if self.predict_noise:
             others["noise1"] = input * mask_noise
 
         return masked, ilens, others
 
-    @property
-    def num_spk(self):
-        return self._num_spk
-
     def forward_streaming(self, input_frame: torch.Tensor, states=None):
-        # input_frame # B, 1, N
-
-        # if complex spectrum,
         if is_complex(input_frame):
             feature = abs(input_frame)
         else:
             feature = input_frame
 
-        ilens = torch.ones(feature.shape[0], device=feature.device)
-
-        x, _, states = self.rnn(feature, ilens, states)
-
-        masks = []
+        B, _, N = feature.shape
 
-        for linear in self.linear:
-            y = linear(x)
-            y = self.nonlinear(y)
-            masks.append(y)
+        processed, states = self.skim.forward_stream(feature, states=states)
 
+        processed = processed.view(B, 1, N, self.num_outputs)
+        masks = self.nonlinear(processed).unbind(dim=3)
         if self.predict_noise:
             *masks, mask_noise = masks
 
         masked = [input_frame * m for m in masks]
 
         others = OrderedDict(
             zip(["mask_spk{}".format(i + 1) for i in range(len(masks))], masks)
         )
         if self.predict_noise:
-            others["noise1"] = input * mask_noise
+            others["noise1"] = input_frame * mask_noise
 
         return masked, states, others
+
+    @property
+    def num_spk(self):
+        return self._num_spk
```

### Comparing `espnet-202304/espnet2/enh/separator/skim_separator.py` & `espnet-202308/espnet2/lm/seq_rnn_lm.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,159 +1,173 @@
-from collections import OrderedDict
-from typing import Dict, List, Optional, Tuple, Union
+"""Sequential implementation of Recurrent Neural Network Language Model."""
+from typing import Tuple, Union
 
 import torch
-from torch_complex.tensor import ComplexTensor
+import torch.nn as nn
+from typeguard import check_argument_types
+
+from espnet2.lm.abs_model import AbsLM
+
+
+class SequentialRNNLM(AbsLM):
+    """Sequential RNNLM.
+
+    See also:
+        https://github.com/pytorch/examples/blob/4581968193699de14b56527296262dd76ab43557/word_language_model/model.py
 
-from espnet2.enh.layers.complex_utils import is_complex
-from espnet2.enh.layers.skim import SkiM
-from espnet2.enh.separator.abs_separator import AbsSeparator
-
-
-class SkiMSeparator(AbsSeparator):
-    """Skipping Memory (SkiM) Separator
-
-    Args:
-        input_dim: input feature dimension
-        causal: bool, whether the system is causal.
-        num_spk: number of target speakers.
-        nonlinear: the nonlinear function for mask estimation,
-            select from 'relu', 'tanh', 'sigmoid'
-        layer: int, number of SkiM blocks. Default is 3.
-        unit: int, dimension of the hidden state.
-        segment_size: segmentation size for splitting long features
-        dropout: float, dropout ratio. Default is 0.
-        mem_type: 'hc', 'h', 'c', 'id' or None.
-            It controls whether the hidden (or cell) state of
-            SegLSTM will be processed by MemLSTM.
-            In 'id' mode, both the hidden and cell states
-            will be identically returned.
-            When mem_type is None, the MemLSTM will be removed.
-        seg_overlap: Bool, whether the segmentation will reserve 50%
-            overlap for adjacent segments. Default is False.
     """
 
     def __init__(
         self,
-        input_dim: int,
-        causal: bool = True,
-        num_spk: int = 2,
-        predict_noise: bool = False,
-        nonlinear: str = "relu",
-        layer: int = 3,
-        unit: int = 512,
-        segment_size: int = 20,
-        dropout: float = 0.0,
-        mem_type: str = "hc",
-        seg_overlap: bool = False,
+        vocab_size: int,
+        unit: int = 650,
+        nhid: int = None,
+        nlayers: int = 2,
+        dropout_rate: float = 0.0,
+        tie_weights: bool = False,
+        rnn_type: str = "lstm",
+        ignore_id: int = 0,
     ):
+        assert check_argument_types()
         super().__init__()
 
-        self._num_spk = num_spk
-        self.predict_noise = predict_noise
-
-        self.segment_size = segment_size
+        ninp = unit
+        if nhid is None:
+            nhid = unit
+        rnn_type = rnn_type.upper()
+
+        self.drop = nn.Dropout(dropout_rate)
+        self.encoder = nn.Embedding(vocab_size, ninp, padding_idx=ignore_id)
+        if rnn_type in ["LSTM", "GRU"]:
+            rnn_class = getattr(nn, rnn_type)
+            self.rnn = rnn_class(
+                ninp, nhid, nlayers, dropout=dropout_rate, batch_first=True
+            )
+        else:
+            try:
+                nonlinearity = {"RNN_TANH": "tanh", "RNN_RELU": "relu"}[rnn_type]
+            except KeyError:
+                raise ValueError(
+                    """An invalid option for `--model` was supplied,
+                    options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']"""
+                )
+            self.rnn = nn.RNN(
+                ninp,
+                nhid,
+                nlayers,
+                nonlinearity=nonlinearity,
+                dropout=dropout_rate,
+                batch_first=True,
+            )
+        self.decoder = nn.Linear(nhid, vocab_size)
+
+        # Optionally tie weights as in:
+        # "Using the Output Embedding to Improve Language Models"
+        # (Press & Wolf 2016) https://arxiv.org/abs/1608.05859
+        # and
+        # "Tying Word Vectors and Word Classifiers:
+        # A Loss Framework for Language Modeling" (Inan et al. 2016)
+        # https://arxiv.org/abs/1611.01462
+        if tie_weights:
+            if nhid != ninp:
+                raise ValueError(
+                    "When using the tied flag, nhid must be equal to emsize"
+                )
+            self.decoder.weight = self.encoder.weight
+
+        self.rnn_type = rnn_type
+        self.nhid = nhid
+        self.nlayers = nlayers
+
+    def zero_state(self):
+        """Initialize LM state filled with zero values."""
+        if isinstance(self.rnn, torch.nn.LSTM):
+            h = torch.zeros((self.nlayers, self.nhid), dtype=torch.float)
+            c = torch.zeros((self.nlayers, self.nhid), dtype=torch.float)
+            state = h, c
+        else:
+            state = torch.zeros((self.nlayers, self.nhid), dtype=torch.float)
 
-        if mem_type not in ("hc", "h", "c", "id", None):
-            raise ValueError("Not supporting mem_type={}".format(mem_type))
+        return state
 
-        self.num_outputs = self.num_spk + 1 if self.predict_noise else self.num_spk
-        self.skim = SkiM(
-            input_size=input_dim,
-            hidden_size=unit,
-            output_size=input_dim * self.num_outputs,
-            dropout=dropout,
-            num_blocks=layer,
-            bidirectional=(not causal),
-            norm_type="cLN" if causal else "gLN",
-            segment_size=segment_size,
-            seg_overlap=seg_overlap,
-            mem_type=mem_type,
+    def forward(
+        self, input: torch.Tensor, hidden: torch.Tensor
+    ) -> Tuple[torch.Tensor, torch.Tensor]:
+        emb = self.drop(self.encoder(input))
+        output, hidden = self.rnn(emb, hidden)
+        output = self.drop(output)
+        decoded = self.decoder(
+            output.contiguous().view(output.size(0) * output.size(1), output.size(2))
+        )
+        return (
+            decoded.view(output.size(0), output.size(1), decoded.size(1)),
+            hidden,
         )
 
-        if nonlinear not in ("sigmoid", "relu", "tanh"):
-            raise ValueError("Not supporting nonlinear={}".format(nonlinear))
-
-        self.nonlinear = {
-            "sigmoid": torch.nn.Sigmoid(),
-            "relu": torch.nn.ReLU(),
-            "tanh": torch.nn.Tanh(),
-        }[nonlinear]
-
-    def forward(
+    def score(
         self,
-        input: Union[torch.Tensor, ComplexTensor],
-        ilens: torch.Tensor,
-        additional: Optional[Dict] = None,
-    ) -> Tuple[List[Union[torch.Tensor, ComplexTensor]], torch.Tensor, OrderedDict]:
-        """Forward.
+        y: torch.Tensor,
+        state: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]],
+        x: torch.Tensor,
+    ) -> Tuple[torch.Tensor, Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]]:
+        """Score new token.
 
         Args:
-            input (torch.Tensor or ComplexTensor): Encoded feature [B, T, N]
-            ilens (torch.Tensor): input lengths [Batch]
-            additional (Dict or None): other data included in model
-                NOTE: not used in this model
+            y: 1D torch.int64 prefix tokens.
+            state: Scorer state for prefix tokens
+            x: 2D encoder feature that generates ys.
 
         Returns:
-            masked (List[Union(torch.Tensor, ComplexTensor)]): [(B, T, N), ...]
-            ilens (torch.Tensor): (B,)
-            others predicted data, e.g. masks: OrderedDict[
-                'mask_spk1': torch.Tensor(Batch, Frames, Freq),
-                'mask_spk2': torch.Tensor(Batch, Frames, Freq),
-                ...
-                'mask_spkn': torch.Tensor(Batch, Frames, Freq),
-            ]
-        """
-
-        # if complex spectrum,
-        if is_complex(input):
-            feature = abs(input)
-        else:
-            feature = input
+            Tuple of
+                torch.float32 scores for next token (n_vocab)
+                and next state for ys
 
-        B, T, N = feature.shape
-
-        processed = self.skim(feature)  # B,T, N
-
-        processed = processed.view(B, T, N, self.num_outputs)
-        masks = self.nonlinear(processed).unbind(dim=3)
-        if self.predict_noise:
-            *masks, mask_noise = masks
-
-        masked = [input * m for m in masks]
+        """
+        y, new_state = self(y[-1].view(1, 1), state)
+        logp = y.log_softmax(dim=-1).view(-1)
+        return logp, new_state
+
+    def batch_score(
+        self, ys: torch.Tensor, states: torch.Tensor, xs: torch.Tensor
+    ) -> Tuple[torch.Tensor, torch.Tensor]:
+        """Score new token batch.
 
-        others = OrderedDict(
-            zip(["mask_spk{}".format(i + 1) for i in range(len(masks))], masks)
-        )
-        if self.predict_noise:
-            others["noise1"] = input * mask_noise
+        Args:
+            ys (torch.Tensor): torch.int64 prefix tokens (n_batch, ylen).
+            states (List[Any]): Scorer states for prefix tokens.
+            xs (torch.Tensor):
+                The encoder feature that generates ys (n_batch, xlen, n_feat).
 
-        return masked, ilens, others
+        Returns:
+            tuple[torch.Tensor, List[Any]]: Tuple of
+                batchfied scores for next token with shape of `(n_batch, n_vocab)`
+                and next state list for ys.
 
-    def forward_streaming(self, input_frame: torch.Tensor, states=None):
-        if is_complex(input_frame):
-            feature = abs(input_frame)
+        """
+        if states[0] is None:
+            states = None
+        elif isinstance(self.rnn, torch.nn.LSTM):
+            # states: Batch x 2 x (Nlayers, Dim) -> 2 x (Nlayers, Batch, Dim)
+            h = torch.stack([h for h, c in states], dim=1)
+            c = torch.stack([c for h, c in states], dim=1)
+            states = h, c
         else:
-            feature = input_frame
+            # states: Batch x (Nlayers, Dim) -> (Nlayers, Batch, Dim)
+            states = torch.stack(states, dim=1)
 
-        B, _, N = feature.shape
-
-        processed, states = self.skim.forward_stream(feature, states=states)
-
-        processed = processed.view(B, 1, N, self.num_outputs)
-        masks = self.nonlinear(processed).unbind(dim=3)
-        if self.predict_noise:
-            *masks, mask_noise = masks
-
-        masked = [input_frame * m for m in masks]
-
-        others = OrderedDict(
-            zip(["mask_spk{}".format(i + 1) for i in range(len(masks))], masks)
-        )
-        if self.predict_noise:
-            others["noise1"] = input_frame * mask_noise
-
-        return masked, states, others
+        ys, states = self(ys[:, -1:], states)
+        # ys: (Batch, 1, Nvocab) -> (Batch, NVocab)
+        assert ys.size(1) == 1, ys.shape
+        ys = ys.squeeze(1)
+        logp = ys.log_softmax(dim=-1)
+
+        # state: Change to batch first
+        if isinstance(self.rnn, torch.nn.LSTM):
+            # h, c: (Nlayers, Batch, Dim)
+            h, c = states
+            # states: Batch x 2 x (Nlayers, Dim)
+            states = [(h[:, i], c[:, i]) for i in range(h.size(1))]
+        else:
+            # states: (Nlayers, Batch, Dim) -> Batch x (Nlayers, Dim)
+            states = [states[:, i] for i in range(states.size(1))]
 
-    @property
-    def num_spk(self):
-        return self._num_spk
+        return logp, states
```

### Comparing `espnet-202304/espnet2/enh/separator/svoice_separator.py` & `espnet-202308/espnet2/enh/separator/svoice_separator.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/separator/tcn_separator.py` & `espnet-202308/espnet2/enh/separator/tcn_separator.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/separator/tfgridnet_separator.py` & `espnet-202308/espnet2/enh/separator/tfgridnet_separator.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/enh/separator/transformer_separator.py` & `espnet-202308/espnet2/enh/separator/transformer_separator.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/fileio/datadir_writer.py` & `espnet-202308/espnet2/fileio/datadir_writer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/fileio/npy_scp.py` & `espnet-202308/espnet2/fileio/npy_scp.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/fileio/rand_gen_dataset.py` & `espnet-202308/espnet2/fileio/rand_gen_dataset.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/fileio/read_text.py` & `espnet-202308/espnet2/fileio/read_text.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/fileio/rttm.py` & `espnet-202308/espnet2/fileio/rttm.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/fileio/score_scp.py` & `espnet-202308/espnet2/fileio/score_scp.py`

 * *Files 13% similar despite different names*

```diff
@@ -8,14 +8,18 @@
 
 from espnet2.fileio.read_text import read_2columns_text
 
 try:
     import music21 as m21  # for CI import
 except ImportError or ModuleNotFoundError:
     m21 = None
+try:
+    import miditoolkit  # for CI import
+except ImportError or ModuleNotFoundError:
+    miditoolkit = None
 
 
 class NOTE(object):
     def __init__(self, lyric, midi, st, et):
         self.lyric = lyric
         self.midi = midi
         self.st = st
@@ -29,15 +33,15 @@
         key1 /some/path/a.xml
         key2 /some/path/b.xml
         key3 /some/path/c.xml
         key4 /some/path/d.xml
         ...
 
         >>> reader = XMLScpReader('xml.scp')
-        >>> lyrics_array, notes_array, segs_array = reader['key1']
+        >>> tempo, note_list = reader['key1']
     """
 
     def __init__(
         self,
         fname,
         dtype=np.int16,
     ):
@@ -64,31 +68,39 @@
             if not note.isRest:  # Note or Chord
                 lr = note.lyric
                 if note.isChord:
                     for n in note:
                         if n.pitch.midi != prepitch:  # Ignore repeat note
                             note = n
                             break
-                if lr is None or lr == "":  # multi note in one syllable
+                if lr is None or lr == "" or lr == "ー":  # multi note in one syllable
                     if note.pitch.midi == prepitch:  # same pitch
                         notes_list[-1].et += dur
                     else:  # different pitch
                         notes_list.append(NOTE("—", note.pitch.midi, st, st + dur))
                 elif lr == "br":  # <br> is tagged as a note
                     if prepitch == 0:
                         notes_list[-1].et += dur
                     else:
                         notes_list.append(NOTE("P", 0, st, st + dur))
                     prepitch = 0
+                    st += dur
+                    continue
                 else:  # normal note for one syllable
-                    notes_list.append(NOTE(note.lyric, note.pitch.midi, st, st + dur))
+                    notes_list.append(NOTE(lr, note.pitch.midi, st, st + dur))
                 prepitch = note.pitch.midi
-                for arti in note.articulations:  # <br> is tagged as a notation
-                    if arti.name in ["breath mark"]:  # up-bow?
-                        notes_list.append(NOTE("B", 0, st, st))  # , 0))
+                for arti in note.articulations:
+                    # NOTE(Yuning): By default, 'breath mark' appears at the end of
+                    # the sentence. In some situations, 'breath mark' doesn't take
+                    # effect in its belonging note. Please handle them under local/.
+                    if arti.name in ["breath mark"]:  # <br> is tagged as a notation
+                        notes_list.append(NOTE("B", 0, st + dur, st + dur))
+                    # NOTE(Yuning): In some datasets, there is a break when 'staccato'
+                    # occurs. We let users to decide whether to perform segmentation
+                    # under local/.
             else:  # rest note
                 if prepitch == 0:
                     notes_list[-1].et += dur
                 else:
                     notes_list.append(NOTE("P", 0, st, st + dur))
                 prepitch = 0
             st += dur
@@ -155,15 +167,15 @@
         bps = 1.0 * tempo / 60
         offset = 0
         for i in range(len(lyrics_seq)):
             duration = int(8 * (segs_seq[i][1] - segs_seq[i][0]) * bps + 0.5)
             duration = 1.0 * duration / 8
             if duration == 0:
                 duration = 1 / 16
-            if notes_seq[i] != -1:  # isNote
+            if notes_seq[i] != 0:  # isNote
                 n = m21.note.Note(notes_seq[i])
                 if lyrics_seq[i] != "—":
                     n.lyric = lyrics_seq[i]
             else:  # isRest
                 n = m21.note.Rest()
             n.offset = offset
             n.duration = m21.duration.Duration(duration)
@@ -182,14 +194,85 @@
     def __exit__(self, exc_type, exc_val, exc_tb):
         self.close()
 
     def close(self):
         self.fscp.close()
 
 
+class MIDReader(collections.abc.Mapping):
+    """Reader class for 'mid.scp'.
+
+    Examples:
+        key1 /some/path/a.mid
+        key2 /some/path/b.mid
+        key3 /some/path/c.mid
+        key4 /some/path/d.mid
+        ...
+
+        >>> reader = XMLScpReader('mid.scp')
+        >>> tempo, note_list = reader['key1']
+    """
+
+    def __init__(
+        self,
+        fname,
+        add_rest=True,
+        dtype=np.int16,
+    ):
+        assert check_argument_types()
+        assert miditoolkit is not None, (
+            "Cannot load miditoolkit package. ",
+            "Please install Muskit modules via ",
+            "(cd tools && make muskit.done)",
+        )
+        self.fname = fname
+        self.dtype = dtype
+        self.add_rest = add_rest  # add rest into note sequencee
+        self.data = read_2columns_text(fname)  # get key-value dict
+
+    def __getitem__(self, key):
+        midi_obj = miditoolkit.midi.parser.MidiFile(self.data[key])
+        # load tempo
+        tempos = midi_obj.tempo_changes
+        tempos.sort(key=lambda x: (x.time, x.tempo))
+        assert len(tempos) == 1
+        tempo = int(tempos[0].tempo + 0.5)
+        # load pitch time sequence
+        tick_to_time = midi_obj.get_tick_to_time_mapping()
+        notes = midi_obj.instruments[0].notes
+        notes.sort(key=lambda x: (x.start, x.pitch))
+        notes_list = []
+        pre_et = 0
+        for note in notes:
+            st = tick_to_time[note.start]
+            et = tick_to_time[note.end]
+            # NOTE(Yuning): MIDIs don't have explicit rest notes.
+            # Explicit rest notes might be needed for stage 1 in svs.
+            if st != pre_et and self.add_rest:
+                notes_list.append(NOTE("P", 0, pre_et, st))
+            notes_list.append(NOTE("*", note.pitch, st, et))
+            pre_et = et
+        return tempo, notes_list
+
+    def get_path(self, key):
+        return self.data[key]
+
+    def __contains__(self, item):
+        return item
+
+    def __len__(self):
+        return len(self.data)
+
+    def __iter__(self):
+        return iter(self.data)
+
+    def keys(self):
+        return self.data.keys()
+
+
 class SingingScoreReader(collections.abc.Mapping):
     """Reader class for 'score.scp'.
 
     Examples:
         key1 /some/path/score.json
         key2 /some/path/score.json
         key3 /some/path/score.json
```

### Comparing `espnet-202304/espnet2/fileio/sound_scp.py` & `espnet-202308/espnet2/fileio/sound_scp.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/fileio/vad_scp.py` & `espnet-202308/espnet2/fileio/vad_scp.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/fst/lm_rescore.py` & `espnet-202308/espnet2/fst/lm_rescore.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/gan_svs/abs_gan_svs.py` & `espnet-202308/espnet2/gan_svs/abs_gan_svs.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/gan_svs/espnet_model.py` & `espnet-202308/espnet2/gan_svs/espnet_model.py`

 * *Files 10% similar despite different names*

```diff
@@ -37,14 +37,15 @@
     def __init__(
         self,
         text_extract: Optional[AbsFeatsExtract],
         feats_extract: Optional[AbsFeatsExtract],
         score_feats_extract: Optional[AbsFeatsExtract],
         label_extract: Optional[AbsFeatsExtract],
         pitch_extract: Optional[AbsFeatsExtract],
+        ying_extract: Optional[AbsFeatsExtract],
         duration_extract: Optional[AbsFeatsExtract],
         energy_extract: Optional[AbsFeatsExtract],
         normalize: Optional[AbsNormalize and InversibleInterface],
         pitch_normalize: Optional[AbsNormalize and InversibleInterface],
         energy_normalize: Optional[AbsNormalize and InversibleInterface],
         svs: AbsGANSVS,
     ):
@@ -54,14 +55,15 @@
         self.text_extract = text_extract
         self.feats_extract = feats_extract
         self.score_feats_extract = score_feats_extract
         self.label_extract = label_extract
         self.pitch_extract = pitch_extract
         self.duration_extract = duration_extract
         self.energy_extract = energy_extract
+        self.ying_extract = ying_extract
         self.normalize = normalize
         self.pitch_normalize = pitch_normalize
         self.energy_normalize = energy_normalize
         self.svs = svs
         assert hasattr(
             svs, "generator"
         ), "generator module must be registered as svs.generator"
@@ -85,19 +87,20 @@
         duration_phn: Optional[torch.Tensor] = None,
         duration_phn_lengths: Optional[torch.Tensor] = None,
         duration_ruled_phn: Optional[torch.Tensor] = None,
         duration_ruled_phn_lengths: Optional[torch.Tensor] = None,
         duration_syb: Optional[torch.Tensor] = None,
         duration_syb_lengths: Optional[torch.Tensor] = None,
         slur: Optional[torch.Tensor] = None,
-        slur_lengths: Optional[torch.Tensor] = None,
         pitch: Optional[torch.Tensor] = None,
         pitch_lengths: Optional[torch.Tensor] = None,
         energy: Optional[torch.Tensor] = None,
         energy_lengths: Optional[torch.Tensor] = None,
+        ying: Optional[torch.Tensor] = None,
+        ying_lengths: Optional[torch.Tensor] = None,
         spembs: Optional[torch.Tensor] = None,
         sids: Optional[torch.Tensor] = None,
         lids: Optional[torch.Tensor] = None,
         forward_generator: bool = True,
         **kwargs,
     ) -> Dict[str, Any]:
         """Return generator or discriminator loss with dict format.
@@ -115,15 +118,14 @@
             duration_phn (Optional[Tensor]): duration tensor (B, T_label).
             duration_phn_lengths (Optional[Tensor]): duration length tensor (B,).
             duration_ruled_phn (Optional[Tensor]): duration tensor (B, T_phone).
             duration_ruled_phn_lengths (Optional[Tensor]): duration length tensor (B,).
             duration_syb (Optional[Tensor]): duration tensor (B, T_syllable).
             duration_syb_lengths (Optional[Tensor]): duration length tensor (B,).
             slur (Optional[Tensor]): slur tensor (B, T_slur).
-            slur_lengths (Optional[Tensor]): slur length tensor (B,).
             pitch (Optional[Tensor]): Pitch tensor (B, T_wav). - f0 sequence
             pitch_lengths (Optional[Tensor]): Pitch length tensor (B,).
             energy (Optional[Tensor]): Energy tensor.
             energy_lengths (Optional[Tensor]): Energy length tensor (B,).
             spembs (Optional[Tensor]): Speaker embedding tensor (B, D).
             sids (Optional[Tensor]): Speaker ID tensor (B, 1).
             lids (Optional[Tensor]): Language ID tensor (B, 1).
@@ -224,15 +226,15 @@
 
                 label_score = label[:, : label_score_lengths.max()]
                 midi_score = midi[:, : midi_score_lengths.max()]
                 duration_score = duration_ruled_phn[
                     :, : duration_score_phn_lengths.max()
                 ]
                 duration_score_syb = duration_syb[:, : duration_score_syb_lengths.max()]
-                slur = slur[:, : slur_lengths.max()]
+                slur = slur[:, : label_score_lengths.max()]
 
             if self.pitch_extract is not None and pitch is None:
                 pitch, pitch_lengths = self.pitch_extract(
                     input=singing,
                     input_lengths=singing_lengths,
                     feats_lengths=feats_lengths,
                 )
@@ -240,14 +242,21 @@
             if self.energy_extract is not None and energy is None:
                 energy, energy_lengths = self.energy_extract(
                     singing,
                     singing_lengths,
                     feats_lengths=feats_lengths,
                 )
 
+            if self.ying_extract is not None and ying is None:
+                ying, ying_lengths = self.ying_extract(
+                    singing,
+                    singing_lengths,
+                    feats_lengths=feats_lengths,
+                )
+
             # Normalize
             if self.normalize is not None:
                 feats, feats_lengths = self.normalize(feats, feats_lengths)
             if self.pitch_normalize is not None:
                 pitch, pitch_lengths = self.pitch_normalize(pitch, pitch_lengths)
             if self.energy_normalize is not None:
                 energy, energy_lengths = self.energy_normalize(energy, energy_lengths)
@@ -272,57 +281,52 @@
             label_score = label_score.to(dtype=torch.long)
             label.update(score=label_score)
             label_lengths.update(score=label_score_lengths)
         batch.update(label=label, label_lengths=label_lengths)
 
         # melody
         melody = dict()
-        melody_lengths = dict()
         if midi_lab is not None:
             midi_lab = midi_lab.to(dtype=torch.long)
             melody.update(lab=midi_lab)
-            melody_lengths.update(lab=midi_lab_lengths)
         if midi_score is not None:
             midi_score = midi_score.to(dtype=torch.long)
             melody.update(score=midi_score)
-            melody_lengths.update(score=midi_score_lengths)
-        batch.update(melody=melody, melody_lengths=melody_lengths)
+        batch.update(melody=melody)
 
         # duration
         # NOTE(Yuning): duration = duration_time / time_shift (same as Xiaoice paper)
         duration = dict()
-        duration_lengths = dict()
         if duration_lab is not None:
             duration_lab = duration_lab.to(dtype=torch.long)
             duration.update(lab=duration_lab)
-            duration_lengths.update(lab=duration_lab_lengths)
         if duration_score is not None:
             duration_phn_score = duration_score.to(dtype=torch.long)
             duration.update(score_phn=duration_phn_score)
-            duration_lengths.update(score_phn=duration_score_phn_lengths)
         if duration_score_syb is not None:
             duration_syb_score = duration_score_syb.to(dtype=torch.long)
             duration.update(score_syb=duration_syb_score)
-            duration_lengths.update(score_syb=duration_score_syb_lengths)
-        batch.update(duration=duration, duration_lengths=duration_lengths)
+        batch.update(duration=duration)
 
         if slur is not None:
-            batch.update(slur=slur, slur_lengths=slur_lengths)
+            batch.update(slur=slur)
         if spembs is not None:
             batch.update(spembs=spembs)
         if sids is not None:
             batch.update(sids=sids)
         if lids is not None:
             batch.update(lids=lids)
         if feats is not None:
             batch.update(feats=feats, feats_lengths=feats_lengths)
         if self.pitch_extract is not None and pitch is not None:
-            batch.update(pitch=pitch, pitch_lengths=pitch_lengths)
+            batch.update(pitch=pitch)
         if self.energy_extract is not None and energy is not None:
-            batch.update(energy=energy, energy_lengths=energy_lengths)
+            batch.update(energy=energy)
+        if self.ying_extract is not None and ying is not None:
+            batch.update(ying=ying)
         if self.svs.require_raw_singing:
             batch.update(singing=singing, singing_lengths=singing_lengths)
         return self.svs(**batch)
 
     def collect_feats(
         self,
         text: torch.Tensor,
@@ -337,19 +341,20 @@
         duration_phn: Optional[torch.Tensor] = None,
         duration_phn_lengths: Optional[torch.Tensor] = None,
         duration_ruled_phn: Optional[torch.Tensor] = None,
         duration_ruled_phn_lengths: Optional[torch.Tensor] = None,
         duration_syb: Optional[torch.Tensor] = None,
         duration_syb_lengths: Optional[torch.Tensor] = None,
         slur: Optional[torch.Tensor] = None,
-        slur_lengths: Optional[torch.Tensor] = None,
         pitch: Optional[torch.Tensor] = None,
         pitch_lengths: Optional[torch.Tensor] = None,
         energy: Optional[torch.Tensor] = None,
         energy_lengths: Optional[torch.Tensor] = None,
+        ying: Optional[torch.Tensor] = None,
+        ying_lengths: Optional[torch.Tensor] = None,
         spembs: Optional[torch.Tensor] = None,
         sids: Optional[torch.Tensor] = None,
         lids: Optional[torch.Tensor] = None,
         **kwargs,
     ) -> Dict[str, torch.Tensor]:
         """Calculate features and return them as a dict.
 
@@ -363,15 +368,14 @@
             phn_cnt (Optional[Tensor]): Number of phones in each syllable (B, T_syb)
             midi (Option[Tensor]): Midi tensor (B, T_label).
             midi_lengths (Optional[Tensor]): Midi lrngth tensor (B,).
             duration_phn (Optional[Tensor]): duration tensor (T_label).
             duration_ruled_phn (Optional[Tensor]): duration tensor (T_phone).
             duration_syb (Optional[Tensor]): duration tensor (T_phone).
             slur (Optional[Tensor]): slur tensor (B, T_slur).
-            slur_lengths (Optional[Tensor]): slur length tensor (B,).
             pitch (Optional[Tensor]): Pitch tensor (B, T_wav). - f0 sequence
             pitch_lengths (Optional[Tensor]): Pitch length tensor (B,).
             energy (Optional[Tensor): Energy tensor.
             energy_lengths (Optional[Tensor): Energy length tensor (B,).
             spembs (Optional[Tensor]): Speaker embedding tensor (B, D).
             sids (Optional[Tensor]): Speaker ID tensor (B, 1).
             lids (Optional[Tensor]): Language ID tensor (B, 1).
@@ -413,18 +417,26 @@
             )
         if self.energy_extract is not None:
             energy, energy_lengths = self.energy_extract(
                 singing,
                 singing_lengths,
                 feats_lengths=feats_lengths,
             )
+        if self.ying_extract is not None and ying is None:
+            ying, ying_lengths = self.ying_extract(
+                singing,
+                singing_lengths,
+                feats_lengths=feats_lengths,
+            )
 
         # store in dict
         feats_dict = {}
         if feats is not None:
             feats_dict.update(feats=feats, feats_lengths=feats_lengths)
         if pitch is not None:
             feats_dict.update(pitch=pitch, pitch_lengths=pitch_lengths)
         if energy is not None:
             feats_dict.update(energy=energy, energy_lengths=energy_lengths)
+        if ying is not None:
+            feats_dict.update(ying=ying, ying_lengths=ying_lengths)
 
         return feats_dict
```

### Comparing `espnet-202304/espnet2/gan_svs/joint/joint_score2wav.py` & `espnet-202308/espnet2/gan_svs/joint/joint_score2wav.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 # Copyright 2021 Tomoki Hayashi
+# Copyright 2023 Renmin University of China (Yuning Wu)
 #  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)
 
-"""Joint text-to-wav module for end-to-end training."""
+"""Joint score-to-wav module for end-to-end training."""
 
 from typing import Any, Dict, Optional
 
 import torch
 from typeguard import check_argument_types
 
 from espnet2.gan_svs.abs_gan_svs import AbsGANSVS
@@ -67,15 +68,14 @@
         odim: int,
         segment_size: int = 32,
         sampling_rate: int = 22050,
         score2mel_type: str = "xiaoice",
         score2mel_params: Dict[str, Any] = {
             "midi_dim": 129,
             "tempo_dim": 500,
-            "embed_dim": 512,
             "adim": 384,
             "aheads": 4,
             "elayers": 6,
             "eunits": 1536,
             "dlayers": 6,
             "dunits": 1536,
             "postnet_layers": 5,
@@ -120,15 +120,20 @@
             "spk_embed_integration_type": "add",
             # training related
             "init_type": "xavier_uniform",
             "init_enc_alpha": 1.0,
             "init_dec_alpha": 1.0,
             "use_masking": False,
             "use_weighted_masking": False,
+            "loss_function": "XiaoiceSing2",
             "loss_type": "L1",
+            "lambda_mel": 1,
+            "lambda_dur": 0.1,
+            "lambda_pitch": 0.01,
+            "lambda_vuv": 0.01,
         },
         vocoder_type: str = "hifigan_generator",
         vocoder_params: Dict[str, Any] = {
             "out_channels": 1,
             "channels": 512,
             "global_channels": -1,
             "kernel_size": 7,
@@ -336,19 +341,17 @@
         feats: torch.Tensor,
         feats_lengths: torch.Tensor,
         singing: torch.Tensor,
         singing_lengths: torch.Tensor,
         label: Optional[Dict[str, torch.Tensor]] = None,
         label_lengths: Optional[Dict[str, torch.Tensor]] = None,
         melody: Optional[Dict[str, torch.Tensor]] = None,
-        melody_lengths: Optional[Dict[str, torch.Tensor]] = None,
         pitch: torch.LongTensor = None,
-        pitch_lengths: torch.Tensor = None,
         duration: Optional[Dict[str, torch.Tensor]] = None,
-        duration_lengths: Optional[Dict[str, torch.Tensor]] = None,
+        slur: torch.LongTensor = None,
         spembs: Optional[torch.Tensor] = None,
         sids: Optional[torch.Tensor] = None,
         lids: Optional[torch.Tensor] = None,
         forward_generator: bool = True,
     ) -> Dict[str, Any]:
         """Perform generator forward.
 
@@ -361,138 +364,111 @@
             singing_lengths (Tensor): Singing length tensor (B,).
             label (Optional[Dict]): key is "lab" or "score";
                 value (LongTensor): Batch of padded label ids (B, Tmax).
             label_lengths (Optional[Dict]): key is "lab" or "score";
                 value (LongTensor): Batch of the lengths of padded label ids (B, ).
             melody (Optional[Dict]): key is "lab" or "score";
                 value (LongTensor): Batch of padded melody (B, Tmax).
-            melody_lengths (Optional[Dict]): key is "lab" or "score";
-                value (LongTensor): Batch of the lengths of padded melody (B, ).
             pitch (FloatTensor): Batch of padded f0 (B, Tmax).
-            pitch_lengths (LongTensor): Batch of the lengths of padded f0 (B, ).
             duration (Optional[Dict]): key is "lab", "score_phn" or "score_syb";
                 value (LongTensor): Batch of padded duration (B, Tmax).
-            duration_length (Optional[Dict]): key is "lab", "score_phn" or "score_syb";
-                value (LongTensor): Batch of the lengths of padded duration (B, ).
             slur (FloatTensor): Batch of padded slur (B, Tmax).
-            slur_lengths (LongTensor): Batch of the lengths of padded slur (B, ).
             spembs (Optional[Tensor]): Batch of speaker embeddings (B, spk_embed_dim).
             sids (Optional[Tensor]): Batch of speaker IDs (B, 1).
             lids (Optional[Tensor]): Batch of language IDs (B, 1).
             forward_generator (bool): Whether to forward generator.
 
         Returns:
             Dict[str, Any]:
                 - loss (Tensor): Loss scalar tensor.
                 - stats (Dict[str, float]): Statistics to be monitored.
                 - weight (Tensor): Weight tensor to summarize losses.
                 - optim_idx (int): Optimizer index (0 for G and 1 for D).
 
         """
-        beat = duration["lab"]
-        beat_lengths = duration_lengths["lab"]
-        duration = duration["lab"]
+
         label = label["score"]
         label_lengths = label_lengths["score"]
         melody = melody["score"]
-        melody_lengths = melody_lengths["score"]
+        duration = duration["lab"]
 
         if forward_generator:
             return self._forward_generator(
                 text=text,
                 text_lengths=text_lengths,
                 feats=feats,
                 feats_lengths=feats_lengths,
                 singing=singing,
                 singing_lengths=singing_lengths,
-                duration=duration,
                 label=label,
                 label_lengths=label_lengths,
                 melody=melody,
-                melody_lengths=melody_lengths,
-                beat=beat,
-                beat_lengths=beat_lengths,
+                duration=duration,
+                slur=slur,
                 pitch=pitch,
-                pitch_lengths=pitch_lengths,
                 sids=sids,
                 spembs=spembs,
                 lids=lids,
             )
         else:
             return self._forward_discrminator(
                 text=text,
                 text_lengths=text_lengths,
                 feats=feats,
                 feats_lengths=feats_lengths,
                 singing=singing,
                 singing_lengths=singing_lengths,
-                duration=duration,
                 label=label,
                 label_lengths=label_lengths,
                 melody=melody,
-                melody_lengths=melody_lengths,
-                beat=beat,
-                beat_lengths=beat_lengths,
+                duration=duration,
+                slur=slur,
                 pitch=pitch,
-                pitch_lengths=pitch_lengths,
                 sids=sids,
                 spembs=spembs,
                 lids=lids,
             )
 
     def _forward_generator(
         self,
         text: torch.Tensor,
         text_lengths: torch.Tensor,
         feats: torch.Tensor,
         feats_lengths: torch.Tensor,
         singing: torch.Tensor,
         singing_lengths: torch.Tensor,
-        duration: torch.Tensor,
         label: Optional[Dict[str, torch.Tensor]] = None,
         label_lengths: Optional[Dict[str, torch.Tensor]] = None,
         melody: Optional[Dict[str, torch.Tensor]] = None,
-        melody_lengths: Optional[Dict[str, torch.Tensor]] = None,
-        beat: Optional[Dict[str, torch.Tensor]] = None,
-        beat_lengths: Optional[Dict[str, torch.Tensor]] = None,
+        duration: Optional[Dict[str, torch.Tensor]] = None,
+        slur: Optional[torch.Tensor] = None,
         pitch: Optional[torch.Tensor] = None,
-        pitch_lengths: Optional[torch.Tensor] = None,
         sids: Optional[torch.Tensor] = None,
         spembs: Optional[torch.Tensor] = None,
         lids: Optional[torch.Tensor] = None,
     ) -> Dict[str, Any]:
         """Perform generator forward.
 
         Args:
             text (Tensor): Text index tensor (B, T_text).
             text_lengths (Tensor): Text length tensor (B,).
             feats (Tensor): Feature tensor (B, T_feats, aux_channels).
             feats_lengths (Tensor): Feature length tensor (B,).
             singing (Tensor): Singing waveform tensor (B, T_wav).
             singing_lengths (Tensor): Singing length tensor (B,).
-            duration (Optional[Dict]): key is "phn", "syb";
-                value (LongTensor): Batch of padded beat (B, Tmax).
             label (Optional[Dict]): key is "lab" or "score";
                 value (LongTensor): Batch of padded label ids (B, Tmax).
             label_lengths (Optional[Dict]): key is "lab" or "score";
                 value (LongTensor): Batch of the lengths of padded label ids (B, ).
             melody (Optional[Dict]): key is "lab" or "score";
                 value (LongTensor): Batch of padded melody (B, Tmax).
-            melody_lengths (Optional[Dict]): key is "lab" or "score";
-                value (LongTensor): Batch of the lengths of padded melody (B, ).
-            tempo (Optional[Dict]): key is "lab" or "score";
-                value (LongTensor): Batch of padded tempo (B, Tmax).
-            tempo_lengths (Optional[Dict]):  key is "lab" or "score";
-                value (LongTensor): Batch of the lengths of padded tempo (B, ).
-            beat (Optional[Dict]): key is "lab", "score_phn" or "score_syb";
-                value (LongTensor): Batch of padded beat (B, Tmax).
-            beat_length (Optional[Dict]): key is "lab", "score_phn" or "score_syb";
-                value (LongTensor): Batch of the lengths of padded beat (B, ).
+            duration (Optional[Dict]): key is "lab", "score_phn" or "score_syb";
+                value (LongTensor): Batch of padded duration (B, Tmax).
+            slur (FloatTensor): Batch of padded slur (B, T_max).
             pitch (FloatTensor): Batch of padded f0 (B, Tmax).
-            pitch_lengths (LongTensor): Batch of the lengths of padded f0 (B, ).
             sids (Optional[Tensor]): Speaker index tensor (B,) or (B, 1).
             spembs (Optional[Tensor]): Speaker embedding tensor (B, spk_embed_dim).
             lids (Optional[Tensor]): Language index tensor (B,) or (B, 1).
 
         Returns:
             Dict[str, Any]:
                 * loss (Tensor): Loss scalar tensor.
@@ -514,19 +490,19 @@
                 text=text,
                 text_lengths=text_lengths,
                 feats=feats,
                 feats_lengths=feats_lengths,
                 label=label,
                 label_lengths=label_lengths,
                 melody=melody,
-                melody_lengths=melody_lengths,
-                duration=beat,
-                duration_lengths=beat_lengths,
+                melody_lengths=label_lengths,
+                duration=duration,
+                duration_lengths=label_lengths,
                 pitch=pitch,
-                pitch_lengths=pitch_lengths,
+                pitch_lengths=feats_lengths,
                 sids=sids,
                 spembs=spembs,
                 lids=lids,
                 joint_training=True,
             )
             # get random segments
             feats_gen_, start_idxs = get_random_segments(
@@ -596,56 +572,43 @@
         self,
         text: torch.Tensor,
         text_lengths: torch.Tensor,
         feats: torch.Tensor,
         feats_lengths: torch.Tensor,
         singing: torch.Tensor,
         singing_lengths: torch.Tensor,
-        duration: torch.Tensor,
         label: Optional[Dict[str, torch.Tensor]] = None,
         label_lengths: Optional[Dict[str, torch.Tensor]] = None,
         melody: Optional[Dict[str, torch.Tensor]] = None,
-        melody_lengths: Optional[Dict[str, torch.Tensor]] = None,
-        beat: Optional[Dict[str, torch.Tensor]] = None,
-        beat_lengths: Optional[Dict[str, torch.Tensor]] = None,
+        duration: Optional[Dict[str, torch.Tensor]] = None,
+        slur: Optional[torch.Tensor] = None,
         pitch: Optional[torch.Tensor] = None,
-        pitch_lengths: Optional[torch.Tensor] = None,
         sids: Optional[torch.Tensor] = None,
         spembs: Optional[torch.Tensor] = None,
         lids: Optional[torch.Tensor] = None,
     ) -> Dict[str, Any]:
         """Perform discriminator forward.
 
         Args:
             text (Tensor): Text index tensor (B, T_text).
             text_lengths (Tensor): Text length tensor (B,).
             feats (Tensor): Feature tensor (B, T_feats, aux_channels).
             feats_lengths (Tensor): Feature length tensor (B,).
             singing (Tensor): Singing waveform tensor (B, T_wav).
             singing_lengths (Tensor): Singing length tensor (B,).
-            duration (Optional[Dict]): key is "phn", "syb";
-                value (LongTensor): Batch of padded beat (B, Tmax).
             label (Optional[Dict]): key is "lab" or "score";
                 value (LongTensor): Batch of padded label ids (B, Tmax).
             label_lengths (Optional[Dict]): key is "lab" or "score";
                 value (LongTensor): Batch of the lengths of padded label ids (B, ).
             melody (Optional[Dict]): key is "lab" or "score";
                 value (LongTensor): Batch of padded melody (B, Tmax).
-            melody_lengths (Optional[Dict]): key is "lab" or "score";
-                value (LongTensor): Batch of the lengths of padded melody (B, ).
-            tempo (Optional[Dict]): key is "lab" or "score";
-                value (LongTensor): Batch of padded tempo (B, Tmax).
-            tempo_lengths (Optional[Dict]):  key is "lab" or "score";
-                value (LongTensor): Batch of the lengths of padded tempo (B, ).
-            beat (Optional[Dict]): key is "lab", "score_phn" or "score_syb";
-                value (LongTensor): Batch of padded beat (B, Tmax).
-            beat_length (Optional[Dict]): key is "lab", "score_phn" or "score_syb";
-                value (LongTensor): Batch of the lengths of padded beat (B, ).
+            duration (Optional[Dict]): key is "lab", "score_phn" or "score_syb";
+                value (LongTensor): Batch of padded duration (B, Tmax).
+            slur (FloatTensor): Batch of padded slur (B, T_max).
             pitch (FloatTensor): Batch of padded f0 (B, Tmax).
-            pitch_lengths (LongTensor): Batch of the lengths of padded f0 (B, ).
             sids (Optional[Tensor]): Speaker index tensor (B,) or (B, 1).
             spembs (Optional[Tensor]): Speaker embedding tensor (B, spk_embed_dim).
             lids (Optional[Tensor]): Language index tensor (B,) or (B, 1).
 
         Returns:
             Dict[str, Any]:
                 * loss (Tensor): Loss scalar tensor.
@@ -667,19 +630,19 @@
                 text=text,
                 text_lengths=text_lengths,
                 feats=feats,
                 feats_lengths=feats_lengths,
                 label=label,
                 label_lengths=label_lengths,
                 melody=melody,
-                melody_lengths=melody_lengths,
-                duration=beat,
-                duration_lengths=beat_lengths,
+                melody_lengths=label_lengths,
+                duration=duration,
+                duration_lengths=label_lengths,
                 pitch=pitch,
-                pitch_lengths=pitch_lengths,
+                pitch_lengths=feats_lengths,
                 sids=sids,
                 spembs=spembs,
                 lids=lids,
                 joint_training=True,
             )
             # get random segments
             feats_gen_, start_idxs = get_random_segments(
@@ -754,21 +717,17 @@
         Args:
             text (Tensor): Input text index tensor (T_text,).
             feats (Tensor): Feature tensor (T_feats, aux_channels).
             label (Optional[Dict]): key is "lab" or "score";
                 value (LongTensor): Batch of padded label ids (B, Tmax).
             melody (Optional[Dict]): key is "lab" or "score";
                 value (LongTensor): Batch of padded melody (B, Tmax).
-            tempo (Optional[Dict]): key is "lab" or "score";
-                value (LongTensor): Batch of padded tempo (B, Tmax).
-            beat (Optional[Dict]): key is "lab", "score_phn" or "score_syb";
-                value (LongTensor): Batch of padded beat (B, Tmax).
+            duration (Optional[Dict]): key is "lab", "score_phn" or "score_syb";
+                value (LongTensor): Batch of padded duration (B, Tmax).
             pitch (FloatTensor): Batch of padded f0 (B, Tmax).
-            duration (Optional[Dict]): key is "phn", "syb";
-                value (LongTensor): Batch of padded beat (B, Tmax).
             slur (LongTensor): Batch of padded slur (B, Tmax).
             sids (Tensor): Speaker index tensor (1,).
             spembs (Optional[Tensor]): Speaker embedding tensor (spk_embed_dim,).
             lids (Tensor): Language index tensor (1,).
             noise_scale (float): Noise scale value for flow.
             noise_scale_dur (float): Noise scale value for duration predictor.
             alpha (float): Alpha parameter to control the speed of generated singing.
```

### Comparing `espnet-202304/espnet2/gan_svs/vits/generator.py` & `espnet-202308/espnet2/gan_tts/vits/generator.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,35 +1,30 @@
 # Copyright 2021 Tomoki Hayashi
-# Copyright 2022 Yifeng Yu
 #  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)
 
-"""Generator module in VISinger.
+"""Generator module in VITS.
 
 This code is based on https://github.com/jaywalnut310/vits.
 
 """
 
-from typing import Dict, List, Optional, Tuple
+import math
+from typing import List, Optional, Tuple
 
 import numpy as np
 import torch
 import torch.nn.functional as F
 
-from espnet2.gan_svs.vits.duration_predictor import DurationPredictor
-from espnet2.gan_svs.vits.frame_prior_net import FramePriorNet
-from espnet2.gan_svs.vits.length_regulator import LengthRegulator
-from espnet2.gan_svs.vits.modules import Projection, sequence_mask
-from espnet2.gan_svs.vits.phoneme_predictor import PhonemePredictor
-from espnet2.gan_svs.vits.pitch_predictor import PitchPredictor
-from espnet2.gan_svs.vits.text_encoder import TextEncoder
 from espnet2.gan_tts.hifigan import HiFiGANGenerator
 from espnet2.gan_tts.utils import get_random_segments
+from espnet2.gan_tts.vits.duration_predictor import StochasticDurationPredictor
 from espnet2.gan_tts.vits.posterior_encoder import PosteriorEncoder
 from espnet2.gan_tts.vits.residual_coupling import ResidualAffineCouplingBlock
-from espnet.nets.pytorch_backend.transformer.embedding import PositionalEncoding
+from espnet2.gan_tts.vits.text_encoder import TextEncoder
+from espnet.nets.pytorch_backend.nets_utils import make_non_pad_mask
 
 
 class VITSGenerator(torch.nn.Module):
     """Generator module in VITS.
 
     This is a module of VITS described in `Conditional Variational Autoencoder
     with Adversarial Learning for End-to-End Text-to-Speech`_.
@@ -41,18 +36,14 @@
         Text-to-Speech`: https://arxiv.org/abs/2006.04558
 
     """
 
     def __init__(
         self,
         vocabs: int,
-        midi_dim: int = 129,
-        tempo_dim: int = 128,
-        beat_dim: int = 600,
-        midi_embed_integration_type: str = "add",
         aux_channels: int = 513,
         hidden_channels: int = 192,
         spks: Optional[int] = None,
         langs: Optional[int] = None,
         spk_embed_dim: Optional[int] = None,
         global_channels: int = -1,
         segment_size: int = 32,
@@ -87,16 +78,18 @@
         flow_flows: int = 4,
         flow_kernel_size: int = 5,
         flow_base_dilation: int = 1,
         flow_layers: int = 4,
         flow_dropout_rate: float = 0.0,
         use_weight_norm_in_flow: bool = True,
         use_only_mean_in_flow: bool = True,
-        use_dp: bool = True,
-        use_visinger: bool = True,
+        stochastic_duration_predictor_kernel_size: int = 3,
+        stochastic_duration_predictor_dropout_rate: float = 0.5,
+        stochastic_duration_predictor_flows: int = 4,
+        stochastic_duration_predictor_dds_conv_layers: int = 3,
     ):
         """Initialize VITS generator module.
 
         Args:
             vocabs (int): Input vocabulary size.
             aux_channels (int): Number of acoustic feature channels.
             hidden_channels (int): Number of hidden channels.
@@ -160,14 +153,23 @@
             flow_kernel_size (int): Kernel size in flow.
             flow_base_dilation (int): Base dilation in flow.
             flow_layers (int): Number of layers in flow.
             flow_dropout_rate (float): Dropout rate in flow
             use_weight_norm_in_flow (bool): Whether to apply weight normalization in
                 flow.
             use_only_mean_in_flow (bool): Whether to use only mean in flow.
+            stochastic_duration_predictor_kernel_size (int): Kernel size in stochastic
+                duration predictor.
+            stochastic_duration_predictor_dropout_rate (float): Dropout rate in
+                stochastic duration predictor.
+            stochastic_duration_predictor_flows (int): Number of flows in stochastic
+                duration predictor.
+            stochastic_duration_predictor_dds_conv_layers (int): Number of DDS conv
+                layers in stochastic duration predictor.
+
         """
         super().__init__()
         self.segment_size = segment_size
         self.text_encoder = TextEncoder(
             vocabs=vocabs,
             attention_dim=hidden_channels,
             attention_heads=text_encoder_attention_heads,
@@ -181,19 +183,15 @@
             normalize_before=text_encoder_normalize_before,
             dropout_rate=text_encoder_dropout_rate,
             positional_dropout_rate=text_encoder_positional_dropout_rate,
             attention_dropout_rate=text_encoder_attention_dropout_rate,
             conformer_kernel_size=text_encoder_conformer_kernel_size,
             use_macaron_style=use_macaron_style_in_text_encoder,
             use_conformer_conv=use_conformer_conv_in_text_encoder,
-            midi_dim=midi_dim,
-            beat_dim=beat_dim,
-            use_visinger=use_visinger,
         )
-
         self.decoder = HiFiGANGenerator(
             in_channels=hidden_channels,
             out_channels=1,
             channels=decoder_channels,
             global_channels=global_channels,
             kernel_size=decoder_kernel_size,
             upsample_scales=decoder_upsample_scales,
@@ -222,48 +220,22 @@
             base_dilation=flow_base_dilation,
             layers=flow_layers,
             global_channels=global_channels,
             dropout_rate=flow_dropout_rate,
             use_weight_norm=use_weight_norm_in_flow,
             use_only_mean=use_only_mean_in_flow,
         )
-        self.use_visinger = use_visinger
-        self.use_dp = use_dp
-        if self.use_visinger:
-            self.project = Projection(hidden_channels, hidden_channels)
-
         # TODO(kan-bayashi): Add deterministic version as an option
-
-        if use_dp:
-            self.duration_predictor = DurationPredictor(
-                channels=hidden_channels,
-                filter_channels=256,
-                kernel_size=3,
-                dropout_rate=0.5,
-                global_channels=global_channels,
-            )
-
-        self.lr = LengthRegulator()
-
-        self.pitch_predictor = PitchPredictor(
-            hidden_channels=hidden_channels,
-            attention_dim=hidden_channels,
-        )
-
-        self.frame_prior_net = FramePriorNet(
-            hidden_channels=hidden_channels,
-            attention_dim=hidden_channels,
-            blocks=4,
-        )
-
-        self.phoneme_predictor = PhonemePredictor(
-            vocabs=vocabs,
-            hidden_channels=hidden_channels,
-            attention_dim=hidden_channels,
-            blocks=2,
+        self.duration_predictor = StochasticDurationPredictor(
+            channels=hidden_channels,
+            kernel_size=stochastic_duration_predictor_kernel_size,
+            dropout_rate=stochastic_duration_predictor_dropout_rate,
+            flows=stochastic_duration_predictor_flows,
+            dds_conv_layers=stochastic_duration_predictor_dds_conv_layers,
+            global_channels=global_channels,
         )
 
         self.upsample_factor = int(np.prod(decoder_upsample_scales))
         self.spks = None
         if spks is not None and spks > 1:
             assert global_channels > 0
             self.spks = spks
@@ -275,29 +247,25 @@
             self.spemb_proj = torch.nn.Linear(spk_embed_dim, global_channels)
         self.langs = None
         if langs is not None and langs > 1:
             assert global_channels > 0
             self.langs = langs
             self.lang_emb = torch.nn.Embedding(langs, global_channels)
 
+        # delayed import
+        from espnet2.gan_tts.vits.monotonic_align import maximum_path
+
+        self.maximum_path = maximum_path
+
     def forward(
         self,
         text: torch.Tensor,
         text_lengths: torch.Tensor,
         feats: torch.Tensor,
         feats_lengths: torch.Tensor,
-        duration: torch.Tensor = None,
-        label: torch.Tensor = None,
-        label_lengths: torch.Tensor = None,
-        melody: torch.Tensor = None,
-        melody_lengths: torch.Tensor = None,
-        beat: torch.Tensor = None,
-        beat_lengths: torch.Tensor = None,
-        pitch: torch.Tensor = None,
-        pitch_lengths: torch.Tensor = None,
         sids: Optional[torch.Tensor] = None,
         spembs: Optional[torch.Tensor] = None,
         lids: Optional[torch.Tensor] = None,
     ) -> Tuple[
         torch.Tensor,
         torch.Tensor,
         torch.Tensor,
@@ -312,30 +280,21 @@
             torch.Tensor,
             torch.Tensor,
         ],
     ]:
         """Calculate forward propagation.
 
         Args:
-            text (LongTensor): Batch of padded character ids (B, Tmax).
-            text_lengths (LongTensor): Batch of lengths of each input batch (B,).
-            feats (Tensor): Batch of padded target features (B, Lmax, odim).
-            feats_lengths (LongTensor): Batch of the lengths of each target (B,).
-            label (LongTensor): Batch of padded label ids (B, Tmax).
-            label_lengths (LongTensor): Batch of the lengths of padded label ids (B, ).
-            melody (LongTensor): Batch of padded melody (B, Tmax).
-            melody_lengths (LongTensor): Batch of the lengths of padded melody (B, ).
-            beat (LongTensor): Batch of padded beat (B, Tmax).
-            beat_lengths (LongTensor): Batch of the lengths of padded beat (B, ).
-            pitch (FloatTensor): Batch of padded f0 (B, Tmax).
-            pitch_lengths (LongTensor): Batch of the lengths of padded f0 (B, ).
-            duration (LongTensor): Batch of padded beat (B, Tmax).
-            spembs (Optional[Tensor]): Batch of speaker embeddings (B, spk_embed_dim).
-            sids (Optional[Tensor]): Batch of speaker IDs (B, 1).
-            lids (Optional[Tensor]): Batch of language IDs (B, 1).
+            text (Tensor): Text index tensor (B, T_text).
+            text_lengths (Tensor): Text length tensor (B,).
+            feats (Tensor): Feature tensor (B, aux_channels, T_feats).
+            feats_lengths (Tensor): Feature length tensor (B,).
+            sids (Optional[Tensor]): Speaker index tensor (B,) or (B, 1).
+            spembs (Optional[Tensor]): Speaker embedding tensor (B, spk_embed_dim).
+            lids (Optional[Tensor]): Language index tensor (B,) or (B, 1).
 
         Returns:
             Tensor: Waveform tensor (B, 1, segment_size * upsample_factor).
             Tensor: Duration negative log-likelihood (NLL) tensor (B,).
             Tensor: Monotonic attention weight tensor (B, 1, T_feats, T_text).
             Tensor: Segments start index tensor (B,).
             Tensor: Text mask tensor (B, 1, T_text).
@@ -345,14 +304,17 @@
                 - Tensor: Flow hidden representation (B, H, T_feats).
                 - Tensor: Expanded text encoder projected mean (B, H, T_feats).
                 - Tensor: Expanded text encoder projected scale (B, H, T_feats).
                 - Tensor: Posterior encoder projected mean (B, H, T_feats).
                 - Tensor: Posterior encoder projected scale (B, H, T_feats).
 
         """
+        # forward text encoder
+        x, m_p, logs_p, x_mask = self.text_encoder(text, text_lengths)
+
         # calculate global conditioning
         g = None
         if self.spks is not None:
             # speaker one-hot vector embedding: (B, global_channels, 1)
             g = self.global_emb(sids.view(-1)).unsqueeze(-1)
         if self.spk_embed_dim is not None:
             # pretreined speaker embedding, e.g., X-vector (B, global_channels, 1)
@@ -365,196 +327,133 @@
             # language one-hot vector embedding: (B, global_channels, 1)
             g_ = self.lang_emb(lids.view(-1)).unsqueeze(-1)
             if g is None:
                 g = g_
             else:
                 g = g + g_
 
-        # forward text encoder
-        if not self.use_dp:
-            # align frame length
-            for i, length in enumerate(label_lengths):
-                if length == label.shape[1]:
-                    label_lengths[i] = feats.shape[2]
-            if label.shape[1] < feats.shape[2]:
-                label = F.pad(
-                    input=label,
-                    pad=(0, feats.shape[2] - label.shape[1], 0, 0),
-                    mode="constant",
-                    value=0,
-                )
-                melody = F.pad(
-                    input=melody,
-                    pad=(0, feats.shape[2] - melody.shape[1], 0, 0),
-                    mode="constant",
-                    value=0,
-                )
-                beat = F.pad(
-                    input=beat,
-                    pad=(0, feats.shape[2] - beat.shape[1], 0, 0),
-                    mode="constant",
-                    value=0,
-                )
-            else:
-                label = label[:, : feats.shape[2]]
-                melody = melody[:, : feats.shape[2]]
-                beat = beat[:, : feats.shape[2]]
-
-            x, m_p, logs_p, x_mask = self.text_encoder(
-                label, label_lengths, melody, beat
-            )
-        else:
-            x, m_p, logs_p, x_mask = self.text_encoder(
-                label, label_lengths, melody, beat
-            )
-            w = duration.unsqueeze(1)
-            logw_gt = w * x_mask
-            logw = self.duration_predictor(x, x_mask, beat, g=g)
-            logw = (torch.exp(logw) - 1) * x_mask
-            logw = torch.mul(logw.squeeze(1), beat).unsqueeze(1)
-
-            x, frame_pitch, x_lengths = self.lr(x, melody, duration, melody_lengths)
-
-            x_mask = torch.unsqueeze(sequence_mask(x_lengths, x.size(2)), 1)
-
-        self.pos_encoder = PositionalEncoding(
-            d_model=x.size(1), dropout_rate=0, max_len=x.size(2)
-        )
-        x = self.pos_encoder(x.transpose(1, 2)).transpose(1, 2)
-
-        if self.use_visinger:
-            pred_pitch, pitch_embedding = self.pitch_predictor(x, x_mask)
-            pred_pitch = torch.squeeze(pred_pitch, 1)
-            if not self.use_dp:
-                gt_pitch = torch.log(440 * (2 ** ((melody - 69) / 12)))  # log f0
-            else:
-                gt_pitch = torch.squeeze(pitch, 2)
-
-            x = self.frame_prior_net(x, pitch_embedding, x_mask)
-            m_p, logs_p = self.project(x, x_mask)
-
         # forward posterior encoder
         z, m_q, logs_q, y_mask = self.posterior_encoder(feats, feats_lengths, g=g)
 
-        # phoneme predictor
-        if self.use_dp:
-            log_probs = self.phoneme_predictor(z, y_mask)
-
         # forward flow
         z_p = self.flow(z, y_mask, g=g)  # (B, H, T_feats)
 
+        # monotonic alignment search
+        with torch.no_grad():
+            # negative cross-entropy
+            s_p_sq_r = torch.exp(-2 * logs_p)  # (B, H, T_text)
+            # (B, 1, T_text)
+            neg_x_ent_1 = torch.sum(
+                -0.5 * math.log(2 * math.pi) - logs_p,
+                [1],
+                keepdim=True,
+            )
+            # (B, T_feats, H) x (B, H, T_text) = (B, T_feats, T_text)
+            neg_x_ent_2 = torch.matmul(
+                -0.5 * (z_p**2).transpose(1, 2),
+                s_p_sq_r,
+            )
+            # (B, T_feats, H) x (B, H, T_text) = (B, T_feats, T_text)
+            neg_x_ent_3 = torch.matmul(
+                z_p.transpose(1, 2),
+                (m_p * s_p_sq_r),
+            )
+            # (B, 1, T_text)
+            neg_x_ent_4 = torch.sum(
+                -0.5 * (m_p**2) * s_p_sq_r,
+                [1],
+                keepdim=True,
+            )
+            # (B, T_feats, T_text)
+            neg_x_ent = neg_x_ent_1 + neg_x_ent_2 + neg_x_ent_3 + neg_x_ent_4
+            # (B, 1, T_feats, T_text)
+            attn_mask = torch.unsqueeze(x_mask, 2) * torch.unsqueeze(y_mask, -1)
+            # monotonic attention weight: (B, 1, T_feats, T_text)
+            attn = (
+                self.maximum_path(
+                    neg_x_ent,
+                    attn_mask.squeeze(1),
+                )
+                .unsqueeze(1)
+                .detach()
+            )
+
+        # forward duration predictor
+        w = attn.sum(2)  # (B, 1, T_text)
+        dur_nll = self.duration_predictor(x, x_mask, w=w, g=g)
+        dur_nll = dur_nll / torch.sum(x_mask)
+
+        # expand the length to match with the feature sequence
+        # (B, T_feats, T_text) x (B, T_text, H) -> (B, H, T_feats)
+        m_p = torch.matmul(attn.squeeze(1), m_p.transpose(1, 2)).transpose(1, 2)
+        # (B, T_feats, T_text) x (B, T_text, H) -> (B, H, T_feats)
+        logs_p = torch.matmul(attn.squeeze(1), logs_p.transpose(1, 2)).transpose(1, 2)
+
         # get random segments
         z_segments, z_start_idxs = get_random_segments(
             z,
             feats_lengths,
             self.segment_size,
         )
 
         # forward decoder with random segments
         wav = self.decoder(z_segments, g=g)
 
-        if self.use_visinger:
-            if self.use_dp:
-                return (
-                    wav,
-                    z_start_idxs,
-                    x_mask,
-                    y_mask,
-                    (
-                        z,
-                        z_p,
-                        m_p,
-                        logs_p,
-                        m_q,
-                        logs_q,
-                        pred_pitch,
-                        gt_pitch,
-                        logw,
-                        logw_gt,
-                        log_probs,
-                    ),
-                )
-            else:
-                return (
-                    wav,
-                    z_start_idxs,
-                    x_mask,
-                    y_mask,
-                    (z, z_p, m_p, logs_p, m_q, logs_q, pred_pitch, gt_pitch),
-                )
-
-        else:
-            return (
-                wav,
-                z_start_idxs,
-                x_mask,
-                y_mask,
-                (z, z_p, m_p, logs_p, m_q, logs_q),
-            )
+        return (
+            wav,
+            dur_nll,
+            attn,
+            z_start_idxs,
+            x_mask,
+            y_mask,
+            (z, z_p, m_p, logs_p, m_q, logs_q),
+        )
 
     def inference(
         self,
         text: torch.Tensor,
         text_lengths: torch.Tensor,
         feats: Optional[torch.Tensor] = None,
         feats_lengths: Optional[torch.Tensor] = None,
-        label: Optional[Dict[str, torch.Tensor]] = None,
-        label_lengths: Optional[Dict[str, torch.Tensor]] = None,
-        melody: Optional[Dict[str, torch.Tensor]] = None,
-        melody_lengths: Optional[Dict[str, torch.Tensor]] = None,
-        beat: Optional[Dict[str, torch.Tensor]] = None,
-        beat_lengths: Optional[Dict[str, torch.Tensor]] = None,
-        pitch: Optional[torch.Tensor] = None,
-        pitch_lengths: Optional[torch.Tensor] = None,
-        # duration: Optional[Dict[str, torch.Tensor]] = None,
         sids: Optional[torch.Tensor] = None,
         spembs: Optional[torch.Tensor] = None,
         lids: Optional[torch.Tensor] = None,
+        dur: Optional[torch.Tensor] = None,
         noise_scale: float = 0.667,
         noise_scale_dur: float = 0.8,
         alpha: float = 1.0,
         max_len: Optional[int] = None,
         use_teacher_forcing: bool = False,
     ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
         """Run inference.
 
         Args:
             text (Tensor): Input text index tensor (B, T_text,).
             text_lengths (Tensor): Text length tensor (B,).
             feats (Tensor): Feature tensor (B, aux_channels, T_feats,).
             feats_lengths (Tensor): Feature length tensor (B,).
-            label (Optional[Dict]): key is "lab" or "score";
-                value (LongTensor): Batch of padded label ids (B, Tmax).
-            melody (Optional[Dict]): key is "lab" or "score";
-                value (LongTensor): Batch of padded melody (B, Tmax).
-            beat (Optional[Dict]): key is "lab", "score_phn" or "score_syb";
-                value (LongTensor): Batch of padded beat (B, Tmax).
-            pitch (FloatTensor): Batch of padded f0 (B, Tmax).
             sids (Optional[Tensor]): Speaker index tensor (B,) or (B, 1).
             spembs (Optional[Tensor]): Speaker embedding tensor (B, spk_embed_dim).
             lids (Optional[Tensor]): Language index tensor (B,) or (B, 1).
+            dur (Optional[Tensor]): Ground-truth duration (B, T_text,). If provided,
+                skip the prediction of durations (i.e., teacher forcing).
             noise_scale (float): Noise scale parameter for flow.
             noise_scale_dur (float): Noise scale parameter for duration predictor.
             alpha (float): Alpha parameter to control the speed of generated speech.
             max_len (Optional[int]): Maximum length of acoustic feature sequence.
             use_teacher_forcing (bool): Whether to use teacher forcing.
 
         Returns:
             Tensor: Generated waveform tensor (B, T_wav).
+            Tensor: Monotonic attention weight tensor (B, T_feats, T_text).
+            Tensor: Duration tensor (B, T_text).
 
         """
         # encoder
-        if self.use_dp:
-            x, m_p, logs_p, x_mask = self.text_encoder(
-                label, label_lengths, melody, beat
-            )
-        else:
-            x, m_p, logs_p, x_mask = self.text_encoder(
-                label, label_lengths, melody, beat
-            )
+        x, m_p, logs_p, x_mask = self.text_encoder(text, text_lengths)
         g = None
         if self.spks is not None:
             # (B, global_channels, 1)
             g = self.global_emb(sids.view(-1)).unsqueeze(-1)
         if self.spk_embed_dim is not None:
             # (B, global_channels, 1)
             g_ = self.spemb_proj(F.normalize(spembs.unsqueeze(0))).unsqueeze(-1)
@@ -573,36 +472,103 @@
         if use_teacher_forcing:
             # forward posterior encoder
             z, m_q, logs_q, y_mask = self.posterior_encoder(feats, feats_lengths, g=g)
 
             # forward flow
             z_p = self.flow(z, y_mask, g=g)  # (B, H, T_feats)
 
+            # monotonic alignment search
+            s_p_sq_r = torch.exp(-2 * logs_p)  # (B, H, T_text)
+            # (B, 1, T_text)
+            neg_x_ent_1 = torch.sum(
+                -0.5 * math.log(2 * math.pi) - logs_p,
+                [1],
+                keepdim=True,
+            )
+            # (B, T_feats, H) x (B, H, T_text) = (B, T_feats, T_text)
+            neg_x_ent_2 = torch.matmul(
+                -0.5 * (z_p**2).transpose(1, 2),
+                s_p_sq_r,
+            )
+            # (B, T_feats, H) x (B, H, T_text) = (B, T_feats, T_text)
+            neg_x_ent_3 = torch.matmul(
+                z_p.transpose(1, 2),
+                (m_p * s_p_sq_r),
+            )
+            # (B, 1, T_text)
+            neg_x_ent_4 = torch.sum(
+                -0.5 * (m_p**2) * s_p_sq_r,
+                [1],
+                keepdim=True,
+            )
+            # (B, T_feats, T_text)
+            neg_x_ent = neg_x_ent_1 + neg_x_ent_2 + neg_x_ent_3 + neg_x_ent_4
+            # (B, 1, T_feats, T_text)
+            attn_mask = torch.unsqueeze(x_mask, 2) * torch.unsqueeze(y_mask, -1)
+            # monotonic attention weight: (B, 1, T_feats, T_text)
+            attn = self.maximum_path(
+                neg_x_ent,
+                attn_mask.squeeze(1),
+            ).unsqueeze(1)
+            dur = attn.sum(2)  # (B, 1, T_text)
+
             # forward decoder with random segments
             wav = self.decoder(z * y_mask, g=g)
         else:
-            if self.use_visinger:
-                if self.use_dp:
-                    logw = self.duration_predictor(x, x_mask, beat, g=g)
-                    logw = (torch.exp(logw) - 1) * x_mask
-                    logw = torch.mul(logw.squeeze(1), beat).unsqueeze(1)
-                    logw[logw < 0] = 0
-                    logw = logw.squeeze(1).to(torch.long)
-
-                    x, frame_pitch, x_lengths = self.lr(x, melody, logw, label_lengths)
-                    x_mask = torch.unsqueeze(sequence_mask(x_lengths, x.size(2)), 1)
-
-                self.pos_encoder = PositionalEncoding(
-                    d_model=x.size(1), dropout_rate=0, max_len=x.size(2)
+            # duration
+            if dur is None:
+                logw = self.duration_predictor(
+                    x,
+                    x_mask,
+                    g=g,
+                    inverse=True,
+                    noise_scale=noise_scale_dur,
                 )
-                x = self.pos_encoder(x.transpose(1, 2)).transpose(1, 2)
-
-                _, pitch_embedding = self.pitch_predictor(x, x_mask)
-                x = self.frame_prior_net(x, pitch_embedding, x_mask)
-                m_p, logs_p = self.project(x, x_mask)
+                w = torch.exp(logw) * x_mask * alpha
+                dur = torch.ceil(w)
+            y_lengths = torch.clamp_min(torch.sum(dur, [1, 2]), 1).long()
+            y_mask = make_non_pad_mask(y_lengths).unsqueeze(1).to(text.device)
+            attn_mask = torch.unsqueeze(x_mask, 2) * torch.unsqueeze(y_mask, -1)
+            attn = self._generate_path(dur, attn_mask)
+
+            # expand the length to match with the feature sequence
+            # (B, T_feats, T_text) x (B, T_text, H) -> (B, H, T_feats)
+            m_p = torch.matmul(
+                attn.squeeze(1),
+                m_p.transpose(1, 2),
+            ).transpose(1, 2)
+            # (B, T_feats, T_text) x (B, T_text, H) -> (B, H, T_feats)
+            logs_p = torch.matmul(
+                attn.squeeze(1),
+                logs_p.transpose(1, 2),
+            ).transpose(1, 2)
 
             # decoder
             z_p = m_p + torch.randn_like(m_p) * torch.exp(logs_p) * noise_scale
-            z = self.flow(z_p, x_mask, g=g, inverse=True)
-            wav = self.decoder((z * x_mask)[:, :, :max_len], g=g)
+            z = self.flow(z_p, y_mask, g=g, inverse=True)
+            wav = self.decoder((z * y_mask)[:, :, :max_len], g=g)
+
+        return wav.squeeze(1), attn.squeeze(1), dur.squeeze(1)
+
+    def _generate_path(self, dur: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:
+        """Generate path a.k.a. monotonic attention.
 
-        return wav.squeeze(1)
+        Args:
+            dur (Tensor): Duration tensor (B, 1, T_text).
+            mask (Tensor): Attention mask tensor (B, 1, T_feats, T_text).
+
+        Returns:
+            Tensor: Path tensor (B, 1, T_feats, T_text).
+
+        """
+        b, _, t_y, t_x = mask.shape
+        cum_dur = torch.cumsum(dur, -1)
+        cum_dur_flat = cum_dur.view(b * t_x)
+        path = torch.arange(t_y, dtype=dur.dtype, device=dur.device)
+        path = path.unsqueeze(0) < cum_dur_flat.unsqueeze(1)
+        path = path.view(b, t_x, t_y).to(dtype=mask.dtype)
+        # path will be like (t_x = 3, t_y = 5):
+        # [[[1., 1., 0., 0., 0.],      [[[1., 1., 0., 0., 0.],
+        #   [1., 1., 1., 1., 0.],  -->   [0., 0., 1., 1., 0.],
+        #   [1., 1., 1., 1., 1.]]]       [0., 0., 0., 0., 1.]]]
+        path = path - F.pad(path, [0, 0, 1, 0, 0, 0])[:, :-1]
+        return path.unsqueeze(1).transpose(2, 3) * mask
```

### Comparing `espnet-202304/espnet2/gan_svs/vits/modules.py` & `espnet-202308/espnet2/gan_svs/vits/modules.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/gan_svs/vits/text_encoder.py` & `espnet-202308/espnet2/gan_svs/vits/text_encoder.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,28 +1,29 @@
 # Copyright 2021 Tomoki Hayashi
 # Copyright 2022 Yifeng Yu
 #  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)
 
-"""Text encoder module in VITS.
+"""Text encoder module in VISinger.
 
-This code is based on https://github.com/jaywalnut310/vits.
+This code is based on https://github.com/jaywalnut310/vits
+and https://github.com/zhangyongmao/VISinger2.
 
 """
 
 import math
-from typing import Tuple
+from typing import Optional, Tuple
 
 import torch
 
 from espnet.nets.pytorch_backend.conformer.encoder import Encoder
 from espnet.nets.pytorch_backend.nets_utils import make_non_pad_mask
 
 
 class TextEncoder(torch.nn.Module):
-    """Text encoder module in VITS.
+    """Text encoder module in VISinger.
 
     This is a module of text encoder described in `Conditional Variational Autoencoder
     with Adversarial Learning for End-to-End Text-to-Speech`_.
 
     Instead of the relative positional Transformer, we use conformer architecture as
     the encoder module, which contains additional convolution layers.
 
@@ -46,17 +47,15 @@
         normalize_before: bool = True,
         use_macaron_style: bool = False,
         use_conformer_conv: bool = False,
         conformer_kernel_size: int = 7,
         dropout_rate: float = 0.1,
         positional_dropout_rate: float = 0.0,
         attention_dropout_rate: float = 0.0,
-        midi_dim: int = 129,
-        beat_dim: int = 600,
-        use_visinger: bool = True,
+        use_slur=True,
     ):
         """Initialize TextEncoder module.
 
         Args:
             vocabs (int): Vocabulary size.
             attention_dim (int): Attention dimension.
             attention_heads (int): Number of attention heads.
@@ -70,23 +69,22 @@
             normalize_before (bool): Whether to apply LayerNorm before attention.
             use_macaron_style (bool): Whether to use macaron style components.
             use_conformer_conv (bool): Whether to use conformer conv layers.
             conformer_kernel_size (int): Conformer's conv kernel size.
             dropout_rate (float): Dropout rate.
             positional_dropout_rate (float): Dropout rate for positional encoding.
             attention_dropout_rate (float): Dropout rate for attention.
+            use_slur (bool): Whether to use slur embedding.
 
         """
         super().__init__()
         # store for forward
         self.attention_dim = attention_dim
 
         # define modules
-        self.emb = torch.nn.Embedding(vocabs, attention_dim)
-        torch.nn.init.normal_(self.emb.weight, 0.0, attention_dim**-0.5)
         self.encoder = Encoder(
             idim=-1,
             input_layer=None,
             attention_dim=attention_dim,
             attention_heads=attention_heads,
             linear_units=linear_units,
             num_blocks=blocks,
@@ -99,57 +97,96 @@
             macaron_style=use_macaron_style,
             pos_enc_layer_type=positional_encoding_layer_type,
             selfattention_layer_type=self_attention_layer_type,
             activation_type=activation_type,
             use_cnn_module=use_conformer_conv,
             cnn_module_kernel=conformer_kernel_size,
         )
-        self.proj = torch.nn.Conv1d(attention_dim, attention_dim * 2, 1)
-        self.pitch_embedding = torch.nn.Embedding(midi_dim, attention_dim)
-        self.beat_embedding = torch.nn.Embedding(beat_dim, attention_dim)
-        self.use_visinger = use_visinger
+        self.emb_phone_dim = 256
+        self.emb_phone = torch.nn.Embedding(vocabs, self.emb_phone_dim)
+        torch.nn.init.normal_(self.emb_phone.weight, 0.0, self.emb_phone_dim**-0.5)
+
+        self.emb_pitch_dim = 128
+        self.emb_pitch = torch.nn.Embedding(
+            129, self.emb_pitch_dim
+        )  # Should we count the number of midis instead of 129?
+        torch.nn.init.normal_(self.emb_pitch.weight, 0.0, self.emb_pitch_dim**-0.5)
+
+        if use_slur:
+            self.emb_slur = torch.nn.Embedding(2, 64)
+            torch.nn.init.normal_(self.emb_slur.weight, 0.0, 64**-0.5)
+
+        if use_slur:
+            self.emb_dur = torch.nn.Linear(1, 64)
+        else:
+            self.emb_dur = torch.nn.Linear(1, 128)
+
+        self.pre_net = torch.nn.Linear(512, attention_dim)
+        self.pre_dur_net = torch.nn.Linear(512, attention_dim)
+
+        self.proj = torch.nn.Conv1d(attention_dim, attention_dim, 1)
+        self.proj_pitch = torch.nn.Conv1d(self.emb_pitch_dim, attention_dim, 1)
 
     def forward(
         self,
-        x: torch.Tensor,
-        x_lengths: torch.Tensor,
-        note_pitch: torch.Tensor,
-        note_beat: torch.Tensor,
+        phone: torch.Tensor,
+        phone_lengths: torch.Tensor,
+        midi_id: torch.Tensor,
+        dur: torch.Tensor,
+        slur: Optional[torch.Tensor] = None,
     ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
         """Calculate forward propagation.
 
         Args:
-            x (Tensor): Input index tensor (B, T_text).
-            x_lengths (Tensor): Length tensor (B,).
+            phone (Tensor): Input index tensor (B, T_text).
+            phone_lengths (Tensor): Length tensor (B,).
+            midi_id (Tensor): Input midi tensor (B, T_text).
+            dur (Tensor): Input duration tensor (B, T_text).
 
         Returns:
             Tensor: Encoded hidden representation (B, attention_dim, T_text).
-            Tensor: Projected mean tensor (B, attention_dim, T_text).
-            Tensor: Projected scale tensor (B, attention_dim, T_text).
-            Tensor: Mask tensor for input tensor (B, 1, T_text).
+            Tensor: Mask tensor for padded part (B, 1, T_text).
+            Tensor: Encoded hidden representation for duration
+                (B, attention_dim, T_text).
+            Tensor: Encoded hidden representation for pitch
+                (B, attention_dim, T_text).
 
         """
-        x = self.emb(x) * math.sqrt(self.attention_dim)
-        note_pitch = self.pitch_embedding(note_pitch)
-        # note_ds = self.beat_embedding(ds)
-        note_beat = self.beat_embedding(note_beat)
-        x = x + note_pitch + note_beat
+        phone_end = self.emb_phone(phone) * math.sqrt(self.emb_phone_dim)
+        pitch_end = self.emb_pitch(midi_id) * math.sqrt(self.emb_pitch_dim)
+
+        if slur is not None:
+            slur_end = self.emb_slur(slur) * math.sqrt(64)
+
+        dur = dur.float()
+        dur_end = self.emb_dur(dur.unsqueeze(-1))
+
+        if slur is not None:
+            x = torch.cat([phone_end, pitch_end, slur_end, dur_end], dim=-1)
+        else:
+            x = torch.cat([phone_end, pitch_end, dur_end], dim=-1)
+
+        dur_input = self.pre_dur_net(x)
+        dur_input = torch.transpose(dur_input, 1, -1)
+
+        x = self.pre_net(x)
+        # x = torch.transpose(x, 1, -1)  # [b, h, t]
+
         x_mask = (
-            make_non_pad_mask(x_lengths)
+            make_non_pad_mask(phone_lengths)
             .to(
                 device=x.device,
                 dtype=x.dtype,
             )
             .unsqueeze(1)
         )
         # encoder assume the channel last (B, T_text, attention_dim)
         # but mask shape shoud be (B, 1, T_text)
         x, _ = self.encoder(x, x_mask)
 
-        # convert the channel first (B, attention_dim, T_text)
+        # convert the channel first to (B, attention_dim, T_text)
         x = x.transpose(1, 2)
-        if not self.use_visinger:
-            stats = self.proj(x) * x_mask
-            m, logs = stats.split(stats.size(1) // 2, dim=1)
-            return x, m, logs, x_mask
-        else:
-            return x, None, None, x_mask
+        x = self.proj(x) * x_mask
+
+        pitch_info = self.proj_pitch(pitch_end.transpose(1, 2))
+
+        return x, x_mask, dur_input, pitch_info
```

### Comparing `espnet-202304/espnet2/gan_svs/vits/vits.py` & `espnet-202308/espnet2/gan_tts/jets/jets.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,123 +1,149 @@
-# Copyright 2021 Tomoki Hayashi
-# Copyright 2022 Yifeng Yu
+# Copyright 2022 Dan Lim
 #  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)
 
-"""VITS/VISinger module for GAN-SVS task."""
+"""JETS module for GAN-TTS task."""
 
-from contextlib import contextmanager
-from distutils.version import LooseVersion
 from typing import Any, Dict, Optional
 
 import torch
 from typeguard import check_argument_types
 
-from espnet2.gan_svs.abs_gan_svs import AbsGANSVS
-from espnet2.gan_svs.vits.generator import VITSGenerator
+from espnet2.gan_tts.abs_gan_tts import AbsGANTTS
 from espnet2.gan_tts.hifigan import (
     HiFiGANMultiPeriodDiscriminator,
     HiFiGANMultiScaleDiscriminator,
     HiFiGANMultiScaleMultiPeriodDiscriminator,
     HiFiGANPeriodDiscriminator,
     HiFiGANScaleDiscriminator,
 )
 from espnet2.gan_tts.hifigan.loss import (
     DiscriminatorAdversarialLoss,
     FeatureMatchLoss,
     GeneratorAdversarialLoss,
     MelSpectrogramLoss,
 )
+from espnet2.gan_tts.jets.generator import JETSGenerator
+from espnet2.gan_tts.jets.loss import ForwardSumLoss, VarianceLoss
 from espnet2.gan_tts.utils import get_segments
-from espnet2.gan_tts.vits.loss import KLDivergenceLoss
 from espnet2.torch_utils.device_funcs import force_gatherable
 
 AVAILABLE_GENERATERS = {
-    "vits_generator": VITSGenerator,
+    "jets_generator": JETSGenerator,
 }
 AVAILABLE_DISCRIMINATORS = {
     "hifigan_period_discriminator": HiFiGANPeriodDiscriminator,
     "hifigan_scale_discriminator": HiFiGANScaleDiscriminator,
     "hifigan_multi_period_discriminator": HiFiGANMultiPeriodDiscriminator,
     "hifigan_multi_scale_discriminator": HiFiGANMultiScaleDiscriminator,
     "hifigan_multi_scale_multi_period_discriminator": HiFiGANMultiScaleMultiPeriodDiscriminator,  # NOQA
 }
 
-if LooseVersion(torch.__version__) >= LooseVersion("1.6.0"):
-    from torch.cuda.amp import autocast
-else:
-    # Nothing to do if torch<1.6.0
-    @contextmanager
-    def autocast(enabled=True):  # NOQA
-        yield
 
+class JETS(AbsGANTTS):
+    """JETS module (generator + discriminator).
 
-class VITS(AbsGANSVS):
-    """VITS module (generator + discriminator).
+    This is a module of JETS described in `JETS: Jointly Training FastSpeech2
+    and HiFi-GAN for End to End Text to Speech'_.
 
-    This is a module of VITS described in `Conditional Variational Autoencoder
-    with Adversarial Learning for End-to-End Text-to-Speech`_.
-
-    .. _`Conditional Variational Autoencoder with Adversarial Learning for End-to-End
-        Text-to-Speech`: https://arxiv.org/abs/2006.04558
+    .. _`JETS: Jointly Training FastSpeech2 and HiFi-GAN for End to End Text to Speech`
+        : https://arxiv.org/abs/2203.16852
 
     """
 
     def __init__(
         self,
         # generator related
         idim: int,
         odim: int,
         sampling_rate: int = 22050,
-        generator_type: str = "vits_generator",
-        use_visinger: bool = True,
-        use_dp: bool = True,
+        generator_type: str = "jets_generator",
         generator_params: Dict[str, Any] = {
-            "midi_dim": 129,
-            "midi_embed_integration_type": "add",
-            "hidden_channels": 192,
-            "spks": None,
-            "langs": None,
+            "adim": 256,
+            "aheads": 2,
+            "elayers": 4,
+            "eunits": 1024,
+            "dlayers": 4,
+            "dunits": 1024,
+            "positionwise_layer_type": "conv1d",
+            "positionwise_conv_kernel_size": 1,
+            "use_scaled_pos_enc": True,
+            "use_batch_norm": True,
+            "encoder_normalize_before": True,
+            "decoder_normalize_before": True,
+            "encoder_concat_after": False,
+            "decoder_concat_after": False,
+            "reduction_factor": 1,
+            "encoder_type": "transformer",
+            "decoder_type": "transformer",
+            "transformer_enc_dropout_rate": 0.1,
+            "transformer_enc_positional_dropout_rate": 0.1,
+            "transformer_enc_attn_dropout_rate": 0.1,
+            "transformer_dec_dropout_rate": 0.1,
+            "transformer_dec_positional_dropout_rate": 0.1,
+            "transformer_dec_attn_dropout_rate": 0.1,
+            "conformer_rel_pos_type": "latest",
+            "conformer_pos_enc_layer_type": "rel_pos",
+            "conformer_self_attn_layer_type": "rel_selfattn",
+            "conformer_activation_type": "swish",
+            "use_macaron_style_in_conformer": True,
+            "use_cnn_in_conformer": True,
+            "zero_triu": False,
+            "conformer_enc_kernel_size": 7,
+            "conformer_dec_kernel_size": 31,
+            "duration_predictor_layers": 2,
+            "duration_predictor_chans": 384,
+            "duration_predictor_kernel_size": 3,
+            "duration_predictor_dropout_rate": 0.1,
+            "energy_predictor_layers": 2,
+            "energy_predictor_chans": 384,
+            "energy_predictor_kernel_size": 3,
+            "energy_predictor_dropout": 0.5,
+            "energy_embed_kernel_size": 1,
+            "energy_embed_dropout": 0.5,
+            "stop_gradient_from_energy_predictor": False,
+            "pitch_predictor_layers": 5,
+            "pitch_predictor_chans": 384,
+            "pitch_predictor_kernel_size": 5,
+            "pitch_predictor_dropout": 0.5,
+            "pitch_embed_kernel_size": 1,
+            "pitch_embed_dropout": 0.5,
+            "stop_gradient_from_pitch_predictor": True,
+            "generator_out_channels": 1,
+            "generator_channels": 512,
+            "generator_global_channels": -1,
+            "generator_kernel_size": 7,
+            "generator_upsample_scales": [8, 8, 2, 2],
+            "generator_upsample_kernel_sizes": [16, 16, 4, 4],
+            "generator_resblock_kernel_sizes": [3, 7, 11],
+            "generator_resblock_dilations": [[1, 3, 5], [1, 3, 5], [1, 3, 5]],
+            "generator_use_additional_convs": True,
+            "generator_bias": True,
+            "generator_nonlinear_activation": "LeakyReLU",
+            "generator_nonlinear_activation_params": {"negative_slope": 0.1},
+            "generator_use_weight_norm": True,
+            "segment_size": 64,
+            "spks": -1,
+            "langs": -1,
             "spk_embed_dim": None,
-            "global_channels": -1,
-            "segment_size": 32,
-            "text_encoder_attention_heads": 2,
-            "text_encoder_ffn_expand": 4,
-            "text_encoder_blocks": 6,
-            "text_encoder_positionwise_layer_type": "conv1d",
-            "text_encoder_positionwise_conv_kernel_size": 1,
-            "text_encoder_positional_encoding_layer_type": "rel_pos",
-            "text_encoder_self_attention_layer_type": "rel_selfattn",
-            "text_encoder_activation_type": "swish",
-            "text_encoder_normalize_before": True,
-            "text_encoder_dropout_rate": 0.1,
-            "text_encoder_positional_dropout_rate": 0.0,
-            "text_encoder_attention_dropout_rate": 0.0,
-            "text_encoder_conformer_kernel_size": 7,
-            "use_macaron_style_in_text_encoder": True,
-            "use_conformer_conv_in_text_encoder": True,
-            "decoder_kernel_size": 7,
-            "decoder_channels": 512,
-            "decoder_upsample_scales": [8, 8, 2, 2],
-            "decoder_upsample_kernel_sizes": [16, 16, 4, 4],
-            "decoder_resblock_kernel_sizes": [3, 7, 11],
-            "decoder_resblock_dilations": [[1, 3, 5], [1, 3, 5], [1, 3, 5]],
-            "use_weight_norm_in_decoder": True,
-            "posterior_encoder_kernel_size": 5,
-            "posterior_encoder_layers": 16,
-            "posterior_encoder_stacks": 1,
-            "posterior_encoder_base_dilation": 1,
-            "posterior_encoder_dropout_rate": 0.0,
-            "use_weight_norm_in_posterior_encoder": True,
-            "flow_flows": 4,
-            "flow_kernel_size": 5,
-            "flow_base_dilation": 1,
-            "flow_layers": 4,
-            "flow_dropout_rate": 0.0,
-            "use_weight_norm_in_flow": True,
-            "use_only_mean_in_flow": True,
+            "spk_embed_integration_type": "add",
+            "use_gst": False,
+            "gst_tokens": 10,
+            "gst_heads": 4,
+            "gst_conv_layers": 6,
+            "gst_conv_chans_list": [32, 32, 64, 64, 128, 128],
+            "gst_conv_kernel_size": 3,
+            "gst_conv_stride": 2,
+            "gst_gru_layers": 1,
+            "gst_gru_units": 128,
+            "init_type": "xavier_uniform",
+            "init_enc_alpha": 1.0,
+            "init_dec_alpha": 1.0,
+            "use_masking": False,
+            "use_weighted_masking": False,
         },
         # discriminator related
         discriminator_type: str = "hifigan_multi_scale_multi_period_discriminator",
         discriminator_params: Dict[str, Any] = {
             "scales": 1,
             "scale_downsample_pooling": "AvgPool1d",
             "scale_downsample_pooling_params": {
@@ -179,26 +205,24 @@
             "fmin": 0,
             "fmax": None,
             "log_base": None,
         },
         lambda_adv: float = 1.0,
         lambda_mel: float = 45.0,
         lambda_feat_match: float = 2.0,
-        lambda_dur: float = 1.0,
-        lambda_kl: float = 1.0,
-        lambda_pitch: float = 1.0,
-        lambda_phoneme: float = 1.0,
+        lambda_var: float = 1.0,
+        lambda_align: float = 2.0,
         cache_generator_outputs: bool = True,
     ):
-        """Initialize VITS module.
+        """Initialize JETS module.
 
         Args:
             idim (int): Input vocabrary size.
             odim (int): Acoustic feature dimension. The actual output channels will
-                be 1 since VITS is the end-to-end text-to-wave model but for the
+                be 1 since JETS is the end-to-end text-to-wave model but for the
                 compatibility odim is used to indicate the acoustic feature dimension.
             sampling_rate (int): Sampling rate, not used for the training but it will
                 be referred in saving waveform during the inference.
             generator_type (str): Generator type.
             generator_params (Dict[str, Any]): Parameter dict for generator.
             discriminator_type (str): Discriminator type.
             discriminator_params (Dict[str, Any]): Parameter dict for discriminator.
@@ -207,34 +231,25 @@
             discriminator_adv_loss_params (Dict[str, Any]): Parameter dict for
                 discriminator adversarial loss.
             feat_match_loss_params (Dict[str, Any]): Parameter dict for feat match loss.
             mel_loss_params (Dict[str, Any]): Parameter dict for mel loss.
             lambda_adv (float): Loss scaling coefficient for adversarial loss.
             lambda_mel (float): Loss scaling coefficient for mel spectrogram loss.
             lambda_feat_match (float): Loss scaling coefficient for feat match loss.
-            lambda_dur (float): Loss scaling coefficient for duration loss.
-            lambda_kl (float): Loss scaling coefficient for KL divergence loss.
+            lambda_var (float): Loss scaling coefficient for variance loss.
+            lambda_align (float): Loss scaling coefficient for alignment loss.
             cache_generator_outputs (bool): Whether to cache generator outputs.
 
         """
         assert check_argument_types()
         super().__init__()
 
         # define modules
         generator_class = AVAILABLE_GENERATERS[generator_type]
-        if generator_type == "vits_generator":
-            # NOTE(kan-bayashi): Update parameters for the compatibility.
-            #   The idim and odim is automatically decided from input data,
-            #   where idim represents #vocabularies and odim represents
-            #   the input acoustic feature dimension.
-            generator_params.update(vocabs=idim, aux_channels=odim)
-        self.use_visinger = use_visinger
-        self.use_dp = use_dp
-        generator_params.update(use_visinger=self.use_visinger)
-        generator_params.update(use_dp=self.use_dp)
+        generator_params.update(idim=idim, odim=odim)
         self.generator = generator_class(
             **generator_params,
         )
         discriminator_class = AVAILABLE_DISCRIMINATORS[discriminator_type]
         self.discriminator = discriminator_class(
             **discriminator_params,
         )
@@ -246,345 +261,227 @@
         )
         self.feat_match_loss = FeatureMatchLoss(
             **feat_match_loss_params,
         )
         self.mel_loss = MelSpectrogramLoss(
             **mel_loss_params,
         )
-        self.kl_loss = KLDivergenceLoss()
-        self.ctc_loss = torch.nn.CTCLoss(idim - 1, reduction="mean")
-        self.mse_loss = torch.nn.MSELoss()
+        self.var_loss = VarianceLoss()
+        self.forwardsum_loss = ForwardSumLoss()
 
         # coefficients
         self.lambda_adv = lambda_adv
         self.lambda_mel = lambda_mel
-        self.lambda_kl = lambda_kl
         self.lambda_feat_match = lambda_feat_match
-        self.lambda_dur = lambda_dur
-        self.lambda_pitch = lambda_pitch
-        self.lambda_phoneme = lambda_phoneme
+        self.lambda_var = lambda_var
+        self.lambda_align = lambda_align
 
         # cache
         self.cache_generator_outputs = cache_generator_outputs
         self._cache = None
 
         # store sampling rate for saving wav file
         # (not used for the training)
         self.fs = sampling_rate
 
         # store parameters for test compatibility
         self.spks = self.generator.spks
         self.langs = self.generator.langs
         self.spk_embed_dim = self.generator.spk_embed_dim
+        self.use_gst = getattr(self.generator, "use_gst", False)
 
     @property
-    def require_raw_singing(self):
-        """Return whether or not singing is required."""
+    def require_raw_speech(self):
+        """Return whether or not speech is required."""
         return True
 
     @property
     def require_vocoder(self):
         """Return whether or not vocoder is required."""
         return False
 
     def forward(
         self,
         text: torch.Tensor,
         text_lengths: torch.Tensor,
         feats: torch.Tensor,
         feats_lengths: torch.Tensor,
-        singing: torch.Tensor,
-        singing_lengths: torch.Tensor,
-        label: Optional[Dict[str, torch.Tensor]] = None,
-        label_lengths: Optional[Dict[str, torch.Tensor]] = None,
-        melody: Optional[Dict[str, torch.Tensor]] = None,
-        melody_lengths: Optional[Dict[str, torch.Tensor]] = None,
-        pitch: torch.LongTensor = None,
-        pitch_lengths: torch.Tensor = None,
-        duration: Optional[Dict[str, torch.Tensor]] = None,
-        duration_lengths: Optional[Dict[str, torch.Tensor]] = None,
-        slur: torch.LongTensor = None,
-        slur_lengths: torch.Tensor = None,
-        spembs: Optional[torch.Tensor] = None,
+        speech: torch.Tensor,
+        speech_lengths: torch.Tensor,
         sids: Optional[torch.Tensor] = None,
+        spembs: Optional[torch.Tensor] = None,
         lids: Optional[torch.Tensor] = None,
         forward_generator: bool = True,
+        **kwargs,
     ) -> Dict[str, Any]:
         """Perform generator forward.
 
         Args:
-            text (LongTensor): Batch of padded character ids (B, Tmax).
-            text_lengths (LongTensor): Batch of lengths of each input batch (B,).
-            feats (Tensor): Batch of padded target features (B, Lmax, odim).
-            feats_lengths (LongTensor): Batch of the lengths of each target (B,).
-            singing (Tensor): Singing waveform tensor (B, T_wav).
-            singing_lengths (Tensor): Singing length tensor (B,).
-            label (Optional[Dict]): key is "lab" or "score";
-                value (LongTensor): Batch of padded label ids (B, Tmax).
-            label_lengths (Optional[Dict]): key is "lab" or "score";
-                value (LongTensor): Batch of the lengths of padded label ids (B, ).
-            melody (Optional[Dict]): key is "lab" or "score";
-                value (LongTensor): Batch of padded melody (B, Tmax).
-            melody_lengths (Optional[Dict]): key is "lab" or "score";
-                value (LongTensor): Batch of the lengths of padded melody (B, ).
-            pitch (FloatTensor): Batch of padded f0 (B, Tmax).
-            pitch_lengths (LongTensor): Batch of the lengths of padded f0 (B, ).
-            duration (Optional[Dict]): key is "lab", "score_phn" or "score_syb";
-                value (LongTensor): Batch of padded duration (B, Tmax).
-            duration_length (Optional[Dict]): key is "lab", "score_phn" or "score_syb";
-                value (LongTensor): Batch of the lengths of padded duration (B, ).
-            slur (FloatTensor): Batch of padded slur (B, Tmax).
-            slur_lengths (LongTensor): Batch of the lengths of padded slur (B, ).
-            spembs (Optional[Tensor]): Batch of speaker embeddings (B, spk_embed_dim).
-            sids (Optional[Tensor]): Batch of speaker IDs (B, 1).
-            lids (Optional[Tensor]): Batch of language IDs (B, 1).
+            text (Tensor): Text index tensor (B, T_text).
+            text_lengths (Tensor): Text length tensor (B,).
+            feats (Tensor): Feature tensor (B, T_feats, aux_channels).
+            feats_lengths (Tensor): Feature length tensor (B,).
+            speech (Tensor): Speech waveform tensor (B, T_wav).
+            speech_lengths (Tensor): Speech length tensor (B,).
+            sids (Optional[Tensor]): Speaker index tensor (B,) or (B, 1).
+            spembs (Optional[Tensor]): Speaker embedding tensor (B, spk_embed_dim).
+            lids (Optional[Tensor]): Language index tensor (B,) or (B, 1).
             forward_generator (bool): Whether to forward generator.
 
         Returns:
             Dict[str, Any]:
                 - loss (Tensor): Loss scalar tensor.
                 - stats (Dict[str, float]): Statistics to be monitored.
                 - weight (Tensor): Weight tensor to summarize losses.
                 - optim_idx (int): Optimizer index (0 for G and 1 for D).
 
         """
-        beat = duration["score_syb"]
-        beat_lengths = duration_lengths["score_syb"]
-        duration = duration["lab"]
-        label = label["lab"]
-        label_lengths = label_lengths["lab"]
-        melody = melody["lab"]
-        melody_lengths = melody_lengths["lab"]
-
         if forward_generator:
             return self._forward_generator(
                 text=text,
                 text_lengths=text_lengths,
                 feats=feats,
                 feats_lengths=feats_lengths,
-                singing=singing,
-                singing_lengths=singing_lengths,
-                duration=duration,
-                label=label,
-                label_lengths=label_lengths,
-                melody=melody,
-                melody_lengths=melody_lengths,
-                beat=beat,
-                beat_lengths=beat_lengths,
-                pitch=pitch,
-                pitch_lengths=pitch_lengths,
+                speech=speech,
+                speech_lengths=speech_lengths,
                 sids=sids,
                 spembs=spembs,
                 lids=lids,
+                **kwargs,
             )
         else:
             return self._forward_discrminator(
                 text=text,
                 text_lengths=text_lengths,
                 feats=feats,
                 feats_lengths=feats_lengths,
-                singing=singing,
-                singing_lengths=singing_lengths,
-                duration=duration,
-                label=label,
-                label_lengths=label_lengths,
-                melody=melody,
-                melody_lengths=melody_lengths,
-                beat=beat,
-                beat_lengths=beat_lengths,
-                pitch=pitch,
-                pitch_lengths=pitch_lengths,
+                speech=speech,
+                speech_lengths=speech_lengths,
                 sids=sids,
                 spembs=spembs,
                 lids=lids,
+                **kwargs,
             )
 
     def _forward_generator(
         self,
         text: torch.Tensor,
         text_lengths: torch.Tensor,
         feats: torch.Tensor,
         feats_lengths: torch.Tensor,
-        singing: torch.Tensor,
-        singing_lengths: torch.Tensor,
-        duration: torch.Tensor,
-        label: Optional[Dict[str, torch.Tensor]] = None,
-        label_lengths: Optional[Dict[str, torch.Tensor]] = None,
-        melody: Optional[Dict[str, torch.Tensor]] = None,
-        melody_lengths: Optional[Dict[str, torch.Tensor]] = None,
-        beat: Optional[Dict[str, torch.Tensor]] = None,
-        beat_lengths: Optional[Dict[str, torch.Tensor]] = None,
-        pitch: Optional[torch.Tensor] = None,
-        pitch_lengths: Optional[torch.Tensor] = None,
+        speech: torch.Tensor,
+        speech_lengths: torch.Tensor,
         sids: Optional[torch.Tensor] = None,
         spembs: Optional[torch.Tensor] = None,
         lids: Optional[torch.Tensor] = None,
+        **kwargs,
     ) -> Dict[str, Any]:
         """Perform generator forward.
 
         Args:
             text (Tensor): Text index tensor (B, T_text).
             text_lengths (Tensor): Text length tensor (B,).
             feats (Tensor): Feature tensor (B, T_feats, aux_channels).
             feats_lengths (Tensor): Feature length tensor (B,).
-            singing (Tensor): Singing waveform tensor (B, T_wav).
-            singing_lengths (Tensor): Singing length tensor (B,).
-            duration (Optional[Dict]): key is "lab", "score_phn", "score_syb;
-                value (LongTensor): Batch of padded beat (B, Tmax).
-            label (Optional[Dict]): key is "lab" or "score";
-                value (LongTensor): Batch of padded label ids (B, Tmax).
-            label_lengths (Optional[Dict]): key is "lab" or "score";
-                value (LongTensor): Batch of the lengths of padded label ids (B, ).
-            melody (Optional[Dict]): key is "lab" or "score";
-                value (LongTensor): Batch of padded melody (B, Tmax).
-            melody_lengths (Optional[Dict]): key is "lab" or "score";
-                value (LongTensor): Batch of the lengths of padded melody (B, ).
-            pitch (FloatTensor): Batch of padded f0 (B, Tmax).
-            pitch_lengths (LongTensor): Batch of the lengths of padded f0 (B, ).
+            speech (Tensor): Speech waveform tensor (B, T_wav).
+            speech_lengths (Tensor): Speech length tensor (B,).
             sids (Optional[Tensor]): Speaker index tensor (B,) or (B, 1).
             spembs (Optional[Tensor]): Speaker embedding tensor (B, spk_embed_dim).
             lids (Optional[Tensor]): Language index tensor (B,) or (B, 1).
 
         Returns:
             Dict[str, Any]:
                 * loss (Tensor): Loss scalar tensor.
                 * stats (Dict[str, float]): Statistics to be monitored.
                 * weight (Tensor): Weight tensor to summarize losses.
                 * optim_idx (int): Optimizer index (0 for G and 1 for D).
 
         """
         # setup
         batch_size = text.size(0)
-        feats = feats.transpose(1, 2)
-        singing = singing.unsqueeze(1)
+        speech = speech.unsqueeze(1)
 
         # calculate generator outputs
         reuse_cache = True
         if not self.cache_generator_outputs or self._cache is None:
             reuse_cache = False
             outs = self.generator(
                 text=text,
                 text_lengths=text_lengths,
                 feats=feats,
                 feats_lengths=feats_lengths,
-                duration=duration,
-                label=label,
-                label_lengths=label_lengths,
-                melody=melody,
-                melody_lengths=melody_lengths,
-                beat=beat,
-                beat_lengths=beat_lengths,
-                pitch=pitch,
-                pitch_lengths=pitch_lengths,
                 sids=sids,
                 spembs=spembs,
                 lids=lids,
+                **kwargs,
             )
         else:
             outs = self._cache
 
         # store cache
         if self.training and self.cache_generator_outputs and not reuse_cache:
             self._cache = outs
 
         # parse outputs
-        singing_hat_, start_idxs, _, z_mask, outs_ = outs
-        if not self.use_visinger:
-            _, z_p, m_p, logs_p, _, logs_q = outs_
-        else:
-            if self.use_dp:
-                (
-                    _,
-                    z_p,
-                    m_p,
-                    logs_p,
-                    _,
-                    logs_q,
-                    pred_pitch,
-                    gt_pitch,
-                    logw,
-                    logw_gt,
-                    log_probs,
-                ) = outs_
-            else:
-                _, z_p, m_p, logs_p, _, logs_q, pred_pitch, gt_pitch = outs_
-        singing_ = get_segments(
-            x=singing,
+        (
+            speech_hat_,
+            bin_loss,
+            log_p_attn,
+            start_idxs,
+            d_outs,
+            ds,
+            p_outs,
+            ps,
+            e_outs,
+            es,
+        ) = outs
+        speech_ = get_segments(
+            x=speech,
             start_idxs=start_idxs * self.generator.upsample_factor,
             segment_size=self.generator.segment_size * self.generator.upsample_factor,
         )
 
         # calculate discriminator outputs
-        p_hat = self.discriminator(singing_hat_)
+        p_hat = self.discriminator(speech_hat_)
         with torch.no_grad():
             # do not store discriminator gradient in generator turn
-            p = self.discriminator(singing_)
+            p = self.discriminator(speech_)
 
         # calculate losses
-        with autocast(enabled=False):
-            mel_loss = self.mel_loss(singing_hat_, singing_)
-            kl_loss = self.kl_loss(z_p, logs_q, m_p, logs_p, z_mask)
-            adv_loss = self.generator_adv_loss(p_hat)
-            feat_match_loss = self.feat_match_loss(p_hat, p)
-
-            if self.use_visinger:
-                pitch_loss = self.mse_loss(pred_pitch, gt_pitch)
-                if self.use_dp:
-                    ctc_loss = self.ctc_loss(
-                        log_probs, label, feats_lengths, label_lengths
-                    )
-                    dur_loss = self.mse_loss(logw, logw_gt)
-
-            mel_loss = mel_loss * self.lambda_mel
-            kl_loss = kl_loss * self.lambda_kl
-
-            adv_loss = adv_loss * self.lambda_adv
-            feat_match_loss = feat_match_loss * self.lambda_feat_match
-            if self.use_visinger:
-                pitch_loss = pitch_loss * self.lambda_pitch
-                if self.use_dp:
-                    ctc_loss = ctc_loss * self.lambda_phoneme
-                    dur_loss = dur_loss * self.lambda_dur
-
-            loss = mel_loss + kl_loss + adv_loss + feat_match_loss
-            if self.use_visinger:
-                loss += pitch_loss
-            if self.use_dp:
-                loss += dur_loss
-                loss += ctc_loss
-
-        if self.use_visinger:
-            if not self.use_dp:
-                stats = dict(
-                    generator_loss=loss.item(),
-                    generator_mel_loss=mel_loss.item(),
-                    generator_kl_loss=kl_loss.item(),
-                    generator_adv_loss=adv_loss.item(),
-                    generator_feat_match_loss=feat_match_loss.item(),
-                    pitch_loss=pitch_loss.item(),
-                )
-            else:
-                stats = dict(
-                    generator_loss=loss.item(),
-                    generator_mel_loss=mel_loss.item(),
-                    generator_kl_loss=kl_loss.item(),
-                    generator_dur_loss=dur_loss.item(),
-                    generator_adv_loss=adv_loss.item(),
-                    generator_feat_match_loss=feat_match_loss.item(),
-                    pitch_loss=pitch_loss.item(),
-                    ctc_loss=ctc_loss.item(),
-                )
-        else:
-            stats = dict(
-                generator_loss=loss.item(),
-                generator_mel_loss=mel_loss.item(),
-                generator_kl_loss=kl_loss.item(),
-                generator_adv_loss=adv_loss.item(),
-                generator_feat_match_loss=feat_match_loss.item(),
-            )
+        mel_loss = self.mel_loss(speech_hat_, speech_)
+        adv_loss = self.generator_adv_loss(p_hat)
+        feat_match_loss = self.feat_match_loss(p_hat, p)
+        dur_loss, pitch_loss, energy_loss = self.var_loss(
+            d_outs, ds, p_outs, ps, e_outs, es, text_lengths
+        )
+        forwardsum_loss = self.forwardsum_loss(log_p_attn, text_lengths, feats_lengths)
+
+        mel_loss = mel_loss * self.lambda_mel
+        adv_loss = adv_loss * self.lambda_adv
+        feat_match_loss = feat_match_loss * self.lambda_feat_match
+        g_loss = mel_loss + adv_loss + feat_match_loss
+        var_loss = (dur_loss + pitch_loss + energy_loss) * self.lambda_var
+        align_loss = (forwardsum_loss + bin_loss) * self.lambda_align
+
+        loss = g_loss + var_loss + align_loss
+
+        stats = dict(
+            generator_loss=loss.item(),
+            generator_g_loss=g_loss.item(),
+            generator_var_loss=var_loss.item(),
+            generator_align_loss=align_loss.item(),
+            generator_g_mel_loss=mel_loss.item(),
+            generator_g_adv_loss=adv_loss.item(),
+            generator_g_feat_match_loss=feat_match_loss.item(),
+            generator_var_dur_loss=dur_loss.item(),
+            generator_var_pitch_loss=pitch_loss.item(),
+            generator_var_energy_loss=energy_loss.item(),
+            generator_align_forwardsum_loss=forwardsum_loss.item(),
+            generator_align_bin_loss=bin_loss.item(),
+        )
 
         loss, stats, weight = force_gatherable((loss, stats, batch_size), loss.device)
 
         # reset cache
         if reuse_cache or not self.training:
             self._cache = None
 
@@ -597,121 +494,82 @@
 
     def _forward_discrminator(
         self,
         text: torch.Tensor,
         text_lengths: torch.Tensor,
         feats: torch.Tensor,
         feats_lengths: torch.Tensor,
-        singing: torch.Tensor,
-        singing_lengths: torch.Tensor,
-        duration: torch.Tensor,
-        label: Optional[Dict[str, torch.Tensor]] = None,
-        label_lengths: Optional[Dict[str, torch.Tensor]] = None,
-        melody: Optional[Dict[str, torch.Tensor]] = None,
-        melody_lengths: Optional[Dict[str, torch.Tensor]] = None,
-        beat: Optional[Dict[str, torch.Tensor]] = None,
-        beat_lengths: Optional[Dict[str, torch.Tensor]] = None,
-        pitch: Optional[torch.Tensor] = None,
-        pitch_lengths: Optional[torch.Tensor] = None,
+        speech: torch.Tensor,
+        speech_lengths: torch.Tensor,
         sids: Optional[torch.Tensor] = None,
         spembs: Optional[torch.Tensor] = None,
         lids: Optional[torch.Tensor] = None,
+        **kwargs,
     ) -> Dict[str, Any]:
         """Perform discriminator forward.
 
         Args:
             text (Tensor): Text index tensor (B, T_text).
             text_lengths (Tensor): Text length tensor (B,).
             feats (Tensor): Feature tensor (B, T_feats, aux_channels).
             feats_lengths (Tensor): Feature length tensor (B,).
-            singing (Tensor): Singing waveform tensor (B, T_wav).
-            singing_lengths (Tensor): Singing length tensor (B,).
-            duration (Optional[Dict]): key is "phn", "syb";
-                value (LongTensor): Batch of padded beat (B, Tmax).
-            label (Optional[Dict]): key is "lab" or "score";
-                value (LongTensor): Batch of padded label ids (B, Tmax).
-            label_lengths (Optional[Dict]): key is "lab" or "score";
-                value (LongTensor): Batch of the lengths of padded label ids (B, ).
-            melody (Optional[Dict]): key is "lab" or "score";
-                value (LongTensor): Batch of padded melody (B, Tmax).
-            melody_lengths (Optional[Dict]): key is "lab" or "score";
-                value (LongTensor): Batch of the lengths of padded melody (B, ).
-            tempo (Optional[Dict]): key is "lab" or "score";
-                value (LongTensor): Batch of padded tempo (B, Tmax).
-            tempo_lengths (Optional[Dict]):  key is "lab" or "score";
-                value (LongTensor): Batch of the lengths of padded tempo (B, ).
-            beat (Optional[Dict]): key is "lab", "score_phn" or "score_syb";
-                value (LongTensor): Batch of padded beat (B, Tmax).
-            beat_length (Optional[Dict]): key is "lab", "score_phn" or "score_syb";
-                value (LongTensor): Batch of the lengths of padded beat (B, ).
-            pitch (FloatTensor): Batch of padded f0 (B, Tmax).
-            pitch_lengths (LongTensor): Batch of the lengths of padded f0 (B, ).
+            speech (Tensor): Speech waveform tensor (B, T_wav).
+            speech_lengths (Tensor): Speech length tensor (B,).
             sids (Optional[Tensor]): Speaker index tensor (B,) or (B, 1).
             spembs (Optional[Tensor]): Speaker embedding tensor (B, spk_embed_dim).
             lids (Optional[Tensor]): Language index tensor (B,) or (B, 1).
 
         Returns:
             Dict[str, Any]:
                 * loss (Tensor): Loss scalar tensor.
                 * stats (Dict[str, float]): Statistics to be monitored.
                 * weight (Tensor): Weight tensor to summarize losses.
                 * optim_idx (int): Optimizer index (0 for G and 1 for D).
 
         """
         # setup
         batch_size = text.size(0)
-        feats = feats.transpose(1, 2)
-        singing = singing.unsqueeze(1)
+        speech = speech.unsqueeze(1)
 
         # calculate generator outputs
         reuse_cache = True
         if not self.cache_generator_outputs or self._cache is None:
             reuse_cache = False
             outs = self.generator(
                 text=text,
                 text_lengths=text_lengths,
                 feats=feats,
                 feats_lengths=feats_lengths,
-                duration=duration,
-                label=label,
-                label_lengths=label_lengths,
-                melody=melody,
-                melody_lengths=melody_lengths,
-                beat=beat,
-                beat_lengths=beat_lengths,
-                pitch=pitch,
-                pitch_lengths=pitch_lengths,
                 sids=sids,
                 spembs=spembs,
                 lids=lids,
+                **kwargs,
             )
         else:
             outs = self._cache
 
         # store cache
         if self.cache_generator_outputs and not reuse_cache:
             self._cache = outs
 
         # parse outputs
-        # remove dp loss
-        singing_hat_, start_idxs, *_ = outs
-        singing_ = get_segments(
-            x=singing,
+        speech_hat_, _, _, start_idxs, *_ = outs
+        speech_ = get_segments(
+            x=speech,
             start_idxs=start_idxs * self.generator.upsample_factor,
             segment_size=self.generator.segment_size * self.generator.upsample_factor,
         )
 
         # calculate discriminator outputs
-        p_hat = self.discriminator(singing_hat_.detach())
-        p = self.discriminator(singing_)
+        p_hat = self.discriminator(speech_hat_.detach())
+        p = self.discriminator(speech_)
 
         # calculate losses
-        with autocast(enabled=False):
-            real_loss, fake_loss = self.discriminator_adv_loss(p_hat, p)
-            loss = real_loss + fake_loss
+        real_loss, fake_loss = self.discriminator_adv_loss(p_hat, p)
+        loss = real_loss + fake_loss
 
         stats = dict(
             discriminator_loss=loss.item(),
             discriminator_real_loss=real_loss.item(),
             discriminator_fake_loss=fake_loss.item(),
         )
         loss, stats, weight = force_gatherable((loss, stats, batch_size), loss.device)
@@ -727,114 +585,72 @@
             "optim_idx": 1,  # needed for trainer
         }
 
     def inference(
         self,
         text: torch.Tensor,
         feats: Optional[torch.Tensor] = None,
-        label: Optional[Dict[str, torch.Tensor]] = None,
-        melody: Optional[Dict[str, torch.Tensor]] = None,
         pitch: Optional[torch.Tensor] = None,
-        duration: Optional[Dict[str, torch.Tensor]] = None,
-        slur: Optional[Dict[str, torch.Tensor]] = None,
-        spembs: Optional[torch.Tensor] = None,
-        sids: Optional[torch.Tensor] = None,
-        lids: Optional[torch.Tensor] = None,
-        noise_scale: float = 0.667,
-        noise_scale_dur: float = 0.8,
-        alpha: float = 1.0,
-        max_len: Optional[int] = None,
+        energy: Optional[torch.Tensor] = None,
         use_teacher_forcing: bool = False,
+        **kwargs,
     ) -> Dict[str, torch.Tensor]:
         """Run inference.
 
         Args:
             text (Tensor): Input text index tensor (T_text,).
             feats (Tensor): Feature tensor (T_feats, aux_channels).
-            label (Optional[Dict]): key is "lab" or "score";
-                value (LongTensor): Batch of padded label ids (B, Tmax).
-            melody (Optional[Dict]): key is "lab" or "score";
-                value (LongTensor): Batch of padded melody (B, Tmax).
-            tempo (Optional[Dict]): key is "lab" or "score";
-                value (LongTensor): Batch of padded tempo (B, Tmax).
-            beat (Optional[Dict]): key is "lab", "score_phn" or "score_syb";
-                value (LongTensor): Batch of padded beat (B, Tmax).
-            pitch (FloatTensor): Batch of padded f0 (B, Tmax).
-            duration (Optional[Dict]): key is "phn", "syb";
-                value (LongTensor): Batch of padded beat (B, Tmax).
-            slur (LongTensor): Batch of padded slur (B, Tmax).
-            sids (Tensor): Speaker index tensor (1,).
-            spembs (Optional[Tensor]): Speaker embedding tensor (spk_embed_dim,).
-            lids (Tensor): Language index tensor (1,).
-            noise_scale (float): Noise scale value for flow.
-            noise_scale_dur (float): Noise scale value for duration predictor.
-            alpha (float): Alpha parameter to control the speed of generated singing.
-            max_len (Optional[int]): Maximum length.
+            pitch (Tensor): Pitch tensor (T_feats, 1).
+            energy (Tensor): Energy tensor (T_feats, 1).
             use_teacher_forcing (bool): Whether to use teacher forcing.
 
         Returns:
             Dict[str, Tensor]:
                 * wav (Tensor): Generated waveform tensor (T_wav,).
+                * duration (Tensor): Predicted duration tensor (T_text,).
 
         """
         # setup
-        label = label["lab"]
-        melody = melody["lab"]
-        beat = duration["score_syb"]
         text = text[None]
         text_lengths = torch.tensor(
             [text.size(1)],
             dtype=torch.long,
             device=text.device,
         )
-        label_lengths = torch.tensor(
-            [label.size(1)],
-            dtype=torch.long,
-            device=text.device,
-        )
-
-        if sids is not None:
-            sids = sids.view(1)
-        if lids is not None:
-            lids = lids.view(1)
+        if "spembs" in kwargs:
+            kwargs["spembs"] = kwargs["spembs"][None]
+        if self.use_gst and "speech" in kwargs:
+            # NOTE(kan-bayashi): Workaround for the use of GST
+            kwargs.pop("speech")
 
         # inference
         if use_teacher_forcing:
             assert feats is not None
-            feats = feats[None].transpose(1, 2)
+            feats = feats[None]
             feats_lengths = torch.tensor(
-                [feats.size(2)],
+                [feats.size(1)],
                 dtype=torch.long,
                 device=feats.device,
             )
-            wav = self.generator.inference(
+            assert pitch is not None
+            pitch = pitch[None]
+            assert energy is not None
+            energy = energy[None]
+
+            wav, dur = self.generator.inference(
                 text=text,
                 text_lengths=text_lengths,
                 feats=feats,
                 feats_lengths=feats_lengths,
-                label=label,
-                label_lengths=label_lengths,
-                melody=melody,
-                beat=beat,
-                sids=sids,
-                spembs=spembs,
-                lids=lids,
-                max_len=max_len,
+                pitch=pitch,
+                energy=energy,
                 use_teacher_forcing=use_teacher_forcing,
+                **kwargs,
             )
         else:
-            wav = self.generator.inference(
+            wav, dur = self.generator.inference(
                 text=text,
                 text_lengths=text_lengths,
-                label=label,
-                label_lengths=label_lengths,
-                melody=melody,
-                beat=beat,
-                sids=sids,
-                spembs=spembs,
-                lids=lids,
-                noise_scale=noise_scale,
-                noise_scale_dur=noise_scale_dur,
-                alpha=alpha,
-                max_len=max_len,
+                feats=feats[None] if self.use_gst else None,
+                **kwargs,
             )
-        return dict(wav=wav.view(-1))
+        return dict(wav=wav.view(-1), duration=dur[0])
```

### Comparing `espnet-202304/espnet2/gan_tts/abs_gan_tts.py` & `espnet-202308/espnet2/gan_tts/abs_gan_tts.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/gan_tts/espnet_model.py` & `espnet-202308/espnet2/gan_tts/espnet_model.py`

 * *Files 8% similar despite different names*

```diff
@@ -51,14 +51,23 @@
         assert hasattr(
             tts, "generator"
         ), "generator module must be registered as tts.generator"
         assert hasattr(
             tts, "discriminator"
         ), "discriminator module must be registered as tts.discriminator"
 
+        if feats_extract is not None:
+            if hasattr(tts.generator, "vocoder"):
+                upsample_factor = tts.generator["vocoder"].upsample_factor
+            else:
+                upsample_factor = tts.generator.upsample_factor
+            assert (
+                feats_extract.get_parameters()["n_shift"] == upsample_factor
+            ), "n_shift must be equal to upsample_factor"
+
     def forward(
         self,
         text: torch.Tensor,
         text_lengths: torch.Tensor,
         speech: torch.Tensor,
         speech_lengths: torch.Tensor,
         durations: Optional[torch.Tensor] = None,
```

### Comparing `espnet-202304/espnet2/gan_tts/hifigan/hifigan.py` & `espnet-202308/espnet2/gan_tts/hifigan/hifigan.py`

 * *Files 20% similar despite different names*

```diff
@@ -524,21 +524,26 @@
             ),
         ]
 
         if use_weight_norm and use_spectral_norm:
             raise ValueError("Either use use_weight_norm or use_spectral_norm.")
 
         # apply weight norm
+        self.use_weight_norm = use_weight_norm
         if use_weight_norm:
             self.apply_weight_norm()
 
         # apply spectral norm
+        self.use_spectral_norm = use_spectral_norm
         if use_spectral_norm:
             self.apply_spectral_norm()
 
+        # backward compatibility
+        self._register_load_state_dict_pre_hook(self._load_state_dict_pre_hook)
+
     def forward(self, x: torch.Tensor) -> List[torch.Tensor]:
         """Calculate forward propagation.
 
         Args:
             x (Tensor): Input noise signal (B, 1, T).
 
         Returns:
@@ -552,30 +557,128 @@
 
         return outs
 
     def apply_weight_norm(self):
         """Apply weight normalization module from all of the layers."""
 
         def _apply_weight_norm(m: torch.nn.Module):
-            if isinstance(m, torch.nn.Conv2d):
+            if isinstance(m, torch.nn.Conv1d):
                 torch.nn.utils.weight_norm(m)
                 logging.debug(f"Weight norm is applied to {m}.")
 
         self.apply(_apply_weight_norm)
 
     def apply_spectral_norm(self):
         """Apply spectral normalization module from all of the layers."""
 
         def _apply_spectral_norm(m: torch.nn.Module):
-            if isinstance(m, torch.nn.Conv2d):
+            if isinstance(m, torch.nn.Conv1d):
                 torch.nn.utils.spectral_norm(m)
                 logging.debug(f"Spectral norm is applied to {m}.")
 
         self.apply(_apply_spectral_norm)
 
+    def remove_weight_norm(self):
+        """Remove weight normalization module from all of the layers."""
+
+        def _remove_weight_norm(m):
+            try:
+                logging.debug(f"Weight norm is removed from {m}.")
+                torch.nn.utils.remove_weight_norm(m)
+            except ValueError:  # this module didn't have weight norm
+                return
+
+        self.apply(_remove_weight_norm)
+
+    def remove_spectral_norm(self):
+        """Remove spectral normalization module from all of the layers."""
+
+        def _remove_spectral_norm(m):
+            try:
+                logging.debug(f"Spectral norm is removed from {m}.")
+                torch.nn.utils.remove_spectral_norm(m)
+            except ValueError:  # this module didn't have weight norm
+                return
+
+        self.apply(_remove_spectral_norm)
+
+    def _load_state_dict_pre_hook(
+        self,
+        state_dict,
+        prefix,
+        local_metadata,
+        strict,
+        missing_keys,
+        unexpected_keys,
+        error_msgs,
+    ):
+        """Fix the compatibility of weight / spectral normalization issue.
+
+        Some pretrained models are trained with configs that use weight / spectral
+        normalization, but actually, the norm is not applied. This causes the mismatch
+        of the parameters with configs. To solve this issue, when parameter mismatch
+        happens in loading pretrained model, we remove the norm from the current model.
+
+        See also:
+            - https://github.com/espnet/espnet/pull/5240
+            - https://github.com/espnet/espnet/pull/5249
+            - https://github.com/kan-bayashi/ParallelWaveGAN/pull/409
+
+        """
+        current_module_keys = [x for x in state_dict.keys() if x.startswith(prefix)]
+        if self.use_weight_norm and any(
+            [k.endswith("weight") for k in current_module_keys]
+        ):
+            logging.warning(
+                "It seems weight norm is not applied in the pretrained model but the"
+                " current model uses it. To keep the compatibility, we remove the norm"
+                " from the current model. This may cause unexpected behavior due to the"
+                " parameter mismatch in finetuning. To avoid this issue, please change"
+                " the following parameters in config to false:\n"
+                " - discriminator_params.follow_official_norm\n"
+                " - discriminator_params.scale_discriminator_params.use_weight_norm\n"
+                " - discriminator_params.scale_discriminator_params.use_spectral_norm\n"
+                "\n"
+                "See also:\n"
+                " - https://github.com/espnet/espnet/pull/5240\n"
+                " - https://github.com/espnet/espnet/pull/5249"
+            )
+            self.remove_weight_norm()
+            self.use_weight_norm = False
+            for k in current_module_keys:
+                if k.endswith("weight_g") or k.endswith("weight_v"):
+                    del state_dict[k]
+
+        if self.use_spectral_norm and any(
+            [k.endswith("weight") for k in current_module_keys]
+        ):
+            logging.warning(
+                "It seems spectral norm is not applied in the pretrained model but the"
+                " current model uses it. To keep the compatibility, we remove the norm"
+                " from the current model. This may cause unexpected behavior due to the"
+                " parameter mismatch in finetuning. To avoid this issue, please change"
+                " the following parameters in config to false:\n"
+                " - discriminator_params.follow_official_norm\n"
+                " - discriminator_params.scale_discriminator_params.use_weight_norm\n"
+                " - discriminator_params.scale_discriminator_params.use_spectral_norm\n"
+                "\n"
+                "See also:\n"
+                " - https://github.com/espnet/espnet/pull/5240\n"
+                " - https://github.com/espnet/espnet/pull/5249"
+            )
+            self.remove_spectral_norm()
+            self.use_spectral_norm = False
+            for k in current_module_keys:
+                if (
+                    k.endswith("weight_u")
+                    or k.endswith("weight_v")
+                    or k.endswith("weight_orig")
+                ):
+                    del state_dict[k]
+
 
 class HiFiGANMultiScaleDiscriminator(torch.nn.Module):
     """HiFi-GAN multi-scale discriminator module."""
 
     def __init__(
         self,
         scales: int = 3,
```

### Comparing `espnet-202304/espnet2/gan_tts/hifigan/loss.py` & `espnet-202308/espnet2/gan_tts/hifigan/loss.py`

 * *Files 1% similar despite different names*

```diff
@@ -273,15 +273,16 @@
     ) -> torch.Tensor:
         """Calculate Mel-spectrogram loss.
 
         Args:
             y_hat (Tensor): Generated waveform tensor (B, 1, T).
             y (Tensor): Groundtruth waveform tensor (B, 1, T).
             spec (Optional[Tensor]): Groundtruth linear amplitude spectrum tensor
-                (B, n_fft, T). if provided, use it instead of groundtruth waveform.
+                (B, T, n_fft // 2 + 1).  if provided, use it instead of groundtruth
+                waveform.
 
         Returns:
             Tensor: Mel-spectrogram loss value.
 
         """
         mel_hat, _ = self.wav_to_mel(y_hat.squeeze(1))
         if spec is None:
```

### Comparing `espnet-202304/espnet2/gan_tts/hifigan/residual_block.py` & `espnet-202308/espnet2/gan_tts/hifigan/residual_block.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/gan_tts/jets/alignments.py` & `espnet-202308/espnet2/gan_tts/jets/alignments.py`

 * *Files 20% similar despite different names*

```diff
@@ -2,38 +2,52 @@
 #  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)
 
 import numpy as np
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 from numba import jit
+from scipy.stats import betabinom
 
 
 class AlignmentModule(nn.Module):
     """Alignment Learning Framework proposed for parallel TTS models in:
 
     https://arxiv.org/abs/2108.10447
 
     """
 
-    def __init__(self, adim, odim):
+    def __init__(self, adim, odim, cache_prior=True):
+        """Initialize AlignmentModule.
+
+        Args:
+            adim (int): Dimension of attention.
+            odim (int): Dimension of feats.
+            cache_prior (bool): Whether to cache beta-binomial prior.
+
+        """
         super().__init__()
+        self.cache_prior = cache_prior
+        self._cache = {}
+
         self.t_conv1 = nn.Conv1d(adim, adim, kernel_size=3, padding=1)
         self.t_conv2 = nn.Conv1d(adim, adim, kernel_size=1, padding=0)
 
         self.f_conv1 = nn.Conv1d(odim, adim, kernel_size=3, padding=1)
         self.f_conv2 = nn.Conv1d(adim, adim, kernel_size=3, padding=1)
         self.f_conv3 = nn.Conv1d(adim, adim, kernel_size=1, padding=0)
 
-    def forward(self, text, feats, x_masks=None):
+    def forward(self, text, feats, text_lengths, feats_lengths, x_masks=None):
         """Calculate alignment loss.
 
         Args:
             text (Tensor): Batched text embedding (B, T_text, adim).
             feats (Tensor): Batched acoustic feature (B, T_feats, odim).
+            text_lengths (Tensor): Text length tensor (B,).
+            feats_lengths (Tensor): Feature length tensor (B,).
             x_masks (Tensor): Mask tensor (B, T_text).
 
         Returns:
             Tensor: Log probability of attention matrix (B, T_feats, T_text).
 
         """
         text = text.transpose(1, 2)
@@ -52,16 +66,64 @@
         score = -dist
 
         if x_masks is not None:
             x_masks = x_masks.unsqueeze(-2)
             score = score.masked_fill(x_masks, -np.inf)
 
         log_p_attn = F.log_softmax(score, dim=-1)
+
+        # add beta-binomial prior
+        bb_prior = self._generate_prior(
+            text_lengths,
+            feats_lengths,
+        ).to(dtype=log_p_attn.dtype, device=log_p_attn.device)
+        log_p_attn = log_p_attn + bb_prior
+
         return log_p_attn
 
+    def _generate_prior(self, text_lengths, feats_lengths, w=1) -> torch.Tensor:
+        """Generate alignment prior formulated as beta-binomial distribution
+
+        Args:
+            text_lengths (Tensor): Batch of the lengths of each input (B,).
+            feats_lengths (Tensor): Batch of the lengths of each target (B,).
+            w (float): Scaling factor; lower -> wider the width.
+
+        Returns:
+            Tensor: Batched 2d static prior matrix (B, T_feats, T_text).
+
+        """
+        B = len(text_lengths)
+        T_text = text_lengths.max()
+        T_feats = feats_lengths.max()
+
+        bb_prior = torch.full((B, T_feats, T_text), fill_value=-np.inf)
+        for bidx in range(B):
+            T = feats_lengths[bidx].item()
+            N = text_lengths[bidx].item()
+
+            key = str(T) + "," + str(N)
+            if self.cache_prior and key in self._cache:
+                prob = self._cache[key]
+            else:
+                alpha = w * np.arange(1, T + 1, dtype=float)  # (T,)
+                beta = w * np.array([T - t + 1 for t in alpha])
+                k = np.arange(N)
+                batched_k = k[..., None]  # (N,1)
+                prob = betabinom.logpmf(batched_k, N, alpha, beta)  # (N,T)
+
+            # store cache
+            if self.cache_prior and key not in self._cache:
+                self._cache[key] = prob
+
+            prob = torch.from_numpy(prob).transpose(0, 1)  # -> (T,N)
+            bb_prior[bidx, :T, :N] = prob
+
+        return bb_prior
+
 
 @jit(nopython=True)
 def _monotonic_alignment_search(log_p_attn):
     # https://arxiv.org/abs/2005.11129
     T_mel = log_p_attn.shape[0]
     T_inp = log_p_attn.shape[1]
     Q = np.full((T_inp, T_mel), fill_value=-np.inf)
```

### Comparing `espnet-202304/espnet2/gan_tts/jets/generator.py` & `espnet-202308/espnet2/gan_tts/jets/generator.py`

 * *Files 1% similar despite different names*

```diff
@@ -573,15 +573,21 @@
 
         # integrate speaker embedding
         if self.spk_embed_dim is not None:
             hs = self._integrate_with_spk_embed(hs, spembs)
 
         # forward alignment module and obtain duration, averaged pitch, energy
         h_masks = make_pad_mask(text_lengths).to(hs.device)
-        log_p_attn = self.alignment_module(hs, feats, h_masks)
+        log_p_attn = self.alignment_module(
+            hs,
+            feats,
+            text_lengths,
+            feats_lengths,
+            h_masks,
+        )
         ds, bin_loss = viterbi_decode(log_p_attn, text_lengths, feats_lengths)
         ps = average_by_duration(
             ds, pitch.squeeze(-1), text_lengths, feats_lengths
         ).unsqueeze(-1)
         es = average_by_duration(
             ds, energy.squeeze(-1), text_lengths, feats_lengths
         ).unsqueeze(-1)
@@ -685,15 +691,21 @@
         # integrate speaker embedding
         if self.spk_embed_dim is not None:
             hs = self._integrate_with_spk_embed(hs, spembs)
 
         h_masks = make_pad_mask(text_lengths).to(hs.device)
         if use_teacher_forcing:
             # forward alignment module and obtain duration, averaged pitch, energy
-            log_p_attn = self.alignment_module(hs, feats, h_masks)
+            log_p_attn = self.alignment_module(
+                hs,
+                feats,
+                text_lengths,
+                feats_lengths,
+                h_masks,
+            )
             d_outs, _ = viterbi_decode(log_p_attn, text_lengths, feats_lengths)
             p_outs = average_by_duration(
                 d_outs, pitch.squeeze(-1), text_lengths, feats_lengths
             ).unsqueeze(-1)
             e_outs = average_by_duration(
                 d_outs, energy.squeeze(-1), text_lengths, feats_lengths
             ).unsqueeze(-1)
```

### Comparing `espnet-202304/espnet2/gan_tts/jets/jets.py` & `espnet-202308/espnet2/gan_tts/vits/vits.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,12 +1,14 @@
-# Copyright 2022 Dan Lim
+# Copyright 2021 Tomoki Hayashi
 #  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)
 
-"""JETS module for GAN-TTS task."""
+"""VITS module for GAN-TTS task."""
 
+from contextlib import contextmanager
+from distutils.version import LooseVersion
 from typing import Any, Dict, Optional
 
 import torch
 from typeguard import check_argument_types
 
 from espnet2.gan_tts.abs_gan_tts import AbsGANTTS
 from espnet2.gan_tts.hifigan import (
@@ -18,132 +20,103 @@
 )
 from espnet2.gan_tts.hifigan.loss import (
     DiscriminatorAdversarialLoss,
     FeatureMatchLoss,
     GeneratorAdversarialLoss,
     MelSpectrogramLoss,
 )
-from espnet2.gan_tts.jets.generator import JETSGenerator
-from espnet2.gan_tts.jets.loss import ForwardSumLoss, VarianceLoss
 from espnet2.gan_tts.utils import get_segments
+from espnet2.gan_tts.vits.generator import VITSGenerator
+from espnet2.gan_tts.vits.loss import KLDivergenceLoss
 from espnet2.torch_utils.device_funcs import force_gatherable
 
 AVAILABLE_GENERATERS = {
-    "jets_generator": JETSGenerator,
+    "vits_generator": VITSGenerator,
 }
 AVAILABLE_DISCRIMINATORS = {
     "hifigan_period_discriminator": HiFiGANPeriodDiscriminator,
     "hifigan_scale_discriminator": HiFiGANScaleDiscriminator,
     "hifigan_multi_period_discriminator": HiFiGANMultiPeriodDiscriminator,
     "hifigan_multi_scale_discriminator": HiFiGANMultiScaleDiscriminator,
     "hifigan_multi_scale_multi_period_discriminator": HiFiGANMultiScaleMultiPeriodDiscriminator,  # NOQA
 }
 
+if LooseVersion(torch.__version__) >= LooseVersion("1.6.0"):
+    from torch.cuda.amp import autocast
+else:
+    # Nothing to do if torch<1.6.0
+    @contextmanager
+    def autocast(enabled=True):  # NOQA
+        yield
 
-class JETS(AbsGANTTS):
-    """JETS module (generator + discriminator).
 
-    This is a module of JETS described in `JETS: Jointly Training FastSpeech2
-    and HiFi-GAN for End to End Text to Speech'_.
+class VITS(AbsGANTTS):
+    """VITS module (generator + discriminator).
 
-    .. _`JETS: Jointly Training FastSpeech2 and HiFi-GAN for End to End Text to Speech`
-        : https://arxiv.org/abs/2203.16852
+    This is a module of VITS described in `Conditional Variational Autoencoder
+    with Adversarial Learning for End-to-End Text-to-Speech`_.
+
+    .. _`Conditional Variational Autoencoder with Adversarial Learning for End-to-End
+        Text-to-Speech`: https://arxiv.org/abs/2006.04558
 
     """
 
     def __init__(
         self,
         # generator related
         idim: int,
         odim: int,
         sampling_rate: int = 22050,
-        generator_type: str = "jets_generator",
+        generator_type: str = "vits_generator",
         generator_params: Dict[str, Any] = {
-            "adim": 256,
-            "aheads": 2,
-            "elayers": 4,
-            "eunits": 1024,
-            "dlayers": 4,
-            "dunits": 1024,
-            "positionwise_layer_type": "conv1d",
-            "positionwise_conv_kernel_size": 1,
-            "use_scaled_pos_enc": True,
-            "use_batch_norm": True,
-            "encoder_normalize_before": True,
-            "decoder_normalize_before": True,
-            "encoder_concat_after": False,
-            "decoder_concat_after": False,
-            "reduction_factor": 1,
-            "encoder_type": "transformer",
-            "decoder_type": "transformer",
-            "transformer_enc_dropout_rate": 0.1,
-            "transformer_enc_positional_dropout_rate": 0.1,
-            "transformer_enc_attn_dropout_rate": 0.1,
-            "transformer_dec_dropout_rate": 0.1,
-            "transformer_dec_positional_dropout_rate": 0.1,
-            "transformer_dec_attn_dropout_rate": 0.1,
-            "conformer_rel_pos_type": "latest",
-            "conformer_pos_enc_layer_type": "rel_pos",
-            "conformer_self_attn_layer_type": "rel_selfattn",
-            "conformer_activation_type": "swish",
-            "use_macaron_style_in_conformer": True,
-            "use_cnn_in_conformer": True,
-            "zero_triu": False,
-            "conformer_enc_kernel_size": 7,
-            "conformer_dec_kernel_size": 31,
-            "duration_predictor_layers": 2,
-            "duration_predictor_chans": 384,
-            "duration_predictor_kernel_size": 3,
-            "duration_predictor_dropout_rate": 0.1,
-            "energy_predictor_layers": 2,
-            "energy_predictor_chans": 384,
-            "energy_predictor_kernel_size": 3,
-            "energy_predictor_dropout": 0.5,
-            "energy_embed_kernel_size": 1,
-            "energy_embed_dropout": 0.5,
-            "stop_gradient_from_energy_predictor": False,
-            "pitch_predictor_layers": 5,
-            "pitch_predictor_chans": 384,
-            "pitch_predictor_kernel_size": 5,
-            "pitch_predictor_dropout": 0.5,
-            "pitch_embed_kernel_size": 1,
-            "pitch_embed_dropout": 0.5,
-            "stop_gradient_from_pitch_predictor": True,
-            "generator_out_channels": 1,
-            "generator_channels": 512,
-            "generator_global_channels": -1,
-            "generator_kernel_size": 7,
-            "generator_upsample_scales": [8, 8, 2, 2],
-            "generator_upsample_kernel_sizes": [16, 16, 4, 4],
-            "generator_resblock_kernel_sizes": [3, 7, 11],
-            "generator_resblock_dilations": [[1, 3, 5], [1, 3, 5], [1, 3, 5]],
-            "generator_use_additional_convs": True,
-            "generator_bias": True,
-            "generator_nonlinear_activation": "LeakyReLU",
-            "generator_nonlinear_activation_params": {"negative_slope": 0.1},
-            "generator_use_weight_norm": True,
-            "segment_size": 64,
-            "spks": -1,
-            "langs": -1,
+            "hidden_channels": 192,
+            "spks": None,
+            "langs": None,
             "spk_embed_dim": None,
-            "spk_embed_integration_type": "add",
-            "use_gst": False,
-            "gst_tokens": 10,
-            "gst_heads": 4,
-            "gst_conv_layers": 6,
-            "gst_conv_chans_list": [32, 32, 64, 64, 128, 128],
-            "gst_conv_kernel_size": 3,
-            "gst_conv_stride": 2,
-            "gst_gru_layers": 1,
-            "gst_gru_units": 128,
-            "init_type": "xavier_uniform",
-            "init_enc_alpha": 1.0,
-            "init_dec_alpha": 1.0,
-            "use_masking": False,
-            "use_weighted_masking": False,
+            "global_channels": -1,
+            "segment_size": 32,
+            "text_encoder_attention_heads": 2,
+            "text_encoder_ffn_expand": 4,
+            "text_encoder_blocks": 6,
+            "text_encoder_positionwise_layer_type": "conv1d",
+            "text_encoder_positionwise_conv_kernel_size": 1,
+            "text_encoder_positional_encoding_layer_type": "rel_pos",
+            "text_encoder_self_attention_layer_type": "rel_selfattn",
+            "text_encoder_activation_type": "swish",
+            "text_encoder_normalize_before": True,
+            "text_encoder_dropout_rate": 0.1,
+            "text_encoder_positional_dropout_rate": 0.0,
+            "text_encoder_attention_dropout_rate": 0.0,
+            "text_encoder_conformer_kernel_size": 7,
+            "use_macaron_style_in_text_encoder": True,
+            "use_conformer_conv_in_text_encoder": True,
+            "decoder_kernel_size": 7,
+            "decoder_channels": 512,
+            "decoder_upsample_scales": [8, 8, 2, 2],
+            "decoder_upsample_kernel_sizes": [16, 16, 4, 4],
+            "decoder_resblock_kernel_sizes": [3, 7, 11],
+            "decoder_resblock_dilations": [[1, 3, 5], [1, 3, 5], [1, 3, 5]],
+            "use_weight_norm_in_decoder": True,
+            "posterior_encoder_kernel_size": 5,
+            "posterior_encoder_layers": 16,
+            "posterior_encoder_stacks": 1,
+            "posterior_encoder_base_dilation": 1,
+            "posterior_encoder_dropout_rate": 0.0,
+            "use_weight_norm_in_posterior_encoder": True,
+            "flow_flows": 4,
+            "flow_kernel_size": 5,
+            "flow_base_dilation": 1,
+            "flow_layers": 4,
+            "flow_dropout_rate": 0.0,
+            "use_weight_norm_in_flow": True,
+            "use_only_mean_in_flow": True,
+            "stochastic_duration_predictor_kernel_size": 3,
+            "stochastic_duration_predictor_dropout_rate": 0.5,
+            "stochastic_duration_predictor_flows": 4,
+            "stochastic_duration_predictor_dds_conv_layers": 3,
         },
         # discriminator related
         discriminator_type: str = "hifigan_multi_scale_multi_period_discriminator",
         discriminator_params: Dict[str, Any] = {
             "scales": 1,
             "scale_downsample_pooling": "AvgPool1d",
             "scale_downsample_pooling_params": {
@@ -205,24 +178,24 @@
             "fmin": 0,
             "fmax": None,
             "log_base": None,
         },
         lambda_adv: float = 1.0,
         lambda_mel: float = 45.0,
         lambda_feat_match: float = 2.0,
-        lambda_var: float = 1.0,
-        lambda_align: float = 2.0,
+        lambda_dur: float = 1.0,
+        lambda_kl: float = 1.0,
         cache_generator_outputs: bool = True,
     ):
-        """Initialize JETS module.
+        """Initialize VITS module.
 
         Args:
             idim (int): Input vocabrary size.
             odim (int): Acoustic feature dimension. The actual output channels will
-                be 1 since JETS is the end-to-end text-to-wave model but for the
+                be 1 since VITS is the end-to-end text-to-wave model but for the
                 compatibility odim is used to indicate the acoustic feature dimension.
             sampling_rate (int): Sampling rate, not used for the training but it will
                 be referred in saving waveform during the inference.
             generator_type (str): Generator type.
             generator_params (Dict[str, Any]): Parameter dict for generator.
             discriminator_type (str): Discriminator type.
             discriminator_params (Dict[str, Any]): Parameter dict for discriminator.
@@ -231,25 +204,30 @@
             discriminator_adv_loss_params (Dict[str, Any]): Parameter dict for
                 discriminator adversarial loss.
             feat_match_loss_params (Dict[str, Any]): Parameter dict for feat match loss.
             mel_loss_params (Dict[str, Any]): Parameter dict for mel loss.
             lambda_adv (float): Loss scaling coefficient for adversarial loss.
             lambda_mel (float): Loss scaling coefficient for mel spectrogram loss.
             lambda_feat_match (float): Loss scaling coefficient for feat match loss.
-            lambda_var (float): Loss scaling coefficient for variance loss.
-            lambda_align (float): Loss scaling coefficient for alignment loss.
+            lambda_dur (float): Loss scaling coefficient for duration loss.
+            lambda_kl (float): Loss scaling coefficient for KL divergence loss.
             cache_generator_outputs (bool): Whether to cache generator outputs.
 
         """
         assert check_argument_types()
         super().__init__()
 
         # define modules
         generator_class = AVAILABLE_GENERATERS[generator_type]
-        generator_params.update(idim=idim, odim=odim)
+        if generator_type == "vits_generator":
+            # NOTE(kan-bayashi): Update parameters for the compatibility.
+            #   The idim and odim is automatically decided from input data,
+            #   where idim represents #vocabularies and odim represents
+            #   the input acoustic feature dimension.
+            generator_params.update(vocabs=idim, aux_channels=odim)
         self.generator = generator_class(
             **generator_params,
         )
         discriminator_class = AVAILABLE_DISCRIMINATORS[discriminator_type]
         self.discriminator = discriminator_class(
             **discriminator_params,
         )
@@ -261,37 +239,35 @@
         )
         self.feat_match_loss = FeatureMatchLoss(
             **feat_match_loss_params,
         )
         self.mel_loss = MelSpectrogramLoss(
             **mel_loss_params,
         )
-        self.var_loss = VarianceLoss()
-        self.forwardsum_loss = ForwardSumLoss()
+        self.kl_loss = KLDivergenceLoss()
 
         # coefficients
         self.lambda_adv = lambda_adv
         self.lambda_mel = lambda_mel
+        self.lambda_kl = lambda_kl
         self.lambda_feat_match = lambda_feat_match
-        self.lambda_var = lambda_var
-        self.lambda_align = lambda_align
+        self.lambda_dur = lambda_dur
 
         # cache
         self.cache_generator_outputs = cache_generator_outputs
         self._cache = None
 
         # store sampling rate for saving wav file
         # (not used for the training)
         self.fs = sampling_rate
 
         # store parameters for test compatibility
         self.spks = self.generator.spks
         self.langs = self.generator.langs
         self.spk_embed_dim = self.generator.spk_embed_dim
-        self.use_gst = getattr(self.generator, "use_gst", False)
 
     @property
     def require_raw_speech(self):
         """Return whether or not speech is required."""
         return True
 
     @property
@@ -307,15 +283,14 @@
         feats_lengths: torch.Tensor,
         speech: torch.Tensor,
         speech_lengths: torch.Tensor,
         sids: Optional[torch.Tensor] = None,
         spembs: Optional[torch.Tensor] = None,
         lids: Optional[torch.Tensor] = None,
         forward_generator: bool = True,
-        **kwargs,
     ) -> Dict[str, Any]:
         """Perform generator forward.
 
         Args:
             text (Tensor): Text index tensor (B, T_text).
             text_lengths (Tensor): Text length tensor (B,).
             feats (Tensor): Feature tensor (B, T_feats, aux_channels).
@@ -342,42 +317,39 @@
                 feats=feats,
                 feats_lengths=feats_lengths,
                 speech=speech,
                 speech_lengths=speech_lengths,
                 sids=sids,
                 spembs=spembs,
                 lids=lids,
-                **kwargs,
             )
         else:
             return self._forward_discrminator(
                 text=text,
                 text_lengths=text_lengths,
                 feats=feats,
                 feats_lengths=feats_lengths,
                 speech=speech,
                 speech_lengths=speech_lengths,
                 sids=sids,
                 spembs=spembs,
                 lids=lids,
-                **kwargs,
             )
 
     def _forward_generator(
         self,
         text: torch.Tensor,
         text_lengths: torch.Tensor,
         feats: torch.Tensor,
         feats_lengths: torch.Tensor,
         speech: torch.Tensor,
         speech_lengths: torch.Tensor,
         sids: Optional[torch.Tensor] = None,
         spembs: Optional[torch.Tensor] = None,
         lids: Optional[torch.Tensor] = None,
-        **kwargs,
     ) -> Dict[str, Any]:
         """Perform generator forward.
 
         Args:
             text (Tensor): Text index tensor (B, T_text).
             text_lengths (Tensor): Text length tensor (B,).
             feats (Tensor): Feature tensor (B, T_feats, aux_channels).
@@ -394,93 +366,74 @@
                 * stats (Dict[str, float]): Statistics to be monitored.
                 * weight (Tensor): Weight tensor to summarize losses.
                 * optim_idx (int): Optimizer index (0 for G and 1 for D).
 
         """
         # setup
         batch_size = text.size(0)
+        feats = feats.transpose(1, 2)
         speech = speech.unsqueeze(1)
 
         # calculate generator outputs
         reuse_cache = True
         if not self.cache_generator_outputs or self._cache is None:
             reuse_cache = False
             outs = self.generator(
                 text=text,
                 text_lengths=text_lengths,
                 feats=feats,
                 feats_lengths=feats_lengths,
                 sids=sids,
                 spembs=spembs,
                 lids=lids,
-                **kwargs,
             )
         else:
             outs = self._cache
 
         # store cache
         if self.training and self.cache_generator_outputs and not reuse_cache:
             self._cache = outs
 
         # parse outputs
-        (
-            speech_hat_,
-            bin_loss,
-            log_p_attn,
-            start_idxs,
-            d_outs,
-            ds,
-            p_outs,
-            ps,
-            e_outs,
-            es,
-        ) = outs
+        speech_hat_, dur_nll, _, start_idxs, _, z_mask, outs_ = outs
+        _, z_p, m_p, logs_p, _, logs_q = outs_
         speech_ = get_segments(
             x=speech,
             start_idxs=start_idxs * self.generator.upsample_factor,
             segment_size=self.generator.segment_size * self.generator.upsample_factor,
         )
 
         # calculate discriminator outputs
         p_hat = self.discriminator(speech_hat_)
         with torch.no_grad():
             # do not store discriminator gradient in generator turn
             p = self.discriminator(speech_)
 
         # calculate losses
-        mel_loss = self.mel_loss(speech_hat_, speech_)
-        adv_loss = self.generator_adv_loss(p_hat)
-        feat_match_loss = self.feat_match_loss(p_hat, p)
-        dur_loss, pitch_loss, energy_loss = self.var_loss(
-            d_outs, ds, p_outs, ps, e_outs, es, text_lengths
-        )
-        forwardsum_loss = self.forwardsum_loss(log_p_attn, text_lengths, feats_lengths)
-
-        mel_loss = mel_loss * self.lambda_mel
-        adv_loss = adv_loss * self.lambda_adv
-        feat_match_loss = feat_match_loss * self.lambda_feat_match
-        g_loss = mel_loss + adv_loss + feat_match_loss
-        var_loss = (dur_loss + pitch_loss + energy_loss) * self.lambda_var
-        align_loss = (forwardsum_loss + bin_loss) * self.lambda_align
-
-        loss = g_loss + var_loss + align_loss
+        with autocast(enabled=False):
+            mel_loss = self.mel_loss(speech_hat_, speech_)
+            kl_loss = self.kl_loss(z_p, logs_q, m_p, logs_p, z_mask)
+            dur_loss = torch.sum(dur_nll.float())
+            adv_loss = self.generator_adv_loss(p_hat)
+            feat_match_loss = self.feat_match_loss(p_hat, p)
+
+            mel_loss = mel_loss * self.lambda_mel
+            kl_loss = kl_loss * self.lambda_kl
+            dur_loss = dur_loss * self.lambda_dur
+            adv_loss = adv_loss * self.lambda_adv
+            feat_match_loss = feat_match_loss * self.lambda_feat_match
+            loss = mel_loss + kl_loss + dur_loss + adv_loss + feat_match_loss
 
         stats = dict(
             generator_loss=loss.item(),
-            generator_g_loss=g_loss.item(),
-            generator_var_loss=var_loss.item(),
-            generator_align_loss=align_loss.item(),
-            generator_g_mel_loss=mel_loss.item(),
-            generator_g_adv_loss=adv_loss.item(),
-            generator_g_feat_match_loss=feat_match_loss.item(),
-            generator_var_dur_loss=dur_loss.item(),
-            generator_var_pitch_loss=pitch_loss.item(),
-            generator_var_energy_loss=energy_loss.item(),
-            generator_align_forwardsum_loss=forwardsum_loss.item(),
-            generator_align_bin_loss=bin_loss.item(),
+            generator_mel_loss=mel_loss.item(),
+            generator_kl_loss=kl_loss.item(),
+            generator_dur_loss=dur_loss.item(),
+            generator_adv_loss=adv_loss.item(),
+            generator_feat_match_loss=feat_match_loss.item(),
         )
 
         loss, stats, weight = force_gatherable((loss, stats, batch_size), loss.device)
 
         # reset cache
         if reuse_cache or not self.training:
             self._cache = None
@@ -499,15 +452,14 @@
         feats: torch.Tensor,
         feats_lengths: torch.Tensor,
         speech: torch.Tensor,
         speech_lengths: torch.Tensor,
         sids: Optional[torch.Tensor] = None,
         spembs: Optional[torch.Tensor] = None,
         lids: Optional[torch.Tensor] = None,
-        **kwargs,
     ) -> Dict[str, Any]:
         """Perform discriminator forward.
 
         Args:
             text (Tensor): Text index tensor (B, T_text).
             text_lengths (Tensor): Text length tensor (B,).
             feats (Tensor): Feature tensor (B, T_feats, aux_channels).
@@ -524,29 +476,29 @@
                 * stats (Dict[str, float]): Statistics to be monitored.
                 * weight (Tensor): Weight tensor to summarize losses.
                 * optim_idx (int): Optimizer index (0 for G and 1 for D).
 
         """
         # setup
         batch_size = text.size(0)
+        feats = feats.transpose(1, 2)
         speech = speech.unsqueeze(1)
 
         # calculate generator outputs
         reuse_cache = True
         if not self.cache_generator_outputs or self._cache is None:
             reuse_cache = False
             outs = self.generator(
                 text=text,
                 text_lengths=text_lengths,
                 feats=feats,
                 feats_lengths=feats_lengths,
                 sids=sids,
                 spembs=spembs,
                 lids=lids,
-                **kwargs,
             )
         else:
             outs = self._cache
 
         # store cache
         if self.cache_generator_outputs and not reuse_cache:
             self._cache = outs
@@ -560,16 +512,17 @@
         )
 
         # calculate discriminator outputs
         p_hat = self.discriminator(speech_hat_.detach())
         p = self.discriminator(speech_)
 
         # calculate losses
-        real_loss, fake_loss = self.discriminator_adv_loss(p_hat, p)
-        loss = real_loss + fake_loss
+        with autocast(enabled=False):
+            real_loss, fake_loss = self.discriminator_adv_loss(p_hat, p)
+            loss = real_loss + fake_loss
 
         stats = dict(
             discriminator_loss=loss.item(),
             discriminator_real_loss=real_loss.item(),
             discriminator_fake_loss=fake_loss.item(),
         )
         loss, stats, weight = force_gatherable((loss, stats, batch_size), loss.device)
@@ -585,72 +538,87 @@
             "optim_idx": 1,  # needed for trainer
         }
 
     def inference(
         self,
         text: torch.Tensor,
         feats: Optional[torch.Tensor] = None,
-        pitch: Optional[torch.Tensor] = None,
-        energy: Optional[torch.Tensor] = None,
+        sids: Optional[torch.Tensor] = None,
+        spembs: Optional[torch.Tensor] = None,
+        lids: Optional[torch.Tensor] = None,
+        durations: Optional[torch.Tensor] = None,
+        noise_scale: float = 0.667,
+        noise_scale_dur: float = 0.8,
+        alpha: float = 1.0,
+        max_len: Optional[int] = None,
         use_teacher_forcing: bool = False,
-        **kwargs,
     ) -> Dict[str, torch.Tensor]:
         """Run inference.
 
         Args:
             text (Tensor): Input text index tensor (T_text,).
             feats (Tensor): Feature tensor (T_feats, aux_channels).
-            pitch (Tensor): Pitch tensor (T_feats, 1).
-            energy (Tensor): Energy tensor (T_feats, 1).
+            sids (Tensor): Speaker index tensor (1,).
+            spembs (Optional[Tensor]): Speaker embedding tensor (spk_embed_dim,).
+            lids (Tensor): Language index tensor (1,).
+            durations (Tensor): Ground-truth duration tensor (T_text,).
+            noise_scale (float): Noise scale value for flow.
+            noise_scale_dur (float): Noise scale value for duration predictor.
+            alpha (float): Alpha parameter to control the speed of generated speech.
+            max_len (Optional[int]): Maximum length.
             use_teacher_forcing (bool): Whether to use teacher forcing.
 
         Returns:
             Dict[str, Tensor]:
                 * wav (Tensor): Generated waveform tensor (T_wav,).
+                * att_w (Tensor): Monotonic attention weight tensor (T_feats, T_text).
                 * duration (Tensor): Predicted duration tensor (T_text,).
 
         """
         # setup
         text = text[None]
         text_lengths = torch.tensor(
             [text.size(1)],
             dtype=torch.long,
             device=text.device,
         )
-        if "spembs" in kwargs:
-            kwargs["spembs"] = kwargs["spembs"][None]
-        if self.use_gst and "speech" in kwargs:
-            # NOTE(kan-bayashi): Workaround for the use of GST
-            kwargs.pop("speech")
+        if sids is not None:
+            sids = sids.view(1)
+        if lids is not None:
+            lids = lids.view(1)
+        if durations is not None:
+            durations = durations.view(1, 1, -1)
 
         # inference
         if use_teacher_forcing:
             assert feats is not None
-            feats = feats[None]
+            feats = feats[None].transpose(1, 2)
             feats_lengths = torch.tensor(
-                [feats.size(1)],
+                [feats.size(2)],
                 dtype=torch.long,
                 device=feats.device,
             )
-            assert pitch is not None
-            pitch = pitch[None]
-            assert energy is not None
-            energy = energy[None]
-
-            wav, dur = self.generator.inference(
+            wav, att_w, dur = self.generator.inference(
                 text=text,
                 text_lengths=text_lengths,
                 feats=feats,
                 feats_lengths=feats_lengths,
-                pitch=pitch,
-                energy=energy,
+                sids=sids,
+                spembs=spembs,
+                lids=lids,
+                max_len=max_len,
                 use_teacher_forcing=use_teacher_forcing,
-                **kwargs,
             )
         else:
-            wav, dur = self.generator.inference(
+            wav, att_w, dur = self.generator.inference(
                 text=text,
                 text_lengths=text_lengths,
-                feats=feats[None] if self.use_gst else None,
-                **kwargs,
+                sids=sids,
+                spembs=spembs,
+                lids=lids,
+                dur=durations,
+                noise_scale=noise_scale,
+                noise_scale_dur=noise_scale_dur,
+                alpha=alpha,
+                max_len=max_len,
             )
-        return dict(wav=wav.view(-1), duration=dur[0])
+        return dict(wav=wav.view(-1), att_w=att_w[0], duration=dur[0])
```

### Comparing `espnet-202304/espnet2/gan_tts/jets/length_regulator.py` & `espnet-202308/espnet2/gan_tts/jets/length_regulator.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/gan_tts/jets/loss.py` & `espnet-202308/espnet2/gan_tts/jets/loss.py`

 * *Files 23% similar despite different names*

```diff
@@ -4,15 +4,14 @@
 """JETS related loss module for ESPnet2."""
 
 from typing import Tuple
 
 import numpy as np
 import torch
 import torch.nn.functional as F
-from scipy.stats import betabinom
 from typeguard import check_argument_types
 
 from espnet.nets.pytorch_backend.fastspeech.duration_predictor import (  # noqa: H301
     DurationPredictorLoss,
 )
 from espnet.nets.pytorch_backend.nets_utils import make_non_pad_mask
 
@@ -104,24 +103,17 @@
 
         return duration_loss, pitch_loss, energy_loss
 
 
 class ForwardSumLoss(torch.nn.Module):
     """Forwardsum loss described at https://openreview.net/forum?id=0NQwnnwAORi"""
 
-    def __init__(self, cache_prior: bool = True):
-        """Initialize forwardsum loss module.
-
-        Args:
-            cache_prior (bool): Whether to cache beta-binomial prior
-
-        """
+    def __init__(self):
+        """Initialize forwardsum loss module."""
         super().__init__()
-        self.cache_prior = cache_prior
-        self._cache = {}
 
     def forward(
         self,
         log_p_attn: torch.Tensor,
         ilens: torch.Tensor,
         olens: torch.Tensor,
         blank_prob: float = np.e**-1,
@@ -137,19 +129,14 @@
 
         Returns:
             Tensor: forwardsum loss value.
 
         """
         B = log_p_attn.size(0)
 
-        # add beta-binomial prior
-        bb_prior = self._generate_prior(ilens, olens)
-        bb_prior = bb_prior.to(dtype=log_p_attn.dtype, device=log_p_attn.device)
-        log_p_attn = log_p_attn + bb_prior
-
         # a row must be added to the attention matrix to account for
         #    blank token of CTC loss
         # (B,T_feats,T_text+1)
         log_p_attn_pd = F.pad(log_p_attn, (1, 0, 0, 0, 0, 0), value=np.log(blank_prob))
 
         loss = 0
         for bidx in range(B):
@@ -157,56 +144,17 @@
             # Every text token is mapped to a unique sequnece number.
             target_seq = torch.arange(1, ilens[bidx] + 1).unsqueeze(0)
             cur_log_p_attn_pd = log_p_attn_pd[
                 bidx, : olens[bidx], : ilens[bidx] + 1
             ].unsqueeze(
                 1
             )  # (T_feats,1,T_text+1)
+            cur_log_p_attn_pd = F.log_softmax(cur_log_p_attn_pd, dim=-1)
             loss += F.ctc_loss(
                 log_probs=cur_log_p_attn_pd,
                 targets=target_seq,
                 input_lengths=olens[bidx : bidx + 1],
                 target_lengths=ilens[bidx : bidx + 1],
                 zero_infinity=True,
             )
         loss = loss / B
         return loss
-
-    def _generate_prior(self, text_lengths, feats_lengths, w=1) -> torch.Tensor:
-        """Generate alignment prior formulated as beta-binomial distribution
-
-        Args:
-            text_lengths (Tensor): Batch of the lengths of each input (B,).
-            feats_lengths (Tensor): Batch of the lengths of each target (B,).
-            w (float): Scaling factor; lower -> wider the width.
-
-        Returns:
-            Tensor: Batched 2d static prior matrix (B, T_feats, T_text).
-
-        """
-        B = len(text_lengths)
-        T_text = text_lengths.max()
-        T_feats = feats_lengths.max()
-
-        bb_prior = torch.full((B, T_feats, T_text), fill_value=-np.inf)
-        for bidx in range(B):
-            T = feats_lengths[bidx].item()
-            N = text_lengths[bidx].item()
-
-            key = str(T) + "," + str(N)
-            if self.cache_prior and key in self._cache:
-                prob = self._cache[key]
-            else:
-                alpha = w * np.arange(1, T + 1, dtype=float)  # (T,)
-                beta = w * np.array([T - t + 1 for t in alpha])
-                k = np.arange(N)
-                batched_k = k[..., None]  # (N,1)
-                prob = betabinom.logpmf(batched_k, N, alpha, beta)  # (N,T)
-
-            # store cache
-            if self.cache_prior and key not in self._cache:
-                self._cache[key] = prob
-
-            prob = torch.from_numpy(prob).transpose(0, 1)  # -> (T,N)
-            bb_prior[bidx, :T, :N] = prob
-
-        return bb_prior
```

### Comparing `espnet-202304/espnet2/gan_tts/joint/joint_text2wav.py` & `espnet-202308/espnet2/gan_tts/joint/joint_text2wav.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/gan_tts/melgan/melgan.py` & `espnet-202308/espnet2/gan_tts/melgan/melgan.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/gan_tts/melgan/pqmf.py` & `espnet-202308/espnet2/gan_tts/melgan/pqmf.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/gan_tts/melgan/residual_stack.py` & `espnet-202308/espnet2/gan_tts/melgan/residual_stack.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/gan_tts/parallel_wavegan/parallel_wavegan.py` & `espnet-202308/espnet2/gan_tts/parallel_wavegan/parallel_wavegan.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/gan_tts/parallel_wavegan/upsample.py` & `espnet-202308/espnet2/gan_tts/parallel_wavegan/upsample.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/gan_tts/style_melgan/style_melgan.py` & `espnet-202308/espnet2/gan_tts/style_melgan/style_melgan.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/gan_tts/style_melgan/tade_res_block.py` & `espnet-202308/espnet2/gan_tts/style_melgan/tade_res_block.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/gan_tts/utils/get_random_segments.py` & `espnet-202308/espnet2/gan_tts/utils/get_random_segments.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 # Copyright 2021 Tomoki Hayashi
 #  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)
 
 """Function to get random segments."""
 
-from typing import Tuple
+from typing import Optional, Tuple
 
 import torch
 
 
 def get_random_segments(
     x: torch.Tensor,
     x_lengths: torch.Tensor,
@@ -23,18 +23,20 @@
     Returns:
         Tensor: Segmented tensor (B, C, segment_size).
         Tensor: Start index tensor (B,).
 
     """
     b, c, t = x.size()
     max_start_idx = x_lengths - segment_size
+    max_start_idx[max_start_idx < 0] = 0
     start_idxs = (torch.rand([b]).to(x.device) * max_start_idx).to(
         dtype=torch.long,
     )
     segments = get_segments(x, start_idxs, segment_size)
+
     return segments, start_idxs
 
 
 def get_segments(
     x: torch.Tensor,
     start_idxs: torch.Tensor,
     segment_size: int,
```

### Comparing `espnet-202304/espnet2/gan_tts/vits/duration_predictor.py` & `espnet-202308/espnet2/gan_tts/vits/duration_predictor.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/gan_tts/vits/flow.py` & `espnet-202308/espnet2/gan_tts/vits/flow.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/gan_tts/vits/loss.py` & `espnet-202308/espnet2/gan_tts/vits/loss.py`

 * *Files 26% similar despite different names*

```diff
@@ -4,14 +4,15 @@
 """VITS-related loss modules.
 
 This code is based on https://github.com/jaywalnut310/vits.
 
 """
 
 import torch
+import torch.distributions as D
 
 
 class KLDivergenceLoss(torch.nn.Module):
     """KL divergence loss."""
 
     def forward(
         self,
@@ -41,7 +42,31 @@
         z_mask = z_mask.float()
         kl = logs_p - logs_q - 0.5
         kl += 0.5 * ((z_p - m_p) ** 2) * torch.exp(-2.0 * logs_p)
         kl = torch.sum(kl * z_mask)
         loss = kl / torch.sum(z_mask)
 
         return loss
+
+
+class KLDivergenceLossWithoutFlow(torch.nn.Module):
+    """KL divergence loss without flow."""
+
+    def forward(
+        self,
+        m_q: torch.Tensor,
+        logs_q: torch.Tensor,
+        m_p: torch.Tensor,
+        logs_p: torch.Tensor,
+    ) -> torch.Tensor:
+        """Calculate KL divergence loss without flow.
+
+        Args:
+            m_q (Tensor): Posterior encoder projected mean (B, H, T_feats).
+            logs_q (Tensor): Posterior encoder projected scale (B, H, T_feats).
+            m_p (Tensor): Expanded text encoder projected mean (B, H, T_feats).
+            logs_p (Tensor): Expanded text encoder projected scale (B, H, T_feats).
+        """
+        posterior_norm = D.Normal(m_q, torch.exp(logs_q))
+        prior_norm = D.Normal(m_p, torch.exp(logs_p))
+        loss = D.kl_divergence(posterior_norm, prior_norm).mean()
+        return loss
```

### Comparing `espnet-202304/espnet2/gan_tts/vits/monotonic_align/__init__.py` & `espnet-202308/espnet2/gan_tts/vits/monotonic_align/__init__.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/gan_tts/vits/monotonic_align/setup.py` & `espnet-202308/espnet2/gan_tts/vits/monotonic_align/setup.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/gan_tts/vits/posterior_encoder.py` & `espnet-202308/espnet2/gan_tts/vits/posterior_encoder.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/gan_tts/vits/residual_coupling.py` & `espnet-202308/espnet2/gan_tts/vits/residual_coupling.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/gan_tts/vits/text_encoder.py` & `espnet-202308/espnet2/gan_tts/vits/text_encoder.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/gan_tts/vits/transform.py` & `espnet-202308/espnet2/gan_tts/vits/transform.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/gan_tts/vits/vits.py` & `espnet-202308/espnet2/svs/naive_rnn/naive_rnn_dp.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,624 +1,629 @@
-# Copyright 2021 Tomoki Hayashi
+# Copyright 2021 Carnegie Mellon University (Jiatong Shi)
 #  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)
 
-"""VITS module for GAN-TTS task."""
+"""NaiveRNN-DP-SVS related modules."""
 
-from contextlib import contextmanager
-from distutils.version import LooseVersion
-from typing import Any, Dict, Optional
+from typing import Dict, Optional, Tuple
 
 import torch
+import torch.nn.functional as F
 from typeguard import check_argument_types
 
-from espnet2.gan_tts.abs_gan_tts import AbsGANTTS
-from espnet2.gan_tts.hifigan import (
-    HiFiGANMultiPeriodDiscriminator,
-    HiFiGANMultiScaleDiscriminator,
-    HiFiGANMultiScaleMultiPeriodDiscriminator,
-    HiFiGANPeriodDiscriminator,
-    HiFiGANScaleDiscriminator,
-)
-from espnet2.gan_tts.hifigan.loss import (
-    DiscriminatorAdversarialLoss,
-    FeatureMatchLoss,
-    GeneratorAdversarialLoss,
-    MelSpectrogramLoss,
-)
-from espnet2.gan_tts.utils import get_segments
-from espnet2.gan_tts.vits.generator import VITSGenerator
-from espnet2.gan_tts.vits.loss import KLDivergenceLoss
+from espnet2.svs.abs_svs import AbsSVS
 from espnet2.torch_utils.device_funcs import force_gatherable
+from espnet2.torch_utils.initialize import initialize
+from espnet.nets.pytorch_backend.e2e_tts_fastspeech import (
+    FeedForwardTransformerLoss as FastSpeechLoss,
+)
+from espnet.nets.pytorch_backend.fastspeech.duration_predictor import DurationPredictor
+from espnet.nets.pytorch_backend.fastspeech.length_regulator import LengthRegulator
+from espnet.nets.pytorch_backend.nets_utils import make_pad_mask
+from espnet.nets.pytorch_backend.tacotron2.decoder import Postnet
+from espnet.nets.pytorch_backend.tacotron2.encoder import Encoder as EncoderPrenet
 
-AVAILABLE_GENERATERS = {
-    "vits_generator": VITSGenerator,
-}
-AVAILABLE_DISCRIMINATORS = {
-    "hifigan_period_discriminator": HiFiGANPeriodDiscriminator,
-    "hifigan_scale_discriminator": HiFiGANScaleDiscriminator,
-    "hifigan_multi_period_discriminator": HiFiGANMultiPeriodDiscriminator,
-    "hifigan_multi_scale_discriminator": HiFiGANMultiScaleDiscriminator,
-    "hifigan_multi_scale_multi_period_discriminator": HiFiGANMultiScaleMultiPeriodDiscriminator,  # NOQA
-}
-
-if LooseVersion(torch.__version__) >= LooseVersion("1.6.0"):
-    from torch.cuda.amp import autocast
-else:
-    # Nothing to do if torch<1.6.0
-    @contextmanager
-    def autocast(enabled=True):  # NOQA
-        yield
-
-
-class VITS(AbsGANTTS):
-    """VITS module (generator + discriminator).
-
-    This is a module of VITS described in `Conditional Variational Autoencoder
-    with Adversarial Learning for End-to-End Text-to-Speech`_.
-
-    .. _`Conditional Variational Autoencoder with Adversarial Learning for End-to-End
-        Text-to-Speech`: https://arxiv.org/abs/2006.04558
 
+class NaiveRNNDP(AbsSVS):
+    """NaiveRNNDP-SVS module.
+
+    This is an implementation of naive RNN with duration prediction
+    for singing voice synthesis
+    The features are processed directly over time-domain from music score and
+    predict the singing voice features
     """
 
     def __init__(
         self,
-        # generator related
+        # network structure related
         idim: int,
         odim: int,
-        sampling_rate: int = 22050,
-        generator_type: str = "vits_generator",
-        generator_params: Dict[str, Any] = {
-            "hidden_channels": 192,
-            "spks": None,
-            "langs": None,
-            "spk_embed_dim": None,
-            "global_channels": -1,
-            "segment_size": 32,
-            "text_encoder_attention_heads": 2,
-            "text_encoder_ffn_expand": 4,
-            "text_encoder_blocks": 6,
-            "text_encoder_positionwise_layer_type": "conv1d",
-            "text_encoder_positionwise_conv_kernel_size": 1,
-            "text_encoder_positional_encoding_layer_type": "rel_pos",
-            "text_encoder_self_attention_layer_type": "rel_selfattn",
-            "text_encoder_activation_type": "swish",
-            "text_encoder_normalize_before": True,
-            "text_encoder_dropout_rate": 0.1,
-            "text_encoder_positional_dropout_rate": 0.0,
-            "text_encoder_attention_dropout_rate": 0.0,
-            "text_encoder_conformer_kernel_size": 7,
-            "use_macaron_style_in_text_encoder": True,
-            "use_conformer_conv_in_text_encoder": True,
-            "decoder_kernel_size": 7,
-            "decoder_channels": 512,
-            "decoder_upsample_scales": [8, 8, 2, 2],
-            "decoder_upsample_kernel_sizes": [16, 16, 4, 4],
-            "decoder_resblock_kernel_sizes": [3, 7, 11],
-            "decoder_resblock_dilations": [[1, 3, 5], [1, 3, 5], [1, 3, 5]],
-            "use_weight_norm_in_decoder": True,
-            "posterior_encoder_kernel_size": 5,
-            "posterior_encoder_layers": 16,
-            "posterior_encoder_stacks": 1,
-            "posterior_encoder_base_dilation": 1,
-            "posterior_encoder_dropout_rate": 0.0,
-            "use_weight_norm_in_posterior_encoder": True,
-            "flow_flows": 4,
-            "flow_kernel_size": 5,
-            "flow_base_dilation": 1,
-            "flow_layers": 4,
-            "flow_dropout_rate": 0.0,
-            "use_weight_norm_in_flow": True,
-            "use_only_mean_in_flow": True,
-            "stochastic_duration_predictor_kernel_size": 3,
-            "stochastic_duration_predictor_dropout_rate": 0.5,
-            "stochastic_duration_predictor_flows": 4,
-            "stochastic_duration_predictor_dds_conv_layers": 3,
-        },
-        # discriminator related
-        discriminator_type: str = "hifigan_multi_scale_multi_period_discriminator",
-        discriminator_params: Dict[str, Any] = {
-            "scales": 1,
-            "scale_downsample_pooling": "AvgPool1d",
-            "scale_downsample_pooling_params": {
-                "kernel_size": 4,
-                "stride": 2,
-                "padding": 2,
-            },
-            "scale_discriminator_params": {
-                "in_channels": 1,
-                "out_channels": 1,
-                "kernel_sizes": [15, 41, 5, 3],
-                "channels": 128,
-                "max_downsample_channels": 1024,
-                "max_groups": 16,
-                "bias": True,
-                "downsample_scales": [2, 2, 4, 4, 1],
-                "nonlinear_activation": "LeakyReLU",
-                "nonlinear_activation_params": {"negative_slope": 0.1},
-                "use_weight_norm": True,
-                "use_spectral_norm": False,
-            },
-            "follow_official_norm": False,
-            "periods": [2, 3, 5, 7, 11],
-            "period_discriminator_params": {
-                "in_channels": 1,
-                "out_channels": 1,
-                "kernel_sizes": [5, 3],
-                "channels": 32,
-                "downsample_scales": [3, 3, 3, 3, 1],
-                "max_downsample_channels": 1024,
-                "bias": True,
-                "nonlinear_activation": "LeakyReLU",
-                "nonlinear_activation_params": {"negative_slope": 0.1},
-                "use_weight_norm": True,
-                "use_spectral_norm": False,
-            },
-        },
-        # loss related
-        generator_adv_loss_params: Dict[str, Any] = {
-            "average_by_discriminators": False,
-            "loss_type": "mse",
-        },
-        discriminator_adv_loss_params: Dict[str, Any] = {
-            "average_by_discriminators": False,
-            "loss_type": "mse",
-        },
-        feat_match_loss_params: Dict[str, Any] = {
-            "average_by_discriminators": False,
-            "average_by_layers": False,
-            "include_final_outputs": True,
-        },
-        mel_loss_params: Dict[str, Any] = {
-            "fs": 22050,
-            "n_fft": 1024,
-            "hop_length": 256,
-            "win_length": None,
-            "window": "hann",
-            "n_mels": 80,
-            "fmin": 0,
-            "fmax": None,
-            "log_base": None,
-        },
-        lambda_adv: float = 1.0,
-        lambda_mel: float = 45.0,
-        lambda_feat_match: float = 2.0,
-        lambda_dur: float = 1.0,
-        lambda_kl: float = 1.0,
-        cache_generator_outputs: bool = True,
+        midi_dim: int = 129,
+        embed_dim: int = 512,
+        duration_dim: int = 500,
+        eprenet_conv_layers: int = 3,
+        eprenet_conv_chans: int = 256,
+        eprenet_conv_filts: int = 5,
+        elayers: int = 3,
+        eunits: int = 1024,
+        ebidirectional: bool = True,
+        midi_embed_integration_type: str = "add",
+        dlayers: int = 3,
+        dunits: int = 1024,
+        dbidirectional: bool = True,
+        postnet_layers: int = 5,
+        postnet_chans: int = 256,
+        postnet_filts: int = 5,
+        use_batch_norm: bool = True,
+        duration_predictor_layers: int = 2,
+        duration_predictor_chans: int = 384,
+        duration_predictor_kernel_size: int = 3,
+        duration_predictor_dropout_rate: float = 0.1,
+        reduction_factor: int = 1,
+        # extra embedding related
+        spks: Optional[int] = None,
+        langs: Optional[int] = None,
+        spk_embed_dim: Optional[int] = None,
+        spk_embed_integration_type: str = "add",
+        eprenet_dropout_rate: float = 0.5,
+        edropout_rate: float = 0.1,
+        ddropout_rate: float = 0.1,
+        postnet_dropout_rate: float = 0.5,
+        init_type: str = "xavier_uniform",
+        use_masking: bool = False,
+        use_weighted_masking: bool = False,
     ):
-        """Initialize VITS module.
+        """Initialize NaiveRNNDP module.
 
         Args:
-            idim (int): Input vocabrary size.
-            odim (int): Acoustic feature dimension. The actual output channels will
-                be 1 since VITS is the end-to-end text-to-wave model but for the
-                compatibility odim is used to indicate the acoustic feature dimension.
-            sampling_rate (int): Sampling rate, not used for the training but it will
-                be referred in saving waveform during the inference.
-            generator_type (str): Generator type.
-            generator_params (Dict[str, Any]): Parameter dict for generator.
-            discriminator_type (str): Discriminator type.
-            discriminator_params (Dict[str, Any]): Parameter dict for discriminator.
-            generator_adv_loss_params (Dict[str, Any]): Parameter dict for generator
-                adversarial loss.
-            discriminator_adv_loss_params (Dict[str, Any]): Parameter dict for
-                discriminator adversarial loss.
-            feat_match_loss_params (Dict[str, Any]): Parameter dict for feat match loss.
-            mel_loss_params (Dict[str, Any]): Parameter dict for mel loss.
-            lambda_adv (float): Loss scaling coefficient for adversarial loss.
-            lambda_mel (float): Loss scaling coefficient for mel spectrogram loss.
-            lambda_feat_match (float): Loss scaling coefficient for feat match loss.
-            lambda_dur (float): Loss scaling coefficient for duration loss.
-            lambda_kl (float): Loss scaling coefficient for KL divergence loss.
-            cache_generator_outputs (bool): Whether to cache generator outputs.
+            idim (int): Dimension of the label inputs.
+            odim (int): Dimension of the outputs.
+            midi_dim (int): Dimension of the midi inputs.
+            embed_dim (int): Dimension of the token embedding.
+            eprenet_conv_layers (int): Number of prenet conv layers.
+            eprenet_conv_filts (int): Number of prenet conv filter size.
+            eprenet_conv_chans (int): Number of prenet conv filter channels.
+            elayers (int): Number of encoder layers.
+            eunits (int): Number of encoder hidden units.
+            ebidirectional (bool): If bidirectional in encoder.
+            midi_embed_integration_type (str): how to integrate midi information,
+                ("add" or "cat").
+            dlayers (int): Number of decoder lstm layers.
+            dunits (int): Number of decoder lstm units.
+            dbidirectional (bool): if bidirectional in decoder.
+            postnet_layers (int): Number of postnet layers.
+            postnet_filts (int): Number of postnet filter size.
+            postnet_chans (int): Number of postnet filter channels.
+            use_batch_norm (bool): Whether to use batch normalization.
+            reduction_factor (int): Reduction factor.
+            duration_predictor_layers (int): Number of duration predictor layers.
+            duration_predictor_chans (int): Number of duration predictor channels.
+            duration_predictor_kernel_size (int): Kernel size of duration predictor.
+            duration_predictor_dropout_rate (float): Dropout rate in duration predictor.
+            # extra embedding related
+            spks (Optional[int]): Number of speakers. If set to > 1, assume that the
+                sids will be provided as the input and use sid embedding layer.
+            langs (Optional[int]): Number of languages. If set to > 1, assume that the
+                lids will be provided as the input and use sid embedding layer.
+            spk_embed_dim (Optional[int]): Speaker embedding dimension. If set to > 0,
+                assume that spembs will be provided as the input.
+            spk_embed_integration_type (str): How to integrate speaker embedding.
+            eprenet_dropout_rate (float): Prenet dropout rate.
+            edropout_rate (float): Encoder dropout rate.
+            ddropout_rate (float): Decoder dropout rate.
+            postnet_dropout_rate (float): Postnet dropout_rate.
+            init_type (str): How to initialize transformer parameters.
+            use_masking (bool): Whether to mask padded part in loss calculation.
+            use_weighted_masking (bool): Whether to apply weighted masking in
+                loss calculation.
 
         """
         assert check_argument_types()
         super().__init__()
 
-        # define modules
-        generator_class = AVAILABLE_GENERATERS[generator_type]
-        if generator_type == "vits_generator":
-            # NOTE(kan-bayashi): Update parameters for the compatibility.
-            #   The idim and odim is automatically decided from input data,
-            #   where idim represents #vocabularies and odim represents
-            #   the input acoustic feature dimension.
-            generator_params.update(vocabs=idim, aux_channels=odim)
-        self.generator = generator_class(
-            **generator_params,
-        )
-        discriminator_class = AVAILABLE_DISCRIMINATORS[discriminator_type]
-        self.discriminator = discriminator_class(
-            **discriminator_params,
-        )
-        self.generator_adv_loss = GeneratorAdversarialLoss(
-            **generator_adv_loss_params,
-        )
-        self.discriminator_adv_loss = DiscriminatorAdversarialLoss(
-            **discriminator_adv_loss_params,
-        )
-        self.feat_match_loss = FeatureMatchLoss(
-            **feat_match_loss_params,
-        )
-        self.mel_loss = MelSpectrogramLoss(
-            **mel_loss_params,
-        )
-        self.kl_loss = KLDivergenceLoss()
-
-        # coefficients
-        self.lambda_adv = lambda_adv
-        self.lambda_mel = lambda_mel
-        self.lambda_kl = lambda_kl
-        self.lambda_feat_match = lambda_feat_match
-        self.lambda_dur = lambda_dur
-
-        # cache
-        self.cache_generator_outputs = cache_generator_outputs
-        self._cache = None
-
-        # store sampling rate for saving wav file
-        # (not used for the training)
-        self.fs = sampling_rate
-
-        # store parameters for test compatibility
-        self.spks = self.generator.spks
-        self.langs = self.generator.langs
-        self.spk_embed_dim = self.generator.spk_embed_dim
-
-    @property
-    def require_raw_speech(self):
-        """Return whether or not speech is required."""
-        return True
-
-    @property
-    def require_vocoder(self):
-        """Return whether or not vocoder is required."""
-        return False
-
-    def forward(
-        self,
-        text: torch.Tensor,
-        text_lengths: torch.Tensor,
-        feats: torch.Tensor,
-        feats_lengths: torch.Tensor,
-        speech: torch.Tensor,
-        speech_lengths: torch.Tensor,
-        sids: Optional[torch.Tensor] = None,
-        spembs: Optional[torch.Tensor] = None,
-        lids: Optional[torch.Tensor] = None,
-        forward_generator: bool = True,
-    ) -> Dict[str, Any]:
-        """Perform generator forward.
-
-        Args:
-            text (Tensor): Text index tensor (B, T_text).
-            text_lengths (Tensor): Text length tensor (B,).
-            feats (Tensor): Feature tensor (B, T_feats, aux_channels).
-            feats_lengths (Tensor): Feature length tensor (B,).
-            speech (Tensor): Speech waveform tensor (B, T_wav).
-            speech_lengths (Tensor): Speech length tensor (B,).
-            sids (Optional[Tensor]): Speaker index tensor (B,) or (B, 1).
-            spembs (Optional[Tensor]): Speaker embedding tensor (B, spk_embed_dim).
-            lids (Optional[Tensor]): Language index tensor (B,) or (B, 1).
-            forward_generator (bool): Whether to forward generator.
-
-        Returns:
-            Dict[str, Any]:
-                - loss (Tensor): Loss scalar tensor.
-                - stats (Dict[str, float]): Statistics to be monitored.
-                - weight (Tensor): Weight tensor to summarize losses.
-                - optim_idx (int): Optimizer index (0 for G and 1 for D).
-
-        """
-        if forward_generator:
-            return self._forward_generator(
-                text=text,
-                text_lengths=text_lengths,
-                feats=feats,
-                feats_lengths=feats_lengths,
-                speech=speech,
-                speech_lengths=speech_lengths,
-                sids=sids,
-                spembs=spembs,
-                lids=lids,
+        # store hyperparameters
+        self.idim = idim
+        self.midi_dim = midi_dim
+        self.duration_dim = duration_dim
+        self.eunits = eunits
+        self.odim = odim
+        self.eos = idim - 1
+        self.reduction_factor = reduction_factor
+
+        self.midi_embed_integration_type = midi_embed_integration_type
+
+        # use idx 0 as padding idx
+        self.padding_idx = 0
+
+        # define transformer encoder
+        if eprenet_conv_layers != 0:
+            # encoder prenet
+            self.encoder_input_layer = torch.nn.Sequential(
+                EncoderPrenet(
+                    idim=idim,
+                    embed_dim=embed_dim,
+                    elayers=0,
+                    econv_layers=eprenet_conv_layers,
+                    econv_chans=eprenet_conv_chans,
+                    econv_filts=eprenet_conv_filts,
+                    use_batch_norm=use_batch_norm,
+                    dropout_rate=eprenet_dropout_rate,
+                    padding_idx=self.padding_idx,
+                ),
+                torch.nn.Linear(eprenet_conv_chans, eunits),
+            )
+            self.midi_encoder_input_layer = torch.nn.Sequential(
+                EncoderPrenet(
+                    idim=midi_dim,
+                    embed_dim=embed_dim,
+                    elayers=0,
+                    econv_layers=eprenet_conv_layers,
+                    econv_chans=eprenet_conv_chans,
+                    econv_filts=eprenet_conv_filts,
+                    use_batch_norm=use_batch_norm,
+                    dropout_rate=eprenet_dropout_rate,
+                    padding_idx=self.padding_idx,
+                ),
+                torch.nn.Linear(eprenet_conv_chans, eunits),
+            )
+            self.duration_encoder_input_layer = torch.nn.Sequential(
+                EncoderPrenet(
+                    idim=midi_dim,
+                    embed_dim=embed_dim,
+                    elayers=0,
+                    econv_layers=eprenet_conv_layers,
+                    econv_chans=eprenet_conv_chans,
+                    econv_filts=eprenet_conv_filts,
+                    use_batch_norm=use_batch_norm,
+                    dropout_rate=eprenet_dropout_rate,
+                    padding_idx=self.padding_idx,
+                ),
+                torch.nn.Linear(eprenet_conv_chans, eunits),
             )
         else:
-            return self._forward_discrminator(
-                text=text,
-                text_lengths=text_lengths,
-                feats=feats,
-                feats_lengths=feats_lengths,
-                speech=speech,
-                speech_lengths=speech_lengths,
-                sids=sids,
-                spembs=spembs,
-                lids=lids,
+            self.encoder_input_layer = torch.nn.Embedding(
+                num_embeddings=idim, embedding_dim=eunits, padding_idx=self.padding_idx
+            )
+            self.midi_encoder_input_layer = torch.nn.Embedding(
+                num_embeddings=midi_dim,
+                embedding_dim=eunits,
+                padding_idx=self.padding_idx,
+            )
+            self.duration_encoder_input_layer = torch.nn.Embedding(
+                num_embeddings=duration_dim,
+                embedding_dim=eunits,
+                padding_idx=self.padding_idx,
             )
 
-    def _forward_generator(
-        self,
-        text: torch.Tensor,
-        text_lengths: torch.Tensor,
-        feats: torch.Tensor,
-        feats_lengths: torch.Tensor,
-        speech: torch.Tensor,
-        speech_lengths: torch.Tensor,
-        sids: Optional[torch.Tensor] = None,
-        spembs: Optional[torch.Tensor] = None,
-        lids: Optional[torch.Tensor] = None,
-    ) -> Dict[str, Any]:
-        """Perform generator forward.
-
-        Args:
-            text (Tensor): Text index tensor (B, T_text).
-            text_lengths (Tensor): Text length tensor (B,).
-            feats (Tensor): Feature tensor (B, T_feats, aux_channels).
-            feats_lengths (Tensor): Feature length tensor (B,).
-            speech (Tensor): Speech waveform tensor (B, T_wav).
-            speech_lengths (Tensor): Speech length tensor (B,).
-            sids (Optional[Tensor]): Speaker index tensor (B,) or (B, 1).
-            spembs (Optional[Tensor]): Speaker embedding tensor (B, spk_embed_dim).
-            lids (Optional[Tensor]): Language index tensor (B,) or (B, 1).
-
-        Returns:
-            Dict[str, Any]:
-                * loss (Tensor): Loss scalar tensor.
-                * stats (Dict[str, float]): Statistics to be monitored.
-                * weight (Tensor): Weight tensor to summarize losses.
-                * optim_idx (int): Optimizer index (0 for G and 1 for D).
-
-        """
-        # setup
-        batch_size = text.size(0)
-        feats = feats.transpose(1, 2)
-        speech = speech.unsqueeze(1)
-
-        # calculate generator outputs
-        reuse_cache = True
-        if not self.cache_generator_outputs or self._cache is None:
-            reuse_cache = False
-            outs = self.generator(
-                text=text,
-                text_lengths=text_lengths,
-                feats=feats,
-                feats_lengths=feats_lengths,
-                sids=sids,
-                spembs=spembs,
-                lids=lids,
+        self.encoder = torch.nn.LSTM(
+            input_size=eunits,
+            hidden_size=eunits,
+            num_layers=elayers,
+            batch_first=True,
+            dropout=edropout_rate,
+            bidirectional=ebidirectional,
+            # proj_size=eunits,
+        )
+
+        self.midi_encoder = torch.nn.LSTM(
+            input_size=eunits,
+            hidden_size=eunits,
+            num_layers=elayers,
+            batch_first=True,
+            dropout=edropout_rate,
+            bidirectional=ebidirectional,
+            # proj_size=eunits,
+        )
+
+        self.duration_encoder = torch.nn.LSTM(
+            input_size=eunits,
+            hidden_size=eunits,
+            num_layers=elayers,
+            batch_first=True,
+            dropout=edropout_rate,
+            bidirectional=ebidirectional,
+            # proj_size=eunits,
+        )
+
+        dim_direction = 2 if ebidirectional is True else 1
+        if self.midi_embed_integration_type == "add":
+            self.midi_projection = torch.nn.Linear(
+                eunits * dim_direction, eunits * dim_direction
             )
         else:
-            outs = self._cache
+            self.midi_projection = torch.nn.Linear(
+                3 * eunits * dim_direction, eunits * dim_direction
+            )
 
-        # store cache
-        if self.training and self.cache_generator_outputs and not reuse_cache:
-            self._cache = outs
-
-        # parse outputs
-        speech_hat_, dur_nll, _, start_idxs, _, z_mask, outs_ = outs
-        _, z_p, m_p, logs_p, _, logs_q = outs_
-        speech_ = get_segments(
-            x=speech,
-            start_idxs=start_idxs * self.generator.upsample_factor,
-            segment_size=self.generator.segment_size * self.generator.upsample_factor,
-        )
-
-        # calculate discriminator outputs
-        p_hat = self.discriminator(speech_hat_)
-        with torch.no_grad():
-            # do not store discriminator gradient in generator turn
-            p = self.discriminator(speech_)
-
-        # calculate losses
-        with autocast(enabled=False):
-            mel_loss = self.mel_loss(speech_hat_, speech_)
-            kl_loss = self.kl_loss(z_p, logs_q, m_p, logs_p, z_mask)
-            dur_loss = torch.sum(dur_nll.float())
-            adv_loss = self.generator_adv_loss(p_hat)
-            feat_match_loss = self.feat_match_loss(p_hat, p)
-
-            mel_loss = mel_loss * self.lambda_mel
-            kl_loss = kl_loss * self.lambda_kl
-            dur_loss = dur_loss * self.lambda_dur
-            adv_loss = adv_loss * self.lambda_adv
-            feat_match_loss = feat_match_loss * self.lambda_feat_match
-            loss = mel_loss + kl_loss + dur_loss + adv_loss + feat_match_loss
+        # define duration predictor
+        self.duration_predictor = DurationPredictor(
+            idim=eunits * dim_direction,
+            n_layers=duration_predictor_layers,
+            n_chans=duration_predictor_chans,
+            kernel_size=duration_predictor_kernel_size,
+            dropout_rate=duration_predictor_dropout_rate,
+        )
+
+        # define length regulator
+        self.length_regulator = LengthRegulator()
+
+        self.decoder = torch.nn.LSTM(
+            input_size=eunits * dim_direction,
+            hidden_size=dunits,
+            num_layers=dlayers,
+            batch_first=True,
+            dropout=ddropout_rate,
+            bidirectional=dbidirectional,
+            # proj_size=dunits,
+        )
+
+        # define spk and lang embedding
+        self.spks = None
+        if spks is not None and spks > 1:
+            self.spks = spks
+            self.sid_emb = torch.nn.Embedding(spks, dunits * dim_direction)
+        self.langs = None
+        if langs is not None and langs > 1:
+            # TODO(Yuning): not encode yet
+            self.langs = langs
+            self.lid_emb = torch.nn.Embedding(langs, dunits * dim_direction)
+
+        # define projection layer
+        self.spk_embed_dim = None
+        if spk_embed_dim is not None and spk_embed_dim > 0:
+            self.spk_embed_dim = spk_embed_dim
+            self.spk_embed_integration_type = spk_embed_integration_type
+        if self.spk_embed_dim is not None:
+            if self.spk_embed_integration_type == "add":
+                self.projection = torch.nn.Linear(
+                    self.spk_embed_dim, dunits * dim_direction
+                )
+            else:
+                self.projection = torch.nn.Linear(
+                    dunits * dim_direction + self.spk_embed_dim, dunits * dim_direction
+                )
+
+        # define final projection
+        self.feat_out = torch.nn.Linear(dunits * dim_direction, odim * reduction_factor)
+
+        # define postnet
+        self.postnet = (
+            None
+            if postnet_layers == 0
+            else Postnet(
+                idim=idim,
+                odim=odim,
+                n_layers=postnet_layers,
+                n_chans=postnet_chans,
+                n_filts=postnet_filts,
+                use_batch_norm=use_batch_norm,
+                dropout_rate=postnet_dropout_rate,
+            )
+        )
 
-        stats = dict(
-            generator_loss=loss.item(),
-            generator_mel_loss=mel_loss.item(),
-            generator_kl_loss=kl_loss.item(),
-            generator_dur_loss=dur_loss.item(),
-            generator_adv_loss=adv_loss.item(),
-            generator_feat_match_loss=feat_match_loss.item(),
+        # define loss function
+        self.criterion = FastSpeechLoss(
+            use_masking=use_masking, use_weighted_masking=use_weighted_masking
         )
 
-        loss, stats, weight = force_gatherable((loss, stats, batch_size), loss.device)
+        # initialize parameters
+        self._reset_parameters(
+            init_type=init_type,
+        )
 
-        # reset cache
-        if reuse_cache or not self.training:
-            self._cache = None
-
-        return {
-            "loss": loss,
-            "stats": stats,
-            "weight": weight,
-            "optim_idx": 0,  # needed for trainer
-        }
+    def _reset_parameters(self, init_type):
+        # initialize parameters
+        if init_type != "pytorch":
+            initialize(self, init_type)
 
-    def _forward_discrminator(
+    def forward(
         self,
         text: torch.Tensor,
         text_lengths: torch.Tensor,
         feats: torch.Tensor,
         feats_lengths: torch.Tensor,
-        speech: torch.Tensor,
-        speech_lengths: torch.Tensor,
-        sids: Optional[torch.Tensor] = None,
+        label: Optional[Dict[str, torch.Tensor]] = None,
+        label_lengths: Optional[Dict[str, torch.Tensor]] = None,
+        melody: Optional[Dict[str, torch.Tensor]] = None,
+        melody_lengths: Optional[Dict[str, torch.Tensor]] = None,
+        pitch: Optional[torch.Tensor] = None,
+        pitch_lengths: Optional[torch.Tensor] = None,
+        duration: Optional[Dict[str, torch.Tensor]] = None,
+        duration_lengths: Optional[Dict[str, torch.Tensor]] = None,
+        slur: torch.LongTensor = None,
+        slur_lengths: torch.Tensor = None,
         spembs: Optional[torch.Tensor] = None,
+        sids: Optional[torch.Tensor] = None,
         lids: Optional[torch.Tensor] = None,
-    ) -> Dict[str, Any]:
-        """Perform discriminator forward.
+        joint_training: bool = False,
+        flag_IsValid=False,
+    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor]:
+        """Calculate forward propagation.
 
         Args:
-            text (Tensor): Text index tensor (B, T_text).
-            text_lengths (Tensor): Text length tensor (B,).
-            feats (Tensor): Feature tensor (B, T_feats, aux_channels).
-            feats_lengths (Tensor): Feature length tensor (B,).
-            speech (Tensor): Speech waveform tensor (B, T_wav).
-            speech_lengths (Tensor): Speech length tensor (B,).
-            sids (Optional[Tensor]): Speaker index tensor (B,) or (B, 1).
-            spembs (Optional[Tensor]): Speaker embedding tensor (B, spk_embed_dim).
-            lids (Optional[Tensor]): Language index tensor (B,) or (B, 1).
+            text (LongTensor): Batch of padded character ids (B, Tmax).
+            text_lengths (LongTensor): Batch of lengths of each input batch (B,).
+            feats (Tensor): Batch of padded target features (B, Lmax, odim).
+            feats_lengths (LongTensor): Batch of the lengths of each target (B,).
+            label (Optional[Dict]): key is "lab" or "score";
+                value (LongTensor): Batch of padded label ids (B, Tmax).
+            label_lengths (Optional[Dict]): key is "lab" or "score";
+                value (LongTensor): Batch of the lengths of padded label ids (B, ).
+            melody (Optional[Dict]): key is "lab" or "score";
+                value (LongTensor): Batch of padded melody (B, Tmax).
+            melody_lengths (Optional[Dict]): key is "lab" or "score";
+                value (LongTensor): Batch of the lengths of padded melody (B, ).
+            pitch (FloatTensor): Batch of padded f0 (B, Tmax).
+            pitch_lengths (LongTensor): Batch of the lengths of padded f0 (B, ).
+            duration (Optional[Dict]): key is "lab", "score_phn" or "score_syb";
+                value (LongTensor): Batch of padded duration (B, Tmax).
+            duration_length (Optional[Dict]): key is "lab", "score_phn" or "score_syb";
+                value (LongTensor): Batch of the lengths of padded duration (B, ).
+            slur (LongTensor): Batch of padded slur (B, Tmax).
+            slur_lengths (LongTensor): Batch of the lengths of padded slur (B, ).
+            spembs (Optional[Tensor]): Batch of speaker embeddings (B, spk_embed_dim).
+            sids (Optional[Tensor]): Batch of speaker IDs (B, 1).
+            lids (Optional[Tensor]): Batch of language IDs (B, 1).
+            joint_training (bool): Whether to perform joint training with vocoder.
+
+        GS Fix:
+            arguements from forward func. V.S. **batch from espnet_model.py
+            label == durations | phone sequence
+            melody -> pitch sequence
 
         Returns:
-            Dict[str, Any]:
-                * loss (Tensor): Loss scalar tensor.
-                * stats (Dict[str, float]): Statistics to be monitored.
-                * weight (Tensor): Weight tensor to summarize losses.
-                * optim_idx (int): Optimizer index (0 for G and 1 for D).
-
+            Tensor: Loss scalar value.
+            Dict: Statistics to be monitored.
+            Tensor: Weight value if not joint training else model outputs.
         """
-        # setup
-        batch_size = text.size(0)
-        feats = feats.transpose(1, 2)
-        speech = speech.unsqueeze(1)
-
-        # calculate generator outputs
-        reuse_cache = True
-        if not self.cache_generator_outputs or self._cache is None:
-            reuse_cache = False
-            outs = self.generator(
-                text=text,
-                text_lengths=text_lengths,
-                feats=feats,
-                feats_lengths=feats_lengths,
-                sids=sids,
-                spembs=spembs,
-                lids=lids,
+        if joint_training:
+            label = label
+            midi = melody
+            label_lengths = label_lengths
+            midi_lengths = melody_lengths
+            duration_ = duration
+            ds = duration
+        else:
+            label = label["score"]
+            midi = melody["score"]
+            duration_ = duration["score_phn"]
+            label_lengths = label_lengths["score"]
+            midi_lengths = melody_lengths["score"]
+            duration_lengths = duration_lengths["score_phn"]
+            ds = duration["lab"]
+
+        feats = feats[:, : feats_lengths.max()]  # for data-parallel
+        midi = midi[:, : midi_lengths.max()]  # for data-parallel
+        label = label[:, : label_lengths.max()]  # for data-parallel
+        duration_ = duration_[:, : duration_lengths.max()]  # for data-parallel
+        batch_size = feats.size(0)
+
+        label_emb = self.encoder_input_layer(label)  # FIX ME: label Float to Int
+        midi_emb = self.midi_encoder_input_layer(midi)
+        duration_emb = self.duration_encoder_input_layer(duration_)
+
+        label_emb = torch.nn.utils.rnn.pack_padded_sequence(
+            label_emb, label_lengths.to("cpu"), batch_first=True, enforce_sorted=False
+        )
+        midi_emb = torch.nn.utils.rnn.pack_padded_sequence(
+            midi_emb, midi_lengths.to("cpu"), batch_first=True, enforce_sorted=False
+        )
+        duration_emb = torch.nn.utils.rnn.pack_padded_sequence(
+            duration_emb,
+            duration_lengths.to("cpu"),
+            batch_first=True,
+            enforce_sorted=False,
+        )
+
+        hs_label, (_, _) = self.encoder(label_emb)
+        hs_midi, (_, _) = self.midi_encoder(midi_emb)
+        hs_duration, (_, _) = self.duration_encoder(duration_emb)
+
+        hs_label, _ = torch.nn.utils.rnn.pad_packed_sequence(hs_label, batch_first=True)
+        hs_midi, _ = torch.nn.utils.rnn.pad_packed_sequence(hs_midi, batch_first=True)
+        hs_duration, _ = torch.nn.utils.rnn.pad_packed_sequence(
+            hs_duration, batch_first=True
+        )
+
+        if self.midi_embed_integration_type == "add":
+            hs = hs_label + hs_midi + hs_duration
+            hs = F.leaky_relu(self.midi_projection(hs))
+        else:
+            hs = torch.cat((hs_label, hs_midi, hs_duration), dim=-1)
+            hs = F.leaky_relu(self.midi_projection(hs))
+        # integrate spk & lang embeddings
+        if self.spks is not None:
+            sid_embs = self.sid_emb(sids.view(-1))
+            hs = hs + sid_embs.unsqueeze(1)
+        if self.langs is not None:
+            lid_embs = self.lid_emb(lids.view(-1))
+            hs = hs + lid_embs.unsqueeze(1)
+
+        # integrate speaker embedding
+        if self.spk_embed_dim is not None:
+            hs = self._integrate_with_spk_embed(hs, spembs)
+
+        # forward duration predictor and length regulator
+        d_masks = make_pad_mask(label_lengths).to(hs.device)
+        d_outs = self.duration_predictor(hs, d_masks)  # (B, T_text)
+        hs = self.length_regulator(hs, ds)  # (B, seq_len, eunits)
+
+        olens = feats_lengths
+        if self.reduction_factor > 1:
+            olens_in = olens.new([olen // self.reduction_factor for olen in olens])
+        else:
+            olens_in = olens
+
+        hs_emb = torch.nn.utils.rnn.pack_padded_sequence(
+            hs, olens_in.to("cpu"), batch_first=True, enforce_sorted=False
+        )
+
+        zs, (_, _) = self.decoder(hs_emb)
+        zs, _ = torch.nn.utils.rnn.pad_packed_sequence(zs, batch_first=True)
+
+        # feat_out: (B, T_feats//r, dunits * dim_direction) -> (B, T_feats//r, odim * r)
+        # view: (B, T_feats//r, odim * r) -> (B, T_feats//r * r, odim)
+        before_outs = F.leaky_relu(self.feat_out(zs).view(zs.size(0), -1, self.odim))
+
+        # postnet -> (B, T_feats//r * r, odim)
+        if self.postnet is None:
+            after_outs = before_outs
+        else:
+            after_outs = before_outs + self.postnet(
+                before_outs.transpose(1, 2)
+            ).transpose(1, 2)
+
+        # modifiy mod part of groundtruth
+        if self.reduction_factor > 1:
+            assert feats_lengths.ge(
+                self.reduction_factor
+            ).all(), "Output length must be greater than or equal to reduction factor."
+            olens = feats_lengths.new(
+                [olen - olen % self.reduction_factor for olen in feats_lengths]
             )
+            max_olen = max(olens)
+            ys = feats[:, :max_olen]
         else:
-            outs = self._cache
+            ys = feats
+            olens = feats_lengths
 
-        # store cache
-        if self.cache_generator_outputs and not reuse_cache:
-            self._cache = outs
-
-        # parse outputs
-        speech_hat_, _, _, start_idxs, *_ = outs
-        speech_ = get_segments(
-            x=speech,
-            start_idxs=start_idxs * self.generator.upsample_factor,
-            segment_size=self.generator.segment_size * self.generator.upsample_factor,
-        )
-
-        # calculate discriminator outputs
-        p_hat = self.discriminator(speech_hat_.detach())
-        p = self.discriminator(speech_)
-
-        # calculate losses
-        with autocast(enabled=False):
-            real_loss, fake_loss = self.discriminator_adv_loss(p_hat, p)
-            loss = real_loss + fake_loss
+        # calculate loss values
+        ilens = label_lengths
+        l1_loss, duration_loss = self.criterion(
+            after_outs, before_outs, d_outs, ys, ds, ilens, olens
+        )
+        loss = l1_loss + duration_loss
 
         stats = dict(
-            discriminator_loss=loss.item(),
-            discriminator_real_loss=real_loss.item(),
-            discriminator_fake_loss=fake_loss.item(),
+            loss=loss.item(), l1_loss=l1_loss.item(), duration_loss=duration_loss.item()
         )
+
         loss, stats, weight = force_gatherable((loss, stats, batch_size), loss.device)
 
-        # reset cache
-        if reuse_cache or not self.training:
-            self._cache = None
-
-        return {
-            "loss": loss,
-            "stats": stats,
-            "weight": weight,
-            "optim_idx": 1,  # needed for trainer
-        }
+        if joint_training:
+            return loss, stats, after_outs if after_outs is not None else before_outs
+        else:
+            if flag_IsValid is False:
+                # training stage
+                return loss, stats, weight
+            else:
+                # validation stage
+                return loss, stats, weight, after_outs[:, : olens.max()], ys, olens
 
     def inference(
         self,
         text: torch.Tensor,
         feats: Optional[torch.Tensor] = None,
-        sids: Optional[torch.Tensor] = None,
+        label: Optional[Dict[str, torch.Tensor]] = None,
+        melody: Optional[Dict[str, torch.Tensor]] = None,
+        pitch: Optional[torch.Tensor] = None,
+        duration: Optional[Dict[str, torch.Tensor]] = None,
+        slur: Optional[Dict[str, torch.Tensor]] = None,
         spembs: Optional[torch.Tensor] = None,
+        sids: Optional[torch.Tensor] = None,
         lids: Optional[torch.Tensor] = None,
-        durations: Optional[torch.Tensor] = None,
-        noise_scale: float = 0.667,
-        noise_scale_dur: float = 0.8,
-        alpha: float = 1.0,
-        max_len: Optional[int] = None,
-        use_teacher_forcing: bool = False,
-    ) -> Dict[str, torch.Tensor]:
-        """Run inference.
+        joint_training: bool = False,
+        use_teacher_forcing: torch.Tensor = False,
+    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor]:
+        """Calculate forward propagation.
 
         Args:
-            text (Tensor): Input text index tensor (T_text,).
-            feats (Tensor): Feature tensor (T_feats, aux_channels).
-            sids (Tensor): Speaker index tensor (1,).
-            spembs (Optional[Tensor]): Speaker embedding tensor (spk_embed_dim,).
-            lids (Tensor): Language index tensor (1,).
-            durations (Tensor): Ground-truth duration tensor (T_text,).
-            noise_scale (float): Noise scale value for flow.
-            noise_scale_dur (float): Noise scale value for duration predictor.
-            alpha (float): Alpha parameter to control the speed of generated speech.
-            max_len (Optional[int]): Maximum length.
-            use_teacher_forcing (bool): Whether to use teacher forcing.
+            text (LongTensor): Batch of padded character ids (Tmax).
+            feats (Tensor): Batch of padded target features (Lmax, odim).
+            label (Optional[Dict]): key is "lab" or "score";
+                value (LongTensor): Batch of padded label ids (Tmax).
+            melody (Optional[Dict]): key is "lab" or "score";
+                value (LongTensor): Batch of padded melody (Tmax).
+            pitch (FloatTensor): Batch of padded f0 (Tmax).
+            duration (Optional[Dict]): key is "lab", "score_phn" or "score_syb";
+                value (LongTensor): Batch of padded duration (Tmax).
+            slur (LongTensor): Batch of padded slur (B, Tmax).
+            spembs (Optional[Tensor]): Batch of speaker embeddings (spk_embed_dim).
+            sids (Optional[Tensor]): Batch of speaker IDs (1).
+            lids (Optional[Tensor]): Batch of language IDs (1).
 
         Returns:
-            Dict[str, Tensor]:
-                * wav (Tensor): Generated waveform tensor (T_wav,).
-                * att_w (Tensor): Monotonic attention weight tensor (T_feats, T_text).
-                * duration (Tensor): Predicted duration tensor (T_text,).
+            Dict[str, Tensor]: Output dict including the following items:
+                * feat_gen (Tensor): Output sequence of features (T_feats, odim).
+        """
+        label = label["score"]
+        midi = melody["score"]
+        if joint_training:
+            duration_ = duration["lab"]
+        else:
+            duration_ = duration["score_phn"]
 
+        label_emb = self.encoder_input_layer(label)  # FIX ME: label Float to Int
+        midi_emb = self.midi_encoder_input_layer(midi)
+        duration_emb = self.duration_encoder_input_layer(duration_)
+
+        hs_label, (_, _) = self.encoder(label_emb)
+        hs_midi, (_, _) = self.midi_encoder(midi_emb)
+        hs_duration, (_, _) = self.duration_encoder(duration_emb)
+
+        if self.midi_embed_integration_type == "add":
+            hs = hs_label + hs_midi + hs_duration
+            hs = F.leaky_relu(self.midi_projection(hs))
+        else:
+            hs = torch.cat((hs_label, hs_midi, hs_duration), dim=-1)
+            hs = F.leaky_relu(self.midi_projection(hs))
+        # integrate spk & lang embeddings
+        if self.spks is not None:
+            sid_embs = self.sid_emb(sids.view(-1))
+            hs = hs + sid_embs.unsqueeze(1)
+        if self.langs is not None:
+            lid_embs = self.lid_emb(lids.view(-1))
+            hs = hs + lid_embs.unsqueeze(1)
+        if spembs is not None:
+            spembs = spembs.unsqueeze(0)
+
+        # integrate speaker embedding
+        if self.spk_embed_dim is not None:
+            hs = self._integrate_with_spk_embed(hs, spembs)
+
+        # forward duration predictor and length regulator
+        d_masks = None  # make_pad_mask(label_lengths).to(input_emb.device)
+        d_outs = self.duration_predictor.inference(hs, d_masks)  # (B, T_text)
+        d_outs_int = torch.floor(d_outs + 0.5).to(dtype=torch.long)  # (B, T_text)
+
+        hs = self.length_regulator(hs, d_outs_int)  # (B, T_feats, adim)
+        zs, (_, _) = self.decoder(hs)
+
+        # feat_out: (B, T_feats//r, dunits * dim_direction) -> (B, T_feats//r, odim * r)
+        # view: (B, T_feats//r, odim * r) -> (B, T_feats//r * r, odim)
+        before_outs = F.leaky_relu(self.feat_out(zs).view(zs.size(0), -1, self.odim))
+        # postnet -> (B, T_feats//r * r, odim)
+        if self.postnet is None:
+            after_outs = before_outs
+        else:
+            after_outs = before_outs + self.postnet(
+                before_outs.transpose(1, 2)
+            ).transpose(1, 2)
+
+        return dict(
+            feat_gen=after_outs[0], prob=None, att_w=None
+        )  # outs, probs, att_ws
+
+    def _integrate_with_spk_embed(
+        self, hs: torch.Tensor, spembs: torch.Tensor
+    ) -> torch.Tensor:
+        """Integrate speaker embedding with hidden states.
+
+        Args:
+            hs (Tensor): Batch of hidden state sequences (B, Tmax, adim).
+            spembs (Tensor): Batch of speaker embeddings (B, spk_embed_dim).
+
+        Returns:
+            Tensor: Batch of integrated hidden state sequences (B, Tmax, adim).
         """
-        # setup
-        text = text[None]
-        text_lengths = torch.tensor(
-            [text.size(1)],
-            dtype=torch.long,
-            device=text.device,
-        )
-        if sids is not None:
-            sids = sids.view(1)
-        if lids is not None:
-            lids = lids.view(1)
-        if durations is not None:
-            durations = durations.view(1, 1, -1)
-
-        # inference
-        if use_teacher_forcing:
-            assert feats is not None
-            feats = feats[None].transpose(1, 2)
-            feats_lengths = torch.tensor(
-                [feats.size(2)],
-                dtype=torch.long,
-                device=feats.device,
-            )
-            wav, att_w, dur = self.generator.inference(
-                text=text,
-                text_lengths=text_lengths,
-                feats=feats,
-                feats_lengths=feats_lengths,
-                sids=sids,
-                spembs=spembs,
-                lids=lids,
-                max_len=max_len,
-                use_teacher_forcing=use_teacher_forcing,
-            )
+
+        if self.spk_embed_integration_type == "add":
+            # apply projection and then add to hidden states
+            spembs = self.projection(F.normalize(spembs))
+            hs = hs + spembs.unsqueeze(1)
+        elif self.spk_embed_integration_type == "concat":
+            # concat hidden states with spk embeds and then apply projection
+            spembs = F.normalize(spembs).unsqueeze(1).expand(-1, hs.size(1), -1)
+            hs = self.projection(torch.cat([hs, spembs], dim=-1))
         else:
-            wav, att_w, dur = self.generator.inference(
-                text=text,
-                text_lengths=text_lengths,
-                sids=sids,
-                spembs=spembs,
-                lids=lids,
-                dur=durations,
-                noise_scale=noise_scale,
-                noise_scale_dur=noise_scale_dur,
-                alpha=alpha,
-                max_len=max_len,
-            )
-        return dict(wav=wav.view(-1), att_w=att_w[0], duration=dur[0])
+            raise NotImplementedError("support only add or concat.")
+
+        return hs
```

### Comparing `espnet-202304/espnet2/gan_tts/wavenet/residual_block.py` & `espnet-202308/espnet2/gan_tts/wavenet/residual_block.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/gan_tts/wavenet/wavenet.py` & `espnet-202308/espnet2/gan_tts/wavenet/wavenet.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/hubert/espnet_model.py` & `espnet-202308/espnet2/uasr/espnet_model.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,483 +1,430 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-
-# Thanks to Abdelrahman Mohamed and Wei-Ning Hsu's help in this implementation,
-# Their origial Hubert work is in:
-#     Paper: https://arxiv.org/pdf/2106.07447.pdf
-#     Code in Fairseq: https://github.com/pytorch/fairseq/tree/master/examples/hubert
-
+import argparse
 import logging
 from contextlib import contextmanager
-from typing import Dict, List, Optional, Tuple, Union
+from typing import Dict, Optional, Tuple
 
+import editdistance
 import torch
+import torch.nn.functional as F
 from packaging.version import parse as V
 from typeguard import check_argument_types
 
-from espnet2.asr.encoder.abs_encoder import AbsEncoder
 from espnet2.asr.frontend.abs_frontend import AbsFrontend
-from espnet2.asr.preencoder.abs_preencoder import AbsPreEncoder
-from espnet2.asr.specaug.abs_specaug import AbsSpecAug
-from espnet2.hubert.hubert_loss import HubertPretrainLoss
-from espnet2.layers.abs_normalize import AbsNormalize
+from espnet2.text.token_id_converter import TokenIDConverter
 from espnet2.torch_utils.device_funcs import force_gatherable
 from espnet2.train.abs_espnet_model import AbsESPnetModel
-from espnet.nets.e2e_asr_common import ErrorCalculator
+from espnet2.uasr.discriminator.abs_discriminator import AbsDiscriminator
+from espnet2.uasr.generator.abs_generator import AbsGenerator
+from espnet2.uasr.loss.abs_loss import AbsUASRLoss
+from espnet2.uasr.segmenter.abs_segmenter import AbsSegmenter
+from espnet2.utils.types import str2bool
+from espnet.nets.pytorch_backend.nets_utils import make_pad_mask
 
 if V(torch.__version__) >= V("1.6.0"):
     from torch.cuda.amp import autocast
 else:
     # Nothing to do if torch<1.6.0
     @contextmanager
     def autocast(enabled=True):
         yield
 
 
-class TorchAudioHubertPretrainModel(AbsESPnetModel):
-    """TorchAudio Hubert Pretrain model"""
+try:
+    import kenlm  # for CI import
+except ImportError or ModuleNotFoundError:
+    kenlm = None
+
+
+class ESPnetUASRModel(AbsESPnetModel):
+    """Unsupervised ASR model.
+
+    The source code is from FAIRSEQ:
+    https://github.com/facebookresearch/fairseq/tree/main/examples/wav2vec/unsupervised
+    """
 
     def __init__(
         self,
-        vocab_size: int,
-        token_list: Union[Tuple[str, ...], List[str]],
         frontend: Optional[AbsFrontend],
-        specaug: Optional[AbsSpecAug],
-        normalize: Optional[AbsNormalize],
-        preencoder: Optional[AbsPreEncoder],
-        encoder: AbsEncoder,
-        ignore_id: int = -1,
+        segmenter: Optional[AbsSegmenter],
+        generator: AbsGenerator,
+        discriminator: AbsDiscriminator,
+        losses: Dict[str, AbsUASRLoss],
+        kenlm_path: Optional[str],
+        token_list: Optional[list],
+        max_epoch: Optional[int],
+        vocab_size: int,
+        cfg: Optional[Dict] = None,
+        pad: int = 1,
+        sil_token: str = "<SIL>",
+        sos_token: str = "<s>",
+        eos_token: str = "</s>",
+        skip_softmax: str2bool = False,
+        use_gumbel: str2bool = False,
+        use_hard_gumbel: str2bool = True,
+        min_temperature: float = 0.1,
+        max_temperature: float = 2.0,
+        decay_temperature: float = 0.99995,
+        use_collected_training_feats: str2bool = False,
     ):
         assert check_argument_types()
 
         super().__init__()
-        self.vocab_size = vocab_size
-        self.ignore_id = ignore_id
-        self.token_list = token_list.copy()
-
+        # note that eos is the same as sos (equivalent ID)
         self.frontend = frontend
-        self.specaug = specaug
-        self.normalize = normalize
-        self.preencoder = preencoder
-        self.encoder = encoder
-        self.error_calculator = None
+        self.segmenter = segmenter
+        self.use_segmenter = True if segmenter is not None else False
+        self.generator = generator
+        self.discriminator = discriminator
+        self.pad = pad
+        if cfg is not None:
+            cfg = argparse.Namespace(**cfg)
+            self.skip_softmax = cfg.no_softmax
+            self.use_gumbel = cfg.gumbel
+            self.use_hard_gumbel = cfg.hard_gumbel
+        else:
+            self.skip_softmax = skip_softmax
+            self.use_gumbel = use_gumbel
+            self.use_hard_gumbel = use_hard_gumbel
+
+        self.use_collected_training_feats = use_collected_training_feats
+
+        self.min_temperature = min_temperature
+        self.max_temperature = max_temperature
+        self.decay_temperature = decay_temperature
+        self.current_temperature = max_temperature
+        self._number_updates = 0
+        self._number_epochs = 0
+
+        self.max_epoch = max_epoch
+        # for loss registration
+        self.losses = torch.nn.ModuleDict(losses)
 
-        self.nan_loss_count = 0.0
+        # for validation
+        self.vocab_size = vocab_size
+        self.token_list = token_list
+        self.token_id_converter = TokenIDConverter(token_list=token_list)
+        self.sil = self.token_id_converter.tokens2ids([sil_token])[0]
+        self.sos = self.token_id_converter.tokens2ids([sos_token])[0]
+        self.eos = self.token_id_converter.tokens2ids([eos_token])[0]
+
+        self.kenlm = None
+        assert (
+            kenlm is not None
+        ), "kenlm is not installed, please install from tools/installers"
+        if kenlm_path:
+            self.kenlm = kenlm.Model(kenlm_path)
+
+    @property
+    def number_updates(self):
+        return self._number_updates
+
+    @number_updates.setter
+    def number_updates(self, iiter: int):
+        assert check_argument_types() and iiter >= 0
+        self._number_updates = iiter
 
     def forward(
         self,
         speech: torch.Tensor,
         speech_lengths: torch.Tensor,
-        text: torch.Tensor,
-        text_lengths: torch.Tensor,
+        text: Optional[torch.Tensor] = None,
+        text_lengths: Optional[torch.Tensor] = None,
+        pseudo_labels: Optional[torch.Tensor] = None,
+        pseudo_labels_lengths: Optional[torch.Tensor] = None,
+        do_validation: Optional[str2bool] = False,
+        print_hyp: Optional[str2bool] = False,
         **kwargs,
     ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor]:
-        """Frontend + Encoder + Calc loss
+        """Frontend + Segmenter + Generator + Discriminator + Calc Loss
 
         Args:
-            speech: (Batch, Length, ...)
-            speech_lengths: (Batch, )
-            text: (Batch, Length)
-            text_lengths: (Batch,)
-            kwargs: "utt_id" is among the input.
         """
+        stats = {}
+
         assert text_lengths.dim() == 1, text_lengths.shape
         # Check that batch_size is unified
         assert (
             speech.shape[0]
             == speech_lengths.shape[0]
             == text.shape[0]
             == text_lengths.shape[0]
-        ), (speech.shape, speech_lengths.shape, text.shape, text_lengths.shape)
+        ), (
+            speech.shape,
+            speech_lengths.shape,
+            text.shape,
+            text_lengths.shape,
+        )
         batch_size = speech.shape[0]
 
         # for data-parallel
         text = text[:, : text_lengths.max()]
 
-        # 1. Encoder
-        logit_m, logit_u, feature_penalty = self.encode(
-            speech, speech_lengths, text, text_lengths
-        )
+        # 1. Feats encode (Extract feats + Apply segmenter)
+        feats, padding_mask = self.encode(speech, speech_lengths)
 
-        # 2a. Hubert criterion
-        loss = self._calc_hubert_loss(
-            logit_m,
-            logit_u,
-            feature_penalty,
-        )
+        # 2. Generate fake samples
+        (
+            generated_sample,
+            real_sample,
+            x_inter,
+            generated_sample_padding_mask,
+        ) = self.generator(feats, text, padding_mask)
+
+        # 3. Reprocess segments
+        if self.use_segmenter:
+            (
+                generated_sample,
+                generated_sample_padding_mask,
+            ) = self.segmenter.logit_segment(
+                generated_sample, generated_sample_padding_mask
+            )
 
-        if not torch.isinf(loss) and not torch.isnan(loss):
-            pass
-            # logging.warning(f"loss, {loss.item() / logit_m.size(0)}")
-        else:
-            self.nan_loss_count += 1
-            logging.warning(f"nan_loss_count, {self.nan_loss_count}")
+        # for phone_diversity_loss
+        generated_sample_logits = generated_sample
 
-        # log accuracies of masked and unmasked frames
-        correct_m, count_m = self._compute_correct(logit_m)
-        correct_u, count_u = self._compute_correct(logit_u)
-
-        stats = dict(
-            loss=loss.detach(),
-            correct_m=correct_m,
-            count_m=count_m,
-            acc_m=correct_m / count_m,
-            correct_u=correct_u,
-            count_u=count_u,
-            acc_u=correct_u / count_u,
+        if not self.skip_softmax:
+            if self.training and self.use_gumbel:
+                generated_sample = F.gumbel_softmax(
+                    generated_sample_logits.float(),
+                    tau=self.curr_temp,
+                    hard=self.use_hard_gumbel,
+                ).type_as(generated_sample_logits)
+            else:
+                generated_sample = generated_sample_logits.softmax(-1)
+
+        # for validation
+        vocab_seen = None
+        if do_validation:
+            batch_num_errors = 0
+            batched_hyp_ids = generated_sample.argmax(-1)
+            batched_hyp_ids[generated_sample_padding_mask] = self.pad
+
+            # for kenlm ppl metric
+            batch_lm_log_prob = 0
+            batch_num_hyp_tokens = 0
+            vocab_seen = torch.zeros(self.vocab_size - 4, dtype=torch.bool)
+
+            for hyp_ids, ref_ids in zip(batched_hyp_ids, text):
+                # remove <pad> and <unk>
+                hyp_ids = hyp_ids[hyp_ids >= 4]
+                # remove duplicate tokens
+                hyp_ids = hyp_ids.unique_consecutive()
+                # remove silence
+                hyp_ids_nosil = hyp_ids[hyp_ids != self.sil]
+                hyp_ids_nosil_list = hyp_ids_nosil.tolist()
+
+                if self.kenlm:
+                    hyp_token_list = self.token_id_converter.ids2tokens(
+                        integers=hyp_ids
+                    )
+                    hyp_tokens = " ".join(hyp_token_list)
+                    lm_log_prob = self.kenlm.score(hyp_tokens)
+                    batch_lm_log_prob += lm_log_prob
+                    batch_num_hyp_tokens += len(hyp_token_list)
+
+                    hyp_tokens_index = hyp_ids[hyp_ids >= 4]
+                    vocab_seen[hyp_tokens_index - 4] = True
+
+                ref_ids = ref_ids[ref_ids != self.pad]
+                ref_ids_list = ref_ids.tolist()
+                num_errors = editdistance.eval(hyp_ids_nosil_list, ref_ids_list)
+                batch_num_errors += num_errors
+
+            stats["batch_num_errors"] = batch_num_errors
+            stats["batch_num_ref_tokens"] = text_lengths.sum().item()
+            if self.kenlm:
+                stats["batch_lm_log_prob"] = batch_lm_log_prob
+                stats["batch_num_hyp_tokens"] = batch_num_hyp_tokens
+                stats["batch_size"] = batch_size
+
+            # print the last sample in the batch
+            if print_hyp:
+                hyp_token_list = self.token_id_converter.ids2tokens(
+                    integers=hyp_ids_nosil
+                )
+                hyp_tokens = " ".join(hyp_token_list)
+
+                ref_token_list = self.token_id_converter.ids2tokens(integers=ref_ids)
+                ref_tokens = " ".join(ref_token_list)
+                logging.info(f"[REF]: {ref_tokens}")
+                logging.info(f"[HYP]: {hyp_tokens}")
+
+        real_sample_padding_mask = text == self.pad
+        # 5. Discriminator condition
+        generated_sample_prediction = self.discriminator(
+            generated_sample, generated_sample_padding_mask
+        )
+        real_sample_prediction = self.discriminator(
+            real_sample, real_sample_padding_mask
         )
 
-        # force_gatherable: to-device and to-tensor if scalar for DataParallel
-        loss, stats, weight = force_gatherable((loss, stats, batch_size), loss.device)
-        return loss, stats, weight
-
-    def collect_feats(
-        self,
-        speech: torch.Tensor,
-        speech_lengths: torch.Tensor,
-        text: torch.Tensor,
-        text_lengths: torch.Tensor,
-        **kwargs,
-    ) -> Dict[str, torch.Tensor]:
-        feats, feats_lengths = self._extract_feats(speech, speech_lengths)
-        return {"feats": feats, "feats_lengths": feats_lengths}
-
-    def encode(
-        self,
-        speech: torch.Tensor,
-        speech_lengths: torch.Tensor,
-        y_pad: torch.Tensor,
-        y_pad_length: torch.Tensor,
-    ) -> Tuple[torch.Tensor, torch.Tensor]:
-        """Frontend + Encoder. Note that this method is used by asr_inference.py
-
-        Args:
-            speech: (Batch, Length, ...)
-            speech_lengths: (Batch, )
-            y_pad: (Batch, Length, ...)
-            y_pad_length: (Batch, )
-        """
-        with autocast(False):
-            # 1. Extract feats
-            feats, feats_lengths = self._extract_feats(speech, speech_lengths)
-
-            # 2. Data augmentation
-            if self.specaug is not None and self.training:
-                feats, feats_lengths = self.specaug(feats, feats_lengths)
-
-            # 3. Normalization for feature: e.g. Global-CMVN, Utterance-CMVN
-            if self.normalize is not None:
-                feats, feats_lengths = self.normalize(feats, feats_lengths)
-
-        # Pre-encoder, e.g. used for raw input data
-        if self.preencoder is not None:
-            feats, feats_lengths = self.preencoder(feats, feats_lengths)
-
-        # 4. Forward encoder
-        # feats: (Batch, Length, Dim)
-        # -> encoder_out: (Batch, Length2, Dim2)
-        encoder_out = self.encoder(feats, feats_lengths, y_pad, y_pad_length)
-
-        return encoder_out
-
-    def _extract_feats(
-        self, speech: torch.Tensor, speech_lengths: torch.Tensor
-    ) -> Tuple[torch.Tensor, torch.Tensor]:
-        assert speech_lengths.dim() == 1, speech_lengths.shape
+        is_discriminative_step = self.is_discriminative_step()
 
-        # for data-parallel
-        speech = speech[:, : speech_lengths.max()]
+        # 5. Calculate losses
+        loss_info = []
 
-        if self.frontend is not None:
-            # Frontend
-            #  e.g. STFT and Feature extract
-            #       data_loader may send time-domain signal in this case
-            # speech (Batch, NSamples) -> feats: (Batch, NFrames, Dim)
-            feats, feats_lengths = self.frontend(speech, speech_lengths)
+        if "discriminator_loss" in self.losses.keys():
+            (
+                generated_sample_prediction_loss,
+                real_sample_prediction_loss,
+            ) = self.losses["discriminator_loss"](
+                generated_sample_prediction,
+                real_sample_prediction,
+                is_discriminative_step,
+            )
+            loss_info.append(
+                generated_sample_prediction_loss
+                * self.losses["discriminator_loss"].weight
+            )
+            if is_discriminative_step:
+                loss_info.append(
+                    real_sample_prediction_loss
+                    * self.losses["discriminator_loss"].weight
+                )
         else:
-            # No frontend and no feature extract
-            feats, feats_lengths = speech, speech_lengths
-        return feats, feats_lengths
+            generated_sample_prediction_loss, real_sample_prediction_loss = None, None
 
-    def _compute_correct(
-        self,
-        logits,
-    ):
-        if logits.numel() == 0:
-            corr, count = 0, 0
+        if "gradient_penalty" in self.losses.keys():
+            gp = self.losses["gradient_penalty"](
+                generated_sample,
+                real_sample,
+                self.training,
+                is_discriminative_step,
+            )
+            loss_info.append(gp * self.losses["gradient_penalty"].weight)
+            loss_info.append(gp * self.losses["gradient_penalty"].weight)
         else:
-            assert logits.dim() > 1, logits.shape
-            max = logits.argmax(-1) == 0
-            min = logits.argmin(-1) == 0
-            both = max & min
-            corr = max.long().sum().item() - both.long().sum().item()
-            count = max.numel()
-        return corr, count
-
-    def _calc_hubert_loss(
-        self,
-        logit_m: Optional[torch.Tensor],
-        logit_u: Optional[torch.Tensor],
-        feature_penalty: torch.Tensor,
-        masked_weight: float = 1.0,
-        unmasked_weight: float = 0.0,
-        feature_weight: float = 10.0,
-        reduction: str = "sum",
-    ) -> torch.Tensor:
-        """Compute the cross-entropy loss on HuBERT masked and non-masked logits.
+            gp = None
 
-        Args:
-            logit_m (Tensor or None): The masked logit Tensor of dimension
-                `(masked_frames, final_dim)`.
-            logit_u (Tensor or None): The non-masked logit Tensor of dimension
-                `(unmasked_frames, final_dim)`.
-            feature_penalty (Tensor): The feature mean value for additional penalty
-                loss.
-            masked_weight (float, optional): The weight for masked cross-entropy loss
-                (Default: ``1.0``).
-            unmasked_weight (float, optional): The weight for non-masked cross-entropy
-                loss (Default: ``0.0``).
-            feature_weight (float, optional): The weight for feature penalty loss
-                (Default: ``10.0``).
-            reduction (str, optional): The reduction method for cross-entropy loss
-                (Default: ``"sum"``).
-        Ref:
-            torchaudio: examples/hubert/loss/hubert_loss.py
-        """
-        loss = feature_penalty * feature_weight * logit_m.shape[0]
-        if logit_m is not None:
-            target_m = torch.zeros(
-                logit_m.shape[0], dtype=torch.long, device=logit_m.device
+        if "phoneme_diversity_loss" in self.losses.keys():
+            pdl = self.losses["phoneme_diversity_loss"](
+                generated_sample_logits, batch_size, is_discriminative_step
             )
-            loss_m = torch.nn.functional.cross_entropy(
-                logit_m, target_m, reduction=reduction
-            )
-            loss += loss_m * masked_weight
-        if logit_u is not None:
-            target_u = torch.zeros(
-                logit_u.shape[0], dtype=torch.long, device=logit_m.device
-            )
-            loss_u = torch.nn.functional.cross_entropy(
-                logit_u, target_u, reduction=reduction
-            )
-            loss += loss_u * unmasked_weight
-        return loss
-
-
-class HubertPretrainModel(AbsESPnetModel):
-    """Hubert Pretrain model"""
-
-    def __init__(
-        self,
-        vocab_size: int,
-        token_list: Union[Tuple[str, ...], List[str]],
-        frontend: Optional[AbsFrontend],
-        specaug: Optional[AbsSpecAug],
-        normalize: Optional[AbsNormalize],
-        preencoder: Optional[AbsPreEncoder],
-        encoder: AbsEncoder,
-        ignore_id: int = -1,
-        lsm_weight: float = 0.0,
-        length_normalized_loss: bool = False,
-        report_cer: bool = False,
-        report_wer: bool = False,
-        sym_space: str = "<space>",
-        sym_blank: str = "<blank>",
-        pred_masked_weight: float = 1.0,
-        pred_nomask_weight: float = 0.0,
-        loss_weights: float = 0.0,
-    ):
-        assert check_argument_types()
+            loss_info.append(pdl * self.losses["phoneme_diversity_loss"].weight)
+        else:
+            pdl = None
 
-        super().__init__()
-        # note that eos is the same as sos (equivalent ID)
-        self.sos = vocab_size - 1
-        self.eos = vocab_size - 1
-        self.vocab_size = vocab_size
-        self.ignore_id = ignore_id
-        self.token_list = token_list.copy()
+        if "smoothness_penalty" in self.losses.keys():
+            sp = self.losses["smoothness_penalty"](
+                generated_sample_logits,
+                generated_sample_padding_mask,
+                batch_size,
+                is_discriminative_step,
+            )
+            loss_info.append(sp * self.losses["smoothness_penalty"].weight)
+        else:
+            sp = None
 
-        self.frontend = frontend
-        self.specaug = specaug
-        self.normalize = normalize
-        self.preencoder = preencoder
-        self.encoder = encoder
-        self.criterion_hubert = HubertPretrainLoss(
-            pred_masked_weight,
-            pred_nomask_weight,
-            loss_weights,
-        )
-        self.pred_masked_weight = pred_masked_weight
-        self.pred_nomask_weight = pred_nomask_weight
-        self.loss_weights = loss_weights
-
-        if report_cer or report_wer:
-            self.error_calculator = ErrorCalculator(
-                token_list, sym_space, sym_blank, report_cer, report_wer
+        if "pseudo_label_loss" in self.losses.keys() and pseudo_labels is not None:
+            mmi = self.losses["pseudo_label_loss"](
+                x_inter, pseudo_labels, is_discriminative_step
             )
+            loss_info.append(mmi * self.losses["pseudo_label_loss"].weight)
         else:
-            self.error_calculator = None
+            mmi = None
 
-    def forward(
+        # Update temperature
+        self._change_temperature()
+        self.number_updates += 1
+
+        loss = sum(loss_info)
+
+        # Collect total loss stats
+        stats["loss"] = loss.detach()
+        stats["generated_sample_prediction_loss"] = generated_sample_prediction_loss
+        stats["real_sample_prediction_loss"] = real_sample_prediction_loss
+        stats["gp"] = gp
+        stats["sp"] = sp
+        stats["pdl"] = pdl
+        stats["mmi"] = mmi
+
+        # force_gatherable: to-device and to-tensor if scalar for DataParallel
+        loss, stats, weight = force_gatherable((loss, stats, batch_size), loss.device)
+        return loss, stats, weight, vocab_seen
+
+    def inference(
         self,
         speech: torch.Tensor,
         speech_lengths: torch.Tensor,
-        text: torch.Tensor,
-        text_lengths: torch.Tensor,
-        **kwargs,
-    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor]:
-        """Frontend + Encoder + Calc loss
-
-        Args:
-            speech: (Batch, Length, ...)
-            speech_lengths: (Batch, )
-            text: (Batch, Length)
-            text_lengths: (Batch,)
-            kwargs: "utt_id" is among the input.
-        """
-        assert text_lengths.dim() == 1, text_lengths.shape
-        # Check that batch_size is unified
-        assert (
-            speech.shape[0]
-            == speech_lengths.shape[0]
-            == text.shape[0]
-            == text_lengths.shape[0]
-        ), (speech.shape, speech_lengths.shape, text.shape, text_lengths.shape)
-        batch_size = speech.shape[0]
-
-        # for data-parallel
-        text = text[:, : text_lengths.max()]
+    ):
+        # 1. Feats encode (Extract feats + Apply segmenter)
+        feats, padding_mask = self.encode(speech, speech_lengths)
 
-        # 1. Encoder
-        encoder_out = self.encode(speech, speech_lengths, text, text_lengths)
+        # 2. Generate fake samples
+        (
+            generated_sample,
+            _,
+            x_inter,
+            generated_sample_padding_mask,
+        ) = self.generator(feats, None, padding_mask)
 
-        # 2a. Hubert criterion
-        loss, acc_mask, acc_unmask = self._calc_hubert_loss(
-            encoder_out,
-        )
+        # generated_sample = generated_sample.softmax(-1)
 
-        stats = dict(
-            loss=loss.detach(),
-            acc_mask=acc_mask,
-            acc_unmask=acc_unmask,
-            acc=acc_mask,
-        )
-
-        # force_gatherable: to-device and to-tensor if scalar for DataParallel
-        loss, stats, weight = force_gatherable((loss, stats, batch_size), loss.device)
-        return loss, stats, weight
+        return generated_sample, generated_sample_padding_mask
 
     def collect_feats(
         self,
         speech: torch.Tensor,
         speech_lengths: torch.Tensor,
-        text: torch.Tensor,
-        text_lengths: torch.Tensor,
+        text: Optional[torch.Tensor] = None,
+        text_lengths: Optional[torch.Tensor] = None,
         **kwargs,
     ) -> Dict[str, torch.Tensor]:
-        feats, feats_lengths = self._extract_feats(speech, speech_lengths)
+        if self.frontend is not None:
+            # Frontend
+            #  e.g. STFT and Feature extract
+            #       data_loader may send time-domain signal in this case
+            # speech (Batch, NSamples) -> feats: (Batch, NFrames, Dim)
+            speech = F.layer_norm(speech, speech.shape)
+            feats, feats_lengths = self.frontend(speech, speech_lengths)
+        else:
+            # No frontend and no feature extract
+            feats, feats_lengths = speech, speech_lengths
         return {"feats": feats, "feats_lengths": feats_lengths}
 
-    def encode(
-        self,
-        speech: torch.Tensor,
-        speech_lengths: torch.Tensor,
-        y_pad: torch.Tensor,
-        y_pad_length: torch.Tensor,
-    ) -> Tuple[torch.Tensor, torch.Tensor]:
-        """Frontend + Encoder. Note that this method is used by asr_inference.py
-
-        Args:
-            speech: (Batch, Length, ...)
-            speech_lengths: (Batch, )
-            y_pad: (Batch, Length, ...)
-            y_pad_length: (Batch, )
-        """
-        with autocast(False):
-            # 1. Extract feats
-            feats, feats_lengths = self._extract_feats(speech, speech_lengths)
-
-            # 2. Data augmentation
-            if self.specaug is not None and self.training:
-                feats, feats_lengths = self.specaug(feats, feats_lengths)
-
-            # 3. Normalization for feature: e.g. Global-CMVN, Utterance-CMVN
-            if self.normalize is not None:
-                feats, feats_lengths = self.normalize(feats, feats_lengths)
-
-        # Pre-encoder, e.g. used for raw input data
-        if self.preencoder is not None:
-            feats, feats_lengths = self.preencoder(feats, feats_lengths)
-
-        # 4. Forward encoder
-        # feats: (Batch, Length, Dim)
-        # -> encoder_out: (Batch, Length2, Dim2)
-        encoder_out = self.encoder(feats, feats_lengths, y_pad, y_pad_length)
-
-        if hasattr(self.encoder, "encoder"):
-            logp_m_list = self.encoder.encoder.get_logits(encoder_out, True)
-            assert self.pred_masked_weight == 0 or len(logp_m_list) > 0
-
-            logp_u_list = self.encoder.encoder.get_logits(encoder_out, False)
-            assert self.pred_nomask_weight == 0 or len(logp_u_list) > 0
-
-        return encoder_out
-
     def _extract_feats(
         self, speech: torch.Tensor, speech_lengths: torch.Tensor
     ) -> Tuple[torch.Tensor, torch.Tensor]:
         assert speech_lengths.dim() == 1, speech_lengths.shape
 
         # for data-parallel
         speech = speech[:, : speech_lengths.max()]
 
-        if self.frontend is not None:
+        if self.frontend is not None and not self.use_collected_training_feats:
             # Frontend
             #  e.g. STFT and Feature extract
             #       data_loader may send time-domain signal in this case
             # speech (Batch, NSamples) -> feats: (Batch, NFrames, Dim)
+            speech = F.layer_norm(speech, speech.shape)
             feats, feats_lengths = self.frontend(speech, speech_lengths)
         else:
-            # No frontend and no feature extract
+            # No frontend and no feature extract (usually with pre-extracted feat)
+            # logging.info("use exisitng features")
             feats, feats_lengths = speech, speech_lengths
         return feats, feats_lengths
 
-    def compute_correct(
-        self,
-        logits,
-    ):
-        if logits.numel() == 0:
-            corr, count = 0, 0
-        else:
-            assert logits.dim() > 1, logits.shape
-            max = logits.argmax(-1) == 0
-            min = logits.argmin(-1) == 0
-            both = max & min
-            corr = max.long().sum().item() - both.long().sum().item()
-            count = max.numel()
-        return corr, count
+    def encode(
+        self, speech: torch.Tensor, speech_lengths: torch.Tensor
+    ) -> Tuple[torch.Tensor, torch.Tensor]:
+        with autocast(False):
+            # 1. Extract feats
+            feats, feats_lengths = self._extract_feats(speech, speech_lengths)
+        padding_mask = make_pad_mask(feats_lengths).to(feats.device)
 
-    def _calc_hubert_loss(
-        self,
-        encoder_out: Dict[str, torch.Tensor],
-    ):
-        # 1. Compute hubert loss
-        loss, logp_m_list, logp_u_list = self.criterion_hubert(
-            self.encoder.encoder, encoder_out
-        )
+        # 2. Apply feats
+        if self.use_segmenter:
+            feats, padding_mask = self.segmenter.pre_segment(feats, padding_mask)
 
-        corr_masked, count_masked = 0, 0
-        corr_unmask, count_unmask = 0, 0
-        with torch.no_grad():
-            for i, logp_m in enumerate(logp_m_list):
-                corr_m, count_m = self.compute_correct(logp_m)
-                corr_masked += corr_m
-                count_masked += count_m
-            for i, logp_u in enumerate(logp_u_list):
-                corr_u, count_u = self.compute_correct(logp_u)
-                corr_unmask += corr_u
-                count_unmask += count_u
+        return feats, padding_mask
 
-        acc_m = corr_masked / (count_masked + 1e-10)
-        acc_u = corr_unmask / (count_unmask + 1e-10)
+    def is_discriminative_step(self):
+        return self.number_updates % 2 == 1
 
-        return loss, acc_m, acc_u
+    def get_optim_index(self):
+        return self.number_updates % 2
+
+    def _change_temperature(self):
+        self.current_temperature = max(
+            self.max_temperature * self.decay_temperature**self.number_updates,
+            self.min_temperature,
+        )
```

### Comparing `espnet-202304/espnet2/hubert/hubert_loss.py` & `espnet-202308/espnet2/hubert/hubert_loss.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/iterators/chunk_iter_factory.py` & `espnet-202308/espnet2/iterators/chunk_iter_factory.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 import logging
 import re
+from collections import defaultdict
 from typing import Any, Dict, Iterator, List, Optional, Sequence, Tuple, Union
 
 import numpy as np
 import torch
 from typeguard import check_argument_types
 
 from espnet2.iterators.abs_iter_factory import AbsIterFactory
@@ -35,15 +36,15 @@
         self,
         dataset,
         batch_size: int,
         batches: Union[AbsSampler, Sequence[Sequence[Any]]],
         chunk_length: Union[int, str],
         chunk_shift_ratio: float = 0.5,
         num_cache_chunks: int = 1024,
-        num_samples_per_epoch: int = None,
+        num_samples_per_epoch: Optional[int] = None,
         seed: int = 0,
         shuffle: bool = False,
         num_workers: int = 0,
         collate_fn=None,
         pin_memory: bool = False,
         excluded_key_prefixes: Optional[List[str]] = None,
     ):
@@ -89,41 +90,43 @@
         self.seed = seed
         self.shuffle = shuffle
 
         # keys that satisfy either condition below will be excluded from the length
         # consistency check:
         #  - exactly match one of the prefixes in `excluded_key_prefixes`
         #  - have one of the prefixes in `excluded_key_prefixes` and end with numbers
+        if excluded_key_prefixes is None:
+            excluded_key_prefixes = ["utt2category"]
+        elif "utt2category" not in excluded_key_prefixes:
+            excluded_key_prefixes = excluded_key_prefixes + ["utt2category"]
         self.excluded_key_pattern = (
             "(" + "[0-9]*)|(".join(excluded_key_prefixes) + "[0-9]*)"
-            if excluded_key_prefixes
-            else None
         )
         if self.excluded_key_pattern:
             logging.info(
                 f"Data keys with the following patterns will be excluded from the "
                 f"length consistency check:\n{self.excluded_key_pattern}"
             )
 
     def build_iter(
         self,
         epoch: int,
-        shuffle: bool = None,
+        shuffle: Optional[bool] = None,
     ) -> Iterator[Tuple[List[str], Dict[str, torch.Tensor]]]:
         per_sample_loader = self.per_sample_iter_factory.build_iter(epoch, shuffle)
 
         if shuffle is None:
             shuffle = self.shuffle
         state = np.random.RandomState(epoch + self.seed)
 
         # NOTE(kamo):
         #   This iterator supports multiple chunk lengths and
         #   keep chunks for each lengths here until collecting specified numbers
-        cache_chunks_dict = {}
-        cache_id_list_dict = {}
+        cache_chunks_dict = defaultdict(dict)
+        cache_id_list_dict = defaultdict(dict)
         for ids, batch in per_sample_loader:
             # Must be per-sample-loader
             assert len(ids) == 1, f"Must be per-sample-loader: {len(ids)}"
             assert all(len(x) == 1 for x in batch.values())
 
             # Get keys of sequence data
             sequence_keys = []
@@ -152,17 +155,24 @@
             if len(chunk_lengths) == 0:
                 logging.warning(
                     f"The length of '{id_}' is {L}, but it is shorter than "
                     f"any candidates of chunk-length: {self.chunk_lengths}"
                 )
                 continue
 
+            # Convert numpy array to number
+            category = (
+                batch.get("utt2category", torch.LongTensor([0]))
+                .type(torch.int64)
+                .item()
+            )
+
             W = int(state.choice(chunk_lengths, 1))
-            cache_id_list = cache_id_list_dict.setdefault(W, [])
-            cache_chunks = cache_chunks_dict.setdefault(W, {})
+            cache_id_list = cache_id_list_dict[category].setdefault(W, [])
+            cache_chunks = cache_chunks_dict[category].setdefault(W, {})
 
             # Shift width to the next chunk
             S = int(W * self.chunk_shift_ratio)
             # Number of chunks
             N = (L - W) // S + 1
             if shuffle:
                 Z = state.randint(0, (L - W) % S + 1)
@@ -194,28 +204,29 @@
                 cache_id_list, cache_chunks = yield from self._generate_mini_batches(
                     cache_id_list,
                     cache_chunks,
                     shuffle,
                     state,
                 )
 
-            cache_id_list_dict[W] = cache_id_list
-            cache_chunks_dict[W] = cache_chunks
+            cache_id_list_dict[category][W] = cache_id_list
+            cache_chunks_dict[category][W] = cache_chunks
 
         else:
-            for W in cache_id_list_dict:
-                cache_id_list = cache_id_list_dict.setdefault(W, [])
-                cache_chunks = cache_chunks_dict.setdefault(W, {})
-
-                yield from self._generate_mini_batches(
-                    cache_id_list,
-                    cache_chunks,
-                    shuffle,
-                    state,
-                )
+            for category in cache_id_list_dict.keys():
+                for W in cache_id_list_dict[category]:
+                    cache_id_list = cache_id_list_dict[category].setdefault(W, [])
+                    cache_chunks = cache_chunks_dict[category].setdefault(W, {})
+
+                    yield from self._generate_mini_batches(
+                        cache_id_list,
+                        cache_chunks,
+                        shuffle,
+                        state,
+                    )
 
     def _generate_mini_batches(
         self,
         id_list: List[str],
         batches: Dict[str, List[torch.Tensor]],
         shuffle: bool,
         state: np.random.RandomState,
```

### Comparing `espnet-202304/espnet2/iterators/multiple_iter_factory.py` & `espnet-202308/espnet2/iterators/multiple_iter_factory.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/iterators/sequence_iter_factory.py` & `espnet-202308/espnet2/iterators/sequence_iter_factory.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,7 +1,8 @@
+import itertools
 import random
 from functools import partial
 from typing import Any, Sequence, Union
 
 import numpy as np
 from torch.utils.data import DataLoader
 from typeguard import check_argument_types
@@ -45,28 +46,30 @@
     def __init__(
         self,
         dataset,
         batches: Union[AbsSampler, Sequence[Sequence[Any]]],
         num_iters_per_epoch: int = None,
         seed: int = 0,
         shuffle: bool = False,
+        shuffle_within_batch: bool = False,
         num_workers: int = 0,
         collate_fn=None,
         pin_memory: bool = False,
     ):
         assert check_argument_types()
 
         if not isinstance(batches, AbsSampler):
             self.sampler = RawSampler(batches)
         else:
             self.sampler = batches
 
         self.dataset = dataset
         self.num_iters_per_epoch = num_iters_per_epoch
         self.shuffle = shuffle
+        self.shuffle_within_batch = shuffle_within_batch
         self.seed = seed
         self.num_workers = num_workers
         self.collate_fn = collate_fn
         # https://discuss.pytorch.org/t/what-is-the-disadvantage-of-using-pin-memory/1702
         self.pin_memory = pin_memory
 
     def build_iter(self, epoch: int, shuffle: bool = None) -> DataLoader:
@@ -136,14 +139,26 @@
 
         # For backward compatibility for pytorch DataLoader
         if self.collate_fn is not None:
             kwargs = dict(collate_fn=self.collate_fn)
         else:
             kwargs = {}
 
+        # reshuffle whole 'batches' so that elements within a batch can move
+        # between different batches
+        if self.shuffle_within_batch:
+            _bs = len(batches[0])
+            batches = list(itertools.chain(*batches))
+            np.random.RandomState(epoch + self.seed).shuffle(batches)
+            _batches = []
+            for ii in range(0, len(batches), _bs):
+                _batches.append(batches[ii : ii + _bs])
+            batches = _batches
+            del _batches
+
         return DataLoader(
             dataset=self.dataset,
             batch_sampler=batches,
             num_workers=self.num_workers,
             pin_memory=self.pin_memory,
             worker_init_fn=partial(worker_init_fn, base_seed=epoch + self.seed),
             **kwargs,
```

### Comparing `espnet-202304/espnet2/layers/global_mvn.py` & `espnet-202308/espnet2/layers/global_mvn.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/layers/label_aggregation.py` & `espnet-202308/espnet2/layers/label_aggregation.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/layers/log_mel.py` & `espnet-202308/espnet2/layers/log_mel.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/layers/mask_along_axis.py` & `espnet-202308/espnet2/layers/mask_along_axis.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/layers/sinc_conv.py` & `espnet-202308/espnet2/layers/sinc_conv.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/layers/stft.py` & `espnet-202308/espnet2/layers/stft.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/layers/time_warp.py` & `espnet-202308/espnet2/layers/time_warp.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/layers/utterance_mvn.py` & `espnet-202308/espnet2/layers/utterance_mvn.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/lm/abs_model.py` & `espnet-202308/espnet2/lm/abs_model.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/lm/espnet_model.py` & `espnet-202308/espnet2/lm/espnet_model.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/lm/seq_rnn_lm.py` & `espnet-202308/espnet2/lm/transformer_lm.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,173 +1,129 @@
-"""Sequential implementation of Recurrent Neural Network Language Model."""
-from typing import Tuple, Union
+from typing import Any, List, Tuple
 
 import torch
 import torch.nn as nn
-from typeguard import check_argument_types
 
 from espnet2.lm.abs_model import AbsLM
+from espnet.nets.pytorch_backend.transformer.embedding import PositionalEncoding
+from espnet.nets.pytorch_backend.transformer.encoder import Encoder
+from espnet.nets.pytorch_backend.transformer.mask import subsequent_mask
 
 
-class SequentialRNNLM(AbsLM):
-    """Sequential RNNLM.
-
-    See also:
-        https://github.com/pytorch/examples/blob/4581968193699de14b56527296262dd76ab43557/word_language_model/model.py
-
-    """
-
+class TransformerLM(AbsLM):
     def __init__(
         self,
         vocab_size: int,
-        unit: int = 650,
-        nhid: int = None,
-        nlayers: int = 2,
-        dropout_rate: float = 0.0,
-        tie_weights: bool = False,
-        rnn_type: str = "lstm",
-        ignore_id: int = 0,
+        pos_enc: str = None,
+        embed_unit: int = 128,
+        att_unit: int = 256,
+        head: int = 2,
+        unit: int = 1024,
+        layer: int = 4,
+        dropout_rate: float = 0.5,
     ):
-        assert check_argument_types()
         super().__init__()
+        if pos_enc == "sinusoidal":
+            pos_enc_class = PositionalEncoding
+        elif pos_enc is None:
 
-        ninp = unit
-        if nhid is None:
-            nhid = unit
-        rnn_type = rnn_type.upper()
-
-        self.drop = nn.Dropout(dropout_rate)
-        self.encoder = nn.Embedding(vocab_size, ninp, padding_idx=ignore_id)
-        if rnn_type in ["LSTM", "GRU"]:
-            rnn_class = getattr(nn, rnn_type)
-            self.rnn = rnn_class(
-                ninp, nhid, nlayers, dropout=dropout_rate, batch_first=True
-            )
-        else:
-            try:
-                nonlinearity = {"RNN_TANH": "tanh", "RNN_RELU": "relu"}[rnn_type]
-            except KeyError:
-                raise ValueError(
-                    """An invalid option for `--model` was supplied,
-                    options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']"""
-                )
-            self.rnn = nn.RNN(
-                ninp,
-                nhid,
-                nlayers,
-                nonlinearity=nonlinearity,
-                dropout=dropout_rate,
-                batch_first=True,
-            )
-        self.decoder = nn.Linear(nhid, vocab_size)
-
-        # Optionally tie weights as in:
-        # "Using the Output Embedding to Improve Language Models"
-        # (Press & Wolf 2016) https://arxiv.org/abs/1608.05859
-        # and
-        # "Tying Word Vectors and Word Classifiers:
-        # A Loss Framework for Language Modeling" (Inan et al. 2016)
-        # https://arxiv.org/abs/1611.01462
-        if tie_weights:
-            if nhid != ninp:
-                raise ValueError(
-                    "When using the tied flag, nhid must be equal to emsize"
-                )
-            self.decoder.weight = self.encoder.weight
-
-        self.rnn_type = rnn_type
-        self.nhid = nhid
-        self.nlayers = nlayers
-
-    def zero_state(self):
-        """Initialize LM state filled with zero values."""
-        if isinstance(self.rnn, torch.nn.LSTM):
-            h = torch.zeros((self.nlayers, self.nhid), dtype=torch.float)
-            c = torch.zeros((self.nlayers, self.nhid), dtype=torch.float)
-            state = h, c
-        else:
-            state = torch.zeros((self.nlayers, self.nhid), dtype=torch.float)
+            def pos_enc_class(*args, **kwargs):
+                return nn.Sequential()  # indentity
 
-        return state
+        else:
+            raise ValueError(f"unknown pos-enc option: {pos_enc}")
 
-    def forward(
-        self, input: torch.Tensor, hidden: torch.Tensor
-    ) -> Tuple[torch.Tensor, torch.Tensor]:
-        emb = self.drop(self.encoder(input))
-        output, hidden = self.rnn(emb, hidden)
-        output = self.drop(output)
-        decoded = self.decoder(
-            output.contiguous().view(output.size(0) * output.size(1), output.size(2))
-        )
-        return (
-            decoded.view(output.size(0), output.size(1), decoded.size(1)),
-            hidden,
+        self.embed = nn.Embedding(vocab_size, embed_unit)
+        self.encoder = Encoder(
+            idim=embed_unit,
+            attention_dim=att_unit,
+            attention_heads=head,
+            linear_units=unit,
+            num_blocks=layer,
+            dropout_rate=dropout_rate,
+            input_layer="linear",
+            pos_enc_class=pos_enc_class,
         )
+        self.decoder = nn.Linear(att_unit, vocab_size)
+
+    def _target_mask(self, ys_in_pad):
+        ys_mask = ys_in_pad != 0
+        m = subsequent_mask(ys_mask.size(-1), device=ys_mask.device).unsqueeze(0)
+        return ys_mask.unsqueeze(-2) & m
+
+    def forward(self, input: torch.Tensor, hidden: None) -> Tuple[torch.Tensor, None]:
+        """Compute LM loss value from buffer sequences.
+
+        Args:
+            input (torch.Tensor): Input ids. (batch, len)
+            hidden (torch.Tensor): Target ids. (batch, len)
+
+        """
+        x = self.embed(input)
+        mask = self._target_mask(input)
+        h, _ = self.encoder(x, mask)
+        y = self.decoder(h)
+        return y, None
 
     def score(
-        self,
-        y: torch.Tensor,
-        state: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]],
-        x: torch.Tensor,
-    ) -> Tuple[torch.Tensor, Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]]:
+        self, y: torch.Tensor, state: Any, x: torch.Tensor
+    ) -> Tuple[torch.Tensor, Any]:
         """Score new token.
 
         Args:
-            y: 1D torch.int64 prefix tokens.
+            y (torch.Tensor): 1D torch.int64 prefix tokens.
             state: Scorer state for prefix tokens
-            x: 2D encoder feature that generates ys.
+            x (torch.Tensor): encoder feature that generates ys.
 
         Returns:
-            Tuple of
-                torch.float32 scores for next token (n_vocab)
+            tuple[torch.Tensor, Any]: Tuple of
+                torch.float32 scores for next token (vocab_size)
                 and next state for ys
 
         """
-        y, new_state = self(y[-1].view(1, 1), state)
-        logp = y.log_softmax(dim=-1).view(-1)
-        return logp, new_state
+        y = y.unsqueeze(0)
+        h, _, cache = self.encoder.forward_one_step(
+            self.embed(y), self._target_mask(y), cache=state
+        )
+        h = self.decoder(h[:, -1])
+        logp = h.log_softmax(dim=-1).squeeze(0)
+        return logp, cache
 
     def batch_score(
-        self, ys: torch.Tensor, states: torch.Tensor, xs: torch.Tensor
-    ) -> Tuple[torch.Tensor, torch.Tensor]:
+        self, ys: torch.Tensor, states: List[Any], xs: torch.Tensor
+    ) -> Tuple[torch.Tensor, List[Any]]:
         """Score new token batch.
 
         Args:
             ys (torch.Tensor): torch.int64 prefix tokens (n_batch, ylen).
             states (List[Any]): Scorer states for prefix tokens.
             xs (torch.Tensor):
                 The encoder feature that generates ys (n_batch, xlen, n_feat).
 
         Returns:
             tuple[torch.Tensor, List[Any]]: Tuple of
-                batchfied scores for next token with shape of `(n_batch, n_vocab)`
+                batchfied scores for next token with shape of `(n_batch, vocab_size)`
                 and next state list for ys.
 
         """
+        # merge states
+        n_batch = len(ys)
+        n_layers = len(self.encoder.encoders)
         if states[0] is None:
-            states = None
-        elif isinstance(self.rnn, torch.nn.LSTM):
-            # states: Batch x 2 x (Nlayers, Dim) -> 2 x (Nlayers, Batch, Dim)
-            h = torch.stack([h for h, c in states], dim=1)
-            c = torch.stack([c for h, c in states], dim=1)
-            states = h, c
-        else:
-            # states: Batch x (Nlayers, Dim) -> (Nlayers, Batch, Dim)
-            states = torch.stack(states, dim=1)
-
-        ys, states = self(ys[:, -1:], states)
-        # ys: (Batch, 1, Nvocab) -> (Batch, NVocab)
-        assert ys.size(1) == 1, ys.shape
-        ys = ys.squeeze(1)
-        logp = ys.log_softmax(dim=-1)
-
-        # state: Change to batch first
-        if isinstance(self.rnn, torch.nn.LSTM):
-            # h, c: (Nlayers, Batch, Dim)
-            h, c = states
-            # states: Batch x 2 x (Nlayers, Dim)
-            states = [(h[:, i], c[:, i]) for i in range(h.size(1))]
+            batch_state = None
         else:
-            # states: (Nlayers, Batch, Dim) -> Batch x (Nlayers, Dim)
-            states = [states[:, i] for i in range(states.size(1))]
+            # transpose state of [batch, layer] into [layer, batch]
+            batch_state = [
+                torch.stack([states[b][i] for b in range(n_batch)])
+                for i in range(n_layers)
+            ]
+
+        # batch decoding
+        h, _, states = self.encoder.forward_one_step(
+            self.embed(ys), self._target_mask(ys), cache=batch_state
+        )
+        h = self.decoder(h[:, -1])
+        logp = h.log_softmax(dim=-1)
 
-        return logp, states
+        # transpose state of [layer, batch] into [batch, layer]
+        state_list = [[states[i][b] for i in range(n_layers)] for b in range(n_batch)]
+        return logp, state_list
```

### Comparing `espnet-202304/espnet2/main_funcs/average_nbest_models.py` & `espnet-202308/espnet2/main_funcs/average_nbest_models.py`

 * *Files 3% similar despite different names*

```diff
@@ -89,14 +89,15 @@
                 for k in avg:
                     if str(avg[k].dtype).startswith("torch.int"):
                         # For int type, not averaged, but only accumulated.
                         # e.g. BatchNorm.num_batches_tracked
                         # (If there are any cases that requires averaging
                         #  or the other reducing method, e.g. max/min, for integer type,
                         #  please report.)
+                        logging.info(f"Accumulating {k} instead of averaging")
                         pass
                     else:
                         avg[k] = avg[k] / n
 
                 # 2.b. Save the ave model and create a symlink
                 torch.save(avg, op)
```

### Comparing `espnet-202304/espnet2/main_funcs/calculate_all_attentions.py` & `espnet-202308/espnet2/main_funcs/calculate_all_attentions.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/main_funcs/collect_stats.py` & `espnet-202308/espnet2/main_funcs/collect_stats.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/main_funcs/pack_funcs.py` & `espnet-202308/espnet2/main_funcs/pack_funcs.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/mt/espnet_model.py` & `espnet-202308/espnet2/mt/espnet_model.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/mt/frontend/embedding.py` & `espnet-202308/espnet2/mt/frontend/embedding.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/optimizers/optim_groups.py` & `espnet-202308/espnet2/optimizers/optim_groups.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/optimizers/sgd.py` & `espnet-202308/espnet2/optimizers/sgd.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/samplers/build_batch_sampler.py` & `espnet-202308/espnet2/samplers/build_batch_sampler.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/samplers/folded_batch_sampler.py` & `espnet-202308/espnet2/samplers/folded_batch_sampler.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/samplers/length_batch_sampler.py` & `espnet-202308/espnet2/samplers/length_batch_sampler.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/samplers/num_elements_batch_sampler.py` & `espnet-202308/espnet2/samplers/num_elements_batch_sampler.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/samplers/sorted_batch_sampler.py` & `espnet-202308/espnet2/samplers/sorted_batch_sampler.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/samplers/unsorted_batch_sampler.py` & `espnet-202308/espnet2/samplers/unsorted_batch_sampler.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/schedulers/abs_scheduler.py` & `espnet-202308/espnet2/schedulers/abs_scheduler.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/schedulers/noam_lr.py` & `espnet-202308/espnet2/schedulers/noam_lr.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/schedulers/warmup_lr.py` & `espnet-202308/espnet2/schedulers/warmup_lr.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/schedulers/warmup_step_lr.py` & `espnet-202308/espnet2/schedulers/warmup_step_lr.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/slu/espnet_model.py` & `espnet-202308/espnet2/slu/espnet_model.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/slu/postdecoder/abs_postdecoder.py` & `espnet-202308/espnet2/slu/postdecoder/abs_postdecoder.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/slu/postdecoder/hugging_face_transformers_postdecoder.py` & `espnet-202308/espnet2/slu/postdecoder/hugging_face_transformers_postdecoder.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/slu/postencoder/conformer_postencoder.py` & `espnet-202308/espnet2/slu/postencoder/conformer_postencoder.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/slu/postencoder/transformer_postencoder.py` & `espnet-202308/espnet2/slu/postencoder/transformer_postencoder.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/st/espnet_model.py` & `espnet-202308/espnet2/asr/discrete_asr_espnet_model.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,436 +1,292 @@
-import logging
 from contextlib import contextmanager
 from typing import Dict, List, Optional, Tuple, Union
 
 import torch
 from packaging.version import parse as V
 from typeguard import check_argument_types
 
 from espnet2.asr.ctc import CTC
 from espnet2.asr.decoder.abs_decoder import AbsDecoder
 from espnet2.asr.encoder.abs_encoder import AbsEncoder
 from espnet2.asr.frontend.abs_frontend import AbsFrontend
 from espnet2.asr.postencoder.abs_postencoder import AbsPostEncoder
 from espnet2.asr.preencoder.abs_preencoder import AbsPreEncoder
 from espnet2.asr.specaug.abs_specaug import AbsSpecAug
-from espnet2.layers.abs_normalize import AbsNormalize
+from espnet2.mt.espnet_model import ESPnetMTModel
 from espnet2.torch_utils.device_funcs import force_gatherable
-from espnet2.train.abs_espnet_model import AbsESPnetModel
 from espnet.nets.e2e_asr_common import ErrorCalculator as ASRErrorCalculator
-from espnet.nets.e2e_mt_common import ErrorCalculator as MTErrorCalculator
 from espnet.nets.pytorch_backend.nets_utils import th_accuracy
 from espnet.nets.pytorch_backend.transformer.add_sos_eos import add_sos_eos
-from espnet.nets.pytorch_backend.transformer.label_smoothing_loss import (  # noqa: H301
-    LabelSmoothingLoss,
-)
 
 if V(torch.__version__) >= V("1.6.0"):
     from torch.cuda.amp import autocast
 else:
     # Nothing to do if torch<1.6.0
     @contextmanager
     def autocast(enabled=True):
         yield
 
 
-class ESPnetSTModel(AbsESPnetModel):
-    """CTC-attention hybrid Encoder-Decoder model"""
+class ESPnetDiscreteASRModel(ESPnetMTModel):
+    """Encoder-Decoder model"""
 
     def __init__(
         self,
         vocab_size: int,
         token_list: Union[Tuple[str, ...], List[str]],
         frontend: Optional[AbsFrontend],
         specaug: Optional[AbsSpecAug],
-        normalize: Optional[AbsNormalize],
         preencoder: Optional[AbsPreEncoder],
         encoder: AbsEncoder,
         postencoder: Optional[AbsPostEncoder],
         decoder: AbsDecoder,
-        extra_asr_decoder: Optional[AbsDecoder],
-        extra_mt_decoder: Optional[AbsDecoder],
         ctc: Optional[CTC],
-        src_vocab_size: Optional[int],
-        src_token_list: Optional[Union[Tuple[str, ...], List[str]]],
-        asr_weight: float = 0.0,
-        mt_weight: float = 0.0,
-        mtlalpha: float = 0.0,
+        ctc_weight: float = 0.5,
+        interctc_weight: float = 0.0,
+        src_vocab_size: int = 0,
+        src_token_list: Union[Tuple[str, ...], List[str]] = [],
         ignore_id: int = -1,
         lsm_weight: float = 0.0,
         length_normalized_loss: bool = False,
-        report_cer: bool = True,
-        report_wer: bool = True,
         report_bleu: bool = True,
         sym_space: str = "<space>",
         sym_blank: str = "<blank>",
         extract_feats_in_collect_stats: bool = True,
+        share_decoder_input_output_embed: bool = False,
+        share_encoder_decoder_input_embed: bool = False,
     ):
         assert check_argument_types()
-        assert 0.0 <= asr_weight < 1.0, "asr_weight should be [0.0, 1.0)"
-        assert 0.0 <= mt_weight < 1.0, "mt_weight should be [0.0, 1.0)"
-        assert 0.0 <= mtlalpha <= 1.0, "mtlalpha should be [0.0, 1.0]"
+        assert 0.0 <= ctc_weight <= 1.0, ctc_weight
 
-        super().__init__()
-        # note that eos is the same as sos (equivalent ID)
-        self.sos = vocab_size - 1
-        self.eos = vocab_size - 1
-        self.src_sos = src_vocab_size - 1 if src_vocab_size else None
-        self.src_eos = src_vocab_size - 1 if src_vocab_size else None
-        self.vocab_size = vocab_size
-        self.src_vocab_size = src_vocab_size
-        self.ignore_id = ignore_id
-        self.asr_weight = asr_weight
-        self.mt_weight = mt_weight
-        self.mtlalpha = mtlalpha
-        self.token_list = token_list.copy()
-
-        self.frontend = frontend
-        self.specaug = specaug
-        self.normalize = normalize
-        self.preencoder = preencoder
-        self.postencoder = postencoder
-        self.encoder = encoder
-        self.decoder = (
-            decoder  # TODO(jiatong): directly implement multi-decoder structure at here
-        )
-
-        self.criterion_st = LabelSmoothingLoss(
-            size=vocab_size,
-            padding_idx=ignore_id,
-            smoothing=lsm_weight,
-            normalize_length=length_normalized_loss,
+        super().__init__(
+            vocab_size=vocab_size,
+            token_list=token_list,
+            frontend=frontend,
+            preencoder=preencoder,
+            encoder=encoder,
+            postencoder=postencoder,
+            decoder=decoder,
+            src_vocab_size=src_vocab_size,
+            src_token_list=src_token_list,
+            ignore_id=ignore_id,
+            lsm_weight=lsm_weight,
+            length_normalized_loss=length_normalized_loss,
+            report_bleu=report_bleu,
+            sym_space=sym_space,
+            sym_blank=sym_blank,
+            extract_feats_in_collect_stats=extract_feats_in_collect_stats,
+            share_decoder_input_output_embed=share_decoder_input_output_embed,
+            share_encoder_decoder_input_embed=share_encoder_decoder_input_embed,
         )
 
-        self.criterion_asr = LabelSmoothingLoss(
-            size=src_vocab_size,
-            padding_idx=ignore_id,
-            smoothing=lsm_weight,
-            normalize_length=length_normalized_loss,
-        )
-
-        # submodule for ASR task
-        if self.asr_weight > 0:
-            assert (
-                src_token_list is not None
-            ), "Missing src_token_list, cannot add asr module to st model"
-            if self.mtlalpha > 0.0:
-                self.ctc = ctc
-            if self.mtlalpha < 1.0:
-                self.extra_asr_decoder = extra_asr_decoder
-            elif extra_asr_decoder is not None:
-                logging.warning(
-                    "Not using extra_asr_decoder because "
-                    "mtlalpha is set as {} (== 1.0)".format(mtlalpha),
-                )
-
-        # submodule for MT task
-        if self.mt_weight > 0:
-            self.extra_mt_decoder = extra_mt_decoder
-        elif extra_mt_decoder is not None:
-            logging.warning(
-                "Not using extra_mt_decoder because "
-                "mt_weight is set as {} (== 0)".format(mt_weight),
-            )
+        self.specaug = specaug
+        # note that eos is the same as sos (equivalent ID)
+        self.blank_id = 0
+        self.ctc_weight = ctc_weight
+        self.interctc_weight = interctc_weight
 
-        # MT error calculator
+        if ctc_weight == 0.0:
+            self.ctc = None
+        else:
+            self.ctc = ctc
         if report_bleu:
-            self.mt_error_calculator = MTErrorCalculator(
-                token_list, sym_space, sym_blank, report_bleu
+            self.error_calculator = ASRErrorCalculator(
+                token_list, sym_space, sym_blank, True, True
             )
-        else:
-            self.mt_error_calculator = None
 
-        # ASR error calculator
-        if self.asr_weight > 0 and (report_cer or report_wer):
-            assert (
-                src_token_list is not None
-            ), "Missing src_token_list, cannot add asr module to st model"
-            self.asr_error_calculator = ASRErrorCalculator(
-                src_token_list, sym_space, sym_blank, report_cer, report_wer
+        if not hasattr(self.encoder, "interctc_use_conditioning"):
+            self.encoder.interctc_use_conditioning = False
+        if self.encoder.interctc_use_conditioning:
+            self.encoder.conditioning_layer = torch.nn.Linear(
+                vocab_size, self.encoder.output_size()
             )
-        else:
-            self.asr_error_calculator = None
-
-        self.extract_feats_in_collect_stats = extract_feats_in_collect_stats
-
-        # TODO(jiatong): add multilingual related functions
 
     def forward(
         self,
-        speech: torch.Tensor,
-        speech_lengths: torch.Tensor,
         text: torch.Tensor,
         text_lengths: torch.Tensor,
-        src_text: Optional[torch.Tensor] = None,
-        src_text_lengths: Optional[torch.Tensor] = None,
+        src_text: torch.Tensor,
+        src_text_lengths: torch.Tensor,
         **kwargs,
     ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor]:
         """Frontend + Encoder + Decoder + Calc loss
 
         Args:
-            speech: (Batch, Length, ...)
-            speech_lengths: (Batch,)
             text: (Batch, Length)
             text_lengths: (Batch,)
             src_text: (Batch, length)
             src_text_lengths: (Batch,)
             kwargs: "utt_id" is among the input.
         """
         assert text_lengths.dim() == 1, text_lengths.shape
         # Check that batch_size is unified
         assert (
-            speech.shape[0]
-            == speech_lengths.shape[0]
-            == text.shape[0]
+            text.shape[0]
             == text_lengths.shape[0]
-        ), (speech.shape, speech_lengths.shape, text.shape, text_lengths.shape)
-
-        # additional checks with valid src_text
-        if src_text is not None:
-            assert src_text_lengths.dim() == 1, src_text_lengths.shape
-            assert text.shape[0] == src_text.shape[0] == src_text_lengths.shape[0], (
-                text.shape,
-                src_text.shape,
-                src_text_lengths.shape,
-            )
+            == src_text.shape[0]
+            == src_text_lengths.shape[0]
+        ), (text.shape, text_lengths.shape, src_text.shape, src_text_lengths.shape)
 
-        batch_size = speech.shape[0]
+        batch_size = src_text.shape[0]
 
         # for data-parallel
         text = text[:, : text_lengths.max()]
-        if src_text is not None:
-            src_text = src_text[:, : src_text_lengths.max()]
+        src_text = src_text[:, : src_text_lengths.max()]
 
         # 1. Encoder
-        encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)
-
-        # 2a. Attention-decoder branch (ST)
-        loss_st_att, acc_st_att, bleu_st_att = self._calc_mt_att_loss(
-            encoder_out, encoder_out_lens, text, text_lengths, st=True
-        )
+        encoder_out, encoder_out_lens = self.encode(src_text, src_text_lengths)
+        intermediate_outs = None
+        if isinstance(encoder_out, tuple):
+            intermediate_outs = encoder_out[1]
+            encoder_out = encoder_out[0]
+        loss_ctc, cer_ctc = None, None
+        stats = dict()
+
+        # 1. CTC branch
+        if self.ctc_weight != 0.0:
+            loss_ctc, cer_ctc = self._calc_ctc_loss(
+                encoder_out, encoder_out_lens, text, text_lengths
+            )
+
+            # Collect CTC branch stats
+            stats["loss_ctc"] = loss_ctc.detach() if loss_ctc is not None else None
+            stats["cer_ctc"] = cer_ctc
+
+        # Intermediate CTC (optional)
+        loss_interctc = 0.0
+        if self.interctc_weight != 0.0 and intermediate_outs is not None:
+            for layer_idx, intermediate_out in intermediate_outs:
+                # we assume intermediate_out has the same length & padding
+                # as those of encoder_out
+                loss_ic, cer_ic = self._calc_ctc_loss(
+                    intermediate_out, encoder_out_lens, text, text_lengths
+                )
+                loss_interctc = loss_interctc + loss_ic
 
-        # 2b. CTC branch
-        if self.asr_weight > 0:
-            assert src_text is not None, "missing source text for asr sub-task of ST"
-
-        if self.asr_weight > 0 and self.mtlalpha > 0:
-            loss_asr_ctc, cer_asr_ctc = self._calc_ctc_loss(
-                encoder_out, encoder_out_lens, src_text, src_text_lengths
-            )
-        else:
-            loss_asr_ctc, cer_asr_ctc = 0, None
+                # Collect Intermedaite CTC stats
+                stats["loss_interctc_layer{}".format(layer_idx)] = (
+                    loss_ic.detach() if loss_ic is not None else None
+                )
+                stats["cer_interctc_layer{}".format(layer_idx)] = cer_ic
 
-        # 2c. Attention-decoder branch (extra ASR)
-        if self.asr_weight > 0 and self.mtlalpha < 1.0:
-            (
-                loss_asr_att,
-                acc_asr_att,
-                cer_asr_att,
-                wer_asr_att,
-            ) = self._calc_asr_att_loss(
-                encoder_out, encoder_out_lens, src_text, src_text_lengths
-            )
-        else:
-            loss_asr_att, acc_asr_att, cer_asr_att, wer_asr_att = 0, None, None, None
+            loss_interctc = loss_interctc / len(intermediate_outs)
 
-        # 2d. Attention-decoder branch (extra MT)
-        if self.mt_weight > 0:
-            loss_mt_att, acc_mt_att = self._calc_mt_att_loss(
-                encoder_out, encoder_out_lens, text, text_lengths, st=False
-            )
-        else:
-            loss_mt_att, acc_mt_att = 0, None
+            # calculate whole encoder loss
+            loss_ctc = (
+                1 - self.interctc_weight
+            ) * loss_ctc + self.interctc_weight * loss_interctc
+
+        # 2a. Attention-decoder branch (MT)
+        loss_att, acc_att, cer_att, wer_att = self._calc_att_loss(
+            encoder_out, encoder_out_lens, text, text_lengths
+        )
 
         # 3. Loss computation
-        asr_ctc_weight = self.mtlalpha
-        loss_st = loss_st_att
-        if asr_ctc_weight == 1.0:
-            loss_asr = loss_asr_ctc
-        elif asr_ctc_weight == 0.0:
-            loss_asr = loss_asr_att
+        if self.ctc_weight > 0.0:
+            loss = self.ctc_weight * loss_ctc + (1 - self.ctc_weight) * loss_att
         else:
-            loss_asr = (
-                asr_ctc_weight * loss_asr_ctc + (1 - asr_ctc_weight) * loss_asr_att
-            )
-        loss_mt = self.mt_weight * loss_mt_att
-        loss = (
-            (1 - self.asr_weight - self.mt_weight) * loss_st
-            + self.asr_weight * loss_asr
-            + self.mt_weight * loss_mt
-        )
+            loss = loss_att
 
-        stats = dict(
-            loss=loss.detach(),
-            loss_asr=loss_asr.detach()
-            if type(loss_asr) not in {float, int}
-            else loss_asr,
-            loss_mt=loss_mt.detach() if type(loss_mt) is not float else loss_mt,
-            loss_st=loss_st.detach(),
-            acc_asr=acc_asr_att,
-            acc_mt=acc_mt_att,
-            acc=acc_st_att,
-            cer_ctc=cer_asr_ctc,
-            cer=cer_asr_att,
-            wer=wer_asr_att,
-            bleu=bleu_st_att,
-        )
+        stats["loss_att"] = loss_att.detach() if loss_att is not None else None
+        stats["acc"] = acc_att
+        stats["cer"] = cer_att
+        stats["wer"] = wer_att
+
+        stats["loss"] = loss.detach()
 
         # force_gatherable: to-device and to-tensor if scalar for DataParallel
         loss, stats, weight = force_gatherable((loss, stats, batch_size), loss.device)
         return loss, stats, weight
 
-    def collect_feats(
-        self,
-        speech: torch.Tensor,
-        speech_lengths: torch.Tensor,
-        text: torch.Tensor,
-        text_lengths: torch.Tensor,
-        src_text: Optional[torch.Tensor] = None,
-        src_text_lengths: Optional[torch.Tensor] = None,
-        **kwargs,
-    ) -> Dict[str, torch.Tensor]:
-        feats, feats_lengths = self._extract_feats(speech, speech_lengths)
-        return {"feats": feats, "feats_lengths": feats_lengths}
-
     def encode(
-        self, speech: torch.Tensor, speech_lengths: torch.Tensor
+        self, src_text: torch.Tensor, src_text_lengths: torch.Tensor
     ) -> Tuple[torch.Tensor, torch.Tensor]:
-        """Frontend + Encoder. Note that this method is used by st_inference.py
+        """Frontend + Encoder. Note that this method is used by mt_inference.py
 
         Args:
-            speech: (Batch, Length, ...)
-            speech_lengths: (Batch, )
+            src_text: (Batch, Length, ...)
+            src_text_lengths: (Batch, )
         """
         with autocast(False):
             # 1. Extract feats
-            feats, feats_lengths = self._extract_feats(speech, speech_lengths)
+            feats, feats_lengths = self._extract_feats(src_text, src_text_lengths)
 
             # 2. Data augmentation
             if self.specaug is not None and self.training:
                 feats, feats_lengths = self.specaug(feats, feats_lengths)
 
-            # 3. Normalization for feature: e.g. Global-CMVN, Utterance-CMVN
-            if self.normalize is not None:
-                feats, feats_lengths = self.normalize(feats, feats_lengths)
-
         # Pre-encoder, e.g. used for raw input data
         if self.preencoder is not None:
             feats, feats_lengths = self.preencoder(feats, feats_lengths)
 
         # 4. Forward encoder
         # feats: (Batch, Length, Dim)
         # -> encoder_out: (Batch, Length2, Dim2)
-        encoder_out, encoder_out_lens, _ = self.encoder(feats, feats_lengths)
+        # encoder_out, encoder_out_lens, _ = self.encoder(feats, feats_lengths)
+        if self.encoder.interctc_use_conditioning:
+            encoder_out, encoder_out_lens, _ = self.encoder(
+                feats, feats_lengths, ctc=self.ctc
+            )
+        else:
+            encoder_out, encoder_out_lens, _ = self.encoder(feats, feats_lengths)
+        intermediate_outs = None
+        if isinstance(encoder_out, tuple):
+            intermediate_outs = encoder_out[1]
+            encoder_out = encoder_out[0]
 
         # Post-encoder, e.g. NLU
         if self.postencoder is not None:
             encoder_out, encoder_out_lens = self.postencoder(
                 encoder_out, encoder_out_lens
             )
 
-        assert encoder_out.size(0) == speech.size(0), (
+        assert encoder_out.size(0) == src_text.size(0), (
             encoder_out.size(),
-            speech.size(0),
+            src_text.size(0),
         )
         assert encoder_out.size(1) <= encoder_out_lens.max(), (
             encoder_out.size(),
             encoder_out_lens.max(),
         )
 
-        return encoder_out, encoder_out_lens
-
-    def _extract_feats(
-        self, speech: torch.Tensor, speech_lengths: torch.Tensor
-    ) -> Tuple[torch.Tensor, torch.Tensor]:
-        assert speech_lengths.dim() == 1, speech_lengths.shape
+        if intermediate_outs is not None:
+            return (encoder_out, intermediate_outs), encoder_out_lens
 
-        # for data-parallel
-        speech = speech[:, : speech_lengths.max()]
-
-        if self.frontend is not None:
-            # Frontend
-            #  e.g. STFT and Feature extract
-            #       data_loader may send time-domain signal in this case
-            # speech (Batch, NSamples) -> feats: (Batch, NFrames, Dim)
-            feats, feats_lengths = self.frontend(speech, speech_lengths)
-        else:
-            # No frontend and no feature extract
-            feats, feats_lengths = speech, speech_lengths
-        return feats, feats_lengths
+        return encoder_out, encoder_out_lens
 
-    def _calc_mt_att_loss(
+    def _calc_att_loss(
         self,
         encoder_out: torch.Tensor,
         encoder_out_lens: torch.Tensor,
         ys_pad: torch.Tensor,
         ys_pad_lens: torch.Tensor,
-        st: bool = True,
     ):
         ys_in_pad, ys_out_pad = add_sos_eos(ys_pad, self.sos, self.eos, self.ignore_id)
         ys_in_lens = ys_pad_lens + 1
 
         # 1. Forward decoder
-        if st:
-            decoder_out, _ = self.decoder(
-                encoder_out, encoder_out_lens, ys_in_pad, ys_in_lens
-            )
-        else:
-            decoder_out, _ = self.extra_mt_decoder(
-                encoder_out, encoder_out_lens, ys_in_pad, ys_in_lens
-            )
-
-        # 2. Compute attention loss
-        loss_att = self.criterion_st(decoder_out, ys_out_pad)
-        acc_att = th_accuracy(
-            decoder_out.view(-1, self.vocab_size),
-            ys_out_pad,
-            ignore_label=self.ignore_id,
-        )
-
-        # Compute cer/wer using attention-decoder
-        if self.training or self.mt_error_calculator is None:
-            bleu_att = None
-        else:
-            ys_hat = decoder_out.argmax(dim=-1)
-            bleu_att = self.mt_error_calculator(ys_hat.cpu(), ys_pad.cpu())
-
-        return loss_att, acc_att, bleu_att
-
-    def _calc_asr_att_loss(
-        self,
-        encoder_out: torch.Tensor,
-        encoder_out_lens: torch.Tensor,
-        ys_pad: torch.Tensor,
-        ys_pad_lens: torch.Tensor,
-    ):
-        ys_in_pad, ys_out_pad = add_sos_eos(
-            ys_pad, self.src_sos, self.src_eos, self.ignore_id
-        )
-        ys_in_lens = ys_pad_lens + 1
-
-        # 1. Forward decoder
-        decoder_out, _ = self.extra_asr_decoder(
+        decoder_out, _ = self.decoder(
             encoder_out, encoder_out_lens, ys_in_pad, ys_in_lens
         )
 
         # 2. Compute attention loss
-        loss_att = self.criterion_asr(decoder_out, ys_out_pad)
+        loss_att = self.criterion_mt(decoder_out, ys_out_pad)
         acc_att = th_accuracy(
-            decoder_out.view(-1, self.src_vocab_size),
+            decoder_out.view(-1, self.vocab_size),
             ys_out_pad,
             ignore_label=self.ignore_id,
         )
 
         # Compute cer/wer using attention-decoder
-        if self.training or self.asr_error_calculator is None:
+        if self.training or self.error_calculator is None:
             cer_att, wer_att = None, None
         else:
             ys_hat = decoder_out.argmax(dim=-1)
-            cer_att, wer_att = self.asr_error_calculator(ys_hat.cpu(), ys_pad.cpu())
+            cer_att, wer_att = self.error_calculator(ys_hat.cpu(), ys_pad.cpu())
 
         return loss_att, acc_att, cer_att, wer_att
 
     def _calc_ctc_loss(
         self,
         encoder_out: torch.Tensor,
         encoder_out_lens: torch.Tensor,
@@ -438,11 +294,11 @@
         ys_pad_lens: torch.Tensor,
     ):
         # Calc CTC loss
         loss_ctc = self.ctc(encoder_out, encoder_out_lens, ys_pad, ys_pad_lens)
 
         # Calc CER using CTC
         cer_ctc = None
-        if not self.training and self.asr_error_calculator is not None:
+        if not self.training and self.error_calculator is not None:
             ys_hat = self.ctc.argmax(encoder_out).data
-            cer_ctc = self.asr_error_calculator(ys_hat.cpu(), ys_pad.cpu(), is_ctc=True)
+            cer_ctc = self.error_calculator(ys_hat.cpu(), ys_pad.cpu(), is_ctc=True)
         return loss_ctc, cer_ctc
```

### Comparing `espnet-202304/espnet2/svs/abs_svs.py` & `espnet-202308/espnet2/svs/abs_svs.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/svs/espnet_model.py` & `espnet-202308/espnet2/svs/espnet_model.py`

 * *Files 12% similar despite different names*

```diff
@@ -38,14 +38,15 @@
     def __init__(
         self,
         text_extract: Optional[AbsFeatsExtract],
         feats_extract: Optional[AbsFeatsExtract],
         score_feats_extract: Optional[AbsFeatsExtract],
         label_extract: Optional[AbsFeatsExtract],
         pitch_extract: Optional[AbsFeatsExtract],
+        ying_extract: Optional[AbsFeatsExtract],
         duration_extract: Optional[AbsFeatsExtract],
         energy_extract: Optional[AbsFeatsExtract],
         normalize: Optional[AbsNormalize and InversibleInterface],
         pitch_normalize: Optional[AbsNormalize and InversibleInterface],
         energy_normalize: Optional[AbsNormalize and InversibleInterface],
         svs: AbsSVS,
     ):
@@ -53,14 +54,15 @@
         assert check_argument_types()
         super().__init__()
         self.text_extract = text_extract
         self.feats_extract = feats_extract
         self.score_feats_extract = score_feats_extract
         self.label_extract = label_extract
         self.pitch_extract = pitch_extract
+        self.ying_extract = ying_extract
         self.duration_extract = duration_extract
         self.energy_extract = energy_extract
         self.normalize = normalize
         self.pitch_normalize = pitch_normalize
         self.energy_normalize = energy_normalize
         self.svs = svs
 
@@ -85,14 +87,16 @@
         duration_syb_lengths: Optional[torch.Tensor] = None,
         slur: Optional[torch.Tensor] = None,
         slur_lengths: Optional[torch.Tensor] = None,
         pitch: Optional[torch.Tensor] = None,
         pitch_lengths: Optional[torch.Tensor] = None,
         energy: Optional[torch.Tensor] = None,
         energy_lengths: Optional[torch.Tensor] = None,
+        ying: Optional[torch.Tensor] = None,
+        ying_lengths: Optional[torch.Tensor] = None,
         spembs: Optional[torch.Tensor] = None,
         sids: Optional[torch.Tensor] = None,
         lids: Optional[torch.Tensor] = None,
         flag_IsValid=False,
         **kwargs,
     ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor]:
         """Caclualte outputs and return the loss tensor.
@@ -216,14 +220,16 @@
                 label_score = label[:, : label_score_lengths.max()]
                 midi_score = midi[:, : midi_score_lengths.max()]
                 duration_score = duration_ruled_phn[
                     :, : duration_score_phn_lengths.max()
                 ]
                 duration_score_syb = duration_syb[:, : duration_score_syb_lengths.max()]
                 slur = slur[:, : slur_lengths.max()]
+            else:
+                raise RuntimeError("Cannot understand score_feats extract type")
 
             if self.pitch_extract is not None and pitch is None:
                 pitch, pitch_lengths = self.pitch_extract(
                     input=singing,
                     input_lengths=singing_lengths,
                     feats_lengths=feats_lengths,
                 )
@@ -231,14 +237,21 @@
             if self.energy_extract is not None and energy is None:
                 energy, energy_lengths = self.energy_extract(
                     singing,
                     singing_lengths,
                     feats_lengths=feats_lengths,
                 )
 
+            if self.ying_extract is not None and ying is None:
+                ying, ying_lengths = self.ying_extract(
+                    singing,
+                    singing_lengths,
+                    feats_lengths=feats_lengths,
+                )
+
             # Normalize
             if self.normalize is not None:
                 feats, feats_lengths = self.normalize(feats, feats_lengths)
             if self.pitch_normalize is not None:
                 pitch, pitch_lengths = self.pitch_normalize(pitch, pitch_lengths)
             if self.energy_normalize is not None:
                 energy, energy_lengths = self.energy_normalize(energy, energy_lengths)
@@ -306,14 +319,16 @@
             batch.update(sids=sids)
         if lids is not None:
             batch.update(lids=lids)
         if self.pitch_extract is not None and pitch is not None:
             batch.update(pitch=pitch, pitch_lengths=pitch_lengths)
         if self.energy_extract is not None and energy is not None:
             batch.update(energy=energy, energy_lengths=energy_lengths)
+        if self.ying_extract is not None and ying is not None:
+            batch.update(ying=ying)
         if self.svs.require_raw_singing:
             batch.update(singing=singing, singing_lengths=singing_lengths)
         return self.svs(**batch)
 
     def collect_feats(
         self,
         text: torch.Tensor,
@@ -333,14 +348,16 @@
         duration_syb_lengths: Optional[torch.Tensor] = None,
         slur: Optional[torch.Tensor] = None,
         slur_lengths: Optional[torch.Tensor] = None,
         pitch: Optional[torch.Tensor] = None,
         pitch_lengths: Optional[torch.Tensor] = None,
         energy: Optional[torch.Tensor] = None,
         energy_lengths: Optional[torch.Tensor] = None,
+        ying: Optional[torch.Tensor] = None,
+        ying_lengths: Optional[torch.Tensor] = None,
         spembs: Optional[torch.Tensor] = None,
         sids: Optional[torch.Tensor] = None,
         lids: Optional[torch.Tensor] = None,
         **kwargs,
     ) -> Dict[str, torch.Tensor]:
         """Caclualte features and return them as a dict.
 
@@ -407,23 +424,31 @@
             )
         if self.energy_extract is not None:
             energy, energy_lengths = self.energy_extract(
                 singing,
                 singing_lengths,
                 feats_lengths=feats_lengths,
             )
+        if self.ying_extract is not None and ying is None:
+            ying, ying_lengths = self.ying_extract(
+                singing,
+                singing_lengths,
+                feats_lengths=feats_lengths,
+            )
 
         # store in dict
         feats_dict = {}
         if feats is not None:
             feats_dict.update(feats=feats, feats_lengths=feats_lengths)
         if pitch is not None:
             feats_dict.update(pitch=pitch, pitch_lengths=pitch_lengths)
         if energy is not None:
             feats_dict.update(energy=energy, energy_lengths=energy_lengths)
+        if ying is not None:
+            feats_dict.update(ying=ying, ying_lengths=ying_lengths)
 
         return feats_dict
 
     def inference(
         self,
         text: torch.Tensor,
         singing: Optional[torch.Tensor] = None,
@@ -535,14 +560,48 @@
             label_score = label[:, : label_lengths.max()]
             midi_score = midi[:, : midi_lengths.max()]
             duration_score = duration_ruled_phn[:, : duration_ruled_phn_lengths.max()]
             duration_score_syb = duration_syb[:, : duration_syb_lengths.max()]
             slur = slur[:, : slur_lengths.max()]
 
         input_dict = dict(text=text)
+        if decode_config["use_teacher_forcing"] or getattr(self.svs, "use_gst", False):
+            if singing is None:
+                raise RuntimeError("missing required argument: 'singing'")
+            if self.feats_extract is not None:
+                feats = self.feats_extract(singing[None])[0][0]
+            else:
+                # Use precalculated feats (feats_type != raw case)
+                feats = singing
+            if self.normalize is not None:
+                feats = self.normalize(feats[None])[0][0]
+            input_dict.update(feats=feats)
+            # if self.svs.require_raw_singing:
+            #     input_dict.update(singing=singing)
+
+        if decode_config["use_teacher_forcing"]:
+            if self.pitch_extract is not None:
+                pitch = self.pitch_extract(
+                    singing[None],
+                    feats_lengths=torch.LongTensor([len(feats)]),
+                )[0][0]
+            if self.pitch_normalize is not None:
+                pitch = self.pitch_normalize(pitch[None])[0][0]
+            if pitch is not None:
+                input_dict.update(pitch=pitch)
+
+            if self.energy_extract is not None:
+                energy = self.energy_extract(
+                    singing[None],
+                    feats_lengths=torch.LongTensor([len(feats)]),
+                )[0][0]
+            if self.energy_normalize is not None:
+                energy = self.energy_normalize(energy[None])[0][0]
+            if energy is not None:
+                input_dict.update(energy=energy)
 
         # label
         label = dict()
         if label_lab is not None:
             label_lab = label_lab.to(dtype=torch.long)
             label.update(lab=label_lab)
         if label_score is not None:
@@ -571,16 +630,14 @@
         if duration_score_syb is not None:
             duration_syb_score = duration_score_syb.to(dtype=torch.long)
             duration.update(score_syb=duration_syb_score)
         input_dict.update(duration=duration)
 
         if slur is not None:
             input_dict.update(slur=slur)
-        if pitch is not None:
-            input_dict.update(pitch=pitch)
         if spembs is not None:
             input_dict.update(spembs=spembs)
         if sids is not None:
             input_dict.update(sids=sids)
         if lids is not None:
             input_dict.update(lids=lids)
```

### Comparing `espnet-202304/espnet2/svs/feats_extract/score_feats_extract.py` & `espnet-202308/espnet2/svs/feats_extract/score_feats_extract.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/svs/naive_rnn/naive_rnn.py` & `espnet-202308/espnet2/svs/naive_rnn/naive_rnn.py`

 * *Files 3% similar despite different names*

```diff
@@ -123,15 +123,53 @@
         init_type: str = "xavier_uniform",
         use_masking: bool = False,
         use_weighted_masking: bool = False,
         loss_type: str = "L1",
     ):
         """Initialize NaiveRNN module.
 
-        Args: TODO(Yuning)
+        Args:
+            idim (int): Dimension of the label inputs.
+            odim (int): Dimension of the outputs.
+            midi_dim (int): Dimension of the midi inputs.
+            embed_dim (int): Dimension of the token embedding.
+            eprenet_conv_layers (int): Number of prenet conv layers.
+            eprenet_conv_filts (int): Number of prenet conv filter size.
+            eprenet_conv_chans (int): Number of prenet conv filter channels.
+            elayers (int): Number of encoder layers.
+            eunits (int): Number of encoder hidden units.
+            ebidirectional (bool): If bidirectional in encoder.
+            midi_embed_integration_type (str): how to integrate midi information,
+                ("add" or "cat").
+            dlayers (int): Number of decoder lstm layers.
+            dunits (int): Number of decoder lstm units.
+            dbidirectional (bool): if bidirectional in decoder.
+            postnet_layers (int): Number of postnet layers.
+            postnet_filts (int): Number of postnet filter size.
+            postnet_chans (int): Number of postnet filter channels.
+            use_batch_norm (bool): Whether to use batch normalization.
+            reduction_factor (int): Reduction factor.
+            # extra embedding related
+            spks (Optional[int]): Number of speakers. If set to > 1, assume that the
+                sids will be provided as the input and use sid embedding layer.
+            langs (Optional[int]): Number of languages. If set to > 1, assume that the
+                lids will be provided as the input and use sid embedding layer.
+            spk_embed_dim (Optional[int]): Speaker embedding dimension. If set to > 0,
+                assume that spembs will be provided as the input.
+            spk_embed_integration_type (str): How to integrate speaker embedding.
+            eprenet_dropout_rate (float): Prenet dropout rate.
+            edropout_rate (float): Encoder dropout rate.
+            ddropout_rate (float): Decoder dropout rate.
+            postnet_dropout_rate (float): Postnet dropout_rate.
+            init_type (str): How to initialize transformer parameters.
+            use_masking (bool): Whether to mask padded part in loss calculation.
+            use_weighted_masking (bool): Whether to apply weighted masking in
+                loss calculation.
+            loss_type (str): Loss function type ("L1", "L2", or "L1+L2").
+
         """
         assert check_argument_types()
         super().__init__()
 
         # store hyperparameters
         self.idim = idim
         self.midi_dim = midi_dim
@@ -207,15 +245,15 @@
 
         dim_direction = 2 if ebidirectional is True else 1
         if self.midi_embed_integration_type == "add":
             self.midi_projection = torch.nn.Linear(
                 eunits * dim_direction, eunits * dim_direction
             )
         else:
-            self.midi_projection = torch.nn.linear(
+            self.midi_projection = torch.nn.Linear(
                 2 * eunits * dim_direction, eunits * dim_direction
             )
 
         self.decoder = torch.nn.LSTM(
             input_size=eunits,
             hidden_size=eunits,
             num_layers=dlayers,
@@ -494,14 +532,16 @@
         # integrate spk & lang embeddings
         if self.spks is not None:
             sid_embs = self.sid_emb(sids.view(-1))
             hs = hs + sid_embs.unsqueeze(1)
         if self.langs is not None:
             lid_embs = self.lid_emb(lids.view(-1))
             hs = hs + lid_embs.unsqueeze(1)
+        if spembs is not None:
+            spembs = spembs.unsqueeze(0)
 
         # integrate speaker embedding
         if self.spk_embed_dim is not None:
             hs = self._integrate_with_spk_embed(hs, spembs)
 
         # (B, T_feats//r, odim * r) -> (B, T_feats//r * r, odim)
         before_outs = F.leaky_relu(self.feat_out(hs).view(hs.size(0), -1, self.odim))
```

### Comparing `espnet-202304/espnet2/svs/naive_rnn/naive_rnn_dp.py` & `espnet-202308/espnet2/tts/fastspeech/fastspeech.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,251 +1,385 @@
-# Copyright 2021 Carnegie Mellon University (Jiatong Shi)
+# Copyright 2020 Nagoya University (Tomoki Hayashi)
 #  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)
 
-"""NaiveRNN-DP-SVS related modules."""
+"""Fastspeech related modules for ESPnet2."""
 
-from typing import Dict, Optional, Tuple
+import logging
+from typing import Dict, Optional, Sequence, Tuple
 
 import torch
 import torch.nn.functional as F
 from typeguard import check_argument_types
 
-from espnet2.svs.abs_svs import AbsSVS
 from espnet2.torch_utils.device_funcs import force_gatherable
 from espnet2.torch_utils.initialize import initialize
+from espnet2.tts.abs_tts import AbsTTS
+from espnet2.tts.gst.style_encoder import StyleEncoder
+from espnet.nets.pytorch_backend.conformer.encoder import Encoder as ConformerEncoder
 from espnet.nets.pytorch_backend.e2e_tts_fastspeech import (
     FeedForwardTransformerLoss as FastSpeechLoss,
 )
 from espnet.nets.pytorch_backend.fastspeech.duration_predictor import DurationPredictor
 from espnet.nets.pytorch_backend.fastspeech.length_regulator import LengthRegulator
-from espnet.nets.pytorch_backend.nets_utils import make_pad_mask
+from espnet.nets.pytorch_backend.nets_utils import make_non_pad_mask, make_pad_mask
 from espnet.nets.pytorch_backend.tacotron2.decoder import Postnet
-from espnet.nets.pytorch_backend.tacotron2.encoder import Encoder as EncoderPrenet
+from espnet.nets.pytorch_backend.transformer.embedding import (
+    PositionalEncoding,
+    ScaledPositionalEncoding,
+)
+from espnet.nets.pytorch_backend.transformer.encoder import (
+    Encoder as TransformerEncoder,
+)
+
 
+class FastSpeech(AbsTTS):
+    """FastSpeech module for end-to-end text-to-speech.
 
-class NaiveRNNDP(AbsSVS):
-    """NaiveRNNDP-SVS module.
+    This is a module of FastSpeech, feed-forward Transformer with duration predictor
+    described in `FastSpeech: Fast, Robust and Controllable Text to Speech`_, which
+    does not require any auto-regressive processing during inference, resulting in
+    fast decoding compared with auto-regressive Transformer.
+
+    .. _`FastSpeech: Fast, Robust and Controllable Text to Speech`:
+        https://arxiv.org/pdf/1905.09263.pdf
 
-    This is an implementation of naive RNN with duration prediction
-    for singing voice synthesis
-    The features are processed directly over time-domain from music score and
-    predict the singing voice features
     """
 
     def __init__(
         self,
         # network structure related
         idim: int,
         odim: int,
-        midi_dim: int = 129,
-        embed_dim: int = 512,
-        tempo_dim: int = 500,
-        eprenet_conv_layers: int = 3,
-        eprenet_conv_chans: int = 256,
-        eprenet_conv_filts: int = 5,
-        elayers: int = 3,
-        eunits: int = 1024,
-        ebidirectional: bool = True,
-        midi_embed_integration_type: str = "add",
-        dlayers: int = 3,
-        dunits: int = 1024,
-        dbidirectional: bool = True,
+        adim: int = 384,
+        aheads: int = 4,
+        elayers: int = 6,
+        eunits: int = 1536,
+        dlayers: int = 6,
+        dunits: int = 1536,
         postnet_layers: int = 5,
-        postnet_chans: int = 256,
+        postnet_chans: int = 512,
         postnet_filts: int = 5,
+        postnet_dropout_rate: float = 0.5,
+        positionwise_layer_type: str = "conv1d",
+        positionwise_conv_kernel_size: int = 1,
+        use_scaled_pos_enc: bool = True,
         use_batch_norm: bool = True,
+        encoder_normalize_before: bool = True,
+        decoder_normalize_before: bool = True,
+        encoder_concat_after: bool = False,
+        decoder_concat_after: bool = False,
         duration_predictor_layers: int = 2,
         duration_predictor_chans: int = 384,
         duration_predictor_kernel_size: int = 3,
         duration_predictor_dropout_rate: float = 0.1,
         reduction_factor: int = 1,
+        encoder_type: str = "transformer",
+        decoder_type: str = "transformer",
+        transformer_enc_dropout_rate: float = 0.1,
+        transformer_enc_positional_dropout_rate: float = 0.1,
+        transformer_enc_attn_dropout_rate: float = 0.1,
+        transformer_dec_dropout_rate: float = 0.1,
+        transformer_dec_positional_dropout_rate: float = 0.1,
+        transformer_dec_attn_dropout_rate: float = 0.1,
+        # only for conformer
+        conformer_rel_pos_type: str = "legacy",
+        conformer_pos_enc_layer_type: str = "rel_pos",
+        conformer_self_attn_layer_type: str = "rel_selfattn",
+        conformer_activation_type: str = "swish",
+        use_macaron_style_in_conformer: bool = True,
+        use_cnn_in_conformer: bool = True,
+        conformer_enc_kernel_size: int = 7,
+        conformer_dec_kernel_size: int = 31,
+        zero_triu: bool = False,
         # extra embedding related
         spks: Optional[int] = None,
         langs: Optional[int] = None,
         spk_embed_dim: Optional[int] = None,
         spk_embed_integration_type: str = "add",
-        eprenet_dropout_rate: float = 0.5,
-        edropout_rate: float = 0.1,
-        ddropout_rate: float = 0.1,
-        postnet_dropout_rate: float = 0.5,
+        use_gst: bool = False,
+        gst_tokens: int = 10,
+        gst_heads: int = 4,
+        gst_conv_layers: int = 6,
+        gst_conv_chans_list: Sequence[int] = (32, 32, 64, 64, 128, 128),
+        gst_conv_kernel_size: int = 3,
+        gst_conv_stride: int = 2,
+        gst_gru_layers: int = 1,
+        gst_gru_units: int = 128,
+        # training related
         init_type: str = "xavier_uniform",
+        init_enc_alpha: float = 1.0,
+        init_dec_alpha: float = 1.0,
         use_masking: bool = False,
         use_weighted_masking: bool = False,
     ):
-        """Initialize NaiveRNN module.
+        """Initialize FastSpeech module.
+
+        Args:
+            idim (int): Dimension of the inputs.
+            odim (int): Dimension of the outputs.
+            elayers (int): Number of encoder layers.
+            eunits (int): Number of encoder hidden units.
+            dlayers (int): Number of decoder layers.
+            dunits (int): Number of decoder hidden units.
+            postnet_layers (int): Number of postnet layers.
+            postnet_chans (int): Number of postnet channels.
+            postnet_filts (int): Kernel size of postnet.
+            postnet_dropout_rate (float): Dropout rate in postnet.
+            use_scaled_pos_enc (bool): Whether to use trainable scaled pos encoding.
+            use_batch_norm (bool): Whether to use batch normalization in encoder prenet.
+            encoder_normalize_before (bool): Whether to apply layernorm layer before
+                encoder block.
+            decoder_normalize_before (bool): Whether to apply layernorm layer before
+                decoder block.
+            encoder_concat_after (bool): Whether to concatenate attention layer's input
+                and output in encoder.
+            decoder_concat_after (bool): Whether to concatenate attention layer's input
+                and output in decoder.
+            duration_predictor_layers (int): Number of duration predictor layers.
+            duration_predictor_chans (int): Number of duration predictor channels.
+            duration_predictor_kernel_size (int): Kernel size of duration predictor.
+            duration_predictor_dropout_rate (float): Dropout rate in duration predictor.
+            reduction_factor (int): Reduction factor.
+            encoder_type (str): Encoder type ("transformer" or "conformer").
+            decoder_type (str): Decoder type ("transformer" or "conformer").
+            transformer_enc_dropout_rate (float): Dropout rate in encoder except
+                attention and positional encoding.
+            transformer_enc_positional_dropout_rate (float): Dropout rate after encoder
+                positional encoding.
+            transformer_enc_attn_dropout_rate (float): Dropout rate in encoder
+                self-attention module.
+            transformer_dec_dropout_rate (float): Dropout rate in decoder except
+                attention & positional encoding.
+            transformer_dec_positional_dropout_rate (float): Dropout rate after decoder
+                positional encoding.
+            transformer_dec_attn_dropout_rate (float): Dropout rate in decoder
+                self-attention module.
+            conformer_rel_pos_type (str): Relative pos encoding type in conformer.
+            conformer_pos_enc_layer_type (str): Pos encoding layer type in conformer.
+            conformer_self_attn_layer_type (str): Self-attention layer type in conformer
+            conformer_activation_type (str): Activation function type in conformer.
+            use_macaron_style_in_conformer: Whether to use macaron style FFN.
+            use_cnn_in_conformer: Whether to use CNN in conformer.
+            conformer_enc_kernel_size: Kernel size of encoder conformer.
+            conformer_dec_kernel_size: Kernel size of decoder conformer.
+            zero_triu: Whether to use zero triu in relative self-attention module.
+            spks (Optional[int]): Number of speakers. If set to > 1, assume that the
+                sids will be provided as the input and use sid embedding layer.
+            langs (Optional[int]): Number of languages. If set to > 1, assume that the
+                lids will be provided as the input and use sid embedding layer.
+            spk_embed_dim (Optional[int]): Speaker embedding dimension. If set to > 0,
+                assume that spembs will be provided as the input.
+            spk_embed_integration_type: How to integrate speaker embedding.
+            use_gst (str): Whether to use global style token.
+            gst_tokens (int): The number of GST embeddings.
+            gst_heads (int): The number of heads in GST multihead attention.
+            gst_conv_layers (int): The number of conv layers in GST.
+            gst_conv_chans_list: (Sequence[int]):
+                List of the number of channels of conv layers in GST.
+            gst_conv_kernel_size (int): Kernel size of conv layers in GST.
+            gst_conv_stride (int): Stride size of conv layers in GST.
+            gst_gru_layers (int): The number of GRU layers in GST.
+            gst_gru_units (int): The number of GRU units in GST.
+            init_type (str): How to initialize transformer parameters.
+            init_enc_alpha (float): Initial value of alpha in scaled pos encoding of the
+                encoder.
+            init_dec_alpha (float): Initial value of alpha in scaled pos encoding of the
+                decoder.
+            use_masking (bool): Whether to apply masking for padded part in loss
+                calculation.
+            use_weighted_masking (bool): Whether to apply weighted masking in loss
+                calculation.
 
-        Args: TODO(Yuning)
         """
         assert check_argument_types()
         super().__init__()
 
         # store hyperparameters
         self.idim = idim
-        self.midi_dim = midi_dim
-        self.tempo_dim = tempo_dim
-        self.eunits = eunits
         self.odim = odim
         self.eos = idim - 1
         self.reduction_factor = reduction_factor
-
-        self.midi_embed_integration_type = midi_embed_integration_type
+        self.encoder_type = encoder_type
+        self.decoder_type = decoder_type
+        self.use_scaled_pos_enc = use_scaled_pos_enc
+        self.use_gst = use_gst
 
         # use idx 0 as padding idx
         self.padding_idx = 0
 
-        # define transformer encoder
-        if eprenet_conv_layers != 0:
-            # encoder prenet
-            self.encoder_input_layer = torch.nn.Sequential(
-                EncoderPrenet(
-                    idim=idim,
-                    embed_dim=embed_dim,
-                    elayers=0,
-                    econv_layers=eprenet_conv_layers,
-                    econv_chans=eprenet_conv_chans,
-                    econv_filts=eprenet_conv_filts,
-                    use_batch_norm=use_batch_norm,
-                    dropout_rate=eprenet_dropout_rate,
-                    padding_idx=self.padding_idx,
-                ),
-                torch.nn.Linear(eprenet_conv_chans, eunits),
-            )
-            self.midi_encoder_input_layer = torch.nn.Sequential(
-                EncoderPrenet(
-                    idim=midi_dim,
-                    embed_dim=embed_dim,
-                    elayers=0,
-                    econv_layers=eprenet_conv_layers,
-                    econv_chans=eprenet_conv_chans,
-                    econv_filts=eprenet_conv_filts,
-                    use_batch_norm=use_batch_norm,
-                    dropout_rate=eprenet_dropout_rate,
-                    padding_idx=self.padding_idx,
-                ),
-                torch.nn.Linear(eprenet_conv_chans, eunits),
-            )
-            self.tempo_encoder_input_layer = torch.nn.Sequential(
-                EncoderPrenet(
-                    idim=midi_dim,
-                    embed_dim=embed_dim,
-                    elayers=0,
-                    econv_layers=eprenet_conv_layers,
-                    econv_chans=eprenet_conv_chans,
-                    econv_filts=eprenet_conv_filts,
-                    use_batch_norm=use_batch_norm,
-                    dropout_rate=eprenet_dropout_rate,
-                    padding_idx=self.padding_idx,
-                ),
-                torch.nn.Linear(eprenet_conv_chans, eunits),
-            )
-        else:
-            self.encoder_input_layer = torch.nn.Embedding(
-                num_embeddings=idim, embedding_dim=eunits, padding_idx=self.padding_idx
-            )
-            self.midi_encoder_input_layer = torch.nn.Embedding(
-                num_embeddings=midi_dim,
-                embedding_dim=eunits,
-                padding_idx=self.padding_idx,
-            )
-            self.tempo_encoder_input_layer = torch.nn.Embedding(
-                num_embeddings=tempo_dim,
-                embedding_dim=eunits,
-                padding_idx=self.padding_idx,
-            )
-
-        self.encoder = torch.nn.LSTM(
-            input_size=eunits,
-            hidden_size=eunits,
-            num_layers=elayers,
-            batch_first=True,
-            dropout=edropout_rate,
-            bidirectional=ebidirectional,
-            # proj_size=eunits,
-        )
-
-        self.midi_encoder = torch.nn.LSTM(
-            input_size=eunits,
-            hidden_size=eunits,
-            num_layers=elayers,
-            batch_first=True,
-            dropout=edropout_rate,
-            bidirectional=ebidirectional,
-            # proj_size=eunits,
-        )
+        # get positional encoding class
+        pos_enc_class = (
+            ScaledPositionalEncoding if self.use_scaled_pos_enc else PositionalEncoding
+        )
+
+        # check relative positional encoding compatibility
+        if "conformer" in [encoder_type, decoder_type]:
+            if conformer_rel_pos_type == "legacy":
+                if conformer_pos_enc_layer_type == "rel_pos":
+                    conformer_pos_enc_layer_type = "legacy_rel_pos"
+                    logging.warning(
+                        "Fallback to conformer_pos_enc_layer_type = 'legacy_rel_pos' "
+                        "due to the compatibility. If you want to use the new one, "
+                        "please use conformer_pos_enc_layer_type = 'latest'."
+                    )
+                if conformer_self_attn_layer_type == "rel_selfattn":
+                    conformer_self_attn_layer_type = "legacy_rel_selfattn"
+                    logging.warning(
+                        "Fallback to "
+                        "conformer_self_attn_layer_type = 'legacy_rel_selfattn' "
+                        "due to the compatibility. If you want to use the new one, "
+                        "please use conformer_pos_enc_layer_type = 'latest'."
+                    )
+            elif conformer_rel_pos_type == "latest":
+                assert conformer_pos_enc_layer_type != "legacy_rel_pos"
+                assert conformer_self_attn_layer_type != "legacy_rel_selfattn"
+            else:
+                raise ValueError(f"Unknown rel_pos_type: {conformer_rel_pos_type}")
 
-        self.tempo_encoder = torch.nn.LSTM(
-            input_size=eunits,
-            hidden_size=eunits,
-            num_layers=elayers,
-            batch_first=True,
-            dropout=edropout_rate,
-            bidirectional=ebidirectional,
-            # proj_size=eunits,
+        # define encoder
+        encoder_input_layer = torch.nn.Embedding(
+            num_embeddings=idim, embedding_dim=adim, padding_idx=self.padding_idx
         )
-
-        dim_direction = 2 if ebidirectional is True else 1
-        if self.midi_embed_integration_type == "add":
-            self.midi_projection = torch.nn.Linear(
-                eunits * dim_direction, eunits * dim_direction
+        if encoder_type == "transformer":
+            self.encoder = TransformerEncoder(
+                idim=idim,
+                attention_dim=adim,
+                attention_heads=aheads,
+                linear_units=eunits,
+                num_blocks=elayers,
+                input_layer=encoder_input_layer,
+                dropout_rate=transformer_enc_dropout_rate,
+                positional_dropout_rate=transformer_enc_positional_dropout_rate,
+                attention_dropout_rate=transformer_enc_attn_dropout_rate,
+                pos_enc_class=pos_enc_class,
+                normalize_before=encoder_normalize_before,
+                concat_after=encoder_concat_after,
+                positionwise_layer_type=positionwise_layer_type,
+                positionwise_conv_kernel_size=positionwise_conv_kernel_size,
             )
-        else:
-            self.midi_projection = torch.nn.linear(
-                3 * eunits * dim_direction, eunits * dim_direction
+        elif encoder_type == "conformer":
+            self.encoder = ConformerEncoder(
+                idim=idim,
+                attention_dim=adim,
+                attention_heads=aheads,
+                linear_units=eunits,
+                num_blocks=elayers,
+                input_layer=encoder_input_layer,
+                dropout_rate=transformer_enc_dropout_rate,
+                positional_dropout_rate=transformer_enc_positional_dropout_rate,
+                attention_dropout_rate=transformer_enc_attn_dropout_rate,
+                normalize_before=encoder_normalize_before,
+                concat_after=encoder_concat_after,
+                positionwise_layer_type=positionwise_layer_type,
+                positionwise_conv_kernel_size=positionwise_conv_kernel_size,
+                macaron_style=use_macaron_style_in_conformer,
+                pos_enc_layer_type=conformer_pos_enc_layer_type,
+                selfattention_layer_type=conformer_self_attn_layer_type,
+                activation_type=conformer_activation_type,
+                use_cnn_module=use_cnn_in_conformer,
+                cnn_module_kernel=conformer_enc_kernel_size,
             )
+        else:
+            raise ValueError(f"{encoder_type} is not supported.")
 
-        # define duration predictor
-        self.duration_predictor = DurationPredictor(
-            idim=eunits * dim_direction,
-            n_layers=duration_predictor_layers,
-            n_chans=duration_predictor_chans,
-            kernel_size=duration_predictor_kernel_size,
-            dropout_rate=duration_predictor_dropout_rate,
-        )
-
-        # define length regulator
-        self.length_regulator = LengthRegulator()
-
-        self.decoder = torch.nn.LSTM(
-            input_size=eunits * dim_direction,
-            hidden_size=dunits,
-            num_layers=dlayers,
-            batch_first=True,
-            dropout=ddropout_rate,
-            bidirectional=dbidirectional,
-            # proj_size=dunits,
-        )
+        # define GST
+        if self.use_gst:
+            self.gst = StyleEncoder(
+                idim=odim,  # the input is mel-spectrogram
+                gst_tokens=gst_tokens,
+                gst_token_dim=adim,
+                gst_heads=gst_heads,
+                conv_layers=gst_conv_layers,
+                conv_chans_list=gst_conv_chans_list,
+                conv_kernel_size=gst_conv_kernel_size,
+                conv_stride=gst_conv_stride,
+                gru_layers=gst_gru_layers,
+                gru_units=gst_gru_units,
+            )
 
         # define spk and lang embedding
         self.spks = None
         if spks is not None and spks > 1:
             self.spks = spks
-            self.sid_emb = torch.nn.Embedding(spks, dunits * dim_direction)
+            self.sid_emb = torch.nn.Embedding(spks, adim)
         self.langs = None
         if langs is not None and langs > 1:
-            # TODO(Yuning): not encode yet
             self.langs = langs
-            self.lid_emb = torch.nn.Embedding(langs, dunits * dim_direction)
+            self.lid_emb = torch.nn.Embedding(langs, adim)
 
-        # define projection layer
+        # define additional projection for speaker embedding
         self.spk_embed_dim = None
         if spk_embed_dim is not None and spk_embed_dim > 0:
             self.spk_embed_dim = spk_embed_dim
             self.spk_embed_integration_type = spk_embed_integration_type
         if self.spk_embed_dim is not None:
             if self.spk_embed_integration_type == "add":
-                self.projection = torch.nn.Linear(
-                    self.spk_embed_dim, dunits * dim_direction
-                )
+                self.projection = torch.nn.Linear(self.spk_embed_dim, adim)
             else:
-                self.projection = torch.nn.Linear(
-                    dunits * dim_direction + self.spk_embed_dim, dunits * dim_direction
-                )
+                self.projection = torch.nn.Linear(adim + self.spk_embed_dim, adim)
+
+        # define duration predictor
+        self.duration_predictor = DurationPredictor(
+            idim=adim,
+            n_layers=duration_predictor_layers,
+            n_chans=duration_predictor_chans,
+            kernel_size=duration_predictor_kernel_size,
+            dropout_rate=duration_predictor_dropout_rate,
+        )
+
+        # define length regulator
+        self.length_regulator = LengthRegulator()
+
+        # define decoder
+        # NOTE: we use encoder as decoder
+        # because fastspeech's decoder is the same as encoder
+        if decoder_type == "transformer":
+            self.decoder = TransformerEncoder(
+                idim=0,
+                attention_dim=adim,
+                attention_heads=aheads,
+                linear_units=dunits,
+                num_blocks=dlayers,
+                input_layer=None,
+                dropout_rate=transformer_dec_dropout_rate,
+                positional_dropout_rate=transformer_dec_positional_dropout_rate,
+                attention_dropout_rate=transformer_dec_attn_dropout_rate,
+                pos_enc_class=pos_enc_class,
+                normalize_before=decoder_normalize_before,
+                concat_after=decoder_concat_after,
+                positionwise_layer_type=positionwise_layer_type,
+                positionwise_conv_kernel_size=positionwise_conv_kernel_size,
+            )
+        elif decoder_type == "conformer":
+            self.decoder = ConformerEncoder(
+                idim=0,
+                attention_dim=adim,
+                attention_heads=aheads,
+                linear_units=dunits,
+                num_blocks=dlayers,
+                input_layer=None,
+                dropout_rate=transformer_dec_dropout_rate,
+                positional_dropout_rate=transformer_dec_positional_dropout_rate,
+                attention_dropout_rate=transformer_dec_attn_dropout_rate,
+                normalize_before=decoder_normalize_before,
+                concat_after=decoder_concat_after,
+                positionwise_layer_type=positionwise_layer_type,
+                positionwise_conv_kernel_size=positionwise_conv_kernel_size,
+                macaron_style=use_macaron_style_in_conformer,
+                pos_enc_layer_type=conformer_pos_enc_layer_type,
+                selfattention_layer_type=conformer_self_attn_layer_type,
+                activation_type=conformer_activation_type,
+                use_cnn_module=use_cnn_in_conformer,
+                cnn_module_kernel=conformer_dec_kernel_size,
+            )
+        else:
+            raise ValueError(f"{decoder_type} is not supported.")
 
         # define final projection
-        self.feat_out = torch.nn.Linear(dunits * dim_direction, odim * reduction_factor)
+        self.feat_out = torch.nn.Linear(adim, odim * reduction_factor)
 
         # define postnet
         self.postnet = (
             None
             if postnet_layers == 0
             else Postnet(
                 idim=idim,
@@ -254,324 +388,315 @@
                 n_chans=postnet_chans,
                 n_filts=postnet_filts,
                 use_batch_norm=use_batch_norm,
                 dropout_rate=postnet_dropout_rate,
             )
         )
 
-        # define loss function
-        self.criterion = FastSpeechLoss(
-            use_masking=use_masking, use_weighted_masking=use_weighted_masking
-        )
-
         # initialize parameters
         self._reset_parameters(
             init_type=init_type,
+            init_enc_alpha=init_enc_alpha,
+            init_dec_alpha=init_dec_alpha,
         )
 
-    def _reset_parameters(self, init_type):
-        # initialize parameters
-        if init_type != "pytorch":
-            initialize(self, init_type)
+        # define criterions
+        self.criterion = FastSpeechLoss(
+            use_masking=use_masking, use_weighted_masking=use_weighted_masking
+        )
+
+    def _forward(
+        self,
+        xs: torch.Tensor,
+        ilens: torch.Tensor,
+        ys: Optional[torch.Tensor] = None,
+        olens: Optional[torch.Tensor] = None,
+        ds: Optional[torch.Tensor] = None,
+        spembs: Optional[torch.Tensor] = None,
+        sids: Optional[torch.Tensor] = None,
+        lids: Optional[torch.Tensor] = None,
+        is_inference: bool = False,
+        alpha: float = 1.0,
+    ) -> Sequence[torch.Tensor]:
+        # forward encoder
+        x_masks = self._source_mask(ilens)
+        hs, _ = self.encoder(xs, x_masks)  # (B, T_text, adim)
+
+        # integrate with GST
+        if self.use_gst:
+            style_embs = self.gst(ys)
+            hs = hs + style_embs.unsqueeze(1)
+
+        # integrate with SID and LID embeddings
+        if self.spks is not None:
+            sid_embs = self.sid_emb(sids.view(-1))
+            hs = hs + sid_embs.unsqueeze(1)
+        if self.langs is not None:
+            lid_embs = self.lid_emb(lids.view(-1))
+            hs = hs + lid_embs.unsqueeze(1)
+
+        # integrate speaker embedding
+        if self.spk_embed_dim is not None:
+            hs = self._integrate_with_spk_embed(hs, spembs)
+
+        # forward duration predictor and length regulator
+        d_masks = make_pad_mask(ilens).to(xs.device)
+        if is_inference:
+            d_outs = self.duration_predictor.inference(hs, d_masks)  # (B, T_text)
+            hs = self.length_regulator(hs, d_outs, alpha)  # (B, T_feats, adim)
+        else:
+            d_outs = self.duration_predictor(hs, d_masks)  # (B, T_text)
+            hs = self.length_regulator(hs, ds)  # (B, T_feats, adim)
+
+        # forward decoder
+        if olens is not None and not is_inference:
+            if self.reduction_factor > 1:
+                olens_in = olens.new([olen // self.reduction_factor for olen in olens])
+            else:
+                olens_in = olens
+            h_masks = self._source_mask(olens_in)
+        else:
+            h_masks = None
+        zs, _ = self.decoder(hs, h_masks)  # (B, T_feats, adim)
+        before_outs = self.feat_out(zs).view(
+            zs.size(0), -1, self.odim
+        )  # (B, T_feats, odim)
+
+        # postnet -> (B, T_feats//r * r, odim)
+        if self.postnet is None:
+            after_outs = before_outs
+        else:
+            after_outs = before_outs + self.postnet(
+                before_outs.transpose(1, 2)
+            ).transpose(1, 2)
+
+        return before_outs, after_outs, d_outs
 
     def forward(
         self,
         text: torch.Tensor,
         text_lengths: torch.Tensor,
         feats: torch.Tensor,
         feats_lengths: torch.Tensor,
-        label: Optional[Dict[str, torch.Tensor]] = None,
-        label_lengths: Optional[Dict[str, torch.Tensor]] = None,
-        melody: Optional[Dict[str, torch.Tensor]] = None,
-        melody_lengths: Optional[Dict[str, torch.Tensor]] = None,
-        pitch: Optional[torch.Tensor] = None,
-        pitch_lengths: Optional[torch.Tensor] = None,
-        duration: Optional[Dict[str, torch.Tensor]] = None,
-        duration_lengths: Optional[Dict[str, torch.Tensor]] = None,
-        slur: torch.LongTensor = None,
-        slur_lengths: torch.Tensor = None,
+        durations: torch.Tensor,
+        durations_lengths: torch.Tensor,
         spembs: Optional[torch.Tensor] = None,
         sids: Optional[torch.Tensor] = None,
         lids: Optional[torch.Tensor] = None,
         joint_training: bool = False,
-        flag_IsValid=False,
     ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor]:
         """Calculate forward propagation.
 
         Args:
-            text (LongTensor): Batch of padded character ids (B, Tmax).
-            text_lengths (LongTensor): Batch of lengths of each input batch (B,).
-            feats (Tensor): Batch of padded target features (B, Lmax, odim).
+            text (LongTensor): Batch of padded character ids (B, T_text).
+            text_lengths (LongTensor): Batch of lengths of each input (B,).
+            feats (Tensor): Batch of padded target features (B, T_feats, odim).
             feats_lengths (LongTensor): Batch of the lengths of each target (B,).
-            label (Optional[Dict]): key is "lab" or "score";
-                value (LongTensor): Batch of padded label ids (B, Tmax).
-            label_lengths (Optional[Dict]): key is "lab" or "score";
-                value (LongTensor): Batch of the lengths of padded label ids (B, ).
-            melody (Optional[Dict]): key is "lab" or "score";
-                value (LongTensor): Batch of padded melody (B, Tmax).
-            melody_lengths (Optional[Dict]): key is "lab" or "score";
-                value (LongTensor): Batch of the lengths of padded melody (B, ).
-            pitch (FloatTensor): Batch of padded f0 (B, Tmax).
-            pitch_lengths (LongTensor): Batch of the lengths of padded f0 (B, ).
-            duration (Optional[Dict]): key is "lab", "score_phn" or "score_syb";
-                value (LongTensor): Batch of padded duration (B, Tmax).
-            duration_length (Optional[Dict]): key is "lab", "score_phn" or "score_syb";
-                value (LongTensor): Batch of the lengths of padded duration (B, ).
-            slur (LongTensor): Batch of padded slur (B, Tmax).
-            slur_lengths (LongTensor): Batch of the lengths of padded slur (B, ).
+            durations (LongTensor): Batch of padded durations (B, T_text + 1).
+            durations_lengths (LongTensor): Batch of duration lengths (B, T_text + 1).
             spembs (Optional[Tensor]): Batch of speaker embeddings (B, spk_embed_dim).
             sids (Optional[Tensor]): Batch of speaker IDs (B, 1).
             lids (Optional[Tensor]): Batch of language IDs (B, 1).
             joint_training (bool): Whether to perform joint training with vocoder.
 
-        GS Fix:
-            arguements from forward func. V.S. **batch from espnet_model.py
-            label == durations | phone sequence
-            melody -> pitch sequence
-
         Returns:
             Tensor: Loss scalar value.
             Dict: Statistics to be monitored.
             Tensor: Weight value if not joint training else model outputs.
-        """
-        if joint_training:
-            label = label
-            midi = melody
-            tempo = duration
-            label_lengths = label_lengths
-            midi_lengths = melody_lengths
-            ds = duration
-        else:
-            label = label["score"]
-            midi = melody["score"]
-            tempo = duration["score_phn"]
-            label_lengths = label_lengths["score"]
-            midi_lengths = melody_lengths["score"]
-            ds = duration["lab"]
 
+        """
         text = text[:, : text_lengths.max()]  # for data-parallel
         feats = feats[:, : feats_lengths.max()]  # for data-parallel
-        midi = midi[:, : midi_lengths.max()]  # for data-parallel
-        label = label[:, : label_lengths.max()]  # for data-parallel
-        batch_size = feats.size(0)
-
-        label_emb = self.encoder_input_layer(label)  # FIX ME: label Float to Int
-        midi_emb = self.midi_encoder_input_layer(midi)
-        tempo_emb = self.tempo_encoder_input_layer(tempo)
-
-        label_emb = torch.nn.utils.rnn.pack_padded_sequence(
-            label_emb, label_lengths.to("cpu"), batch_first=True, enforce_sorted=False
-        )
-        midi_emb = torch.nn.utils.rnn.pack_padded_sequence(
-            midi_emb, midi_lengths.to("cpu"), batch_first=True, enforce_sorted=False
-        )
-        tempo_emb = torch.nn.utils.rnn.pack_padded_sequence(
-            tempo_emb, midi_lengths.to("cpu"), batch_first=True, enforce_sorted=False
-        )
-
-        hs_label, (_, _) = self.encoder(label_emb)
-        hs_midi, (_, _) = self.midi_encoder(midi_emb)
-        hs_tempo, (_, _) = self.tempo_encoder(tempo_emb)
-
-        hs_label, _ = torch.nn.utils.rnn.pad_packed_sequence(hs_label, batch_first=True)
-        hs_midi, _ = torch.nn.utils.rnn.pad_packed_sequence(hs_midi, batch_first=True)
-        hs_tempo, _ = torch.nn.utils.rnn.pad_packed_sequence(hs_tempo, batch_first=True)
-
-        if self.midi_embed_integration_type == "add":
-            hs = hs_label + hs_midi + hs_tempo
-            hs = F.leaky_relu(self.midi_projection(hs))
-        else:
-            hs = torch.cat((hs_label, hs_midi, hs_tempo), dim=-1)
-            hs = F.leaky_relu(self.midi_projection(hs))
-        # integrate spk & lang embeddings
-        if self.spks is not None:
-            sid_embs = self.sid_emb(sids.view(-1))
-            hs = hs + sid_embs.unsqueeze(1)
-        if self.langs is not None:
-            lid_embs = self.lid_emb(lids.view(-1))
-            hs = hs + lid_embs.unsqueeze(1)
-
-        # integrate speaker embedding
-        if self.spk_embed_dim is not None:
-            hs = self._integrate_with_spk_embed(hs, spembs)
+        durations = durations[:, : durations_lengths.max()]  # for data-parallel
 
-        # forward duration predictor and length regulator
-        d_masks = make_pad_mask(label_lengths).to(hs.device)
-        d_outs = self.duration_predictor(hs, d_masks)  # (B, T_text)
-        hs = self.length_regulator(hs, ds)  # (B, seq_len, eunits)
+        batch_size = text.size(0)
 
-        hs_emb = torch.nn.utils.rnn.pack_padded_sequence(
-            hs, feats_lengths.to("cpu"), batch_first=True, enforce_sorted=False
+        # Add eos at the last of sequence
+        xs = F.pad(text, [0, 1], "constant", self.padding_idx)
+        for i, l in enumerate(text_lengths):
+            xs[i, l] = self.eos
+        ilens = text_lengths + 1
+
+        ys, ds = feats, durations
+        olens = feats_lengths
+
+        # forward propagation
+        before_outs, after_outs, d_outs = self._forward(
+            xs,
+            ilens,
+            ys,
+            olens,
+            ds,
+            spembs=spembs,
+            sids=sids,
+            lids=lids,
+            is_inference=False,
         )
 
-        zs, (_, _) = self.decoder(hs_emb)
-        zs, _ = torch.nn.utils.rnn.pad_packed_sequence(zs, batch_first=True)
-
-        zs = zs[:, self.reduction_factor - 1 :: self.reduction_factor]
-
-        # (B, T_feats//r, odim * r) -> (B, T_feats//r * r, odim)
-        before_outs = F.leaky_relu(self.feat_out(zs).view(zs.size(0), -1, self.odim))
-
-        # postnet -> (B, T_feats//r * r, odim)
-        if self.postnet is None:
-            after_outs = before_outs
-        else:
-            after_outs = before_outs + self.postnet(
-                before_outs.transpose(1, 2)
-            ).transpose(1, 2)
-
         # modifiy mod part of groundtruth
         if self.reduction_factor > 1:
-            assert feats_lengths.ge(
-                self.reduction_factor
-            ).all(), "Output length must be greater than or equal to reduction factor."
-            olens = feats_lengths.new(
-                [olen - olen % self.reduction_factor for olen in feats_lengths]
-            )
+            olens = olens.new([olen - olen % self.reduction_factor for olen in olens])
             max_olen = max(olens)
-            ys = feats[:, :max_olen]
-        else:
-            ys = feats
-            olens = feats_lengths
+            ys = ys[:, :max_olen]
 
-        # calculate loss values
-        ilens = label_lengths
+        # calculate loss
+        if self.postnet is None:
+            after_outs = None
         l1_loss, duration_loss = self.criterion(
             after_outs, before_outs, d_outs, ys, ds, ilens, olens
         )
         loss = l1_loss + duration_loss
 
         stats = dict(
-            loss=loss.item(), l1_loss=l1_loss.item(), duration_loss=duration_loss.item()
+            l1_loss=l1_loss.item(),
+            duration_loss=duration_loss.item(),
         )
 
-        loss, stats, weight = force_gatherable((loss, stats, batch_size), loss.device)
+        # report extra information
+        if self.encoder_type == "transformer" and self.use_scaled_pos_enc:
+            stats.update(
+                encoder_alpha=self.encoder.embed[-1].alpha.data.item(),
+            )
+        if self.decoder_type == "transformer" and self.use_scaled_pos_enc:
+            stats.update(
+                decoder_alpha=self.decoder.embed[-1].alpha.data.item(),
+            )
 
-        if joint_training:
-            return loss, stats, after_outs if after_outs is not None else before_outs
+        if not joint_training:
+            stats.update(loss=loss.item())
+            loss, stats, weight = force_gatherable(
+                (loss, stats, batch_size), loss.device
+            )
+            return loss, stats, weight
         else:
-            if flag_IsValid is False:
-                # training stage
-                return loss, stats, weight
-            else:
-                # validation stage
-                return loss, stats, weight, after_outs[:, : olens.max()], ys, olens
+            return loss, stats, after_outs if after_outs is not None else before_outs
 
     def inference(
         self,
         text: torch.Tensor,
         feats: Optional[torch.Tensor] = None,
-        label: Optional[Dict[str, torch.Tensor]] = None,
-        melody: Optional[Dict[str, torch.Tensor]] = None,
-        pitch: Optional[torch.Tensor] = None,
-        duration: Optional[Dict[str, torch.Tensor]] = None,
-        slur: Optional[Dict[str, torch.Tensor]] = None,
+        durations: Optional[torch.Tensor] = None,
         spembs: Optional[torch.Tensor] = None,
         sids: Optional[torch.Tensor] = None,
         lids: Optional[torch.Tensor] = None,
-        joint_training: bool = False,
-        use_teacher_forcing: torch.Tensor = False,
-    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor]:
-        """Calculate forward propagation.
+        alpha: float = 1.0,
+        use_teacher_forcing: bool = False,
+    ) -> Dict[str, torch.Tensor]:
+        """Generate the sequence of features given the sequences of characters.
 
         Args:
-            text (LongTensor): Batch of padded character ids (Tmax).
-            feats (Tensor): Batch of padded target features (Lmax, odim).
-            label (Optional[Dict]): key is "lab" or "score";
-                value (LongTensor): Batch of padded label ids (Tmax).
-            melody (Optional[Dict]): key is "lab" or "score";
-                value (LongTensor): Batch of padded melody (Tmax).
-            pitch (FloatTensor): Batch of padded f0 (Tmax).
-            duration (Optional[Dict]): key is "lab", "score_phn" or "score_syb";
-                value (LongTensor): Batch of padded duration (Tmax).
-            slur (LongTensor): Batch of padded slur (B, Tmax).
-            spembs (Optional[Tensor]): Batch of speaker embeddings (spk_embed_dim).
-            sids (Optional[Tensor]): Batch of speaker IDs (1).
-            lids (Optional[Tensor]): Batch of language IDs (1).
+            text (LongTensor): Input sequence of characters (T_text,).
+            feats (Optional[Tensor]): Feature sequence to extract style (N, idim).
+            durations (Optional[LongTensor]): Groundtruth of duration (T_text + 1,).
+            spembs (Optional[Tensor]): Speaker embedding (spk_embed_dim,).
+            sids (Optional[Tensor]): Speaker ID (1,).
+            lids (Optional[Tensor]): Language ID (1,).
+            alpha (float): Alpha to control the speed.
+            use_teacher_forcing (bool): Whether to use teacher forcing.
+                If true, groundtruth of duration, pitch and energy will be used.
 
         Returns:
             Dict[str, Tensor]: Output dict including the following items:
                 * feat_gen (Tensor): Output sequence of features (T_feats, odim).
-        """
-        label = label["score"]
-        midi = melody["score"]
-        if joint_training:
-            tempo = duration["lab"]
-        else:
-            tempo = duration["score_phn"]
-
-        label_emb = self.encoder_input_layer(label)  # FIX ME: label Float to Int
-        midi_emb = self.midi_encoder_input_layer(midi)
-        tempo_emb = self.tempo_encoder_input_layer(tempo)
-
-        hs_label, (_, _) = self.encoder(label_emb)
-        hs_midi, (_, _) = self.midi_encoder(midi_emb)
-        hs_tempo, (_, _) = self.tempo_encoder(tempo_emb)
-
-        if self.midi_embed_integration_type == "add":
-            hs = hs_label + hs_midi + hs_tempo
-            hs = F.leaky_relu(self.midi_projection(hs))
-        else:
-            hs = torch.cat((hs_label, hs_midi, hs_tempo), dim=-1)
-            hs = F.leaky_relu(self.midi_projection(hs))
-        # integrate spk & lang embeddings
-        if self.spks is not None:
-            sid_embs = self.sid_emb(sids.view(-1))
-            hs = hs + sid_embs.unsqueeze(1)
-        if self.langs is not None:
-            lid_embs = self.lid_emb(lids.view(-1))
-            hs = hs + lid_embs.unsqueeze(1)
+                * duration (Tensor): Duration sequence (T_text + 1,).
 
-        # integrate speaker embedding
-        if self.spk_embed_dim is not None:
-            hs = self._integrate_with_spk_embed(hs, spembs)
-
-        # forward duration predictor and length regulator
-        d_masks = None  # make_pad_mask(label_lengths).to(input_emb.device)
-        d_outs = self.duration_predictor.inference(hs, d_masks)  # (B, T_text)
-        d_outs_int = torch.floor(d_outs + 0.5).to(dtype=torch.long)  # (B, T_text)
-
-        hs = self.length_regulator(hs, d_outs_int)  # (B, T_feats, adim)
-        zs, (_, _) = self.decoder(hs)
-
-        zs = zs[:, self.reduction_factor - 1 :: self.reduction_factor]
+        """
+        x, y = text, feats
+        spemb, d = spembs, durations
 
-        # (B, T_feats//r, odim * r) -> (B, T_feats//r * r, odim)
-        before_outs = F.leaky_relu(self.feat_out(zs).view(zs.size(0), -1, self.odim))
+        # add eos at the last of sequence
+        x = F.pad(x, [0, 1], "constant", self.eos)
 
-        # postnet -> (B, T_feats//r * r, odim)
-        if self.postnet is None:
-            after_outs = before_outs
-        else:
-            after_outs = before_outs + self.postnet(
-                before_outs.transpose(1, 2)
-            ).transpose(1, 2)
+        # setup batch axis
+        ilens = torch.tensor([x.shape[0]], dtype=torch.long, device=x.device)
+        xs, ys = x.unsqueeze(0), None
+        if y is not None:
+            ys = y.unsqueeze(0)
+        if spemb is not None:
+            spembs = spemb.unsqueeze(0)
+
+        if use_teacher_forcing:
+            # use groundtruth of duration
+            ds = d.unsqueeze(0)
+            _, outs, d_outs = self._forward(
+                xs,
+                ilens,
+                ys,
+                ds=ds,
+                spembs=spembs,
+                sids=sids,
+                lids=lids,
+            )  # (1, T_feats, odim)
+        else:
+            # inference
+            _, outs, d_outs = self._forward(
+                xs,
+                ilens,
+                ys,
+                spembs=spembs,
+                sids=sids,
+                lids=lids,
+                is_inference=True,
+                alpha=alpha,
+            )  # (1, T_feats, odim)
 
-        return dict(
-            feat_gen=after_outs[0], prob=None, att_w=None
-        )  # outs, probs, att_ws
+        return dict(feat_gen=outs[0], duration=d_outs[0])
 
     def _integrate_with_spk_embed(
         self, hs: torch.Tensor, spembs: torch.Tensor
     ) -> torch.Tensor:
         """Integrate speaker embedding with hidden states.
 
         Args:
-            hs (Tensor): Batch of hidden state sequences (B, Tmax, adim).
+            hs (Tensor): Batch of hidden state sequences (B, T_text, adim).
             spembs (Tensor): Batch of speaker embeddings (B, spk_embed_dim).
 
         Returns:
-            Tensor: Batch of integrated hidden state sequences (B, Tmax, adim).
-        """
+            Tensor: Batch of integrated hidden state sequences (B, T_text, adim).
 
+        """
         if self.spk_embed_integration_type == "add":
             # apply projection and then add to hidden states
             spembs = self.projection(F.normalize(spembs))
             hs = hs + spembs.unsqueeze(1)
         elif self.spk_embed_integration_type == "concat":
             # concat hidden states with spk embeds and then apply projection
             spembs = F.normalize(spembs).unsqueeze(1).expand(-1, hs.size(1), -1)
             hs = self.projection(torch.cat([hs, spembs], dim=-1))
         else:
             raise NotImplementedError("support only add or concat.")
 
-        return
+        return hs
+
+    def _source_mask(self, ilens: torch.Tensor) -> torch.Tensor:
+        """Make masks for self-attention.
+
+        Args:
+            ilens (LongTensor): Batch of lengths (B,).
+
+        Returns:
+            Tensor: Mask tensor for self-attention.
+                dtype=torch.uint8 in PyTorch 1.2-
+                dtype=torch.bool in PyTorch 1.2+ (including 1.2)
+
+        Examples:
+            >>> ilens = [5, 3]
+            >>> self._source_mask(ilens)
+            tensor([[[1, 1, 1, 1, 1],
+                     [1, 1, 1, 0, 0]]], dtype=torch.uint8)
+
+        """
+        x_masks = make_non_pad_mask(ilens).to(next(self.parameters()).device)
+        return x_masks.unsqueeze(-2)
+
+    def _reset_parameters(
+        self, init_type: str, init_enc_alpha: float, init_dec_alpha: float
+    ):
+        # initialize parameters
+        if init_type != "pytorch":
+            initialize(self, init_type)
+
+        # initialize alpha in scaled positional encoding
+        if self.encoder_type == "transformer" and self.use_scaled_pos_enc:
+            self.encoder.embed[-1].alpha.data = torch.tensor(init_enc_alpha)
+        if self.decoder_type == "transformer" and self.use_scaled_pos_enc:
+            self.decoder.embed[-1].alpha.data = torch.tensor(init_dec_alpha)
```

### Comparing `espnet-202304/espnet2/svs/xiaoice/XiaoiceSing.py` & `espnet-202308/espnet2/svs/xiaoice/XiaoiceSing.py`

 * *Files 1% similar despite different names*

```diff
@@ -50,16 +50,15 @@
 
     def __init__(
         self,
         # network structure related
         idim: int,
         odim: int,
         midi_dim: int = 129,
-        tempo_dim: int = 500,
-        embed_dim: int = 512,
+        duration_dim: int = 500,
         adim: int = 384,
         aheads: int = 4,
         elayers: int = 6,
         eunits: int = 1536,
         dlayers: int = 6,
         dunits: int = 1536,
         postnet_layers: int = 5,
@@ -114,16 +113,18 @@
         lambda_dur: float = 0.1,
         lambda_pitch: float = 0.01,
         lambda_vuv: float = 0.01,
     ):
         """Initialize XiaoiceSing module.
 
         Args:
-            idim (int): Dimension of the inputs.
+            idim (int): Dimension of the label inputs.
             odim (int): Dimension of the outputs.
+            midi_dim (int): Dimension of the midi inputs.
+            duration_dim (int): Dimension of the duration inputs.
             elayers (int): Number of encoder layers.
             eunits (int): Number of encoder hidden units.
             dlayers (int): Number of decoder layers.
             dunits (int): Number of decoder hidden units.
             postnet_layers (int): Number of postnet layers.
             postnet_chans (int): Number of postnet channels.
             postnet_filts (int): Kernel size of postnet.
@@ -170,30 +171,29 @@
             init_dec_alpha (float): Initial value of alpha in scaled pos encoding of the
                 decoder.
             use_masking (bool): Whether to apply masking for padded part in loss
                 calculation.
             use_weighted_masking (bool): Whether to apply weighted masking in loss
                 calculation.
             loss_function (str): Loss functions ("FastSpeech1" or "XiaoiceSing2")
-            loss_type (str): Loss type ("L1" (MAE) or "L2" (MSE))
+            loss_type (str): Mel loss type ("L1" (MAE), "L2" (MSE) or "L1+L2")
             lambda_mel (float): Loss scaling coefficient for Mel loss.
             lambda_dur (float): Loss scaling coefficient for duration loss.
             lambda_pitch (float): Loss scaling coefficient for pitch loss.
             lambda_vuv (float): Loss scaling coefficient for VUV loss.
 
         """
         assert check_argument_types()
         super().__init__()
 
         # store hyperparameters
         self.idim = idim
         self.midi_dim = midi_dim
-        self.tempo_dim = tempo_dim
+        self.duration_dim = duration_dim
         self.odim = odim
-        self.embed_dim = embed_dim
         self.eos = idim - 1
         self.reduction_factor = reduction_factor
         self.encoder_type = encoder_type
         self.decoder_type = decoder_type
         self.use_scaled_pos_enc = use_scaled_pos_enc
         self.loss_function = loss_function
         self.loss_type = loss_type
@@ -232,24 +232,24 @@
                 assert conformer_pos_enc_layer_type != "legacy_rel_pos"
                 assert conformer_self_attn_layer_type != "legacy_rel_selfattn"
             else:
                 raise ValueError(f"Unknown rel_pos_type: {conformer_rel_pos_type}")
 
         # define encoder
         self.phone_encode_layer = torch.nn.Embedding(
-            num_embeddings=idim, embedding_dim=embed_dim, padding_idx=self.padding_idx
+            num_embeddings=idim, embedding_dim=adim, padding_idx=self.padding_idx
         )
         self.midi_encode_layer = torch.nn.Embedding(
             num_embeddings=midi_dim,
-            embedding_dim=embed_dim,
+            embedding_dim=adim,
             padding_idx=self.padding_idx,
         )
-        self.tempo_encode_layer = torch.nn.Embedding(
-            num_embeddings=tempo_dim,
-            embedding_dim=embed_dim,
+        self.duration_encode_layer = torch.nn.Embedding(
+            num_embeddings=duration_dim,
+            embedding_dim=adim,
             padding_idx=self.padding_idx,
         )
         if encoder_type == "transformer":
             self.encoder = TransformerEncoder(
                 idim=0,
                 attention_dim=adim,
                 attention_heads=aheads,
@@ -366,15 +366,15 @@
                 use_cnn_module=use_cnn_in_conformer,
                 cnn_module_kernel=conformer_dec_kernel_size,
             )
         else:
             raise ValueError(f"{decoder_type} is not supported.")
 
         # define final projection
-        self.linear_projection = torch.nn.Linear(adim, odim * reduction_factor + 2)
+        self.linear_projection = torch.nn.Linear(adim, (odim + 2) * reduction_factor)
 
         # define postnet
         self.postnet = (
             None
             if postnet_layers == 0
             else Postnet(
                 idim=idim,
@@ -461,43 +461,44 @@
             Dict: Statistics to be monitored.
             Tensor: Weight value if not joint training else model outputs.
         """
 
         if joint_training:
             label = label
             midi = melody
-            tempo = duration
             label_lengths = label_lengths
             midi_lengths = melody_lengths
-            tempo_lengths = duration_lengths
+            duration_lengths = duration_lengths
+            duration_ = duration
             ds = duration
         else:
             label = label["score"]
             midi = melody["score"]
-            tempo = duration["score_phn"]
+            duration_ = duration["score_phn"]
             label_lengths = label_lengths["score"]
             midi_lengths = melody_lengths["score"]
-            tempo_lengths = duration_lengths["score_phn"]
+            duration_lengths = duration_lengths["score_phn"]
             ds = duration["lab"]
 
-        text = text[:, : text_lengths.max()]  # for data-parallel
         feats = feats[:, : feats_lengths.max()]  # for data-parallel
         midi = midi[:, : midi_lengths.max()]  # for data-parallel
         label = label[:, : label_lengths.max()]  # for data-parallel
-        tempo = tempo[:, : tempo_lengths.max()]  # for data-parallel
+        duration_ = duration_[:, : duration_lengths.max()]  # for data-parallel
+        olens = feats_lengths
+
         if self.loss_function == "XiaoiceSing2":
             pitch = pitch[:, : pitch_lengths.max()]
             log_f0 = torch.clamp(pitch, min=0)
             vuv = log_f0 != 0
         batch_size = text.size(0)
 
         label_emb = self.phone_encode_layer(label)
         midi_emb = self.midi_encode_layer(midi)
-        tempo_emb = self.tempo_encode_layer(tempo)
-        input_emb = label_emb + midi_emb + tempo_emb
+        duration_emb = self.duration_encode_layer(duration_)
+        input_emb = label_emb + midi_emb + duration_emb
 
         x_masks = self._source_mask(label_lengths)
         hs, _ = self.encoder(input_emb, x_masks)  # (B, T_text, adim)
 
         # integrate with SID and LID embeddings
         if self.spks is not None:
             sid_embs = self.sid_emb(sids.view(-1))
@@ -513,20 +514,27 @@
         # forward duration predictor and length regulator
         d_masks = make_pad_mask(label_lengths).to(input_emb.device)
         d_outs = self.duration_predictor(hs, d_masks)  # (B, T_text)
 
         hs = self.length_regulator(hs, ds)  # (B, T_feats, adim)
 
         # forward decoder
-        h_masks = self._source_mask(feats_lengths)
+        if self.reduction_factor > 1:
+            olens_in = olens.new([olen // self.reduction_factor for olen in olens])
+        else:
+            olens_in = olens
+        h_masks = self._source_mask(olens_in)
+
         zs, _ = self.decoder(hs, h_masks)  # (B, T_feats, adim)
-        before_outs, log_f0_outs, vuv_outs = self.linear_projection(
-            zs
-        ).split_with_sizes([self.odim * self.reduction_factor, 1, 1], dim=2)
-        before_outs = before_outs.view(zs.size(0), -1, self.odim)  # (B. T_feats, odim)
+        before_outs, log_f0_outs, vuv_outs = (
+            self.linear_projection(zs)
+            .view((zs.size(0), -1, self.odim + 2))
+            .split_with_sizes([self.odim, 1, 1], dim=2)
+        )
+        # (B. T_feats, odim), (B. T_feats, 1), (B. T_feats, 1)
 
         # postnet -> (B, Lmax//r * r, odim)
         if self.postnet is None:
             after_outs = before_outs
         else:
             after_outs = before_outs + self.postnet(
                 before_outs.transpose(1, 2)
@@ -641,34 +649,36 @@
                 * feat_gen (Tensor): Output sequence of features (T_feats, odim).
                 * duration (Tensor): Duration sequence (T_text + 1,).
         """
 
         label = label["score"]
         midi = melody["score"]
         if joint_training:
-            tempo = duration["lab"]
+            duration_ = duration["lab"]
         else:
-            tempo = duration["score_phn"]
+            duration_ = duration["score_phn"]
         ds = duration["lab"]
 
         label_emb = self.phone_encode_layer(label)
         midi_emb = self.midi_encode_layer(midi)
-        tempo_emb = self.tempo_encode_layer(tempo)
-        input_emb = label_emb + midi_emb + tempo_emb
+        duration_emb = self.duration_encode_layer(duration_)
+        input_emb = label_emb + midi_emb + duration_emb
 
         x_masks = None  # self._source_mask(label_lengths)
         hs, _ = self.encoder(input_emb, x_masks)  # (B, T_text, adim)
 
         # integrate with SID and LID embeddings
         if self.spks is not None:
             sid_embs = self.sid_emb(sids.view(-1))
             hs = hs + sid_embs.unsqueeze(1)
         if self.langs is not None:
             lid_embs = self.lid_emb(lids.view(-1))
             hs = hs + lid_embs.unsqueeze(1)
+        if spembs is not None:
+            spembs = spembs.unsqueeze(0)
 
         # integrate speaker embedding
         if self.spk_embed_dim is not None:
             hs = self._integrate_with_spk_embed(hs, spembs)
 
         # forward duration predictor and length regulator
         d_masks = None  # make_pad_mask(label_lengths).to(input_emb.device)
@@ -682,19 +692,20 @@
 
         # use duration model output
         hs = self.length_regulator(hs, d_outs_int)  # (B, T_feats, adim)
 
         # forward decoder
         h_masks = None  # self._source_mask(feats_lengths)
         zs, _ = self.decoder(hs, h_masks)  # (B, T_feats, adim)
-        before_outs, _, _ = self.linear_projection(zs).split_with_sizes(
-            [self.odim * self.reduction_factor, 1, 1], dim=2
+        before_outs, _, _ = (
+            self.linear_projection(zs)
+            .view((zs.size(0), -1, self.odim + 2))
+            .split_with_sizes([self.odim, 1, 1], dim=2)
         )
-        before_outs = before_outs.view(zs.size(0), -1, self.odim)  # (B. T_feats, odim)
-        # (B, T_feats, odim)
+        # (B, T_feats, odim), (B, T_feats, 1), (B, T_feats, 1)
 
         # postnet -> (B, Lmax//r * r, odim)
         if self.postnet is None:
             after_outs = before_outs
         else:
             after_outs = before_outs + self.postnet(
                 before_outs.transpose(1, 2)
```

### Comparing `espnet-202304/espnet2/svs/xiaoice/loss.py` & `espnet-202308/espnet2/tts/fastspeech2/loss.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,25 +1,24 @@
 # Copyright 2020 Nagoya University (Tomoki Hayashi)
-# Copyright 2023 Renmin University of China (Yuning Wu)
 #  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)
 
-"""XiaoiceSing2 related loss module for ESPnet2."""
+"""Fastspeech2 related loss module for ESPnet2."""
 
 from typing import Tuple
 
 import torch
 from typeguard import check_argument_types
 
 from espnet.nets.pytorch_backend.fastspeech.duration_predictor import (  # noqa: H301
     DurationPredictorLoss,
 )
 from espnet.nets.pytorch_backend.nets_utils import make_non_pad_mask
 
 
-class XiaoiceSing2Loss(torch.nn.Module):
+class FastSpeech2Loss(torch.nn.Module):
     """Loss function module for FastSpeech2."""
 
     def __init__(self, use_masking: bool = True, use_weighted_masking: bool = False):
         """Initialize feed-forward Transformer loss module.
 
         Args:
             use_masking (bool): Whether to apply masking for padded part in loss
@@ -35,95 +34,93 @@
         self.use_masking = use_masking
         self.use_weighted_masking = use_weighted_masking
 
         # define criterions
         reduction = "none" if self.use_weighted_masking else "mean"
         self.l1_criterion = torch.nn.L1Loss(reduction=reduction)
         self.mse_criterion = torch.nn.MSELoss(reduction=reduction)
-        self.bce_criterion = torch.nn.BCEWithLogitsLoss(reduction=reduction)
         self.duration_criterion = DurationPredictorLoss(reduction=reduction)
 
     def forward(
         self,
         after_outs: torch.Tensor,
         before_outs: torch.Tensor,
         d_outs: torch.Tensor,
         p_outs: torch.Tensor,
-        v_outs: torch.Tensor,
+        e_outs: torch.Tensor,
         ys: torch.Tensor,
         ds: torch.Tensor,
         ps: torch.Tensor,
-        vs: torch.Tensor,
+        es: torch.Tensor,
         ilens: torch.Tensor,
         olens: torch.Tensor,
-        loss_type: str = "L2",
     ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
         """Calculate forward propagation.
 
         Args:
             after_outs (Tensor): Batch of outputs after postnets (B, T_feats, odim).
             before_outs (Tensor): Batch of outputs before postnets (B, T_feats, odim).
             d_outs (LongTensor): Batch of outputs of duration predictor (B, T_text).
-            p_outs (Tensor): Batch of outputs of log_f0 (B, T_text, 1).
-            v_outs (Tensor): Batch of outputs of VUV (B, T_text, 1).
+            p_outs (Tensor): Batch of outputs of pitch predictor (B, T_text, 1).
+            e_outs (Tensor): Batch of outputs of energy predictor (B, T_text, 1).
             ys (Tensor): Batch of target features (B, T_feats, odim).
             ds (LongTensor): Batch of durations (B, T_text).
-            ps (Tensor): Batch of target log_f0 (B, T_text, 1).
-            vs (Tensor): Batch of target VUV (B, T_text, 1).
+            ps (Tensor): Batch of target token-averaged pitch (B, T_text, 1).
+            es (Tensor): Batch of target token-averaged energy (B, T_text, 1).
             ilens (LongTensor): Batch of the lengths of each input (B,).
             olens (LongTensor): Batch of the lengths of each target (B,).
 
         Returns:
-            Tensor: Mel loss value.
+            Tensor: L1 loss value.
             Tensor: Duration predictor loss value.
             Tensor: Pitch predictor loss value.
-            Tensor: VUV predictor loss value.
+            Tensor: Energy predictor loss value.
 
         """
         # apply mask to remove padded part
         if self.use_masking:
             out_masks = make_non_pad_mask(olens).unsqueeze(-1).to(ys.device)
             before_outs = before_outs.masked_select(out_masks)
             if after_outs is not None:
                 after_outs = after_outs.masked_select(out_masks)
             ys = ys.masked_select(out_masks)
             duration_masks = make_non_pad_mask(ilens).to(ys.device)
             d_outs = d_outs.masked_select(duration_masks)
             ds = ds.masked_select(duration_masks)
-            p_outs = p_outs.masked_select(out_masks)
-            v_outs = v_outs.masked_select(out_masks)
-            ps = ps.masked_select(out_masks)
-            vs = vs.masked_select(out_masks)
+            pitch_masks = make_non_pad_mask(ilens).unsqueeze(-1).to(ys.device)
+            p_outs = p_outs.masked_select(pitch_masks)
+            e_outs = e_outs.masked_select(pitch_masks)
+            ps = ps.masked_select(pitch_masks)
+            es = es.masked_select(pitch_masks)
 
         # calculate loss
-        if loss_type == "L1":
-            mel_loss = self.l1_criterion(before_outs, ys)
-            if after_outs is not None:
-                mel_loss += self.l1_criterion(after_outs, ys)
-        else:
-            mel_loss = self.mse_criterion(before_outs, ys)
-            if after_outs is not None:
-                mel_loss += self.mse_criterion(after_outs, ys)
+        l1_loss = self.l1_criterion(before_outs, ys)
+        if after_outs is not None:
+            l1_loss += self.l1_criterion(after_outs, ys)
         duration_loss = self.duration_criterion(d_outs, ds)
         pitch_loss = self.mse_criterion(p_outs, ps)
-        vuv_loss = self.bce_criterion(v_outs, vs.float())
+        energy_loss = self.mse_criterion(e_outs, es)
 
         # make weighted mask and apply it
         if self.use_weighted_masking:
             out_masks = make_non_pad_mask(olens).unsqueeze(-1).to(ys.device)
             out_weights = out_masks.float() / out_masks.sum(dim=1, keepdim=True).float()
             out_weights /= ys.size(0) * ys.size(2)
             duration_masks = make_non_pad_mask(ilens).to(ys.device)
             duration_weights = (
                 duration_masks.float() / duration_masks.sum(dim=1, keepdim=True).float()
             )
             duration_weights /= ds.size(0)
 
             # apply weight
-            mel_loss = mel_loss.mul(out_weights).masked_select(out_masks).sum()
+            l1_loss = l1_loss.mul(out_weights).masked_select(out_masks).sum()
             duration_loss = (
                 duration_loss.mul(duration_weights).masked_select(duration_masks).sum()
             )
-            pitch_loss = pitch_loss.mul(pitch_weights).masked_select(out_masks).sum()
-            vuv_loss = vuv_loss.mul(pitch_weights).masked_select(pitch_masks).sum()
+            pitch_masks = duration_masks.unsqueeze(-1)
+            pitch_weights = duration_weights.unsqueeze(-1)
+            pitch_loss = pitch_loss.mul(pitch_weights).masked_select(pitch_masks).sum()
+            energy_loss = (
+                energy_loss.mul(pitch_weights).masked_select(pitch_masks).sum()
+            )
 
-        return mel_loss, duration_loss, pitch_loss, vuv_loss
+        return l1_loss, duration_loss, pitch_loss, energy_loss
```

### Comparing `espnet-202304/espnet2/tasks/abs_task.py` & `espnet-202308/espnet2/tasks/abs_task.py`

 * *Files 1% similar despite different names*

```diff
@@ -26,16 +26,20 @@
 from espnet2.iterators.multiple_iter_factory import MultipleIterFactory
 from espnet2.iterators.sequence_iter_factory import SequenceIterFactory
 from espnet2.main_funcs.collect_stats import collect_stats
 from espnet2.optimizers.optim_groups import configure_optimizer
 from espnet2.optimizers.sgd import SGD
 from espnet2.samplers.build_batch_sampler import BATCH_TYPES, build_batch_sampler
 from espnet2.samplers.unsorted_batch_sampler import UnsortedBatchSampler
+from espnet2.schedulers.cosine_anneal_warmup_restart import (
+    CosineAnnealingWarmupRestarts,
+)
 from espnet2.schedulers.noam_lr import NoamLR
 from espnet2.schedulers.warmup_lr import WarmupLR
+from espnet2.schedulers.warmup_reducelronplateau import WarmupReduceLROnPlateau
 from espnet2.schedulers.warmup_step_lr import WarmupStepLR
 from espnet2.torch_utils.load_pretrained_model import load_pretrained_model
 from espnet2.torch_utils.model_summary import model_summary
 from espnet2.torch_utils.pytorch_version import pytorch_cudnn_version
 from espnet2.torch_utils.set_all_random_seed import set_all_random_seed
 from espnet2.train.abs_espnet_model import AbsESPnetModel
 from espnet2.train.class_choices import ClassChoices
@@ -140,19 +144,21 @@
     ReduceLROnPlateau=torch.optim.lr_scheduler.ReduceLROnPlateau,
     lambdalr=torch.optim.lr_scheduler.LambdaLR,
     steplr=torch.optim.lr_scheduler.StepLR,
     multisteplr=torch.optim.lr_scheduler.MultiStepLR,
     exponentiallr=torch.optim.lr_scheduler.ExponentialLR,
     CosineAnnealingLR=torch.optim.lr_scheduler.CosineAnnealingLR,
     noamlr=NoamLR,
-    warmupsteplr=WarmupStepLR,
     warmuplr=WarmupLR,
+    warmupsteplr=WarmupStepLR,
+    warmupReducelronplateau=WarmupReduceLROnPlateau,
     cycliclr=torch.optim.lr_scheduler.CyclicLR,
     onecyclelr=torch.optim.lr_scheduler.OneCycleLR,
     CosineAnnealingWarmRestarts=torch.optim.lr_scheduler.CosineAnnealingWarmRestarts,
+    CosineAnnealingWarmupRestarts=CosineAnnealingWarmupRestarts,
 )
 # To lower keys
 optim_classes = {k.lower(): v for k, v in optim_classes.items()}
 scheduler_classes = {k.lower(): v for k, v in scheduler_classes.items()}
 
 
 @dataclass
@@ -717,14 +723,21 @@
             type=str,
             default="descending",
             choices=["descending", "ascending"],
             help="Sort the samples in each mini-batches by the sample "
             'lengths. To enable this, "shape_file" must have the length information.',
         )
         group.add_argument(
+            "--shuffle_within_batch",
+            type=str2bool,
+            default=False,
+            help="Shuffles wholes batches in sample-wise. Required for"
+            "Classification tasks normally.",
+        )
+        group.add_argument(
             "--sort_batch",
             type=str,
             default="descending",
             choices=["descending", "ascending"],
             help="Sort mini-batches by the sample lengths",
         )
         group.add_argument(
@@ -1233,14 +1246,18 @@
             else:
                 train_key_file = None
             if len(args.valid_shape_file) != 0:
                 valid_key_file = args.valid_shape_file[0]
             else:
                 valid_key_file = None
 
+            if model and not getattr(model, "extract_feats_in_collect_stats", True):
+                model = None
+                logging.info("Skipping collect_feats in collect_stats stage.")
+
             collect_stats(
                 model=model,
                 train_iter=cls.build_streaming_iterator(
                     data_path_and_name_and_type=args.train_data_path_and_name_and_type,
                     key_file=train_key_file,
                     batch_size=args.batch_size,
                     dtype=args.train_dtype,
@@ -1481,15 +1498,15 @@
         Note that the definition of "epoch" doesn't always indicate
         to run out of the whole training corpus.
         "--num_iters_per_epoch" option restricts the number of iterations for each epoch
         and the rest of samples for the originally epoch are left for the next epoch.
         e.g. If The number of mini-batches equals to 4, the following two are same:
 
         - 1 epoch without "--num_iters_per_epoch"
-        - 4 epoch with "--num_iters_per_epoch" == 4
+        - 4 epoch with "--num_iters_per_epoch" == 1
 
         """
         assert check_argument_types()
         iter_options = cls.build_iter_options(args, distributed_option, mode)
 
         # Overwrite iter_options if any kwargs is given
         if kwargs is not None:
@@ -1587,14 +1604,15 @@
 
         return SequenceIterFactory(
             dataset=dataset,
             batches=batches,
             seed=args.seed,
             num_iters_per_epoch=iter_options.num_iters_per_epoch,
             shuffle=iter_options.train,
+            shuffle_within_batch=args.shuffle_within_batch,
             num_workers=args.num_workers,
             collate_fn=iter_options.collate_fn,
             pin_memory=args.ngpu > 0,
         )
 
     @classmethod
     def build_chunk_iter_factory(
```

### Comparing `espnet-202304/espnet2/tasks/asr.py` & `espnet-202308/espnet2/tasks/asr.py`

 * *Files 1% similar despite different names*

```diff
@@ -56,14 +56,15 @@
 from espnet2.asr.frontend.windowing import SlidingWindow
 from espnet2.asr.maskctc_model import MaskCTCModel
 from espnet2.asr.pit_espnet_model import ESPnetASRModel as PITESPnetModel
 from espnet2.asr.postencoder.abs_postencoder import AbsPostEncoder
 from espnet2.asr.postencoder.hugging_face_transformers_postencoder import (
     HuggingFaceTransformersPostEncoder,
 )
+from espnet2.asr.postencoder.length_adaptor_postencoder import LengthAdaptorPostEncoder
 from espnet2.asr.preencoder.abs_preencoder import AbsPreEncoder
 from espnet2.asr.preencoder.linear import LinearProjection
 from espnet2.asr.preencoder.sinc import LightweightSincConvs
 from espnet2.asr.specaug.abs_specaug import AbsSpecAug
 from espnet2.asr.specaug.specaug import SpecAug
 from espnet2.asr_transducer.joint_network import JointNetwork
 from espnet2.layers.abs_normalize import AbsNormalize
@@ -158,14 +159,15 @@
     type_check=AbsEncoder,
     default="rnn",
 )
 postencoder_choices = ClassChoices(
     name="postencoder",
     classes=dict(
         hugging_face_transformers=HuggingFaceTransformersPostEncoder,
+        length_adaptor=LengthAdaptorPostEncoder,
     ),
     type_check=AbsPostEncoder,
     default=None,
     optional=True,
 )
 decoder_choices = ClassChoices(
     "decoder",
```

### Comparing `espnet-202304/espnet2/tasks/asr_transducer.py` & `espnet-202308/espnet2/tasks/asr_transducer.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,24 +1,27 @@
 """ASR Transducer Task."""
 
 import argparse
 import logging
+import os
 from typing import Callable, Collection, Dict, List, Optional, Tuple
 
 import numpy as np
 import torch
 from typeguard import check_argument_types, check_return_type
 
 from espnet2.asr.frontend.abs_frontend import AbsFrontend
 from espnet2.asr.frontend.default import DefaultFrontend
 from espnet2.asr.frontend.windowing import SlidingWindow
 from espnet2.asr.specaug.abs_specaug import AbsSpecAug
 from espnet2.asr.specaug.specaug import SpecAug
 from espnet2.asr_transducer.decoder.abs_decoder import AbsDecoder
+from espnet2.asr_transducer.decoder.mega_decoder import MEGADecoder
 from espnet2.asr_transducer.decoder.rnn_decoder import RNNDecoder
+from espnet2.asr_transducer.decoder.rwkv_decoder import RWKVDecoder
 from espnet2.asr_transducer.decoder.stateless_decoder import StatelessDecoder
 from espnet2.asr_transducer.encoder.encoder import Encoder
 from espnet2.asr_transducer.espnet_transducer_model import ESPnetASRTransducerModel
 from espnet2.asr_transducer.joint_network import JointNetwork
 from espnet2.layers.abs_normalize import AbsNormalize
 from espnet2.layers.global_mvn import GlobalMVN
 from espnet2.layers.utterance_mvn import UtteranceMVN
@@ -59,15 +62,17 @@
     type_check=AbsNormalize,
     default="utterance_mvn",
     optional=True,
 )
 decoder_choices = ClassChoices(
     "decoder",
     classes=dict(
+        mega=MEGADecoder,
         rnn=RNNDecoder,
+        rwkv=RWKVDecoder,
         stateless=StatelessDecoder,
     ),
     type_check=AbsDecoder,
     default="rnn",
 )
 
 
@@ -350,14 +355,20 @@
             # Overwriting token_list to keep it as "portable".
             args.token_list = list(token_list)
         elif isinstance(args.token_list, (tuple, list)):
             token_list = list(args.token_list)
         else:
             raise RuntimeError("token_list must be str or list")
         vocab_size = len(token_list)
+
+        if hasattr(args, "scheduler_conf"):
+            args.model_conf["warmup_steps"] = args.scheduler_conf.get(
+                "warmup_steps", 25000
+            )
+
         logging.info(f"Vocabulary size: {vocab_size }")
 
         # 1. frontend
         if args.input_size is None:
             # Extract features in the model
             frontend_class = frontend_choices.get_class(args.frontend)
             frontend = frontend_class(**args.frontend_conf)
@@ -383,14 +394,15 @@
 
         # 4. Encoder
         encoder = Encoder(input_size, **args.encoder_conf)
         encoder_output_size = encoder.output_size
 
         # 5. Decoder
         decoder_class = decoder_choices.get_class(args.decoder)
+
         decoder = decoder_class(
             vocab_size,
             **args.decoder_conf,
         )
         decoder_output_size = decoder.output_size
 
         # 6. Joint Network
```

### Comparing `espnet-202304/espnet2/tasks/diar.py` & `espnet-202308/espnet2/tasks/diar.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/tasks/enh.py` & `espnet-202308/espnet2/tasks/enh.py`

 * *Files 4% similar despite different names*

```diff
@@ -309,29 +309,42 @@
         )
         group.add_argument(
             "--force_single_channel",
             type=str2bool,
             default=False,
             help="Whether to force all data to be single-channel.",
         )
+        group.add_argument(
+            "--channel_reordering",
+            type=str2bool,
+            default=False,
+            help="Whether to randomly reorder the channels of the "
+            "multi-channel signals.",
+        )
+        group.add_argument(
+            "--categories",
+            nargs="+",
+            default=[],
+            type=str,
+            help="The set of all possible categories in the dataset. Used to add the "
+            "category information to each sample",
+        )
 
         group.add_argument(
             "--dynamic_mixing",
             type=str2bool,
             default=False,
             help="Apply dynamic mixing",
         )
-
         group.add_argument(
             "--utt2spk",
             type=str_or_none,
             default=None,
             help="The file path of utt2spk file. Only used in dynamic_mixing mode.",
         )
-
         group.add_argument(
             "--dynamic_mixing_gain_db",
             type=float,
             default=0.0,
             help="Random gain (in dB) for dynamic mixing sources",
         )
 
@@ -358,81 +371,56 @@
         assert check_argument_types()
 
         use_preprocessor = getattr(args, "preprocessor", None) is not None
 
         if use_preprocessor:
             # TODO(simpleoier): To make this as simple as model parts, e.g. encoder
             if args.preprocessor == "dynamic_mixing":
-                if train:
-                    retval = preprocessor_choices.get_class(args.preprocessor)(
-                        train=train,
-                        source_scp=os.path.join(
-                            os.path.dirname(
-                                args.train_data_path_and_name_and_type[0][0]
-                            ),
-                            args.preprocessor_conf.get("source_scp_name", "spk1.scp"),
-                        ),
-                        ref_num=args.preprocessor_conf.get(
-                            "ref_num",
-                            args.separator_conf["num_spk"],
-                        ),
-                        dynamic_mixing_gain_db=args.preprocessor_conf.get(
-                            "dynamic_mixing_gain_db",
-                            0.0,
-                        ),
-                        speech_name=args.preprocessor_conf.get(
-                            "speech_name",
-                            "speech_mix",
-                        ),
-                        speech_ref_name_prefix=args.preprocessor_conf.get(
-                            "speech_ref_name_prefix",
-                            "speech_ref",
-                        ),
-                        mixture_source_name=args.preprocessor_conf.get(
-                            "mixture_source_name",
-                            None,
-                        ),
-                        utt2spk=getattr(args, "utt2spk", None),
-                    )
-                else:
-                    retval = None
+                retval = preprocessor_choices.get_class(args.preprocessor)(
+                    train=train,
+                    source_scp=os.path.join(
+                        os.path.dirname(args.train_data_path_and_name_and_type[0][0]),
+                        args.preprocessor_conf.get("source_scp_name", "spk1.scp"),
+                    ),
+                    ref_num=args.preprocessor_conf.get(
+                        "ref_num", args.separator_conf["num_spk"]
+                    ),
+                    dynamic_mixing_gain_db=args.preprocessor_conf.get(
+                        "dynamic_mixing_gain_db", 0.0
+                    ),
+                    speech_name=args.preprocessor_conf.get("speech_name", "speech_mix"),
+                    speech_ref_name_prefix=args.preprocessor_conf.get(
+                        "speech_ref_name_prefix", "speech_ref"
+                    ),
+                    mixture_source_name=args.preprocessor_conf.get(
+                        "mixture_source_name", None
+                    ),
+                    utt2spk=getattr(args, "utt2spk", None),
+                    categories=args.preprocessor_conf.get("categories", None),
+                )
             elif args.preprocessor == "enh":
                 retval = preprocessor_choices.get_class(args.preprocessor)(
                     train=train,
                     # NOTE(kamo): Check attribute existence for backward compatibility
-                    rir_scp=args.rir_scp if hasattr(args, "rir_scp") else None,
-                    rir_apply_prob=args.rir_apply_prob
-                    if hasattr(args, "rir_apply_prob")
-                    else 1.0,
-                    noise_scp=args.noise_scp if hasattr(args, "noise_scp") else None,
-                    noise_apply_prob=args.noise_apply_prob
-                    if hasattr(args, "noise_apply_prob")
-                    else 1.0,
-                    noise_db_range=args.noise_db_range
-                    if hasattr(args, "noise_db_range")
-                    else "13_15",
-                    short_noise_thres=args.short_noise_thres
-                    if hasattr(args, "short_noise_thres")
-                    else 0.5,
-                    speech_volume_normalize=args.speech_volume_normalize
-                    if hasattr(args, "speech_volume_normalize")
-                    else None,
-                    use_reverberant_ref=args.use_reverberant_ref
-                    if hasattr(args, "use_reverberant_ref")
-                    else None,
-                    num_spk=args.num_spk if hasattr(args, "num_spk") else 1,
-                    num_noise_type=args.num_noise_type
-                    if hasattr(args, "num_noise_type")
-                    else 1,
-                    sample_rate=args.sample_rate
-                    if hasattr(args, "sample_rate")
-                    else 8000,
-                    force_single_channel=args.force_single_channel
-                    if hasattr(args, "force_single_channel")
-                    else False,
+                    rir_scp=getattr(args, "rir_scp", None),
+                    rir_apply_prob=getattr(args, "rir_apply_prob", 1.0),
+                    noise_scp=getattr(args, "noise_scp", None),
+                    noise_apply_prob=getattr(args, "noise_apply_prob", 1.0),
+                    noise_db_range=getattr(args, "noise_db_range", "13_15"),
+                    short_noise_thres=getattr(args, "short_noise_thres", 0.5),
+                    speech_volume_normalize=getattr(
+                        args, "speech_volume_normalize", None
+                    ),
+                    use_reverberant_ref=getattr(args, "use_reverberant_ref", None),
+                    num_spk=getattr(args, "num_spk", 1),
+                    num_noise_type=getattr(args, "num_noise_type", 1),
+                    sample_rate=getattr(args, "sample_rate", 8000),
+                    force_single_channel=getattr(args, "force_single_channel", False),
+                    channel_reordering=getattr(args, "channel_reordering", False),
+                    categories=getattr(args, "categories", None),
                 )
             else:
                 raise ValueError(
                     f"Preprocessor type {args.preprocessor} is not supported."
                 )
         else:
             retval = None
@@ -454,14 +442,15 @@
     def optional_data_names(
         cls, train: bool = True, inference: bool = False
     ) -> Tuple[str, ...]:
         retval = ["speech_mix"]
         retval += ["dereverb_ref{}".format(n) for n in range(1, MAX_REFERENCE_NUM + 1)]
         retval += ["speech_ref{}".format(n) for n in range(2, MAX_REFERENCE_NUM + 1)]
         retval += ["noise_ref{}".format(n) for n in range(1, MAX_REFERENCE_NUM + 1)]
+        retval += ["category"]
         retval = tuple(retval)
         assert check_return_type(retval)
         return retval
 
     @classmethod
     def build_model(cls, args: argparse.Namespace) -> ESPnetEnhancementModel:
         assert check_argument_types()
```

### Comparing `espnet-202304/espnet2/tasks/enh_s2t.py` & `espnet-202308/espnet2/tasks/enh_s2t.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/tasks/enh_tse.py` & `espnet-202308/espnet2/tasks/enh_tse.py`

 * *Files 9% similar despite different names*

```diff
@@ -207,14 +207,29 @@
         )
         group.add_argument(
             "--force_single_channel",
             type=str2bool,
             default=False,
             help="Whether to force all data to be single-channel.",
         )
+        group.add_argument(
+            "--channel_reordering",
+            type=str2bool,
+            default=False,
+            help="Whether to randomly reorder the channels of the "
+            "multi-channel signals.",
+        )
+        group.add_argument(
+            "--categories",
+            nargs="+",
+            default=[],
+            type=str,
+            help="The set of all possible categories in the dataset. Used to add the "
+            "category information to each sample",
+        )
 
         for class_choices in cls.class_choices_list:
             # Append --<name> and --<name>_conf.
             # e.g. --encoder and --encoder_conf
             class_choices.add_arguments(group)
 
     @classmethod
@@ -236,42 +251,28 @@
         retval = TSEPreprocessor(
             train=train,
             train_spk2enroll=args.train_spk2enroll,
             enroll_segment=getattr(args, "enroll_segment", None),
             load_spk_embedding=getattr(args, "load_spk_embedding", False),
             load_all_speakers=getattr(args, "load_all_speakers", False),
             # inherited from EnhPreprocessor
-            rir_scp=args.rir_scp if hasattr(args, "rir_scp") else None,
-            rir_apply_prob=args.rir_apply_prob
-            if hasattr(args, "rir_apply_prob")
-            else 1.0,
-            noise_scp=args.noise_scp if hasattr(args, "noise_scp") else None,
-            noise_apply_prob=args.noise_apply_prob
-            if hasattr(args, "noise_apply_prob")
-            else 1.0,
-            noise_db_range=args.noise_db_range
-            if hasattr(args, "noise_db_range")
-            else "13_15",
-            short_noise_thres=args.short_noise_thres
-            if hasattr(args, "short_noise_thres")
-            else 0.5,
-            speech_volume_normalize=args.speech_volume_normalize
-            if hasattr(args, "speech_volume_normalize")
-            else None,
-            use_reverberant_ref=args.use_reverberant_ref
-            if hasattr(args, "use_reverberant_ref")
-            else None,
-            num_spk=args.num_spk if hasattr(args, "num_spk") else 1,
-            num_noise_type=args.num_noise_type
-            if hasattr(args, "num_noise_type")
-            else 1,
-            sample_rate=args.sample_rate if hasattr(args, "sample_rate") else 8000,
-            force_single_channel=args.force_single_channel
-            if hasattr(args, "force_single_channel")
-            else False,
+            rir_scp=getattr(args, "rir_scp", None),
+            rir_apply_prob=getattr(args, "rir_apply_prob", 1.0),
+            noise_scp=getattr(args, "noise_scp", None),
+            noise_apply_prob=getattr(args, "noise_apply_prob", 1.0),
+            noise_db_range=getattr(args, "noise_db_range", "13_15"),
+            short_noise_thres=getattr(args, "short_noise_thres", 0.5),
+            speech_volume_normalize=getattr(args, "speech_volume_normalize", None),
+            use_reverberant_ref=getattr(args, "use_reverberant_ref", None),
+            num_spk=getattr(args, "num_spk", 1),
+            num_noise_type=getattr(args, "num_noise_type", 1),
+            sample_rate=getattr(args, "sample_rate", 8000),
+            force_single_channel=getattr(args, "force_single_channel", False),
+            channel_reordering=getattr(args, "channel_reordering", False),
+            categories=getattr(args, "categories", None),
         )
         assert check_return_type(retval)
         return retval
 
     @classmethod
     def required_data_names(
         cls, train: bool = True, inference: bool = False
@@ -292,14 +293,15 @@
             retval += [
                 "speech_ref{}".format(n) for n in range(2, MAX_REFERENCE_NUM + 1)
             ]
         else:
             retval += [
                 "speech_ref{}".format(n) for n in range(1, MAX_REFERENCE_NUM + 1)
             ]
+        retval += ["category"]
         retval = tuple(retval)
         assert check_return_type(retval)
         return retval
 
     @classmethod
     def build_model(cls, args: argparse.Namespace) -> ESPnetExtractionModel:
         assert check_argument_types()
```

### Comparing `espnet-202304/espnet2/tasks/gan_svs.py` & `espnet-202308/espnet2/tasks/gan_svs.py`

 * *Files 4% similar despite different names*

```diff
@@ -31,14 +31,15 @@
 from espnet2.train.preprocessor import SVSPreprocessor
 from espnet2.tts.feats_extract.abs_feats_extract import AbsFeatsExtract
 from espnet2.tts.feats_extract.dio import Dio
 from espnet2.tts.feats_extract.energy import Energy
 from espnet2.tts.feats_extract.linear_spectrogram import LinearSpectrogram
 from espnet2.tts.feats_extract.log_mel_fbank import LogMelFbank
 from espnet2.tts.feats_extract.log_spectrogram import LogSpectrogram
+from espnet2.tts.feats_extract.ying import Ying
 from espnet2.utils.get_default_kwargs import get_default_kwargs
 from espnet2.utils.nested_dict_action import NestedDictAction
 from espnet2.utils.types import int_or_none, str2bool, str_or_none
 
 feats_extractor_choices = ClassChoices(
     "feats_extract",
     classes=dict(
@@ -62,14 +63,21 @@
 pitch_extractor_choices = ClassChoices(
     "pitch_extract",
     classes=dict(dio=Dio),
     type_check=AbsFeatsExtract,
     default=None,
     optional=True,
 )
+ying_extractor_choices = ClassChoices(
+    "ying_extract",
+    classes=dict(ying=Ying),
+    type_check=AbsFeatsExtract,
+    default=None,
+    optional=True,
+)
 energy_extractor_choices = ClassChoices(
     "energy_extract",
     classes=dict(energy=Energy),
     type_check=AbsFeatsExtract,
     default=None,
     optional=True,
 )
@@ -130,14 +138,16 @@
         normalize_choices,
         # --svs and --svs_conf
         svs_choices,
         # --pitch_extract and --pitch_extract_conf
         pitch_extractor_choices,
         # --pitch_normalize and --pitch_normalize_conf
         pitch_normalize_choices,
+        # --ying_extract and --ying_extract_conf
+        ying_extractor_choices,
         # --energy_extract and --energy_extract_conf
         energy_extractor_choices,
         # --energy_normalize and --energy_normalize_conf
         energy_normalize_choices,
     ]
 
     # Use GANTrainer instead of Trainer
@@ -275,15 +285,24 @@
         return retval
 
     @classmethod
     def optional_data_names(
         cls, train: bool = True, inference: bool = False
     ) -> Tuple[str, ...]:
         if not inference:
-            retval = ("spembs", "durations", "pitch", "energy", "sids", "lids", "feats")
+            retval = (
+                "spembs",
+                "durations",
+                "pitch",
+                "energy",
+                "sids",
+                "lids",
+                "feats",
+                "ying",
+            )
         else:
             # Inference mode
             retval = ("spembs", "singing", "pitch", "durations", "sids", "lids")
         return retval
 
     @classmethod
     def build_model(cls, args: argparse.Namespace) -> ESPnetGANSVSModel:
@@ -326,14 +345,15 @@
         # 3. SVS
         svs_class = svs_choices.get_class(args.svs)
         svs = svs_class(idim=vocab_size, odim=odim, **args.svs_conf)
 
         # 4. Extra components
         score_feats_extract = None
         pitch_extract = None
+        ying_extract = None
         energy_extract = None
         pitch_normalize = None
         energy_normalize = None
         logging.info(f"args:{args}")
         if getattr(args, "score_feats_extract", None) is not None:
             score_feats_extract_class = score_feats_extractor_choices.get_class(
                 args.score_feats_extract
@@ -345,14 +365,22 @@
             pitch_extract_class = pitch_extractor_choices.get_class(
                 args.pitch_extract,
             )
 
             pitch_extract = pitch_extract_class(
                 **args.pitch_extract_conf,
             )
+        if getattr(args, "ying_extract", None) is not None:
+            ying_extract_class = ying_extractor_choices.get_class(
+                args.ying_extract,
+            )
+
+            ying_extract = ying_extract_class(
+                **args.ying_extract_conf,
+            )
         if getattr(args, "energy_extract", None) is not None:
             energy_extract_class = energy_extractor_choices.get_class(
                 args.energy_extract,
             )
             energy_extract = energy_extract_class(
                 **args.energy_extract_conf,
             )
@@ -374,14 +402,15 @@
         # 5. Build model
         model = ESPnetGANSVSModel(
             text_extract=score_feats_extract,
             feats_extract=feats_extract,
             score_feats_extract=score_feats_extract,
             label_extract=score_feats_extract,
             pitch_extract=pitch_extract,
+            ying_extract=ying_extract,
             duration_extract=score_feats_extract,
             energy_extract=energy_extract,
             normalize=normalize,
             pitch_normalize=pitch_normalize,
             energy_normalize=energy_normalize,
             svs=svs,
             **args.model_conf,
```

### Comparing `espnet-202304/espnet2/tasks/gan_tts.py` & `espnet-202308/espnet2/tasks/gan_tts.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/tasks/hubert.py` & `espnet-202308/espnet2/tasks/hubert.py`

 * *Files 12% similar despite different names*

```diff
@@ -5,14 +5,15 @@
 # Their origial Hubert work is in:
 #     Paper: https://arxiv.org/pdf/2106.07447.pdf
 #     Code in Fairseq: https://github.com/pytorch/fairseq/tree/master/examples/hubert
 import argparse
 import logging
 from typing import Callable, Collection, Dict, List, Optional, Tuple, Union
 
+import humanfriendly
 import numpy as np
 import torch
 from typeguard import check_argument_types, check_return_type
 
 from espnet2.asr.encoder.abs_encoder import AbsEncoder
 from espnet2.asr.encoder.hubert_encoder import (  # noqa: H301
     FairseqHubertPretrainEncoder,
@@ -271,21 +272,45 @@
     def build_collate_fn(
         cls, args: argparse.Namespace, train: bool
     ) -> Callable[
         [Collection[Tuple[str, Dict[str, np.ndarray]]]],
         Tuple[List[str], Dict[str, torch.Tensor]],
     ]:
         assert check_argument_types()
+
+        # default sampling rate is 16000
+        fs = args.frontend_conf.get("fs", 16000)
+        if isinstance(fs, str):
+            fs = humanfriendly.parse_size(fs)
+        sample_rate = fs / 1000
+
+        if args.encoder_conf.get("extractor_conv_layer_config", None) is None:
+            # corresponding to default conv extractor
+            # refer to espnet2/asr/encoder/hubert_encoder.py
+            reception_field = 400
+            stride_field = 320
+        else:
+            stride_field, reception_field = 1, 1
+            for conv_config in args.encoder_conf["extractor_conv_layer_config"][::-1]:
+                _, kernel, stride = conv_config
+                stride_field *= stride
+                reception_field = stride * (reception_field - 1) + kernel
+
+        window_size = reception_field / sample_rate
+        window_shift = stride_field / sample_rate
         return HuBERTCollateFn(
             float_pad_value=0.0,
             int_pad_value=-1,
             label_downsampling=args.collate_fn_conf.get("label_downsampling", 1),
             pad=args.collate_fn_conf.get("pad", False),
             rand_crop=args.collate_fn_conf.get("rand_crop", True),
             crop_audio=not args.collect_stats,
+            window_size=window_size,
+            window_shift=window_shift,
+            sample_rate=sample_rate,
         )
 
     @classmethod
     def build_preprocess_fn(
         cls, args: argparse.Namespace, train: bool
     ) -> Optional[Callable[[str, Dict[str, np.array]], Dict[str, np.ndarray]]]:
         assert check_argument_types()
@@ -363,15 +388,15 @@
             # Extract features in the model
             frontend_class = frontend_choices.get_class(args.frontend)
             frontend = frontend_class(**args.frontend_conf)
             input_size = frontend.output_size()
         else:
             # Give features from data-loader
             args.frontend = None
-            args.frontend_conf = {}
+            args.frontend_conf = {**args.frontend_conf}
             frontend = None
             input_size = args.input_size
 
         # 2. Data augmentation for spectrogram
         if args.specaug is not None:
             specaug_class = specaug_choices.get_class(args.specaug)
             specaug = specaug_class(**args.specaug_conf)
```

### Comparing `espnet-202304/espnet2/tasks/lm.py` & `espnet-202308/espnet2/tasks/lm.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/tasks/mt.py` & `espnet-202308/espnet2/tasks/mt.py`

 * *Files 9% similar despite different names*

```diff
@@ -2,45 +2,51 @@
 import logging
 from typing import Callable, Collection, Dict, List, Optional, Tuple
 
 import numpy as np
 import torch
 from typeguard import check_argument_types, check_return_type
 
+from espnet2.asr.ctc import CTC
 from espnet2.asr.decoder.abs_decoder import AbsDecoder
 from espnet2.asr.decoder.rnn_decoder import RNNDecoder
 from espnet2.asr.decoder.transformer_decoder import (
     DynamicConvolution2DTransformerDecoder,
     DynamicConvolutionTransformerDecoder,
     LightweightConvolution2DTransformerDecoder,
     LightweightConvolutionTransformerDecoder,
     TransformerDecoder,
 )
+from espnet2.asr.discrete_asr_espnet_model import ESPnetDiscreteASRModel
 from espnet2.asr.encoder.abs_encoder import AbsEncoder
 from espnet2.asr.encoder.branchformer_encoder import BranchformerEncoder
 from espnet2.asr.encoder.conformer_encoder import ConformerEncoder
 from espnet2.asr.encoder.contextual_block_transformer_encoder import (
     ContextualBlockTransformerEncoder,
 )
+from espnet2.asr.encoder.e_branchformer_encoder import EBranchformerEncoder
 from espnet2.asr.encoder.rnn_encoder import RNNEncoder
 from espnet2.asr.encoder.transformer_encoder import TransformerEncoder
 from espnet2.asr.encoder.vgg_rnn_encoder import VGGRNNEncoder
 from espnet2.asr.frontend.abs_frontend import AbsFrontend
 from espnet2.asr.postencoder.abs_postencoder import AbsPostEncoder
 from espnet2.asr.postencoder.hugging_face_transformers_postencoder import (
     HuggingFaceTransformersPostEncoder,
 )
 from espnet2.asr.preencoder.abs_preencoder import AbsPreEncoder
 from espnet2.asr.preencoder.linear import LinearProjection
 from espnet2.asr.preencoder.sinc import LightweightSincConvs
+from espnet2.asr.specaug.abs_specaug import AbsSpecAug
+from espnet2.asr.specaug.specaug import SpecAug
 from espnet2.mt.espnet_model import ESPnetMTModel
 from espnet2.mt.frontend.embedding import Embedding
 from espnet2.tasks.abs_task import AbsTask
 from espnet2.text.phoneme_tokenizer import g2p_choices
 from espnet2.torch_utils.initialize import initialize
+from espnet2.train.abs_espnet_model import AbsESPnetModel
 from espnet2.train.class_choices import ClassChoices
 from espnet2.train.collate_fn import CommonCollateFn
 from espnet2.train.preprocessor import MutliTokenizerCommonPreprocessor
 from espnet2.train.trainer import Trainer
 from espnet2.utils.get_default_kwargs import get_default_kwargs
 from espnet2.utils.nested_dict_action import NestedDictAction
 from espnet2.utils.types import int_or_none, str2bool, str_or_none
@@ -49,14 +55,23 @@
     name="frontend",
     classes=dict(
         embed=Embedding,
     ),
     type_check=AbsFrontend,
     default="embed",
 )
+specaug_choices = ClassChoices(
+    name="specaug",
+    classes=dict(
+        specaug=SpecAug,
+    ),
+    type_check=AbsSpecAug,
+    default=None,
+    optional=True,
+)
 preencoder_choices = ClassChoices(
     name="preencoder",
     classes=dict(
         sinc=LightweightSincConvs,
         linear=LinearProjection,
     ),
     type_check=AbsPreEncoder,
@@ -68,14 +83,15 @@
     classes=dict(
         conformer=ConformerEncoder,
         transformer=TransformerEncoder,
         contextual_block_transformer=ContextualBlockTransformerEncoder,
         vgg_rnn=VGGRNNEncoder,
         rnn=RNNEncoder,
         branchformer=BranchformerEncoder,
+        e_branchformer=EBranchformerEncoder,
     ),
     type_check=AbsEncoder,
     default="rnn",
 )
 postencoder_choices = ClassChoices(
     name="postencoder",
     classes=dict(
@@ -94,32 +110,45 @@
         dynamic_conv=DynamicConvolutionTransformerDecoder,
         dynamic_conv2d=DynamicConvolution2DTransformerDecoder,
         rnn=RNNDecoder,
     ),
     type_check=AbsDecoder,
     default="rnn",
 )
+model_choices = ClassChoices(
+    "model",
+    classes=dict(
+        mt=ESPnetMTModel,
+        discrete_asr=ESPnetDiscreteASRModel,
+    ),
+    type_check=AbsESPnetModel,
+    default="mt",
+)
 
 
 class MTTask(AbsTask):
     # If you need more than one optimizers, change this value
     num_optimizers: int = 1
 
     # Add variable objects configurations
     class_choices_list = [
         # --frontend and --frontend_conf
         frontend_choices,
+        # --specaug and --specaug_conf
+        specaug_choices,
         # --preencoder and --preencoder_conf
         preencoder_choices,
         # --encoder and --encoder_conf
         encoder_choices,
         # --postencoder and --postencoder_conf
         postencoder_choices,
         # --decoder and --decoder_conf
         decoder_choices,
+        # --model and --model_conf
+        model_choices,
     ]
 
     # If you need to modify train() or eval() procedures, change Trainer class here
     trainer = Trainer
 
     @classmethod
     def add_task_arguments(cls, parser: argparse.ArgumentParser):
@@ -159,20 +188,19 @@
 
         group.add_argument(
             "--input_size",
             type=int_or_none,
             default=None,
             help="The number of input dimension of the feature",
         )
-
         group.add_argument(
-            "--model_conf",
+            "--ctc_conf",
             action=NestedDictAction,
-            default=get_default_kwargs(ESPnetMTModel),
-            help="The keyword arguments for model class.",
+            default=get_default_kwargs(CTC),
+            help="The keyword arguments for CTC class.",
         )
 
         group = parser.add_argument_group(description="Preprocess related")
         group.add_argument(
             "--use_preprocessor",
             type=str2bool,
             default=True,
@@ -219,14 +247,28 @@
         parser.add_argument(
             "--g2p",
             type=str_or_none,
             choices=g2p_choices,
             default=None,
             help="Specify g2p method if --token_type=phn",
         )
+        parser.add_argument(
+            "--tokenizer_encode_conf",
+            type=dict,
+            default=None,
+            help="Tokenization encoder conf, "
+            "e.g. BPE dropout: enable_sampling=True, alpha=0.1, nbest_size=-1",
+        )
+        parser.add_argument(
+            "--src_tokenizer_encode_conf",
+            type=dict,
+            default=None,
+            help="Src tokenization encoder conf, "
+            "e.g. BPE dropout: enable_sampling=True, alpha=0.1, nbest_size=-1",
+        )
 
         for class_choices in cls.class_choices_list:
             # Append --<name> and --<name>_conf.
             # e.g. --encoder and --encoder_conf
             class_choices.add_arguments(group)
 
     @classmethod
@@ -251,14 +293,20 @@
                 token_type=[args.token_type, args.src_token_type],
                 token_list=[args.token_list, args.src_token_list],
                 bpemodel=[args.bpemodel, args.src_bpemodel],
                 non_linguistic_symbols=args.non_linguistic_symbols,
                 text_cleaner=args.cleaner,
                 g2p_type=args.g2p,
                 text_name=["text", "src_text"],
+                tokenizer_encode_conf=[
+                    args.tokenizer_encode_conf,
+                    args.src_tokenizer_encode_conf,
+                ]
+                if train
+                else [dict(), dict()],
             )
         else:
             retval = None
         assert check_return_type(retval)
         return retval
 
     @classmethod
@@ -324,14 +372,21 @@
         else:
             # Give features from data-loader
             args.frontend = None
             args.frontend_conf = {}
             frontend = None
             input_size = args.input_size
 
+        # 2. Data augmentation for spectrogram
+        if getattr(args, "specaug", None) is not None:
+            specaug_class = specaug_choices.get_class(args.specaug)
+            specaug = specaug_class(**args.specaug_conf)
+        else:
+            specaug = None
+
         # 3. Pre-encoder input block
         # NOTE(kan-bayashi): Use getattr to keep the compatibility
         if getattr(args, "preencoder", None) is not None:
             preencoder_class = preencoder_choices.get_class(args.preencoder)
             preencoder = preencoder_class(**args.preencoder_conf)
             input_size = preencoder.output_size()
         else:
@@ -358,26 +413,41 @@
 
         decoder = decoder_class(
             vocab_size=vocab_size,
             encoder_output_size=encoder_output_size,
             **args.decoder_conf,
         )
 
+        # 6. CTC
+        ctc = CTC(
+            odim=vocab_size, encoder_output_size=encoder_output_size, **args.ctc_conf
+        )
+
         # 8. Build model
-        model = ESPnetMTModel(
+        try:
+            model_class = model_choices.get_class(args.model)
+            if args.model == "discrete_asr":
+                extra_model_conf = dict(ctc=ctc, specaug=specaug)
+            else:
+                extra_model_conf = dict()
+        except AttributeError:
+            model_class = model_choices.get_class("mt")
+            extra_model_conf = dict()
+        model = model_class(
             vocab_size=vocab_size,
             src_vocab_size=src_vocab_size,
             frontend=frontend,
             preencoder=preencoder,
             encoder=encoder,
             postencoder=postencoder,
             decoder=decoder,
             token_list=token_list,
             src_token_list=src_token_list,
             **args.model_conf,
+            **extra_model_conf,
         )
 
         # FIXME(kamo): Should be done in model?
         # 9. Initialize
         if args.init is not None:
             initialize(model, args.init)
```

### Comparing `espnet-202304/espnet2/tasks/slu.py` & `espnet-202308/espnet2/tasks/slu.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/tasks/st.py` & `espnet-202308/espnet2/tasks/st.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,51 +1,66 @@
 import argparse
 import logging
-from typing import Callable, Collection, Dict, List, Optional, Tuple
+from typing import Callable, Collection, Dict, List, Optional, Tuple, Union
 
 import numpy as np
 import torch
 from typeguard import check_argument_types, check_return_type
 
 from espnet2.asr.ctc import CTC
 from espnet2.asr.decoder.abs_decoder import AbsDecoder
+from espnet2.asr.decoder.hugging_face_transformers_decoder import (  # noqa: H301
+    HuggingFaceTransformersDecoder,
+)
 from espnet2.asr.decoder.rnn_decoder import RNNDecoder
+from espnet2.asr.decoder.transducer_decoder import TransducerDecoder
 from espnet2.asr.decoder.transformer_decoder import (
     DynamicConvolution2DTransformerDecoder,
     DynamicConvolutionTransformerDecoder,
     LightweightConvolution2DTransformerDecoder,
     LightweightConvolutionTransformerDecoder,
     TransformerDecoder,
+    TransformerMDDecoder,
 )
 from espnet2.asr.encoder.abs_encoder import AbsEncoder
+from espnet2.asr.encoder.branchformer_encoder import BranchformerEncoder
 from espnet2.asr.encoder.conformer_encoder import ConformerEncoder
+from espnet2.asr.encoder.contextual_block_conformer_encoder import (
+    ContextualBlockConformerEncoder,
+)
 from espnet2.asr.encoder.contextual_block_transformer_encoder import (
     ContextualBlockTransformerEncoder,
 )
+from espnet2.asr.encoder.e_branchformer_encoder import EBranchformerEncoder
 from espnet2.asr.encoder.hubert_encoder import (
     FairseqHubertEncoder,
     FairseqHubertPretrainEncoder,
 )
+from espnet2.asr.encoder.hugging_face_transformers_encoder import (
+    HuggingFaceTransformersEncoder,
+)
 from espnet2.asr.encoder.rnn_encoder import RNNEncoder
 from espnet2.asr.encoder.transformer_encoder import TransformerEncoder
 from espnet2.asr.encoder.vgg_rnn_encoder import VGGRNNEncoder
 from espnet2.asr.encoder.wav2vec2_encoder import FairSeqWav2Vec2Encoder
 from espnet2.asr.frontend.abs_frontend import AbsFrontend
 from espnet2.asr.frontend.default import DefaultFrontend
 from espnet2.asr.frontend.s3prl import S3prlFrontend
 from espnet2.asr.frontend.windowing import SlidingWindow
 from espnet2.asr.postencoder.abs_postencoder import AbsPostEncoder
 from espnet2.asr.postencoder.hugging_face_transformers_postencoder import (
     HuggingFaceTransformersPostEncoder,
 )
+from espnet2.asr.postencoder.length_adaptor_postencoder import LengthAdaptorPostEncoder
 from espnet2.asr.preencoder.abs_preencoder import AbsPreEncoder
 from espnet2.asr.preencoder.linear import LinearProjection
 from espnet2.asr.preencoder.sinc import LightweightSincConvs
 from espnet2.asr.specaug.abs_specaug import AbsSpecAug
 from espnet2.asr.specaug.specaug import SpecAug
+from espnet2.asr_transducer.joint_network import JointNetwork
 from espnet2.layers.abs_normalize import AbsNormalize
 from espnet2.layers.global_mvn import GlobalMVN
 from espnet2.layers.utterance_mvn import UtteranceMVN
 from espnet2.st.espnet_model import ESPnetSTModel
 from espnet2.tasks.abs_task import AbsTask
 from espnet2.text.phoneme_tokenizer import g2p_choices
 from espnet2.torch_utils.initialize import initialize
@@ -96,70 +111,129 @@
 )
 encoder_choices = ClassChoices(
     "encoder",
     classes=dict(
         conformer=ConformerEncoder,
         transformer=TransformerEncoder,
         contextual_block_transformer=ContextualBlockTransformerEncoder,
+        contextual_block_conformer=ContextualBlockConformerEncoder,
         vgg_rnn=VGGRNNEncoder,
         rnn=RNNEncoder,
         wav2vec2=FairSeqWav2Vec2Encoder,
         hubert=FairseqHubertEncoder,
         hubert_pretrain=FairseqHubertPretrainEncoder,
+        branchformer=BranchformerEncoder,
+        e_branchformer=EBranchformerEncoder,
     ),
     type_check=AbsEncoder,
     default="rnn",
 )
 postencoder_choices = ClassChoices(
     name="postencoder",
     classes=dict(
         hugging_face_transformers=HuggingFaceTransformersPostEncoder,
+        length_adaptor=LengthAdaptorPostEncoder,
     ),
     type_check=AbsPostEncoder,
     default=None,
     optional=True,
 )
 decoder_choices = ClassChoices(
     "decoder",
     classes=dict(
         transformer=TransformerDecoder,
+        transformer_md=TransformerMDDecoder,
         lightweight_conv=LightweightConvolutionTransformerDecoder,
         lightweight_conv2d=LightweightConvolution2DTransformerDecoder,
         dynamic_conv=DynamicConvolutionTransformerDecoder,
         dynamic_conv2d=DynamicConvolution2DTransformerDecoder,
         rnn=RNNDecoder,
+        transducer=TransducerDecoder,
+        hugging_face_transformers=HuggingFaceTransformersDecoder,
     ),
     type_check=AbsDecoder,
     default="rnn",
 )
 extra_asr_decoder_choices = ClassChoices(
     "extra_asr_decoder",
     classes=dict(
         transformer=TransformerDecoder,
+        transformer_md=TransformerMDDecoder,
         lightweight_conv=LightweightConvolutionTransformerDecoder,
         lightweight_conv2d=LightweightConvolution2DTransformerDecoder,
         dynamic_conv=DynamicConvolutionTransformerDecoder,
         dynamic_conv2d=DynamicConvolution2DTransformerDecoder,
         rnn=RNNDecoder,
     ),
     type_check=AbsDecoder,
-    default="rnn",
+    default=None,
+    optional=True,
 )
 extra_mt_decoder_choices = ClassChoices(
     "extra_mt_decoder",
     classes=dict(
         transformer=TransformerDecoder,
         lightweight_conv=LightweightConvolutionTransformerDecoder,
         lightweight_conv2d=LightweightConvolution2DTransformerDecoder,
         dynamic_conv=DynamicConvolutionTransformerDecoder,
         dynamic_conv2d=DynamicConvolution2DTransformerDecoder,
         rnn=RNNDecoder,
     ),
     type_check=AbsDecoder,
-    default="rnn",
+    default=None,
+    optional=True,
+)
+extra_mt_encoder_choices = ClassChoices(
+    "extra_mt_encoder",
+    classes=dict(
+        conformer=ConformerEncoder,
+        transformer=TransformerEncoder,
+        contextual_block_transformer=ContextualBlockTransformerEncoder,
+        contextual_block_conformer=ContextualBlockConformerEncoder,
+        vgg_rnn=VGGRNNEncoder,
+        rnn=RNNEncoder,
+        branchformer=BranchformerEncoder,
+        e_branchformer=EBranchformerEncoder,
+        hugging_face_transformers=HuggingFaceTransformersEncoder,
+    ),
+    type_check=AbsEncoder,
+    default=None,
+    optional=True,
+)
+md_encoder_choices = ClassChoices(
+    "md_encoder",
+    classes=dict(
+        conformer=ConformerEncoder,
+        transformer=TransformerEncoder,
+        contextual_block_transformer=ContextualBlockTransformerEncoder,
+        contextual_block_conformer=ContextualBlockConformerEncoder,
+        vgg_rnn=VGGRNNEncoder,
+        rnn=RNNEncoder,
+        branchformer=BranchformerEncoder,
+        e_branchformer=EBranchformerEncoder,
+    ),
+    type_check=AbsEncoder,
+    default=None,
+    optional=True,
+)
+hier_encoder_choices = ClassChoices(
+    "hier_encoder",
+    classes=dict(
+        conformer=ConformerEncoder,
+        transformer=TransformerEncoder,
+        contextual_block_transformer=ContextualBlockTransformerEncoder,
+        contextual_block_conformer=ContextualBlockConformerEncoder,
+        vgg_rnn=VGGRNNEncoder,
+        rnn=RNNEncoder,
+        branchformer=BranchformerEncoder,
+        e_branchformer=EBranchformerEncoder,
+    ),
+    type_check=AbsEncoder,
+    default=None,
+    optional=True,
 )
 
 
 class STTask(AbsTask):
     # If you need more than one optimizers, change this value
     num_optimizers: int = 1
 
@@ -179,14 +253,20 @@
         postencoder_choices,
         # --decoder and --decoder_conf
         decoder_choices,
         # --extra_asr_decoder and --extra_asr_decoder_conf
         extra_asr_decoder_choices,
         # --extra_mt_decoder and --extra_mt_decoder_conf
         extra_mt_decoder_choices,
+        # --md_encoder and --md_encoder_conf
+        md_encoder_choices,
+        # --hier_encoder and --hier_encoder_conf
+        hier_encoder_choices,
+        # --extra_mt_encoder and --extra_mt_encoder_conf
+        extra_mt_encoder_choices,
     ]
 
     # If you need to modify train() or eval() procedures, change Trainer class here
     trainer = Trainer
 
     @classmethod
     def add_task_arguments(cls, parser: argparse.ArgumentParser):
@@ -234,14 +314,20 @@
         group.add_argument(
             "--ctc_conf",
             action=NestedDictAction,
             default=get_default_kwargs(CTC),
             help="The keyword arguments for CTC class.",
         )
         group.add_argument(
+            "--st_joint_net_conf",
+            action=NestedDictAction,
+            default=None,
+            help="The keyword arguments for joint network class.",
+        )
+        group.add_argument(
             "--model_conf",
             action=NestedDictAction,
             default=get_default_kwargs(ESPnetSTModel),
             help="The keyword arguments for model class.",
         )
 
         group = parser.add_argument_group(description="Preprocess related")
@@ -251,15 +337,15 @@
             default=True,
             help="Apply preprocessing to data or not",
         )
         group.add_argument(
             "--token_type",
             type=str,
             default="bpe",
-            choices=["bpe", "char", "word", "phn"],
+            choices=["bpe", "char", "word", "phn", "hugging_face"],
             help="The target text will be tokenized " "in the specified level token",
         )
         group.add_argument(
             "--src_token_type",
             type=str,
             default="bpe",
             choices=["bpe", "char", "word", "phn", "none"],
@@ -335,14 +421,20 @@
         group.add_argument(
             "--short_noise_thres",
             type=float,
             default=0.5,
             help="If len(noise) / len(speech) is smaller than this threshold during "
             "dynamic mixing, a warning will be displayed.",
         )
+        group.add_argument(
+            "--ctc_sample_rate",
+            type=float,
+            default=0.0,
+            help="Sample greedy CTC output as AR decoder target.",
+        )
 
         for class_choices in cls.class_choices_list:
             # Append --<name> and --<name>_conf.
             # e.g. --encoder and --encoder_conf
             class_choices.add_arguments(group)
 
     @classmethod
@@ -359,14 +451,15 @@
     @classmethod
     def build_preprocess_fn(
         cls, args: argparse.Namespace, train: bool
     ) -> Optional[Callable[[str, Dict[str, np.array]], Dict[str, np.ndarray]]]:
         assert check_argument_types()
         if args.src_token_type == "none":
             args.src_token_type = None
+
         if args.use_preprocessor:
             retval = MutliTokenizerCommonPreprocessor(
                 train=train,
                 token_type=[args.token_type, args.src_token_type],
                 token_list=[args.token_list, args.src_token_list],
                 bpemodel=[args.bpemodel, args.src_bpemodel],
                 non_linguistic_symbols=args.non_linguistic_symbols,
@@ -417,15 +510,15 @@
             retval = ("src_text",)
         else:
             retval = ()
         assert check_return_type(retval)
         return retval
 
     @classmethod
-    def build_model(cls, args: argparse.Namespace) -> ESPnetSTModel:
+    def build_model(cls, args: argparse.Namespace) -> Union[ESPnetSTModel]:
         assert check_argument_types()
         if isinstance(args.token_list, str):
             with open(args.token_list, encoding="utf-8") as f:
                 token_list = [line.rstrip() for line in f]
 
             # Overwriting token_list to keep it as "portable".
             args.token_list = list(token_list)
@@ -488,56 +581,88 @@
         else:
             preencoder = None
 
         # 4. Encoder
         encoder_class = encoder_choices.get_class(args.encoder)
         encoder = encoder_class(input_size=input_size, **args.encoder_conf)
 
+        asr_encoder_output_size = encoder.output_size()
+        if getattr(args, "hier_encoder", None) is not None:
+            hier_encoder_class = hier_encoder_choices.get_class(args.hier_encoder)
+            hier_encoder = hier_encoder_class(
+                input_size=asr_encoder_output_size, **args.hier_encoder_conf
+            )
+            encoder_output_size = hier_encoder.output_size()
+        else:
+            hier_encoder = None
+            encoder_output_size = asr_encoder_output_size
+
         # 5. Post-encoder block
         # NOTE(kan-bayashi): Use getattr to keep the compatibility
-        encoder_output_size = encoder.output_size()
         if getattr(args, "postencoder", None) is not None:
             postencoder_class = postencoder_choices.get_class(args.postencoder)
             postencoder = postencoder_class(
                 input_size=encoder_output_size, **args.postencoder_conf
             )
             encoder_output_size = postencoder.output_size()
         else:
             postencoder = None
 
         # 5. Decoder
         decoder_class = decoder_choices.get_class(args.decoder)
 
-        decoder = decoder_class(
-            vocab_size=vocab_size,
-            encoder_output_size=encoder_output_size,
-            **args.decoder_conf,
-        )
+        if args.decoder == "transducer":
+            decoder = decoder_class(
+                vocab_size,
+                embed_pad=0,
+                **args.decoder_conf,
+            )
+
+            st_joint_network = JointNetwork(
+                vocab_size,
+                encoder_output_size,
+                decoder.dunits,
+                **args.st_joint_net_conf,
+            )
+        else:
+            decoder = decoder_class(
+                vocab_size=vocab_size,
+                encoder_output_size=encoder_output_size,
+                **args.decoder_conf,
+            )
+
+            st_joint_network = None
 
         # 6. CTC
         if src_token_list is not None:
             ctc = CTC(
                 odim=src_vocab_size,
-                encoder_output_size=encoder_output_size,
+                encoder_output_size=asr_encoder_output_size,
                 **args.ctc_conf,
             )
         else:
             ctc = None
 
+        st_ctc = CTC(
+            odim=vocab_size,
+            encoder_output_size=encoder_output_size,
+            **args.ctc_conf,
+        )
+
         # 7. ASR extra decoder
         if (
             getattr(args, "extra_asr_decoder", None) is not None
             and src_token_list is not None
         ):
             extra_asr_decoder_class = extra_asr_decoder_choices.get_class(
                 args.extra_asr_decoder
             )
             extra_asr_decoder = extra_asr_decoder_class(
                 vocab_size=src_vocab_size,
-                encoder_output_size=encoder_output_size,
+                encoder_output_size=asr_encoder_output_size,
                 **args.extra_asr_decoder_conf,
             )
         else:
             extra_asr_decoder = None
 
         # 8. MT extra decoder
         if getattr(args, "extra_mt_decoder", None) is not None:
@@ -546,30 +671,55 @@
             )
             extra_mt_decoder = extra_mt_decoder_class(
                 vocab_size=vocab_size,
                 encoder_output_size=encoder_output_size,
                 **args.extra_mt_decoder_conf,
             )
         else:
-            extra_asr_decoder = None
+            extra_mt_decoder = None
+
+        # 9. MD encoder
+        if getattr(args, "md_encoder", None) is not None:
+            md_encoder_class = md_encoder_choices.get_class(args.md_encoder)
+            md_encoder = md_encoder_class(
+                input_size=extra_asr_decoder._output_size_bf_softmax,
+                **args.md_encoder_conf,
+            )
+        else:
+            md_encoder = None
+
+        if getattr(args, "extra_mt_encoder", None) is not None:
+            extra_mt_encoder_class = extra_mt_encoder_choices.get_class(
+                args.extra_mt_encoder
+            )
+            extra_mt_encoder = extra_mt_encoder_class(
+                input_size=vocab_size,  # hacked for mbart
+                **args.extra_mt_encoder_conf,
+            )
+        else:
+            extra_mt_encoder = None
 
-        # 8. Build model
         model = ESPnetSTModel(
             vocab_size=vocab_size,
             src_vocab_size=src_vocab_size,
             frontend=frontend,
             specaug=specaug,
             normalize=normalize,
             preencoder=preencoder,
             encoder=encoder,
+            hier_encoder=hier_encoder,
+            md_encoder=md_encoder,
             postencoder=postencoder,
             decoder=decoder,
             ctc=ctc,
+            st_ctc=st_ctc,
+            st_joint_network=st_joint_network,
             extra_asr_decoder=extra_asr_decoder,
             extra_mt_decoder=extra_mt_decoder,
+            extra_mt_encoder=extra_mt_encoder,
             token_list=token_list,
             src_token_list=src_token_list,
             **args.model_conf,
         )
 
         # FIXME(kamo): Should be done in model?
         # 9. Initialize
```

### Comparing `espnet-202304/espnet2/tasks/svs.py` & `espnet-202308/espnet2/tasks/svs.py`

 * *Files 3% similar despite different names*

```diff
@@ -18,31 +18,34 @@
 from espnet2.svs.espnet_model import ESPnetSVSModel
 from espnet2.svs.feats_extract.score_feats_extract import (
     FrameScoreFeats,
     SyllableScoreFeats,
 )
 from espnet2.svs.naive_rnn.naive_rnn import NaiveRNN
 from espnet2.svs.naive_rnn.naive_rnn_dp import NaiveRNNDP
-from espnet2.svs.xiaoice.XiaoiceSing import XiaoiceSing
 
 # TODO(Yuning): Models to be added
+from espnet2.svs.singing_tacotron.singing_tacotron import singing_tacotron
+from espnet2.svs.xiaoice.XiaoiceSing import XiaoiceSing
+
 # from espnet2.svs.encoder_decoder.transformer.transformer import Transformer
 # from espnet2.svs.mlp_singer.mlp_singer import MLPSinger
 # from espnet2.svs.glu_transformer.glu_transformer import GLU_Transformer
 from espnet2.tasks.abs_task import AbsTask
 from espnet2.train.class_choices import ClassChoices
 from espnet2.train.collate_fn import CommonCollateFn
 from espnet2.train.preprocessor import SVSPreprocessor
 from espnet2.train.trainer import Trainer
 from espnet2.tts.feats_extract.abs_feats_extract import AbsFeatsExtract
 from espnet2.tts.feats_extract.dio import Dio
 from espnet2.tts.feats_extract.energy import Energy
 from espnet2.tts.feats_extract.linear_spectrogram import LinearSpectrogram
 from espnet2.tts.feats_extract.log_mel_fbank import LogMelFbank
 from espnet2.tts.feats_extract.log_spectrogram import LogSpectrogram
+from espnet2.tts.feats_extract.ying import Ying
 
 # from espnet2.svs.xiaoice.XiaoiceSing import XiaoiceSing_noDP
 # from espnet2.svs.bytesing.bytesing import ByteSing
 from espnet2.tts.utils import ParallelWaveGANPretrainedVocoder
 from espnet2.utils.get_default_kwargs import get_default_kwargs
 from espnet2.utils.griffin_lim import Spectrogram2Waveform
 from espnet2.utils.nested_dict_action import NestedDictAction
@@ -94,14 +97,21 @@
 pitch_normalize_choices = ClassChoices(
     "pitch_normalize",
     classes=dict(global_mvn=GlobalMVN),
     type_check=AbsNormalize,
     default=None,
     optional=True,
 )
+ying_extractor_choices = ClassChoices(
+    "ying_extract",
+    classes=dict(ying=Ying),
+    type_check=AbsFeatsExtract,
+    default=None,
+    optional=True,
+)
 energy_normalize_choices = ClassChoices(
     "energy_normalize",
     classes=dict(global_mvn=GlobalMVN),
     type_check=AbsNormalize,
     default=None,
     optional=True,
 )
@@ -114,14 +124,15 @@
         naive_rnn=NaiveRNN,
         naive_rnn_dp=NaiveRNNDP,
         xiaoice=XiaoiceSing,
         # xiaoice_noDP=XiaoiceSing_noDP,
         vits=VITS,
         joint_score2wav=JointScore2Wav,
         # mlp=MLPSinger,
+        singing_tacotron=singing_tacotron,
     ),
     type_check=AbsSVS,
     default="naive_rnn",
 )
 
 
 class SVSTask(AbsTask):
@@ -137,14 +148,16 @@
         normalize_choices,
         # --svs and --svs_conf
         svs_choices,
         # --pitch_extract and --pitch_extract_conf
         pitch_extractor_choices,
         # --pitch_normalize and --pitch_normalize_conf
         pitch_normalize_choices,
+        # --ying_extract and --ying_extract_conf
+        ying_extractor_choices,
         # --energy_extract and --energy_extract_conf
         energy_extractor_choices,
         # --energy_normalize and --energy_normalize_conf
         energy_normalize_choices,
     ]
 
     # If you need to modify train() or eval() procedures, change Trainer class here
@@ -277,15 +290,14 @@
             )
         else:
             retval = None
         # FIXME (jiatong): sometimes checking is not working here
         # assert check_return_type(retval)
         return retval
 
-    # TODO(Yuning): check new names
     @classmethod
     def required_data_names(
         cls, train: bool = True, inference: bool = False
     ) -> Tuple[str, ...]:
         if not inference:
             retval = ("text", "singing", "score", "label")
         else:
@@ -294,15 +306,24 @@
         return retval
 
     @classmethod
     def optional_data_names(
         cls, train: bool = True, inference: bool = False
     ) -> Tuple[str, ...]:
         if not inference:
-            retval = ("spembs", "durations", "pitch", "energy", "sids", "lids", "feats")
+            retval = (
+                "spembs",
+                "durations",
+                "pitch",
+                "energy",
+                "sids",
+                "lids",
+                "feats",
+                "ying",
+            )
         else:
             # Inference mode
             retval = ("spembs", "singing", "pitch", "durations", "sids", "lids")
         return retval
 
     @classmethod
     def build_model(cls, args: argparse.Namespace) -> ESPnetSVSModel:
@@ -345,14 +366,15 @@
         # 3. SVS
         svs_class = svs_choices.get_class(args.svs)
         svs = svs_class(idim=vocab_size, odim=odim, **args.svs_conf)
 
         # 4. Extra components
         score_feats_extract = None
         pitch_extract = None
+        ying_extract = None
         energy_extract = None
         pitch_normalize = None
         energy_normalize = None
         logging.info(f"args:{args}")
         if getattr(args, "score_feats_extract", None) is not None:
             score_feats_extract_class = score_feats_extractor_choices.get_class(
                 args.score_feats_extract
@@ -367,14 +389,22 @@
                     "reduction_factor", None
                 ) == args.svs_conf.get("reduction_factor", 1)
             else:
                 args.pitch_extract_conf["reduction_factor"] = args.svs_conf.get(
                     "reduction_factor", 1
                 )
             pitch_extract = pitch_extract_class(**args.pitch_extract_conf)
+        if getattr(args, "ying_extract", None) is not None:
+            ying_extract_class = ying_extractor_choices.get_class(
+                args.ying_extract,
+            )
+
+            ying_extract = ying_extract_class(
+                **args.ying_extract_conf,
+            )
         if getattr(args, "energy_extract", None) is not None:
             if args.energy_extract_conf.get("reduction_factor", None) is not None:
                 assert args.energy_extract_conf.get(
                     "reduction_factor", None
                 ) == args.svs_conf.get("reduction_factor", 1)
             else:
                 args.energy_extract_conf["reduction_factor"] = args.svs_conf.get(
@@ -398,14 +428,15 @@
         # 5. Build model
         model = ESPnetSVSModel(
             text_extract=score_feats_extract,
             feats_extract=feats_extract,
             score_feats_extract=score_feats_extract,
             label_extract=score_feats_extract,
             pitch_extract=pitch_extract,
+            ying_extract=ying_extract,
             duration_extract=score_feats_extract,
             energy_extract=energy_extract,
             normalize=normalize,
             pitch_normalize=pitch_normalize,
             energy_normalize=energy_normalize,
             svs=svs,
             **args.model_conf,
```

### Comparing `espnet-202304/espnet2/tasks/tts.py` & `espnet-202308/espnet2/tasks/tts.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/tasks/uasr.py` & `espnet-202308/espnet2/tasks/uasr.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/text/build_tokenizer.py` & `espnet-202308/espnet2/text/build_tokenizer.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 from pathlib import Path
-from typing import Iterable, Union
+from typing import Dict, Iterable, Union
 
 from typeguard import check_argument_types
 
 from espnet2.text.abs_tokenizer import AbsTokenizer
 from espnet2.text.char_tokenizer import CharTokenizer
 from espnet2.text.hugging_face_tokenizer import HuggingFaceTokenizer
 from espnet2.text.phoneme_tokenizer import PhonemeTokenizer
@@ -17,26 +17,30 @@
     bpemodel: Union[Path, str, Iterable[str]] = None,
     non_linguistic_symbols: Union[Path, str, Iterable[str]] = None,
     remove_non_linguistic_symbols: bool = False,
     space_symbol: str = "<space>",
     delimiter: str = None,
     g2p_type: str = None,
     nonsplit_symbol: Iterable[str] = None,
+    # tokenization encode (text2token) args, e.g. BPE dropout, only applied in training
+    encode_kwargs: Dict = None,
 ) -> AbsTokenizer:
     """A helper function to instantiate Tokenizer"""
     assert check_argument_types()
     if token_type == "bpe":
         if bpemodel is None:
             raise ValueError('bpemodel is required if token_type = "bpe"')
 
         if remove_non_linguistic_symbols:
             raise RuntimeError(
                 "remove_non_linguistic_symbols is not implemented for token_type=bpe"
             )
-        return SentencepiecesTokenizer(bpemodel)
+        if encode_kwargs is None:
+            encode_kwargs = dict()
+        return SentencepiecesTokenizer(bpemodel, encode_kwargs)
 
     if token_type == "hugging_face":
         if bpemodel is None:
             raise ValueError('bpemodel is required if token_type = "hugging_face"')
 
         if remove_non_linguistic_symbols:
             raise RuntimeError(
```

### Comparing `espnet-202304/espnet2/text/char_tokenizer.py` & `espnet-202308/espnet2/text/char_tokenizer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/text/cleaner.py` & `espnet-202308/espnet2/text/cleaner.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/text/hugging_face_tokenizer.py` & `espnet-202308/espnet2/text/hugging_face_tokenizer.py`

 * *Files 4% similar despite different names*

```diff
@@ -42,10 +42,14 @@
 
     def text2tokens(self, line: str) -> List[str]:
         self._build_tokenizer()
         return self.tokenizer.tokenize(line)
 
     def tokens2text(self, tokens: Iterable[str]) -> str:
         self._build_tokenizer()
-        return self.tokenizer.batch_decode(
-            [self.tokenizer.convert_tokens_to_ids(tokens)], skip_special_tokens=True
-        )[0]
+        return (
+            self.tokenizer.batch_decode(
+                [self.tokenizer.convert_tokens_to_ids(tokens)], skip_special_tokens=True
+            )[0]
+            .replace("\n", " ")
+            .strip()
+        )
```

### Comparing `espnet-202304/espnet2/text/korean_cleaner.py` & `espnet-202308/espnet2/text/korean_cleaner.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/text/phoneme_tokenizer.py` & `espnet-202308/espnet2/text/phoneme_tokenizer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/text/sentencepiece_tokenizer.py` & `espnet-202308/espnet2/text/sentencepiece_tokenizer.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,36 +1,37 @@
 from pathlib import Path
-from typing import Iterable, List, Union
+from typing import Dict, Iterable, List, Union
 
 import sentencepiece as spm
 from typeguard import check_argument_types
 
 from espnet2.text.abs_tokenizer import AbsTokenizer
 
 
 class SentencepiecesTokenizer(AbsTokenizer):
-    def __init__(self, model: Union[Path, str]):
+    def __init__(self, model: Union[Path, str], encode_kwargs: Dict = dict()):
         assert check_argument_types()
         self.model = str(model)
         # NOTE(kamo):
         # Don't build SentencePieceProcessor in __init__()
         # because it's not picklable and it may cause following error,
         # "TypeError: can't pickle SwigPyObject objects",
         # when giving it as argument of "multiprocessing.Process()".
         self.sp = None
+        self.encode_kwargs = encode_kwargs
 
     def __repr__(self):
         return f'{self.__class__.__name__}(model="{self.model}")'
 
     def _build_sentence_piece_processor(self):
         # Build SentencePieceProcessor lazily.
         if self.sp is None:
             self.sp = spm.SentencePieceProcessor()
             self.sp.load(self.model)
 
     def text2tokens(self, line: str) -> List[str]:
         self._build_sentence_piece_processor()
-        return self.sp.EncodeAsPieces(line)
+        return self.sp.EncodeAsPieces(line, **self.encode_kwargs)
 
     def tokens2text(self, tokens: Iterable[str]) -> str:
         self._build_sentence_piece_processor()
         return self.sp.DecodePieces(list(tokens))
```

### Comparing `espnet-202304/espnet2/text/token_id_converter.py` & `espnet-202308/espnet2/text/token_id_converter.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/text/whisper_token_id_converter.py` & `espnet-202308/espnet2/text/whisper_token_id_converter.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/text/whisper_tokenizer.py` & `espnet-202308/espnet2/text/whisper_tokenizer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/text/word_tokenizer.py` & `espnet-202308/espnet2/text/word_tokenizer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/torch_utils/add_gradient_noise.py` & `espnet-202308/espnet2/torch_utils/add_gradient_noise.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/torch_utils/device_funcs.py` & `espnet-202308/espnet2/torch_utils/device_funcs.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/torch_utils/forward_adaptor.py` & `espnet-202308/espnet2/torch_utils/forward_adaptor.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/torch_utils/get_layer_from_string.py` & `espnet-202308/espnet2/torch_utils/get_layer_from_string.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/torch_utils/initialize.py` & `espnet-202308/espnet2/torch_utils/initialize.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/torch_utils/load_pretrained_model.py` & `espnet-202308/espnet2/torch_utils/load_pretrained_model.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/torch_utils/model_summary.py` & `espnet-202308/espnet2/torch_utils/model_summary.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/torch_utils/recursive_op.py` & `espnet-202308/espnet2/torch_utils/recursive_op.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/train/abs_espnet_model.py` & `espnet-202308/espnet2/train/abs_espnet_model.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/train/abs_gan_espnet_model.py` & `espnet-202308/espnet2/train/abs_gan_espnet_model.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/train/class_choices.py` & `espnet-202308/espnet2/train/class_choices.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/train/collate_fn.py` & `espnet-202308/espnet2/train/collate_fn.py`

 * *Files 12% similar despite different names*

```diff
@@ -47,28 +47,34 @@
         float_pad_value: Union[float, int] = 0.0,
         int_pad_value: int = -32768,
         label_downsampling: int = 1,
         pad: bool = False,
         rand_crop: bool = True,
         crop_audio: bool = True,
         not_sequence: Collection[str] = (),
+        window_size: float = 25,
+        window_shift: float = 20,
+        sample_rate: float = 16,
     ):
         assert check_argument_types()
         super().__init__(
             float_pad_value=float_pad_value,
             int_pad_value=int_pad_value,
             not_sequence=not_sequence,
         )
         self.float_pad_value = float_pad_value
         self.int_pad_value = int_pad_value
         self.label_downsampling = label_downsampling
         self.pad = pad
         self.rand_crop = rand_crop
         self.crop_audio = crop_audio
         self.not_sequence = set(not_sequence)
+        self.window_size = window_size
+        self.window_shift = window_shift
+        self.sample_rate = sample_rate
 
     def __repr__(self):
         return (
             f"{self.__class__}(float_pad_value={self.float_pad_value}, "
             f"int_pad_value={self.float_pad_value}, "
             f"label_downsampling={self.label_downsampling}, "
             f"pad_value={self.pad}, rand_crop={self.rand_crop}) "
@@ -92,15 +98,22 @@
             # The MFCC feature is 10ms per frame, while the HuBERT's transformer output
             # is 20ms per frame. Downsample the KMeans label if it's generated by MFCC
             # features.
             if self.label_downsampling > 1:
                 label = label[:: self.label_downsampling]
             if self.crop_audio:
                 waveform, label, length = _crop_audio_label(
-                    waveform, label, length, num_frames, self.rand_crop
+                    waveform,
+                    label,
+                    length,
+                    num_frames,
+                    self.rand_crop,
+                    self.window_size,
+                    self.window_shift,
+                    self.sample_rate,
                 )
             new_data.append((uid, dict(speech=waveform, text=label)))
 
         return common_collate_fn(
             new_data,
             float_pad_value=self.float_pad_value,
             int_pad_value=self.int_pad_value,
@@ -110,48 +123,57 @@
 
 def _crop_audio_label(
     waveform: torch.Tensor,
     label: torch.Tensor,
     length: torch.Tensor,
     num_frames: int,
     rand_crop: bool,
+    window_size: int = 25,
+    window_shift: int = 20,
+    sample_rate: int = 16,
 ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
     """Collate the audio and label at the same time.
 
     Args:
         waveform (Tensor): The waveform Tensor with dimensions `(time)`.
         label (Tensor): The label Tensor with dimensions `(seq)`.
         length (Tensor): The length Tensor with dimension `(1,)`.
         num_frames (int): The final length of the waveform.
         rand_crop (bool): if ``rand_crop`` is True, the starting index of the
             waveform and label is random if the length is longer than the minimum
             length in the mini-batch.
+        window_size (int): reception field of conv feature extractor (in ms).
+            In default, calculated by [400 (samples) / 16 (sample_rate)].
+        window_shift (int): the stride of conv feature extractor (in ms).
+            In default, calculated by [320 (samples) / 16 (sample_rate)].
+        sample_rate (int): number of samples in audio signal per millisecond.
 
     Returns:
         (Tuple(Tensor, Tensor, Tensor)): Returns the Tensors for the waveform,
             label, and the waveform length.
 
     """
 
-    kernel_size = 25
-    stride = 20
-    sample_rate = 16  # 16 per millisecond
     frame_offset = 0
     if waveform.size > num_frames and rand_crop:
         diff = waveform.size - num_frames
         frame_offset = torch.randint(diff, size=(1,))
     elif waveform.size < num_frames:
         num_frames = waveform.size
     label_offset = max(
-        math.floor((frame_offset - kernel_size * sample_rate) / (stride * sample_rate))
+        math.floor(
+            (frame_offset - window_size * sample_rate) / (window_shift * sample_rate)
+        )
         + 1,
         0,
     )
     num_label = (
-        math.floor((num_frames - kernel_size * sample_rate) / (stride * sample_rate))
+        math.floor(
+            (num_frames - window_size * sample_rate) / (window_shift * sample_rate)
+        )
         + 1
     )
     waveform = waveform[frame_offset : frame_offset + num_frames]
     label = label[label_offset : label_offset + num_label]
     length = num_frames
 
     return waveform, label, length
```

### Comparing `espnet-202304/espnet2/train/dataset.py` & `espnet-202308/espnet2/train/dataset.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/train/distributed_utils.py` & `espnet-202308/espnet2/train/distributed_utils.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/train/gan_trainer.py` & `espnet-202308/espnet2/train/gan_trainer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/train/iterable_dataset.py` & `espnet-202308/espnet2/train/iterable_dataset.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/train/preprocessor.py` & `espnet-202308/espnet2/train/preprocessor.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,22 +1,23 @@
 import json
 import logging
 import random
 import re
 from abc import ABC, abstractmethod
 from pathlib import Path
-from typing import Collection, Dict, Iterable, List, Union
+from typing import Collection, Dict, Iterable, List, Optional, Tuple, Union
 
 import numpy as np
 import scipy.signal
 import soundfile
 from typeguard import check_argument_types, check_return_type
 
 from espnet2.text.build_tokenizer import build_tokenizer
 from espnet2.text.cleaner import TextCleaner
+from espnet2.text.hugging_face_token_id_converter import HuggingFaceTokenIDConverter
 from espnet2.text.token_id_converter import TokenIDConverter
 from espnet2.text.whisper_token_id_converter import OpenAIWhisperTokenIDConverter
 
 
 class AbsPreprocessor(ABC):
     def __init__(self, train: bool):
         self.train = train
@@ -169,15 +170,19 @@
                 bpemodel=bpemodel,
                 delimiter=delimiter,
                 space_symbol=space_symbol,
                 non_linguistic_symbols=non_linguistic_symbols,
                 g2p_type=g2p_type,
                 nonsplit_symbol=nonsplit_symbol,
             )
-            if bpemodel not in ["whisper_en", "whisper_multilingual"]:
+            if token_type == "hugging_face":
+                self.token_id_converter = HuggingFaceTokenIDConverter(
+                    model_name_or_path=bpemodel
+                )
+            elif bpemodel not in ["whisper_en", "whisper_multilingual"]:
                 self.token_id_converter = TokenIDConverter(
                     token_list=token_list,
                     unk_symbol=unk_symbol,
                 )
             else:
                 self.token_id_converter = OpenAIWhisperTokenIDConverter(
                     model_type=bpemodel
@@ -216,16 +221,16 @@
             else:
                 raise ValueError(
                     "Format error: '{noise_db_range}' e.g. -3_4 -> [-3db,4db]"
                 )
         else:
             self.noises = None
 
-    def _convolve_rir(self, speech, power):
-        rir_path = np.random.choice(self.rirs)
+    def _convolve_rir(self, speech, power, rirs):
+        rir_path = np.random.choice(rirs)
         rir = None
         if rir_path is not None:
             rir, _ = soundfile.read(rir_path, dtype=np.float64, always_2d=True)
 
             # rir: (Nmic, Time)
             rir = rir.T
 
@@ -235,20 +240,20 @@
                 :, : speech.shape[1]
             ]
             # Reverse mean power to the original power
             power2 = (speech[detect_non_silence(speech)] ** 2).mean()
             speech = np.sqrt(power / max(power2, 1e-10)) * speech
         return speech, rir
 
-    def _add_noise(self, speech, power):
+    def _add_noise(self, speech, power, noises, noise_db_low, noise_db_high):
         nsamples = speech.shape[1]
-        noise_path = np.random.choice(self.noises)
+        noise_path = np.random.choice(noises)
         noise = None
         if noise_path is not None:
-            noise_db = np.random.uniform(self.noise_db_low, self.noise_db_high)
+            noise_db = np.random.uniform(noise_db_low, noise_db_high)
             with soundfile.SoundFile(noise_path) as f:
                 if f.frames == nsamples:
                     noise = f.read(dtype=np.float64, always_2d=True)
                 elif f.frames < nsamples:
                     if f.frames / nsamples < self.short_noise_thres:
                         logging.warning(
                             f"Noise ({f.frames}) is much shorter than "
@@ -296,22 +301,28 @@
                 else:
                     speech = speech.T
                 # Calc power on non silence region
                 power = (speech[detect_non_silence(speech)] ** 2).mean()
 
                 # 1. Convolve RIR
                 if self.rirs is not None and self.rir_apply_prob >= np.random.random():
-                    speech, _ = self._convolve_rir(speech, power)
+                    speech, _ = self._convolve_rir(speech, power, self.rirs)
 
                 # 2. Add Noise
                 if (
                     self.noises is not None
                     and self.noise_apply_prob >= np.random.random()
                 ):
-                    speech, _ = self._add_noise(speech, power)
+                    speech, _ = self._add_noise(
+                        speech,
+                        power,
+                        self.noises,
+                        self.noise_db_low,
+                        self.noise_db_high,
+                    )
 
                 speech = speech.T
                 ma = np.max(np.abs(speech))
                 if ma > 1.0:
                     speech /= ma
                 data[self.speech_name] = speech
 
@@ -328,14 +339,20 @@
         if self.text_name in data and self.tokenizer is not None:
             text = data[self.text_name]
             if isinstance(text, np.ndarray):
                 return data
             text = self.text_cleaner(text)
             tokens = self.tokenizer.text2tokens(text)
             text_ints = self.token_id_converter.tokens2ids(tokens)
+            if len(text_ints) > 500:
+                logging.warning(
+                    "The length of the text output exceeds 500, "
+                    "which may cause OOM on the GPU."
+                    "Please ensure that the data processing is correct and verify it."
+                )
             data[self.text_name] = np.array(text_ints, dtype=np.int64)
         if self.aux_task_names is not None and self.tokenizer is not None:
             for name in self.aux_task_names:
                 if name in data:
                     text = data[name]
                     text = self.text_cleaner(text)
                     tokens = self.tokenizer.text2tokens(text)
@@ -545,14 +562,15 @@
         noise_scp: str = None,
         noise_apply_prob: float = 1.0,
         noise_db_range: str = "3_10",
         short_noise_thres: float = 0.5,
         speech_volume_normalize: float = None,
         speech_name: str = "speech",
         text_name: List[str] = ["text"],
+        tokenizer_encode_conf: List[Dict] = [dict(), dict()],
     ):
         # TODO(jiatong): sync with Kamo and Jing on interface for preprocessor
         super().__init__(
             train=train,
             token_type=token_type[0],
             token_list=token_list[0],
             bpemodel=bpemodel[0],
@@ -589,14 +607,17 @@
                     build_tokenizer(
                         token_type=token_type[i],
                         bpemodel=bpemodel[i],
                         delimiter=delimiter,
                         space_symbol=space_symbol,
                         non_linguistic_symbols=non_linguistic_symbols,
                         g2p_type=g2p_type,
+                        encode_kwargs=tokenizer_encode_conf[i]
+                        if i < len(tokenizer_encode_conf)
+                        else None,
                     )
                 )
                 self.token_id_converter.append(
                     TokenIDConverter(
                         token_list=token_list[i],
                         unk_symbol=unk_symbol,
                     )
@@ -630,14 +651,15 @@
         source_scp: str = None,
         ref_num: int = 2,
         dynamic_mixing_gain_db: float = 0.0,
         speech_name: str = "speech_mix",
         speech_ref_name_prefix: str = "speech_ref",
         mixture_source_name: str = None,
         utt2spk: str = None,
+        categories: Optional[List] = None,
     ):
         super().__init__(train)
         self.source_scp = source_scp
         self.ref_num = ref_num
         self.dynamic_mixing_gain_db = dynamic_mixing_gain_db
         self.speech_name = speech_name
         self.speech_ref_name_prefix = speech_ref_name_prefix
@@ -670,14 +692,23 @@
                     self.utt2spk[sps[0]] = sps[1]
 
             for key in self.sources.keys():
                 assert key in self.utt2spk
 
         self.source_keys = list(self.sources.keys())
 
+        # Map each category into a unique integer
+        self.categories = {}
+        if categories:
+            count = 0
+            for c in categories:
+                if c not in self.categories:
+                    self.categories[c] = count
+                    count += 1
+
     def _pick_source_utterances_(self, uid):
         # return (ref_num - 1) uid of reference sources.
 
         source_keys = [uid]
 
         spk_ids = [self.utt2spk[uid]]
 
@@ -750,14 +781,25 @@
         self, uid: str, data: Dict[str, Union[str, np.ndarray]]
     ) -> Dict[str, np.ndarray]:
         # TODO(Chenda): need to test for multi-channel data.
         assert (
             len(data[self.mixture_source_name].shape) == 1
         ), "Multi-channel input has not been tested"
 
+        # Add the category information (an integer) to `data`
+        if not self.categories and "category" in data:
+            raise ValueError(
+                "categories must be set in the config file when utt2category files "
+                "exist in the data directory (e.g., dump/raw/*/utt2category)"
+            )
+        if self.categories and "category" in data:
+            category = data.pop("category")
+            assert category in self.categories, category
+            data["utt2category"] = np.array([self.categories[category]])
+
         if self.train:
             data = self._mix_speech_(uid, data)
 
         assert check_return_type(data)
         return data
 
 
@@ -779,14 +821,16 @@
         noise_ref_name_prefix: str = "noise_ref",
         dereverb_ref_name_prefix: str = "dereverb_ref",
         use_reverberant_ref: bool = False,
         num_spk: int = 1,
         num_noise_type: int = 1,
         sample_rate: int = 8000,
         force_single_channel: bool = False,
+        channel_reordering: bool = False,
+        categories: Optional[List] = None,
     ):
         super().__init__(
             train=train,
             token_type=None,
             token_list=None,
             bpemodel=None,
             text_cleaner=None,
@@ -807,28 +851,74 @@
         self.speech_ref_name_prefix = speech_ref_name_prefix
         self.noise_ref_name_prefix = noise_ref_name_prefix
         self.dereverb_ref_name_prefix = dereverb_ref_name_prefix
         self.use_reverberant_ref = use_reverberant_ref
         self.num_spk = num_spk
         self.num_noise_type = num_noise_type
         self.sample_rate = sample_rate
+        self.rir_scp = rir_scp
+        self.noise_scp = noise_scp
+        self.noise_db_range = noise_db_range
+        # Whether to always convert the signals to single-channel
         self.force_single_channel = force_single_channel
+        # If True, randomly reorder the channels of the multi-channel signals
+        self.channel_reordering = channel_reordering
+
+        # Map each category into a unique integer
+        self.categories = {}
+        if categories:
+            count = 0
+            for c in categories:
+                if c not in self.categories:
+                    self.categories[c] = count
+                    count += 1
 
         if self.speech_volume_normalize is not None:
             sps = speech_volume_normalize.split("_")
             if len(sps) == 1:
                 self.volume_low, self.volume_high = float(sps[0])
             elif len(sps) == 2:
                 self.volume_low, self.volume_high = float(sps[0]), float(sps[1])
             else:
                 raise ValueError(
                     "Format error for --speech_volume_normalize: "
                     f"'{speech_volume_normalize}'"
                 )
 
+    def __basic_str__(self):
+        msg = f", num_spk={self.num_spk}"
+        for key in (
+            "force_single_channel",
+            "channel_reordering",
+            "speech_volume_normalize",
+        ):
+            if getattr(self, key):
+                msg += f", {key}={getattr(self, key)}"
+        if self.rirs is not None and self.rir_apply_prob > 0:
+            msg += f", sample_rate={self.sample_rate}"
+            msg += f", rir_scp={self.rir_scp}, rir_apply_prob={self.rir_apply_prob}"
+            if self.use_reverberant_ref:
+                msg += f", use_reverberant_ref={self.use_reverberant_ref}"
+        if self.noises is not None and self.noise_apply_prob > 0:
+            msg += f", noise_scp={self.noise_scp}"
+            msg += f", noise_apply_prob={self.noise_apply_prob}"
+            msg += f", noise_db_range={self.noise_db_range}"
+        if self.categories:
+            if len(self.categories) <= 10:
+                msg += f", categories={self.categories}"
+            else:
+                msg += f", num_category={len(self.categories)}"
+        return msg
+
+    def __repr__(self):
+        name = self.__class__.__module__ + "." + self.__class__.__name__
+        msg = f"{name}(train={self.train}"
+        msg += self.__basic_str__()
+        return msg + ")"
+
     def _ensure_2d(self, signal):
         if isinstance(signal, tuple):
             return tuple(self._ensure_2d(sig) for sig in signal)
         elif isinstance(signal, list):
             return [self._ensure_2d(sig) for sig in signal]
         else:
             # (Nmic, Time)
@@ -861,22 +951,46 @@
                 data_dict[speech_ref_name] = func(data_dict[speech_ref_name])
 
             dereverb_ref_name = self.dereverb_ref_name_prefix + str(spk + 1)
             if dereverb_ref_name in data_dict:
                 data_dict[dereverb_ref_name] = func(data_dict[dereverb_ref_name])
 
     def _speech_process(
-        self, data: Dict[str, Union[str, np.ndarray]]
+        self, uid: str, data: Dict[str, Union[str, np.ndarray]]
     ) -> Dict[str, Union[str, np.ndarray]]:
         assert check_argument_types()
 
         if self.speech_name not in data:
             assert check_return_type(data)
             return data
 
+        speech_mix = data[self.speech_name]
+        # Reorder channels of the multi-channel signals
+        if speech_mix.ndim > 1 and self.channel_reordering and self.train:
+            num_ch = speech_mix.shape[-1]
+            # chs = np.random.choice(range(num_ch), size=num_ch, replace=False).tolist()
+            chs = np.random.permutation(num_ch).tolist()
+            data[self.speech_name] = speech_mix[..., chs]
+            for i in range(self.num_spk):
+                k = self.speech_ref_name_prefix + str(i + 1)
+                if data[k].ndim > 1:
+                    assert data[k].shape == speech_mix.shape
+                    data[k] = data[k][..., chs]
+
+        # Add the category information (an integer) to `data`
+        if not self.categories and "category" in data:
+            raise ValueError(
+                "categories must be set in the config file when utt2category files "
+                "exist in the data directory (e.g., dump/raw/*/utt2category)"
+            )
+        if self.categories and "category" in data:
+            category = data.pop("category")
+            assert category in self.categories, category
+            data["utt2category"] = np.array([self.categories[category]])
+
         if self.train:
             # clean speech signal (Nmic, Time)
             speech_ref = [
                 self._ensure_2d(data[self.speech_ref_name_prefix + str(i + 1)])
                 for i in range(self.num_spk)
             ]
 
@@ -909,15 +1023,15 @@
                 else:
                     np.testing.assert_allclose(
                         np.squeeze(sum(speech_ref)), np.squeeze(speech_mix)
                     )
 
                 speech_ref, rir_ref = zip(
                     *[
-                        self._convolve_rir(sp, power)
+                        self._convolve_rir(sp, power, self.rirs)
                         for sp, power in zip(speech_ref, power_ref)
                     ]
                 )
                 if self.force_single_channel:
                     speech_ref = list(
                         map(lambda x: x if x.shape[0] == 1 else x[:1], speech_ref)
                     )
@@ -958,15 +1072,21 @@
                     speech_mix = sum(speech_ref)
 
             # 2. Add Noise
             if self.noises is not None and self.noise_apply_prob >= np.random.random():
                 if self.noise_ref_name_prefix + "1" in data:
                     speech_mix -= data[self.noise_ref_name_prefix + "1"]
                 power_mix = (speech_mix[detect_non_silence(speech_mix)] ** 2).mean()
-                speech_mix, noise = self._add_noise(speech_mix, power_mix)
+                speech_mix, noise = self._add_noise(
+                    speech_mix,
+                    power_mix,
+                    self.noises,
+                    self.noise_db_low,
+                    self.noise_db_high,
+                )
                 if self.force_single_channel:
                     if speech_mix.shape[0] > 1:
                         speech_mix = speech_mix[:1]
                     if noise.shape[0] > 1:
                         noise = noise[:1]
 
                 for n in range(1, self.num_noise_type):
@@ -994,14 +1114,23 @@
             speech_mix = data[self.speech_name]
             ma = np.max(np.abs(speech_mix))
             self._apply_to_all_signals(data, lambda x: x * volume_scale / ma)
 
         assert check_return_type(data)
         return data
 
+    def __call__(
+        self, uid: str, data: Dict[str, Union[str, np.ndarray]]
+    ) -> Dict[str, np.ndarray]:
+        assert check_argument_types()
+
+        data = self._speech_process(uid, data)
+        data = self._text_process(data)
+        return data
+
 
 class SVSPreprocessor(AbsPreprocessor):
     """Preprocessor for Sing Voice Sythesis (SVS) task."""
 
     def __init__(
         self,
         train: bool,
@@ -1129,21 +1258,21 @@
                         slur[index_lab] = 0
                     index_lab += 1
 
             assert index_lab == lab_len
             data.pop(self.midi_name)
 
             phn_cnt = np.array(phn_cnt)
-            label.astype(np.int64)
-            midi.astype(np.int64)
-            duration_phn.astype(np.int64)
-            duration_syb.astype(np.int64)
-            duration_ruled_phn.astype(np.int64)
-            phn_cnt.astype(np.int64)
-            slur.astype(np.int64)
+            label = label.astype(np.int64)
+            midi = midi.astype(np.int64)
+            duration_phn = duration_phn.astype(np.int64)
+            duration_syb = duration_syb.astype(np.int64)
+            duration_ruled_phn = duration_ruled_phn.astype(np.int64)
+            phn_cnt = phn_cnt.astype(np.int64)
+            slur = slur.astype(np.int64)
 
             data["label"] = label
             data["midi"] = midi
             data["duration_phn"] = duration_phn
             data["duration_ruled_phn"] = duration_ruled_phn
             data["duration_syb"] = duration_syb
             data["phn_cnt"] = phn_cnt
@@ -1188,14 +1317,16 @@
         noise_ref_name_prefix: str = "noise_ref",
         dereverb_ref_name_prefix: str = "dereverb_ref",
         use_reverberant_ref: bool = False,
         num_spk: int = 1,
         num_noise_type: int = 1,
         sample_rate: int = 8000,
         force_single_channel: bool = False,
+        channel_reordering: bool = False,
+        categories: Optional[List] = None,
     ):
         super().__init__(
             train,
             rir_scp=rir_scp,
             rir_apply_prob=rir_apply_prob,
             noise_scp=noise_scp,
             noise_apply_prob=noise_apply_prob,
@@ -1207,14 +1338,16 @@
             noise_ref_name_prefix=noise_ref_name_prefix,
             dereverb_ref_name_prefix=dereverb_ref_name_prefix,
             use_reverberant_ref=use_reverberant_ref,
             num_spk=num_spk,
             num_noise_type=num_noise_type,
             sample_rate=sample_rate,
             force_single_channel=force_single_channel,
+            channel_reordering=channel_reordering,
+            categories=categories,
         )
         # If specified, the enrollment will be chomped to the specified length
         self.enroll_segment = enroll_segment
         # If True, the speaker embedding will be loaded instead of enrollment audios
         self.load_spk_embedding = load_spk_embedding
         # If False, only one of the speakers in each mixture sample will be loaded
         self.load_all_speakers = load_all_speakers
@@ -1233,14 +1366,25 @@
                 logging.info("Using dynamically sampled enrollment for each sample")
                 with open(train_spk2enroll, "r", encoding="utf-8") as f:
                     # {spkID: [(uid1, path1), (uid2, path2), ...]}
                     self.train_spk2enroll = json.load(f)
         else:
             self.train_spk2enroll = None
 
+    def __repr__(self):
+        name = self.__class__.__module__ + "." + self.__class__.__name__
+        msg = f"{name}(train={self.train}"
+        if self.train_spk2enroll:
+            msg += f", len(train_spk2enroll)={len(self.train_spk2enroll)}"
+        for key in ("enroll_segment", "load_spk_embedding", "load_all_speakers"):
+            if getattr(self, key):
+                msg += f", {key}={getattr(self, key)}"
+        msg += self.__basic_str__()
+        return msg + ")"
+
     def _read_audio_segment(self, path, seg_len=None):
         with soundfile.SoundFile(path) as f:
             if seg_len is None or f.frames == seg_len:
                 audio = f.read(dtype=np.float32, always_2d=True)
             elif f.frames < seg_len:
                 offset = np.random.randint(0, seg_len - f.frames)
                 # audio: (Time, Nmic)
@@ -1327,10 +1471,282 @@
         return data
 
     def __call__(
         self, uid: str, data: Dict[str, Union[str, np.ndarray]]
     ) -> Dict[str, np.ndarray]:
         assert check_argument_types()
 
-        data = super()._speech_process(data)
+        data = super()._speech_process(uid, data)
         data = self._speech_process(uid, data)
         return data
+
+
+class SpkPreprocessor(CommonPreprocessor):
+    """Preprocessor for Speaker tasks.
+
+    Args:
+        train (bool): Whether to use in training mode.
+        spk2utt (str): Path to the `spk2utt` file.
+        target_duration (float): Target duration in seconds.
+        sample_rate (int): Sampling rate.
+        num_eval (int): Number of utterances to be used for evaluation.
+        rir_scp (str): Path to the RIR scp file.
+        rir_apply_prob (float): Probability of applying RIR.
+        noise_info (List[Tuple[float, str, Tuple[int, int], Tuple[float, float]]]):
+            List of tuples of noise information. Each tuple represents a noise type.
+            Each tuple consists of `(prob, noise_scp, num_to_mix, db_range)`.
+                - `prob` (float) is the probability of applying the noise type.
+                - `noise_scp` (str) is the path to the noise scp file.
+                - `num_to_mix` (Tuple[int, int]) is the range of the number of noises
+                    to be mixed.
+                - `db_range` (Tuple[float, float]) is the range of noise levels in dB.
+        noise_apply_prob (float): Probability of applying noise.
+        short_noise_thres (float): Threshold of short noise.
+    """
+
+    def __init__(
+        self,
+        train: bool,
+        spk2utt: str,
+        target_duration: float,  # in seconds
+        sample_rate: int = 16000,
+        num_eval: int = 10,
+        rir_scp: str = None,
+        rir_apply_prob: float = 1.0,
+        noise_info: List[
+            Tuple[float, str, Tuple[int, int], Tuple[float, float]]
+        ] = None,
+        noise_apply_prob: float = 1.0,
+        short_noise_thres: float = 0.5,
+    ):
+        super().__init__(train, rir_scp=rir_scp, rir_apply_prob=rir_apply_prob)
+        with open(spk2utt, "r") as f_s2u:
+            self.spk2utt = f_s2u.readlines()
+
+        self.nspk = len(self.spk2utt)
+        self.spk2label = None  # a dictionary that maps string speaker label to int
+        self.sample_rate = sample_rate
+        self.target_duration = int(target_duration * sample_rate)
+        self.num_eval = num_eval
+        self._make_label_mapping()
+
+        self.rir_scp = rir_scp
+
+        self.noise_apply_prob = noise_apply_prob
+        self.short_noise_thres = short_noise_thres
+        self.noises = []
+        self.noise_probs = []
+        self.noise_db_ranges = []
+        self.noise_num_to_mix = []
+        if noise_apply_prob > 0:
+            for prob, noise_scp, num_to_mix, db_range in noise_info:
+                if prob > 0:
+                    assert len(db_range) == 2, db_range
+                    assert db_range[0] <= db_range[1], db_range
+                    assert len(num_to_mix) == 2, num_to_mix
+                    assert num_to_mix[0] <= num_to_mix[1], num_to_mix
+                    self.noise_probs.append(prob)
+                    self.noise_db_ranges.append(tuple(db_range))
+                    self.noise_num_to_mix.append(num_to_mix)
+                    noises = []
+                    with open(noise_scp, "r", encoding="utf-8") as f:
+                        for line in f:
+                            sps = line.strip().split(None, 1)
+                            if len(sps) == 1:
+                                noises.append(sps[0])
+                            else:
+                                noises.append(sps[1])
+                    self.noises.append(noises)
+
+    def __repr__(self):
+        name = self.__class__.__module__ + "." + self.__class__.__name__
+        msg = f"{name}(train={self.train}"
+        if self.spk2label:
+            msg += f", len(spk2label)={len(self.spk2label)}"
+        for key in ("target_duration", "sample_rate", "num_eval"):
+            if getattr(self, key):
+                msg += f", {key}={getattr(self, key)}"
+        if self.rirs is not None and self.rir_apply_prob > 0:
+            msg += f", rir_scp={self.rir_scp}, rir_apply_prob={self.rir_apply_prob}"
+        if self.noise_apply_prob > 0 and self.noises:
+            msg += f", noise_apply_prob={self.noise_apply_prob}"
+            msg += f", noises.shapes={[len(n) for n in self.noises]}"
+            msg += f", noise_probs={self.noise_probs}"
+            msg += f", noise_db_ranges={self.noise_db_ranges}"
+            msg += f", noise_num_to_mix={self.noise_num_to_mix}"
+        return msg + ")"
+
+    def _make_label_mapping(self):
+        label_idx = 0
+        self.spk2label = {}
+        for spk in self.spk2utt:
+            spk = spk.strip().split(" ")[0]
+            self.spk2label[spk] = label_idx
+            label_idx += 1
+
+    def _speech_process(self, data: Dict[np.ndarray, str]):
+        if self.train:
+            audio = data["speech"]
+
+            # duplicate if utt is shorter than minimum required duration
+            if len(audio) < self.target_duration:
+                shortage = self.target_duration - len(audio) + 1
+                audio = np.pad(audio, (0, shortage), "wrap")
+
+            startframe = np.array(
+                [np.int64(random.random() * (len(audio) - self.target_duration))]
+            )
+
+            data["speech"] = audio[
+                int(startframe) : int(startframe) + self.target_duration
+            ]
+
+            if self.noise_apply_prob > 0 or self.rir_apply_prob > 0:
+                data["speech"] = self._apply_data_augmentation(data["speech"])
+        else:
+            audio = data["speech"]
+            audio2 = data["speech2"]
+
+            # duplicate if utt is shorter than minimum required duration
+            if len(audio) < self.target_duration:
+                shortage = self.target_duration - len(audio) + 1
+                audio = np.pad(audio, (0, shortage), "wrap")
+            if len(audio2) < self.target_duration:
+                shortage = self.target_duration - len(audio2) + 1
+                audio2 = np.pad(audio2, (0, shortage), "wrap")
+
+            startframe = np.linspace(
+                0, len(audio) - self.target_duration, num=self.num_eval
+            )
+            audios = []
+            for frame in startframe:
+                audios.append(audio[int(frame) : int(frame) + self.target_duration])
+            audios = np.stack(audios, axis=0)
+
+            startframe2 = np.linspace(
+                0, len(audio2) - self.target_duration, num=self.num_eval
+            )
+            audios2 = []
+            for frame in startframe2:
+                audios2.append(audio2[int(frame) : int(frame) + self.target_duration])
+            audios2 = np.stack(audios2, axis=0)
+
+            data["speech"] = audios
+            data["speech2"] = audios2
+
+        return data
+
+    def _convolve_rir(self, speech, rirs):
+        rir_path = np.random.choice(rirs)
+        rir = None
+        if rir_path is not None:
+            rir, _ = soundfile.read(rir_path, dtype=np.float64, always_2d=True)
+
+            # rir: (Nmic, Time)
+            rir = rir.T
+
+            # normalize rir
+            rir = rir / np.sqrt(np.sum(rir**2))
+
+            # speech: (Nmic, Time)
+            # Note that this operation doesn't change the signal length
+            speech = scipy.signal.convolve(speech, rir, mode="full")[
+                :, : speech.shape[1]
+            ]
+        return speech, rir
+
+    def _load_noise(self, speech, speech_db, noises, noise_db_low, noise_db_high):
+        nsamples = speech.shape[1]
+        noise_path = np.random.choice(noises)
+        noise = None
+        if noise_path is not None:
+            noise_snr = np.random.uniform(noise_db_low, noise_db_high)
+            with soundfile.SoundFile(noise_path) as f:
+                if f.frames == nsamples:
+                    noise = f.read(dtype=np.float64)
+                elif f.frames < nsamples:
+                    # noise: (Time,)
+                    noise = f.read(dtype=np.float64)
+                    # Repeat noise
+                    noise = np.pad(
+                        noise,
+                        (0, nsamples - f.frames),
+                        mode="wrap",
+                    )
+                else:
+                    offset = np.random.randint(0, f.frames - nsamples)
+                    f.seek(offset)
+                    # noise: (Time,)
+                    noise = f.read(nsamples, dtype=np.float64)
+                    if len(noise) != nsamples:
+                        raise RuntimeError(f"Something wrong: {noise_path}")
+            # noise: (Nmic, Time)
+            noise = noise[None, :]
+
+            noise_power = np.mean(noise**2)
+            noise_db = 10 * np.log10(noise_power + 1e-4)
+            scale = np.sqrt(10 ** ((speech_db - noise_db - noise_snr) / 10))
+
+            noise = noise * scale
+        return noise
+
+    def _apply_data_augmentation(self, speech):
+        # speech: (Nmic, Time)
+        if speech.ndim == 1:
+            speech = speech[None, :]
+        else:
+            speech = speech.T
+
+        if self.rirs is not None and self.rir_apply_prob >= np.random.random():
+            speech, _ = self._convolve_rir(speech, self.rirs)
+
+        if self.noises and self.noise_apply_prob >= np.random.random():
+            idx = random.choices(
+                range(len(self.noises)), weights=self.noise_probs, k=1
+            )[0]
+            low, high = self.noise_num_to_mix[idx]
+            if low == high:
+                num_to_mix = low
+            else:
+                num_to_mix = np.random.randint(low, high + 1)
+
+            # add eps of 1e-4 to avoid negative value before log
+            speech_db = 10 * np.log10(np.mean(speech**2) + 1e-4)
+            noiselist = []
+            for _ in range(num_to_mix):
+                noise = self._load_noise(
+                    speech,  # original speech
+                    speech_db,  # db of speech
+                    self.noises[idx],  # a list of a type of noise
+                    self.noise_db_ranges[idx][0],  # min db
+                    self.noise_db_ranges[idx][1],  # max db
+                )
+                noiselist.append(noise)
+            noise = np.sum(np.concatenate(noiselist, axis=0), axis=0, keepdims=True)
+            speech = speech + noise
+
+        speech = np.squeeze(speech, axis=0)
+        return speech
+
+    def _text_process(
+        self, data: Dict[str, Union[str, np.ndarray]]
+    ) -> Dict[str, np.ndarray]:
+        """
+        Make speaker labels into integers
+        """
+        if self.train:
+            int_label = self.spk2label[data["spk_labels"]]
+            data["spk_labels"] = np.asarray([int_label], dtype=np.int64)
+        else:
+            data["spk_labels"] = np.asarray([int(data["spk_labels"])])
+
+        return data
+
+    def __call__(
+        self, uid: str, data: Dict[str, Union[str, np.ndarray]]
+    ) -> Dict[str, np.ndarray]:
+        assert check_argument_types()
+
+        data = self._text_process(data)
+        data = self._speech_process(data)
+
+        return data
```

### Comparing `espnet-202304/espnet2/train/reporter.py` & `espnet-202308/espnet2/train/reporter.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/train/trainer.py` & `espnet-202308/espnet2/train/trainer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/train/uasr_trainer.py` & `espnet-202308/espnet2/train/uasr_trainer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/tts/abs_tts.py` & `espnet-202308/espnet2/tts/abs_tts.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/tts/espnet_model.py` & `espnet-202308/espnet2/tts/espnet_model.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/tts/fastspeech/fastspeech.py` & `espnet-202308/espnet2/tts/fastspeech2/fastspeech2.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,50 +1,51 @@
 # Copyright 2020 Nagoya University (Tomoki Hayashi)
 #  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)
 
-"""Fastspeech related modules for ESPnet2."""
+"""Fastspeech2 related modules for ESPnet2."""
 
 import logging
 from typing import Dict, Optional, Sequence, Tuple
 
 import torch
 import torch.nn.functional as F
 from typeguard import check_argument_types
 
 from espnet2.torch_utils.device_funcs import force_gatherable
 from espnet2.torch_utils.initialize import initialize
 from espnet2.tts.abs_tts import AbsTTS
+from espnet2.tts.fastspeech2.loss import FastSpeech2Loss
+from espnet2.tts.fastspeech2.variance_predictor import VariancePredictor
 from espnet2.tts.gst.style_encoder import StyleEncoder
 from espnet.nets.pytorch_backend.conformer.encoder import Encoder as ConformerEncoder
-from espnet.nets.pytorch_backend.e2e_tts_fastspeech import (
-    FeedForwardTransformerLoss as FastSpeechLoss,
-)
 from espnet.nets.pytorch_backend.fastspeech.duration_predictor import DurationPredictor
 from espnet.nets.pytorch_backend.fastspeech.length_regulator import LengthRegulator
 from espnet.nets.pytorch_backend.nets_utils import make_non_pad_mask, make_pad_mask
 from espnet.nets.pytorch_backend.tacotron2.decoder import Postnet
 from espnet.nets.pytorch_backend.transformer.embedding import (
     PositionalEncoding,
     ScaledPositionalEncoding,
 )
 from espnet.nets.pytorch_backend.transformer.encoder import (
     Encoder as TransformerEncoder,
 )
 
 
-class FastSpeech(AbsTTS):
-    """FastSpeech module for end-to-end text-to-speech.
-
-    This is a module of FastSpeech, feed-forward Transformer with duration predictor
-    described in `FastSpeech: Fast, Robust and Controllable Text to Speech`_, which
-    does not require any auto-regressive processing during inference, resulting in
-    fast decoding compared with auto-regressive Transformer.
+class FastSpeech2(AbsTTS):
+    """FastSpeech2 module.
 
-    .. _`FastSpeech: Fast, Robust and Controllable Text to Speech`:
-        https://arxiv.org/pdf/1905.09263.pdf
+    This is a module of FastSpeech2 described in `FastSpeech 2: Fast and
+    High-Quality End-to-End Text to Speech`_. Instead of quantized pitch and
+    energy, we use token-averaged value introduced in `FastPitch: Parallel
+    Text-to-speech with Pitch Prediction`_.
+
+    .. _`FastSpeech 2: Fast and High-Quality End-to-End Text to Speech`:
+        https://arxiv.org/abs/2006.04558
+    .. _`FastPitch: Parallel Text-to-speech with Pitch Prediction`:
+        https://arxiv.org/abs/2006.06873
 
     """
 
     def __init__(
         self,
         # network structure related
         idim: int,
@@ -63,18 +64,14 @@
         positionwise_conv_kernel_size: int = 1,
         use_scaled_pos_enc: bool = True,
         use_batch_norm: bool = True,
         encoder_normalize_before: bool = True,
         decoder_normalize_before: bool = True,
         encoder_concat_after: bool = False,
         decoder_concat_after: bool = False,
-        duration_predictor_layers: int = 2,
-        duration_predictor_chans: int = 384,
-        duration_predictor_kernel_size: int = 3,
-        duration_predictor_dropout_rate: float = 0.1,
         reduction_factor: int = 1,
         encoder_type: str = "transformer",
         decoder_type: str = "transformer",
         transformer_enc_dropout_rate: float = 0.1,
         transformer_enc_positional_dropout_rate: float = 0.1,
         transformer_enc_attn_dropout_rate: float = 0.1,
         transformer_dec_dropout_rate: float = 0.1,
@@ -83,17 +80,38 @@
         # only for conformer
         conformer_rel_pos_type: str = "legacy",
         conformer_pos_enc_layer_type: str = "rel_pos",
         conformer_self_attn_layer_type: str = "rel_selfattn",
         conformer_activation_type: str = "swish",
         use_macaron_style_in_conformer: bool = True,
         use_cnn_in_conformer: bool = True,
+        zero_triu: bool = False,
         conformer_enc_kernel_size: int = 7,
         conformer_dec_kernel_size: int = 31,
-        zero_triu: bool = False,
+        # duration predictor
+        duration_predictor_layers: int = 2,
+        duration_predictor_chans: int = 384,
+        duration_predictor_kernel_size: int = 3,
+        duration_predictor_dropout_rate: float = 0.1,
+        # energy predictor
+        energy_predictor_layers: int = 2,
+        energy_predictor_chans: int = 384,
+        energy_predictor_kernel_size: int = 3,
+        energy_predictor_dropout: float = 0.5,
+        energy_embed_kernel_size: int = 9,
+        energy_embed_dropout: float = 0.5,
+        stop_gradient_from_energy_predictor: bool = False,
+        # pitch predictor
+        pitch_predictor_layers: int = 2,
+        pitch_predictor_chans: int = 384,
+        pitch_predictor_kernel_size: int = 3,
+        pitch_predictor_dropout: float = 0.5,
+        pitch_embed_kernel_size: int = 9,
+        pitch_embed_dropout: float = 0.5,
+        stop_gradient_from_pitch_predictor: bool = False,
         # extra embedding related
         spks: Optional[int] = None,
         langs: Optional[int] = None,
         spk_embed_dim: Optional[int] = None,
         spk_embed_integration_type: str = "add",
         use_gst: bool = False,
         gst_tokens: int = 10,
@@ -107,15 +125,15 @@
         # training related
         init_type: str = "xavier_uniform",
         init_enc_alpha: float = 1.0,
         init_dec_alpha: float = 1.0,
         use_masking: bool = False,
         use_weighted_masking: bool = False,
     ):
-        """Initialize FastSpeech module.
+        """Initialize FastSpeech2 module.
 
         Args:
             idim (int): Dimension of the inputs.
             odim (int): Dimension of the outputs.
             elayers (int): Number of encoder layers.
             eunits (int): Number of encoder hidden units.
             dlayers (int): Number of decoder layers.
@@ -130,18 +148,14 @@
                 encoder block.
             decoder_normalize_before (bool): Whether to apply layernorm layer before
                 decoder block.
             encoder_concat_after (bool): Whether to concatenate attention layer's input
                 and output in encoder.
             decoder_concat_after (bool): Whether to concatenate attention layer's input
                 and output in decoder.
-            duration_predictor_layers (int): Number of duration predictor layers.
-            duration_predictor_chans (int): Number of duration predictor channels.
-            duration_predictor_kernel_size (int): Kernel size of duration predictor.
-            duration_predictor_dropout_rate (float): Dropout rate in duration predictor.
             reduction_factor (int): Reduction factor.
             encoder_type (str): Encoder type ("transformer" or "conformer").
             decoder_type (str): Decoder type ("transformer" or "conformer").
             transformer_enc_dropout_rate (float): Dropout rate in encoder except
                 attention and positional encoding.
             transformer_enc_positional_dropout_rate (float): Dropout rate after encoder
                 positional encoding.
@@ -155,17 +169,37 @@
                 self-attention module.
             conformer_rel_pos_type (str): Relative pos encoding type in conformer.
             conformer_pos_enc_layer_type (str): Pos encoding layer type in conformer.
             conformer_self_attn_layer_type (str): Self-attention layer type in conformer
             conformer_activation_type (str): Activation function type in conformer.
             use_macaron_style_in_conformer: Whether to use macaron style FFN.
             use_cnn_in_conformer: Whether to use CNN in conformer.
+            zero_triu: Whether to use zero triu in relative self-attention module.
             conformer_enc_kernel_size: Kernel size of encoder conformer.
             conformer_dec_kernel_size: Kernel size of decoder conformer.
-            zero_triu: Whether to use zero triu in relative self-attention module.
+            duration_predictor_layers (int): Number of duration predictor layers.
+            duration_predictor_chans (int): Number of duration predictor channels.
+            duration_predictor_kernel_size (int): Kernel size of duration predictor.
+            duration_predictor_dropout_rate (float): Dropout rate in duration predictor.
+            pitch_predictor_layers (int): Number of pitch predictor layers.
+            pitch_predictor_chans (int): Number of pitch predictor channels.
+            pitch_predictor_kernel_size (int): Kernel size of pitch predictor.
+            pitch_predictor_dropout_rate (float): Dropout rate in pitch predictor.
+            pitch_embed_kernel_size (float): Kernel size of pitch embedding.
+            pitch_embed_dropout_rate (float): Dropout rate for pitch embedding.
+            stop_gradient_from_pitch_predictor: Whether to stop gradient from pitch
+                predictor to encoder.
+            energy_predictor_layers (int): Number of energy predictor layers.
+            energy_predictor_chans (int): Number of energy predictor channels.
+            energy_predictor_kernel_size (int): Kernel size of energy predictor.
+            energy_predictor_dropout_rate (float): Dropout rate in energy predictor.
+            energy_embed_kernel_size (float): Kernel size of energy embedding.
+            energy_embed_dropout_rate (float): Dropout rate for energy embedding.
+            stop_gradient_from_energy_predictor: Whether to stop gradient from energy
+                predictor to encoder.
             spks (Optional[int]): Number of speakers. If set to > 1, assume that the
                 sids will be provided as the input and use sid embedding layer.
             langs (Optional[int]): Number of languages. If set to > 1, assume that the
                 lids will be provided as the input and use sid embedding layer.
             spk_embed_dim (Optional[int]): Speaker embedding dimension. If set to > 0,
                 assume that spembs will be provided as the input.
             spk_embed_integration_type: How to integrate speaker embedding.
@@ -196,14 +230,16 @@
         # store hyperparameters
         self.idim = idim
         self.odim = odim
         self.eos = idim - 1
         self.reduction_factor = reduction_factor
         self.encoder_type = encoder_type
         self.decoder_type = decoder_type
+        self.stop_gradient_from_pitch_predictor = stop_gradient_from_pitch_predictor
+        self.stop_gradient_from_energy_predictor = stop_gradient_from_energy_predictor
         self.use_scaled_pos_enc = use_scaled_pos_enc
         self.use_gst = use_gst
 
         # use idx 0 as padding idx
         self.padding_idx = 0
 
         # get positional encoding class
@@ -273,14 +309,15 @@
                 positionwise_conv_kernel_size=positionwise_conv_kernel_size,
                 macaron_style=use_macaron_style_in_conformer,
                 pos_enc_layer_type=conformer_pos_enc_layer_type,
                 selfattention_layer_type=conformer_self_attn_layer_type,
                 activation_type=conformer_activation_type,
                 use_cnn_module=use_cnn_in_conformer,
                 cnn_module_kernel=conformer_enc_kernel_size,
+                zero_triu=zero_triu,
             )
         else:
             raise ValueError(f"{encoder_type} is not supported.")
 
         # define GST
         if self.use_gst:
             self.gst = StyleEncoder(
@@ -322,14 +359,52 @@
             idim=adim,
             n_layers=duration_predictor_layers,
             n_chans=duration_predictor_chans,
             kernel_size=duration_predictor_kernel_size,
             dropout_rate=duration_predictor_dropout_rate,
         )
 
+        # define pitch predictor
+        self.pitch_predictor = VariancePredictor(
+            idim=adim,
+            n_layers=pitch_predictor_layers,
+            n_chans=pitch_predictor_chans,
+            kernel_size=pitch_predictor_kernel_size,
+            dropout_rate=pitch_predictor_dropout,
+        )
+        # NOTE(kan-bayashi): We use continuous pitch + FastPitch style avg
+        self.pitch_embed = torch.nn.Sequential(
+            torch.nn.Conv1d(
+                in_channels=1,
+                out_channels=adim,
+                kernel_size=pitch_embed_kernel_size,
+                padding=(pitch_embed_kernel_size - 1) // 2,
+            ),
+            torch.nn.Dropout(pitch_embed_dropout),
+        )
+
+        # define energy predictor
+        self.energy_predictor = VariancePredictor(
+            idim=adim,
+            n_layers=energy_predictor_layers,
+            n_chans=energy_predictor_chans,
+            kernel_size=energy_predictor_kernel_size,
+            dropout_rate=energy_predictor_dropout,
+        )
+        # NOTE(kan-bayashi): We use continuous enegy + FastPitch style avg
+        self.energy_embed = torch.nn.Sequential(
+            torch.nn.Conv1d(
+                in_channels=1,
+                out_channels=adim,
+                kernel_size=energy_embed_kernel_size,
+                padding=(energy_embed_kernel_size - 1) // 2,
+            ),
+            torch.nn.Dropout(energy_embed_dropout),
+        )
+
         # define length regulator
         self.length_regulator = LengthRegulator()
 
         # define decoder
         # NOTE: we use encoder as decoder
         # because fastspeech's decoder is the same as encoder
         if decoder_type == "transformer":
@@ -396,163 +471,122 @@
         self._reset_parameters(
             init_type=init_type,
             init_enc_alpha=init_enc_alpha,
             init_dec_alpha=init_dec_alpha,
         )
 
         # define criterions
-        self.criterion = FastSpeechLoss(
+        self.criterion = FastSpeech2Loss(
             use_masking=use_masking, use_weighted_masking=use_weighted_masking
         )
 
-    def _forward(
-        self,
-        xs: torch.Tensor,
-        ilens: torch.Tensor,
-        ys: Optional[torch.Tensor] = None,
-        olens: Optional[torch.Tensor] = None,
-        ds: Optional[torch.Tensor] = None,
-        spembs: Optional[torch.Tensor] = None,
-        sids: Optional[torch.Tensor] = None,
-        lids: Optional[torch.Tensor] = None,
-        is_inference: bool = False,
-        alpha: float = 1.0,
-    ) -> Sequence[torch.Tensor]:
-        # forward encoder
-        x_masks = self._source_mask(ilens)
-        hs, _ = self.encoder(xs, x_masks)  # (B, T_text, adim)
-
-        # integrate with GST
-        if self.use_gst:
-            style_embs = self.gst(ys)
-            hs = hs + style_embs.unsqueeze(1)
-
-        # integrate with SID and LID embeddings
-        if self.spks is not None:
-            sid_embs = self.sid_emb(sids.view(-1))
-            hs = hs + sid_embs.unsqueeze(1)
-        if self.langs is not None:
-            lid_embs = self.lid_emb(lids.view(-1))
-            hs = hs + lid_embs.unsqueeze(1)
-
-        # integrate speaker embedding
-        if self.spk_embed_dim is not None:
-            hs = self._integrate_with_spk_embed(hs, spembs)
-
-        # forward duration predictor and length regulator
-        d_masks = make_pad_mask(ilens).to(xs.device)
-        if is_inference:
-            d_outs = self.duration_predictor.inference(hs, d_masks)  # (B, T_text)
-            hs = self.length_regulator(hs, d_outs, alpha)  # (B, T_feats, adim)
-        else:
-            d_outs = self.duration_predictor(hs, d_masks)  # (B, T_text)
-            hs = self.length_regulator(hs, ds)  # (B, T_feats, adim)
-
-        # forward decoder
-        if olens is not None and not is_inference:
-            if self.reduction_factor > 1:
-                olens_in = olens.new([olen // self.reduction_factor for olen in olens])
-            else:
-                olens_in = olens
-            h_masks = self._source_mask(olens_in)
-        else:
-            h_masks = None
-        zs, _ = self.decoder(hs, h_masks)  # (B, T_feats, adim)
-        before_outs = self.feat_out(zs).view(
-            zs.size(0), -1, self.odim
-        )  # (B, T_feats, odim)
-
-        # postnet -> (B, T_feats//r * r, odim)
-        if self.postnet is None:
-            after_outs = before_outs
-        else:
-            after_outs = before_outs + self.postnet(
-                before_outs.transpose(1, 2)
-            ).transpose(1, 2)
-
-        return before_outs, after_outs, d_outs
-
     def forward(
         self,
         text: torch.Tensor,
         text_lengths: torch.Tensor,
         feats: torch.Tensor,
         feats_lengths: torch.Tensor,
         durations: torch.Tensor,
         durations_lengths: torch.Tensor,
+        pitch: torch.Tensor,
+        pitch_lengths: torch.Tensor,
+        energy: torch.Tensor,
+        energy_lengths: torch.Tensor,
         spembs: Optional[torch.Tensor] = None,
         sids: Optional[torch.Tensor] = None,
         lids: Optional[torch.Tensor] = None,
         joint_training: bool = False,
     ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor]:
         """Calculate forward propagation.
 
         Args:
-            text (LongTensor): Batch of padded character ids (B, T_text).
+            text (LongTensor): Batch of padded token ids (B, T_text).
             text_lengths (LongTensor): Batch of lengths of each input (B,).
             feats (Tensor): Batch of padded target features (B, T_feats, odim).
             feats_lengths (LongTensor): Batch of the lengths of each target (B,).
             durations (LongTensor): Batch of padded durations (B, T_text + 1).
             durations_lengths (LongTensor): Batch of duration lengths (B, T_text + 1).
+            pitch (Tensor): Batch of padded token-averaged pitch (B, T_text + 1, 1).
+            pitch_lengths (LongTensor): Batch of pitch lengths (B, T_text + 1).
+            energy (Tensor): Batch of padded token-averaged energy (B, T_text + 1, 1).
+            energy_lengths (LongTensor): Batch of energy lengths (B, T_text + 1).
             spembs (Optional[Tensor]): Batch of speaker embeddings (B, spk_embed_dim).
             sids (Optional[Tensor]): Batch of speaker IDs (B, 1).
             lids (Optional[Tensor]): Batch of language IDs (B, 1).
             joint_training (bool): Whether to perform joint training with vocoder.
 
         Returns:
             Tensor: Loss scalar value.
             Dict: Statistics to be monitored.
             Tensor: Weight value if not joint training else model outputs.
 
         """
         text = text[:, : text_lengths.max()]  # for data-parallel
         feats = feats[:, : feats_lengths.max()]  # for data-parallel
         durations = durations[:, : durations_lengths.max()]  # for data-parallel
+        pitch = pitch[:, : pitch_lengths.max()]  # for data-parallel
+        energy = energy[:, : energy_lengths.max()]  # for data-parallel
 
         batch_size = text.size(0)
 
         # Add eos at the last of sequence
         xs = F.pad(text, [0, 1], "constant", self.padding_idx)
         for i, l in enumerate(text_lengths):
             xs[i, l] = self.eos
         ilens = text_lengths + 1
 
-        ys, ds = feats, durations
+        ys, ds, ps, es = feats, durations, pitch, energy
         olens = feats_lengths
 
         # forward propagation
-        before_outs, after_outs, d_outs = self._forward(
+        before_outs, after_outs, d_outs, p_outs, e_outs = self._forward(
             xs,
             ilens,
             ys,
             olens,
             ds,
+            ps,
+            es,
             spembs=spembs,
             sids=sids,
             lids=lids,
             is_inference=False,
         )
 
-        # modifiy mod part of groundtruth
+        # modify mod part of groundtruth
         if self.reduction_factor > 1:
             olens = olens.new([olen - olen % self.reduction_factor for olen in olens])
             max_olen = max(olens)
             ys = ys[:, :max_olen]
 
         # calculate loss
         if self.postnet is None:
             after_outs = None
-        l1_loss, duration_loss = self.criterion(
-            after_outs, before_outs, d_outs, ys, ds, ilens, olens
+
+        # calculate loss
+        l1_loss, duration_loss, pitch_loss, energy_loss = self.criterion(
+            after_outs=after_outs,
+            before_outs=before_outs,
+            d_outs=d_outs,
+            p_outs=p_outs,
+            e_outs=e_outs,
+            ys=ys,
+            ds=ds,
+            ps=ps,
+            es=es,
+            ilens=ilens,
+            olens=olens,
         )
-        loss = l1_loss + duration_loss
+        loss = l1_loss + duration_loss + pitch_loss + energy_loss
 
         stats = dict(
             l1_loss=l1_loss.item(),
             duration_loss=duration_loss.item(),
+            pitch_loss=pitch_loss.item(),
+            energy_loss=energy_loss.item(),
         )
 
         # report extra information
         if self.encoder_type == "transformer" and self.use_scaled_pos_enc:
             stats.update(
                 encoder_alpha=self.encoder.embed[-1].alpha.data.item(),
             )
@@ -566,84 +600,183 @@
             loss, stats, weight = force_gatherable(
                 (loss, stats, batch_size), loss.device
             )
             return loss, stats, weight
         else:
             return loss, stats, after_outs if after_outs is not None else before_outs
 
+    def _forward(
+        self,
+        xs: torch.Tensor,
+        ilens: torch.Tensor,
+        ys: Optional[torch.Tensor] = None,
+        olens: Optional[torch.Tensor] = None,
+        ds: Optional[torch.Tensor] = None,
+        ps: Optional[torch.Tensor] = None,
+        es: Optional[torch.Tensor] = None,
+        spembs: Optional[torch.Tensor] = None,
+        sids: Optional[torch.Tensor] = None,
+        lids: Optional[torch.Tensor] = None,
+        is_inference: bool = False,
+        alpha: float = 1.0,
+    ) -> Sequence[torch.Tensor]:
+        # forward encoder
+        x_masks = self._source_mask(ilens)
+        hs, _ = self.encoder(xs, x_masks)  # (B, T_text, adim)
+
+        # integrate with GST
+        if self.use_gst:
+            style_embs = self.gst(ys)
+            hs = hs + style_embs.unsqueeze(1)
+
+        # integrate with SID and LID embeddings
+        if self.spks is not None:
+            sid_embs = self.sid_emb(sids.view(-1))
+            hs = hs + sid_embs.unsqueeze(1)
+        if self.langs is not None:
+            lid_embs = self.lid_emb(lids.view(-1))
+            hs = hs + lid_embs.unsqueeze(1)
+
+        # integrate speaker embedding
+        if self.spk_embed_dim is not None:
+            hs = self._integrate_with_spk_embed(hs, spembs)
+
+        # forward duration predictor and variance predictors
+        d_masks = make_pad_mask(ilens).to(xs.device)
+
+        if self.stop_gradient_from_pitch_predictor:
+            p_outs = self.pitch_predictor(hs.detach(), d_masks.unsqueeze(-1))
+        else:
+            p_outs = self.pitch_predictor(hs, d_masks.unsqueeze(-1))
+        if self.stop_gradient_from_energy_predictor:
+            e_outs = self.energy_predictor(hs.detach(), d_masks.unsqueeze(-1))
+        else:
+            e_outs = self.energy_predictor(hs, d_masks.unsqueeze(-1))
+
+        if is_inference:
+            d_outs = self.duration_predictor.inference(hs, d_masks)  # (B, T_text)
+            # use prediction in inference
+            p_embs = self.pitch_embed(p_outs.transpose(1, 2)).transpose(1, 2)
+            e_embs = self.energy_embed(e_outs.transpose(1, 2)).transpose(1, 2)
+            hs = hs + e_embs + p_embs
+            hs = self.length_regulator(hs, d_outs, alpha)  # (B, T_feats, adim)
+        else:
+            d_outs = self.duration_predictor(hs, d_masks)
+            # use groundtruth in training
+            p_embs = self.pitch_embed(ps.transpose(1, 2)).transpose(1, 2)
+            e_embs = self.energy_embed(es.transpose(1, 2)).transpose(1, 2)
+            hs = hs + e_embs + p_embs
+            hs = self.length_regulator(hs, ds)  # (B, T_feats, adim)
+
+        # forward decoder
+        if olens is not None and not is_inference:
+            if self.reduction_factor > 1:
+                olens_in = olens.new([olen // self.reduction_factor for olen in olens])
+            else:
+                olens_in = olens
+            h_masks = self._source_mask(olens_in)
+        else:
+            h_masks = None
+        zs, _ = self.decoder(hs, h_masks)  # (B, T_feats, adim)
+        before_outs = self.feat_out(zs).view(
+            zs.size(0), -1, self.odim
+        )  # (B, T_feats, odim)
+
+        # postnet -> (B, T_feats//r * r, odim)
+        if self.postnet is None:
+            after_outs = before_outs
+        else:
+            after_outs = before_outs + self.postnet(
+                before_outs.transpose(1, 2)
+            ).transpose(1, 2)
+
+        return before_outs, after_outs, d_outs, p_outs, e_outs
+
     def inference(
         self,
         text: torch.Tensor,
         feats: Optional[torch.Tensor] = None,
         durations: Optional[torch.Tensor] = None,
-        spembs: Optional[torch.Tensor] = None,
+        spembs: torch.Tensor = None,
         sids: Optional[torch.Tensor] = None,
         lids: Optional[torch.Tensor] = None,
+        pitch: Optional[torch.Tensor] = None,
+        energy: Optional[torch.Tensor] = None,
         alpha: float = 1.0,
         use_teacher_forcing: bool = False,
     ) -> Dict[str, torch.Tensor]:
         """Generate the sequence of features given the sequences of characters.
 
         Args:
             text (LongTensor): Input sequence of characters (T_text,).
-            feats (Optional[Tensor]): Feature sequence to extract style (N, idim).
-            durations (Optional[LongTensor]): Groundtruth of duration (T_text + 1,).
-            spembs (Optional[Tensor]): Speaker embedding (spk_embed_dim,).
+            feats (Optional[Tensor): Feature sequence to extract style (N, idim).
+            durations (Optional[Tensor): Groundtruth of duration (T_text + 1,).
+            spembs (Optional[Tensor): Speaker embedding vector (spk_embed_dim,).
             sids (Optional[Tensor]): Speaker ID (1,).
             lids (Optional[Tensor]): Language ID (1,).
+            pitch (Optional[Tensor]): Groundtruth of token-avg pitch (T_text + 1, 1).
+            energy (Optional[Tensor]): Groundtruth of token-avg energy (T_text + 1, 1).
             alpha (float): Alpha to control the speed.
             use_teacher_forcing (bool): Whether to use teacher forcing.
                 If true, groundtruth of duration, pitch and energy will be used.
 
         Returns:
             Dict[str, Tensor]: Output dict including the following items:
                 * feat_gen (Tensor): Output sequence of features (T_feats, odim).
                 * duration (Tensor): Duration sequence (T_text + 1,).
+                * pitch (Tensor): Pitch sequence (T_text + 1,).
+                * energy (Tensor): Energy sequence (T_text + 1,).
 
         """
         x, y = text, feats
-        spemb, d = spembs, durations
+        spemb, d, p, e = spembs, durations, pitch, energy
 
         # add eos at the last of sequence
         x = F.pad(x, [0, 1], "constant", self.eos)
 
         # setup batch axis
         ilens = torch.tensor([x.shape[0]], dtype=torch.long, device=x.device)
         xs, ys = x.unsqueeze(0), None
         if y is not None:
             ys = y.unsqueeze(0)
         if spemb is not None:
             spembs = spemb.unsqueeze(0)
 
         if use_teacher_forcing:
-            # use groundtruth of duration
-            ds = d.unsqueeze(0)
-            _, outs, d_outs = self._forward(
+            # use groundtruth of duration, pitch, and energy
+            ds, ps, es = d.unsqueeze(0), p.unsqueeze(0), e.unsqueeze(0)
+            _, outs, d_outs, p_outs, e_outs = self._forward(
                 xs,
                 ilens,
                 ys,
                 ds=ds,
+                ps=ps,
+                es=es,
                 spembs=spembs,
                 sids=sids,
                 lids=lids,
             )  # (1, T_feats, odim)
         else:
-            # inference
-            _, outs, d_outs = self._forward(
+            _, outs, d_outs, p_outs, e_outs = self._forward(
                 xs,
                 ilens,
                 ys,
                 spembs=spembs,
                 sids=sids,
                 lids=lids,
                 is_inference=True,
                 alpha=alpha,
             )  # (1, T_feats, odim)
 
-        return dict(feat_gen=outs[0], duration=d_outs[0])
+        return dict(
+            feat_gen=outs[0],
+            duration=d_outs[0],
+            pitch=p_outs[0],
+            energy=e_outs[0],
+        )
 
     def _integrate_with_spk_embed(
         self, hs: torch.Tensor, spembs: torch.Tensor
     ) -> torch.Tensor:
         """Integrate speaker embedding with hidden states.
 
         Args:
```

### Comparing `espnet-202304/espnet2/tts/fastspeech2/fastspeech2.py` & `espnet-202308/espnet2/tts/prodiff/prodiff.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,96 +1,94 @@
-# Copyright 2020 Nagoya University (Tomoki Hayashi)
+# Copyright 2022 Hitachi LTD. (Nelson Yalta)
 #  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)
+# Based in FastSpeech2
 
-"""Fastspeech2 related modules for ESPnet2."""
+"""ProDiff related modules for ESPnet2."""
 
 import logging
 from typing import Dict, Optional, Sequence, Tuple
 
 import torch
 import torch.nn.functional as F
 from typeguard import check_argument_types
 
 from espnet2.torch_utils.device_funcs import force_gatherable
 from espnet2.torch_utils.initialize import initialize
 from espnet2.tts.abs_tts import AbsTTS
-from espnet2.tts.fastspeech2.loss import FastSpeech2Loss
 from espnet2.tts.fastspeech2.variance_predictor import VariancePredictor
 from espnet2.tts.gst.style_encoder import StyleEncoder
+from espnet2.tts.prodiff.denoiser import SpectogramDenoiser
+from espnet2.tts.prodiff.loss import ProDiffLoss
 from espnet.nets.pytorch_backend.conformer.encoder import Encoder as ConformerEncoder
 from espnet.nets.pytorch_backend.fastspeech.duration_predictor import DurationPredictor
 from espnet.nets.pytorch_backend.fastspeech.length_regulator import LengthRegulator
 from espnet.nets.pytorch_backend.nets_utils import make_non_pad_mask, make_pad_mask
 from espnet.nets.pytorch_backend.tacotron2.decoder import Postnet
 from espnet.nets.pytorch_backend.transformer.embedding import (
     PositionalEncoding,
     ScaledPositionalEncoding,
 )
 from espnet.nets.pytorch_backend.transformer.encoder import (
     Encoder as TransformerEncoder,
 )
 
 
-class FastSpeech2(AbsTTS):
-    """FastSpeech2 module.
+class ProDiff(AbsTTS):
+    """ProDiff module.
 
-    This is a module of FastSpeech2 described in `FastSpeech 2: Fast and
-    High-Quality End-to-End Text to Speech`_. Instead of quantized pitch and
-    energy, we use token-averaged value introduced in `FastPitch: Parallel
-    Text-to-speech with Pitch Prediction`_.
-
-    .. _`FastSpeech 2: Fast and High-Quality End-to-End Text to Speech`:
-        https://arxiv.org/abs/2006.04558
-    .. _`FastPitch: Parallel Text-to-speech with Pitch Prediction`:
-        https://arxiv.org/abs/2006.06873
+    This is a module of ProDiff described in `ProDiff: Progressive Fast Diffusion Model
+    for High-Quality Text-to-Speech`_.
+
+    .. _`ProDiff: Progressive Fast Diffusion Model for High-Quality Text-to-Speech`:
+        https://arxiv.org/abs/2207.06389
 
     """
 
     def __init__(
         self,
         # network structure related
         idim: int,
         odim: int,
         adim: int = 384,
         aheads: int = 4,
         elayers: int = 6,
         eunits: int = 1536,
-        dlayers: int = 6,
-        dunits: int = 1536,
-        postnet_layers: int = 5,
+        postnet_layers: int = 0,
         postnet_chans: int = 512,
         postnet_filts: int = 5,
         postnet_dropout_rate: float = 0.5,
         positionwise_layer_type: str = "conv1d",
         positionwise_conv_kernel_size: int = 1,
         use_scaled_pos_enc: bool = True,
         use_batch_norm: bool = True,
         encoder_normalize_before: bool = True,
-        decoder_normalize_before: bool = True,
         encoder_concat_after: bool = False,
-        decoder_concat_after: bool = False,
         reduction_factor: int = 1,
         encoder_type: str = "transformer",
-        decoder_type: str = "transformer",
+        decoder_type: str = "diffusion",
         transformer_enc_dropout_rate: float = 0.1,
         transformer_enc_positional_dropout_rate: float = 0.1,
         transformer_enc_attn_dropout_rate: float = 0.1,
-        transformer_dec_dropout_rate: float = 0.1,
-        transformer_dec_positional_dropout_rate: float = 0.1,
-        transformer_dec_attn_dropout_rate: float = 0.1,
+        # Denoiser Decoder
+        denoiser_layers: int = 20,
+        denoiser_channels: int = 256,
+        diffusion_steps: int = 1000,
+        diffusion_timescale: int = 1,
+        diffusion_beta: float = 40.0,
+        diffusion_scheduler: str = "vpsde",
+        diffusion_cycle_ln: int = 1,
         # only for conformer
         conformer_rel_pos_type: str = "legacy",
         conformer_pos_enc_layer_type: str = "rel_pos",
         conformer_self_attn_layer_type: str = "rel_selfattn",
         conformer_activation_type: str = "swish",
         use_macaron_style_in_conformer: bool = True,
         use_cnn_in_conformer: bool = True,
         zero_triu: bool = False,
         conformer_enc_kernel_size: int = 7,
-        conformer_dec_kernel_size: int = 31,
         # duration predictor
         duration_predictor_layers: int = 2,
         duration_predictor_chans: int = 384,
         duration_predictor_kernel_size: int = 3,
         duration_predictor_dropout_rate: float = 0.1,
         # energy predictor
         energy_predictor_layers: int = 2,
@@ -125,15 +123,15 @@
         # training related
         init_type: str = "xavier_uniform",
         init_enc_alpha: float = 1.0,
         init_dec_alpha: float = 1.0,
         use_masking: bool = False,
         use_weighted_masking: bool = False,
     ):
-        """Initialize FastSpeech2 module.
+        """Initialize ProDiff module.
 
         Args:
             idim (int): Dimension of the inputs.
             odim (int): Dimension of the outputs.
             elayers (int): Number of encoder layers.
             eunits (int): Number of encoder hidden units.
             dlayers (int): Number of decoder layers.
@@ -401,60 +399,34 @@
             torch.nn.Dropout(energy_embed_dropout),
         )
 
         # define length regulator
         self.length_regulator = LengthRegulator()
 
         # define decoder
-        # NOTE: we use encoder as decoder
-        # because fastspeech's decoder is the same as encoder
-        if decoder_type == "transformer":
-            self.decoder = TransformerEncoder(
-                idim=0,
-                attention_dim=adim,
-                attention_heads=aheads,
-                linear_units=dunits,
-                num_blocks=dlayers,
-                input_layer=None,
-                dropout_rate=transformer_dec_dropout_rate,
-                positional_dropout_rate=transformer_dec_positional_dropout_rate,
-                attention_dropout_rate=transformer_dec_attn_dropout_rate,
-                pos_enc_class=pos_enc_class,
-                normalize_before=decoder_normalize_before,
-                concat_after=decoder_concat_after,
-                positionwise_layer_type=positionwise_layer_type,
-                positionwise_conv_kernel_size=positionwise_conv_kernel_size,
-            )
-        elif decoder_type == "conformer":
-            self.decoder = ConformerEncoder(
-                idim=0,
-                attention_dim=adim,
-                attention_heads=aheads,
-                linear_units=dunits,
-                num_blocks=dlayers,
-                input_layer=None,
-                dropout_rate=transformer_dec_dropout_rate,
-                positional_dropout_rate=transformer_dec_positional_dropout_rate,
-                attention_dropout_rate=transformer_dec_attn_dropout_rate,
-                normalize_before=decoder_normalize_before,
-                concat_after=decoder_concat_after,
-                positionwise_layer_type=positionwise_layer_type,
-                positionwise_conv_kernel_size=positionwise_conv_kernel_size,
-                macaron_style=use_macaron_style_in_conformer,
-                pos_enc_layer_type=conformer_pos_enc_layer_type,
-                selfattention_layer_type=conformer_self_attn_layer_type,
-                activation_type=conformer_activation_type,
-                use_cnn_module=use_cnn_in_conformer,
-                cnn_module_kernel=conformer_dec_kernel_size,
+        if decoder_type == "diffusion":
+            self.decoder = SpectogramDenoiser(
+                odim,
+                adim=adim,
+                layers=denoiser_layers,
+                channels=denoiser_channels,
+                timesteps=diffusion_steps,
+                timescale=diffusion_timescale,
+                max_beta=diffusion_beta,
+                scheduler=diffusion_scheduler,
+                cycle_length=diffusion_cycle_ln,
             )
         else:
-            raise ValueError(f"{decoder_type} is not supported.")
+            raise NotImplementedError(decoder_type)
 
         # define final projection
-        self.feat_out = torch.nn.Linear(adim, odim * reduction_factor)
+        if decoder_type != "diffusion":
+            self.feat_out = torch.nn.Linear(adim, odim * reduction_factor)
+        if reduction_factor > 1:
+            raise NotImplementedError()
 
         # define postnet
         self.postnet = (
             None
             if postnet_layers == 0
             else Postnet(
                 idim=idim,
@@ -471,15 +443,15 @@
         self._reset_parameters(
             init_type=init_type,
             init_enc_alpha=init_enc_alpha,
             init_dec_alpha=init_dec_alpha,
         )
 
         # define criterions
-        self.criterion = FastSpeech2Loss(
+        self.criterion = ProDiffLoss(
             use_masking=use_masking, use_weighted_masking=use_weighted_masking
         )
 
     def forward(
         self,
         text: torch.Tensor,
         text_lengths: torch.Tensor,
@@ -559,31 +531,32 @@
             ys = ys[:, :max_olen]
 
         # calculate loss
         if self.postnet is None:
             after_outs = None
 
         # calculate loss
-        l1_loss, duration_loss, pitch_loss, energy_loss = self.criterion(
+        l1_loss, ssim_loss, duration_loss, pitch_loss, energy_loss = self.criterion(
             after_outs=after_outs,
             before_outs=before_outs,
             d_outs=d_outs,
             p_outs=p_outs,
             e_outs=e_outs,
             ys=ys,
             ds=ds,
             ps=ps,
             es=es,
             ilens=ilens,
             olens=olens,
         )
-        loss = l1_loss + duration_loss + pitch_loss + energy_loss
+        loss = l1_loss + ssim_loss + duration_loss + pitch_loss + energy_loss
 
         stats = dict(
             l1_loss=l1_loss.item(),
+            ssim_loss=ssim_loss.item(),
             duration_loss=duration_loss.item(),
             pitch_loss=pitch_loss.item(),
             energy_loss=energy_loss.item(),
         )
 
         # report extra information
         if self.encoder_type == "transformer" and self.use_scaled_pos_enc:
@@ -615,14 +588,24 @@
         es: Optional[torch.Tensor] = None,
         spembs: Optional[torch.Tensor] = None,
         sids: Optional[torch.Tensor] = None,
         lids: Optional[torch.Tensor] = None,
         is_inference: bool = False,
         alpha: float = 1.0,
     ) -> Sequence[torch.Tensor]:
+        """Calculate forward propagation without loss.
+
+        Args:
+            xs (Tensor): Batch of padded target features (B, T_feats, odim).
+            ilens (LongTensor): Batch of the lengths of each target (B,).
+
+        Returns:
+            Tensor: Weight value if not joint training else model outputs.
+
+        """
         # forward encoder
         x_masks = self._source_mask(ilens)
         hs, _ = self.encoder(xs, x_masks)  # (B, T_text, adim)
 
         # integrate with GST
         if self.use_gst:
             style_embs = self.gst(ys)
@@ -672,35 +655,42 @@
             if self.reduction_factor > 1:
                 olens_in = olens.new([olen // self.reduction_factor for olen in olens])
             else:
                 olens_in = olens
             h_masks = self._source_mask(olens_in)
         else:
             h_masks = None
-        zs, _ = self.decoder(hs, h_masks)  # (B, T_feats, adim)
-        before_outs = self.feat_out(zs).view(
-            zs.size(0), -1, self.odim
-        )  # (B, T_feats, odim)
+
+        if self.decoder_type == "diffusion":
+            before_outs = self.decoder(
+                hs, ys, h_masks, is_inference
+            )  # (B, T_feats, odim)
+        else:
+            zs, _ = self.decoder(hs, h_masks)  # (B, T_feats, adim)
+            before_outs = self.feat_out(zs).view(
+                zs.size(0), -1, self.odim
+            )  # (B, T_feats, odim)
 
         # postnet -> (B, T_feats//r * r, odim)
         if self.postnet is None:
             after_outs = before_outs
         else:
             after_outs = before_outs + self.postnet(
                 before_outs.transpose(1, 2)
             ).transpose(1, 2)
 
         return before_outs, after_outs, d_outs, p_outs, e_outs
 
+    @torch.no_grad()
     def inference(
         self,
         text: torch.Tensor,
         feats: Optional[torch.Tensor] = None,
         durations: Optional[torch.Tensor] = None,
-        spembs: torch.Tensor = None,
+        spembs: Optional[torch.Tensor] = None,
         sids: Optional[torch.Tensor] = None,
         lids: Optional[torch.Tensor] = None,
         pitch: Optional[torch.Tensor] = None,
         energy: Optional[torch.Tensor] = None,
         alpha: float = 1.0,
         use_teacher_forcing: bool = False,
     ) -> Dict[str, torch.Tensor]:
@@ -750,14 +740,15 @@
                 ys,
                 ds=ds,
                 ps=ps,
                 es=es,
                 spembs=spembs,
                 sids=sids,
                 lids=lids,
+                is_inference=True,
             )  # (1, T_feats, odim)
         else:
             _, outs, d_outs, p_outs, e_outs = self._forward(
                 xs,
                 ilens,
                 ys,
                 spembs=spembs,
@@ -820,14 +811,22 @@
         """
         x_masks = make_non_pad_mask(ilens).to(next(self.parameters()).device)
         return x_masks.unsqueeze(-2)
 
     def _reset_parameters(
         self, init_type: str, init_enc_alpha: float, init_dec_alpha: float
     ):
+        """Reset parameters of the model.
+
+        Args:
+            init_type (str): Type of initialization.
+            init_enc_alpha (float): Value of the initialization for the encoder.
+            init_dec_alpha (float): Value of the initialization for the decoder.
+
+        """
         # initialize parameters
         if init_type != "pytorch":
             initialize(self, init_type)
 
         # initialize alpha in scaled positional encoding
         if self.encoder_type == "transformer" and self.use_scaled_pos_enc:
             self.encoder.embed[-1].alpha.data = torch.tensor(init_enc_alpha)
```

### Comparing `espnet-202304/espnet2/tts/fastspeech2/loss.py` & `espnet-202308/espnet2/svs/xiaoice/loss.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,24 +1,25 @@
 # Copyright 2020 Nagoya University (Tomoki Hayashi)
+# Copyright 2023 Renmin University of China (Yuning Wu)
 #  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)
 
-"""Fastspeech2 related loss module for ESPnet2."""
+"""XiaoiceSing2 related loss module for ESPnet2."""
 
 from typing import Tuple
 
 import torch
 from typeguard import check_argument_types
 
 from espnet.nets.pytorch_backend.fastspeech.duration_predictor import (  # noqa: H301
     DurationPredictorLoss,
 )
 from espnet.nets.pytorch_backend.nets_utils import make_non_pad_mask
 
 
-class FastSpeech2Loss(torch.nn.Module):
+class XiaoiceSing2Loss(torch.nn.Module):
     """Loss function module for FastSpeech2."""
 
     def __init__(self, use_masking: bool = True, use_weighted_masking: bool = False):
         """Initialize feed-forward Transformer loss module.
 
         Args:
             use_masking (bool): Whether to apply masking for padded part in loss
@@ -34,93 +35,106 @@
         self.use_masking = use_masking
         self.use_weighted_masking = use_weighted_masking
 
         # define criterions
         reduction = "none" if self.use_weighted_masking else "mean"
         self.l1_criterion = torch.nn.L1Loss(reduction=reduction)
         self.mse_criterion = torch.nn.MSELoss(reduction=reduction)
+        self.bce_criterion = torch.nn.BCEWithLogitsLoss(reduction=reduction)
         self.duration_criterion = DurationPredictorLoss(reduction=reduction)
 
     def forward(
         self,
         after_outs: torch.Tensor,
         before_outs: torch.Tensor,
         d_outs: torch.Tensor,
         p_outs: torch.Tensor,
-        e_outs: torch.Tensor,
+        v_outs: torch.Tensor,
         ys: torch.Tensor,
         ds: torch.Tensor,
         ps: torch.Tensor,
-        es: torch.Tensor,
+        vs: torch.Tensor,
         ilens: torch.Tensor,
         olens: torch.Tensor,
+        loss_type: str = "L1",
     ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
         """Calculate forward propagation.
 
         Args:
             after_outs (Tensor): Batch of outputs after postnets (B, T_feats, odim).
             before_outs (Tensor): Batch of outputs before postnets (B, T_feats, odim).
             d_outs (LongTensor): Batch of outputs of duration predictor (B, T_text).
-            p_outs (Tensor): Batch of outputs of pitch predictor (B, T_text, 1).
-            e_outs (Tensor): Batch of outputs of energy predictor (B, T_text, 1).
+            p_outs (Tensor): Batch of outputs of log_f0 (B, T_text, 1).
+            v_outs (Tensor): Batch of outputs of VUV (B, T_text, 1).
             ys (Tensor): Batch of target features (B, T_feats, odim).
             ds (LongTensor): Batch of durations (B, T_text).
-            ps (Tensor): Batch of target token-averaged pitch (B, T_text, 1).
-            es (Tensor): Batch of target token-averaged energy (B, T_text, 1).
+            ps (Tensor): Batch of target log_f0 (B, T_text, 1).
+            vs (Tensor): Batch of target VUV (B, T_text, 1).
             ilens (LongTensor): Batch of the lengths of each input (B,).
             olens (LongTensor): Batch of the lengths of each target (B,).
+            loss_type (str): Mel loss type ("L1" (MAE), "L2" (MSE) or "L1+L2")
 
         Returns:
-            Tensor: L1 loss value.
+            Tensor: Mel loss value.
             Tensor: Duration predictor loss value.
             Tensor: Pitch predictor loss value.
-            Tensor: Energy predictor loss value.
+            Tensor: VUV predictor loss value.
 
         """
         # apply mask to remove padded part
         if self.use_masking:
             out_masks = make_non_pad_mask(olens).unsqueeze(-1).to(ys.device)
             before_outs = before_outs.masked_select(out_masks)
             if after_outs is not None:
                 after_outs = after_outs.masked_select(out_masks)
             ys = ys.masked_select(out_masks)
             duration_masks = make_non_pad_mask(ilens).to(ys.device)
             d_outs = d_outs.masked_select(duration_masks)
             ds = ds.masked_select(duration_masks)
-            pitch_masks = make_non_pad_mask(ilens).unsqueeze(-1).to(ys.device)
-            p_outs = p_outs.masked_select(pitch_masks)
-            e_outs = e_outs.masked_select(pitch_masks)
-            ps = ps.masked_select(pitch_masks)
-            es = es.masked_select(pitch_masks)
+            p_outs = p_outs.masked_select(out_masks)
+            v_outs = v_outs.masked_select(out_masks)
+            ps = ps.masked_select(out_masks)
+            vs = vs.masked_select(out_masks)
 
         # calculate loss
-        l1_loss = self.l1_criterion(before_outs, ys)
-        if after_outs is not None:
-            l1_loss += self.l1_criterion(after_outs, ys)
+        if loss_type == "L1":
+            mel_loss = self.l1_criterion(before_outs, ys)
+            if after_outs is not None:
+                mel_loss += self.l1_criterion(after_outs, ys)
+        elif loss_type == "L2":
+            mel_loss = self.mse_criterion(before_outs, ys)
+            if after_outs is not None:
+                mel_loss += self.mse_criterion(after_outs, ys)
+        elif loss_type == "L1+L2":
+            mel_loss = self.l1_criterion(before_outs, ys) + self.mse_criterion(
+                before_outs, ys
+            )
+            if after_outs is not None:
+                mel_loss += self.l1_criterion(after_outs, ys) + self.mse_criterion(
+                    after_outs, ys
+                )
+        else:
+            raise NotImplementedError("Mel loss support only L1, L2 or L1+L2.")
         duration_loss = self.duration_criterion(d_outs, ds)
         pitch_loss = self.mse_criterion(p_outs, ps)
-        energy_loss = self.mse_criterion(e_outs, es)
+        vuv_loss = self.bce_criterion(v_outs, vs.float())
 
         # make weighted mask and apply it
         if self.use_weighted_masking:
             out_masks = make_non_pad_mask(olens).unsqueeze(-1).to(ys.device)
             out_weights = out_masks.float() / out_masks.sum(dim=1, keepdim=True).float()
             out_weights /= ys.size(0) * ys.size(2)
             duration_masks = make_non_pad_mask(ilens).to(ys.device)
             duration_weights = (
                 duration_masks.float() / duration_masks.sum(dim=1, keepdim=True).float()
             )
             duration_weights /= ds.size(0)
 
             # apply weight
-            l1_loss = l1_loss.mul(out_weights).masked_select(out_masks).sum()
+            mel_loss = mel_loss.mul(out_weights).masked_select(out_masks).sum()
             duration_loss = (
                 duration_loss.mul(duration_weights).masked_select(duration_masks).sum()
             )
-            pitch_masks = duration_masks.unsqueeze(-1)
-            pitch_weights = duration_weights.unsqueeze(-1)
-            pitch_loss = pitch_loss.mul(pitch_weights).masked_select(pitch_masks).sum()
-            energy_loss = (
-                energy_loss.mul(pitch_weights).masked_select(pitch_masks).sum()
-            )
+            pitch_loss = pitch_loss.mul(out_weights).masked_select(out_masks).sum()
+            vuv_loss = vuv_loss.mul(out_weights).masked_select(out_masks).sum()
 
-        return l1_loss, duration_loss, pitch_loss, energy_loss
+        return mel_loss, duration_loss, pitch_loss, vuv_loss
```

### Comparing `espnet-202304/espnet2/tts/fastspeech2/variance_predictor.py` & `espnet-202308/espnet2/tts/fastspeech2/variance_predictor.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/tts/feats_extract/dio.py` & `espnet-202308/espnet2/tts/feats_extract/dio.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/tts/feats_extract/energy.py` & `espnet-202308/espnet2/tts/feats_extract/energy.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/tts/feats_extract/linear_spectrogram.py` & `espnet-202308/espnet2/tts/feats_extract/linear_spectrogram.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/tts/feats_extract/log_mel_fbank.py` & `espnet-202308/espnet2/tts/feats_extract/log_mel_fbank.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/tts/feats_extract/log_spectrogram.py` & `espnet-202308/espnet2/tts/feats_extract/log_spectrogram.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/tts/gst/style_encoder.py` & `espnet-202308/espnet2/tts/gst/style_encoder.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/tts/prodiff/denoiser.py` & `espnet-202308/espnet2/tts/prodiff/denoiser.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/tts/prodiff/loss.py` & `espnet-202308/espnet2/tts/prodiff/loss.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/tts/prodiff/prodiff.py` & `espnet-202308/espnet2/gan_svs/vits/generator.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,834 +1,927 @@
-# Copyright 2022 Hitachi LTD. (Nelson Yalta)
+# Copyright 2021 Tomoki Hayashi
+# Copyright 2022 Yifeng Yu
 #  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)
-# Based in FastSpeech2
 
-"""ProDiff related modules for ESPnet2."""
+"""Generator module in VISinger.
 
-import logging
-from typing import Dict, Optional, Sequence, Tuple
+This code is based on https://github.com/jaywalnut310/vits.
 
-import torch
-import torch.nn.functional as F
-from typeguard import check_argument_types
+    This is a module of VISinger described in `VISinger: Variational Inference
+      with Adversarial Learning for End-to-End Singing Voice Synthesis`_.
 
-from espnet2.torch_utils.device_funcs import force_gatherable
-from espnet2.torch_utils.initialize import initialize
-from espnet2.tts.abs_tts import AbsTTS
-from espnet2.tts.fastspeech2.variance_predictor import VariancePredictor
-from espnet2.tts.gst.style_encoder import StyleEncoder
-from espnet2.tts.prodiff.denoiser import SpectogramDenoiser
-from espnet2.tts.prodiff.loss import ProDiffLoss
-from espnet.nets.pytorch_backend.conformer.encoder import Encoder as ConformerEncoder
-from espnet.nets.pytorch_backend.fastspeech.duration_predictor import DurationPredictor
-from espnet.nets.pytorch_backend.fastspeech.length_regulator import LengthRegulator
-from espnet.nets.pytorch_backend.nets_utils import make_non_pad_mask, make_pad_mask
-from espnet.nets.pytorch_backend.tacotron2.decoder import Postnet
-from espnet.nets.pytorch_backend.transformer.embedding import (
-    PositionalEncoding,
-    ScaledPositionalEncoding,
-)
-from espnet.nets.pytorch_backend.transformer.encoder import (
-    Encoder as TransformerEncoder,
-)
+    .. _`VISinger: Variational Inference with Adversarial Learning for
+      End-to-End Singing Voice Synthesis`: https://arxiv.org/abs/2110.08813
 
+"""
 
-class ProDiff(AbsTTS):
-    """ProDiff module.
+import math
+from typing import List, Optional, Tuple
 
-    This is a module of ProDiff described in `ProDiff: Progressive Fast Diffusion Model
-    for High-Quality Text-to-Speech`_.
+import numpy as np
+import torch
+import torch.nn.functional as F
+
+from espnet2.gan_svs.avocodo import AvocodoGenerator
+from espnet2.gan_svs.uhifigan import UHiFiGANGenerator
+from espnet2.gan_svs.uhifigan.sine_generator import SineGen
+from espnet2.gan_svs.utils.expand_f0 import expand_f0
+from espnet2.gan_svs.visinger2 import (
+    Generator_Harm,
+    Generator_Noise,
+    VISinger2VocoderGenerator,
+)
+from espnet2.gan_svs.visinger2.ddsp import upsample
+from espnet2.gan_svs.vits.duration_predictor import DurationPredictor
+from espnet2.gan_svs.vits.length_regulator import LengthRegulator
+from espnet2.gan_svs.vits.phoneme_predictor import PhonemePredictor
+from espnet2.gan_svs.vits.pitch_predictor import Decoder
+from espnet2.gan_svs.vits.prior_decoder import PriorDecoder
+from espnet2.gan_svs.vits.text_encoder import TextEncoder
+from espnet2.gan_tts.hifigan import HiFiGANGenerator
+from espnet2.gan_tts.utils import get_random_segments, get_segments
+from espnet2.gan_tts.vits.posterior_encoder import PosteriorEncoder
+from espnet2.gan_tts.vits.residual_coupling import ResidualAffineCouplingBlock
 
-    .. _`ProDiff: Progressive Fast Diffusion Model for High-Quality Text-to-Speech`:
-        https://arxiv.org/abs/2207.06389
 
-    """
+class VISingerGenerator(torch.nn.Module):
+    """Generator module in VISinger."""
 
     def __init__(
         self,
-        # network structure related
-        idim: int,
-        odim: int,
-        adim: int = 384,
-        aheads: int = 4,
-        elayers: int = 6,
-        eunits: int = 1536,
-        postnet_layers: int = 0,
-        postnet_chans: int = 512,
-        postnet_filts: int = 5,
-        postnet_dropout_rate: float = 0.5,
-        positionwise_layer_type: str = "conv1d",
-        positionwise_conv_kernel_size: int = 1,
-        use_scaled_pos_enc: bool = True,
-        use_batch_norm: bool = True,
-        encoder_normalize_before: bool = True,
-        encoder_concat_after: bool = False,
-        reduction_factor: int = 1,
-        encoder_type: str = "transformer",
-        decoder_type: str = "diffusion",
-        transformer_enc_dropout_rate: float = 0.1,
-        transformer_enc_positional_dropout_rate: float = 0.1,
-        transformer_enc_attn_dropout_rate: float = 0.1,
-        # Denoiser Decoder
-        denoiser_layers: int = 20,
-        denoiser_channels: int = 256,
-        diffusion_steps: int = 1000,
-        diffusion_timescale: int = 1,
-        diffusion_beta: float = 40.0,
-        diffusion_scheduler: str = "vpsde",
-        diffusion_cycle_ln: int = 1,
-        # only for conformer
-        conformer_rel_pos_type: str = "legacy",
-        conformer_pos_enc_layer_type: str = "rel_pos",
-        conformer_self_attn_layer_type: str = "rel_selfattn",
-        conformer_activation_type: str = "swish",
-        use_macaron_style_in_conformer: bool = True,
-        use_cnn_in_conformer: bool = True,
-        zero_triu: bool = False,
-        conformer_enc_kernel_size: int = 7,
-        # duration predictor
-        duration_predictor_layers: int = 2,
-        duration_predictor_chans: int = 384,
-        duration_predictor_kernel_size: int = 3,
-        duration_predictor_dropout_rate: float = 0.1,
-        # energy predictor
-        energy_predictor_layers: int = 2,
-        energy_predictor_chans: int = 384,
-        energy_predictor_kernel_size: int = 3,
-        energy_predictor_dropout: float = 0.5,
-        energy_embed_kernel_size: int = 9,
-        energy_embed_dropout: float = 0.5,
-        stop_gradient_from_energy_predictor: bool = False,
-        # pitch predictor
-        pitch_predictor_layers: int = 2,
-        pitch_predictor_chans: int = 384,
-        pitch_predictor_kernel_size: int = 3,
-        pitch_predictor_dropout: float = 0.5,
-        pitch_embed_kernel_size: int = 9,
-        pitch_embed_dropout: float = 0.5,
-        stop_gradient_from_pitch_predictor: bool = False,
-        # extra embedding related
+        vocabs: int,
+        aux_channels: int = 513,
+        hidden_channels: int = 192,
         spks: Optional[int] = None,
         langs: Optional[int] = None,
         spk_embed_dim: Optional[int] = None,
-        spk_embed_integration_type: str = "add",
-        use_gst: bool = False,
-        gst_tokens: int = 10,
-        gst_heads: int = 4,
-        gst_conv_layers: int = 6,
-        gst_conv_chans_list: Sequence[int] = (32, 32, 64, 64, 128, 128),
-        gst_conv_kernel_size: int = 3,
-        gst_conv_stride: int = 2,
-        gst_gru_layers: int = 1,
-        gst_gru_units: int = 128,
-        # training related
-        init_type: str = "xavier_uniform",
-        init_enc_alpha: float = 1.0,
-        init_dec_alpha: float = 1.0,
-        use_masking: bool = False,
-        use_weighted_masking: bool = False,
+        global_channels: int = -1,
+        segment_size: int = 32,
+        text_encoder_attention_heads: int = 2,
+        text_encoder_ffn_expand: int = 4,
+        text_encoder_blocks: int = 6,
+        text_encoder_positionwise_layer_type: str = "conv1d",
+        text_encoder_positionwise_conv_kernel_size: int = 1,
+        text_encoder_positional_encoding_layer_type: str = "rel_pos",
+        text_encoder_self_attention_layer_type: str = "rel_selfattn",
+        text_encoder_activation_type: str = "swish",
+        text_encoder_normalize_before: bool = True,
+        text_encoder_dropout_rate: float = 0.1,
+        text_encoder_positional_dropout_rate: float = 0.0,
+        text_encoder_attention_dropout_rate: float = 0.0,
+        text_encoder_conformer_kernel_size: int = 7,
+        use_macaron_style_in_text_encoder: bool = True,
+        use_conformer_conv_in_text_encoder: bool = True,
+        decoder_kernel_size: int = 7,
+        decoder_channels: int = 512,
+        decoder_downsample_scales: List[int] = [2, 2, 8, 8],
+        decoder_downsample_kernel_sizes: List[int] = [4, 4, 16, 16],
+        decoder_upsample_scales: List[int] = [8, 8, 2, 2],
+        decoder_upsample_kernel_sizes: List[int] = [16, 16, 4, 4],
+        decoder_resblock_kernel_sizes: List[int] = [3, 7, 11],
+        decoder_resblock_dilations: List[List[int]] = [[1, 3, 5], [1, 3, 5], [1, 3, 5]],
+        # avocodo
+        use_avocodo=False,
+        projection_filters: List[int] = [0, 1, 1, 1],
+        projection_kernels: List[int] = [0, 5, 7, 11],
+        # visinger 2
+        n_harmonic: int = 64,
+        use_weight_norm_in_decoder: bool = True,
+        posterior_encoder_kernel_size: int = 5,
+        posterior_encoder_layers: int = 16,
+        posterior_encoder_stacks: int = 1,
+        posterior_encoder_base_dilation: int = 1,
+        posterior_encoder_dropout_rate: float = 0.0,
+        use_weight_norm_in_posterior_encoder: bool = True,
+        flow_flows: int = 4,
+        flow_kernel_size: int = 5,
+        flow_base_dilation: int = 1,
+        flow_layers: int = 4,
+        flow_dropout_rate: float = 0.0,
+        use_weight_norm_in_flow: bool = True,
+        use_only_mean_in_flow: bool = True,
+        generator_type: str = "visinger",
+        vocoder_generator_type: str = "hifigan",
+        fs: int = 22050,
+        hop_length: int = 256,
+        win_length: int = 1024,
+        n_fft: int = 1024,
+        use_phoneme_predictor: bool = False,
+        expand_f0_method: str = "repeat",
     ):
-        """Initialize ProDiff module.
+        """Initialize VITS generator module.
 
         Args:
-            idim (int): Dimension of the inputs.
-            odim (int): Dimension of the outputs.
-            elayers (int): Number of encoder layers.
-            eunits (int): Number of encoder hidden units.
-            dlayers (int): Number of decoder layers.
-            dunits (int): Number of decoder hidden units.
-            postnet_layers (int): Number of postnet layers.
-            postnet_chans (int): Number of postnet channels.
-            postnet_filts (int): Kernel size of postnet.
-            postnet_dropout_rate (float): Dropout rate in postnet.
-            use_scaled_pos_enc (bool): Whether to use trainable scaled pos encoding.
-            use_batch_norm (bool): Whether to use batch normalization in encoder prenet.
-            encoder_normalize_before (bool): Whether to apply layernorm layer before
-                encoder block.
-            decoder_normalize_before (bool): Whether to apply layernorm layer before
-                decoder block.
-            encoder_concat_after (bool): Whether to concatenate attention layer's input
-                and output in encoder.
-            decoder_concat_after (bool): Whether to concatenate attention layer's input
-                and output in decoder.
-            reduction_factor (int): Reduction factor.
-            encoder_type (str): Encoder type ("transformer" or "conformer").
-            decoder_type (str): Decoder type ("transformer" or "conformer").
-            transformer_enc_dropout_rate (float): Dropout rate in encoder except
-                attention and positional encoding.
-            transformer_enc_positional_dropout_rate (float): Dropout rate after encoder
-                positional encoding.
-            transformer_enc_attn_dropout_rate (float): Dropout rate in encoder
-                self-attention module.
-            transformer_dec_dropout_rate (float): Dropout rate in decoder except
-                attention & positional encoding.
-            transformer_dec_positional_dropout_rate (float): Dropout rate after decoder
-                positional encoding.
-            transformer_dec_attn_dropout_rate (float): Dropout rate in decoder
-                self-attention module.
-            conformer_rel_pos_type (str): Relative pos encoding type in conformer.
-            conformer_pos_enc_layer_type (str): Pos encoding layer type in conformer.
-            conformer_self_attn_layer_type (str): Self-attention layer type in conformer
-            conformer_activation_type (str): Activation function type in conformer.
-            use_macaron_style_in_conformer: Whether to use macaron style FFN.
-            use_cnn_in_conformer: Whether to use CNN in conformer.
-            zero_triu: Whether to use zero triu in relative self-attention module.
-            conformer_enc_kernel_size: Kernel size of encoder conformer.
-            conformer_dec_kernel_size: Kernel size of decoder conformer.
-            duration_predictor_layers (int): Number of duration predictor layers.
-            duration_predictor_chans (int): Number of duration predictor channels.
-            duration_predictor_kernel_size (int): Kernel size of duration predictor.
-            duration_predictor_dropout_rate (float): Dropout rate in duration predictor.
-            pitch_predictor_layers (int): Number of pitch predictor layers.
-            pitch_predictor_chans (int): Number of pitch predictor channels.
-            pitch_predictor_kernel_size (int): Kernel size of pitch predictor.
-            pitch_predictor_dropout_rate (float): Dropout rate in pitch predictor.
-            pitch_embed_kernel_size (float): Kernel size of pitch embedding.
-            pitch_embed_dropout_rate (float): Dropout rate for pitch embedding.
-            stop_gradient_from_pitch_predictor: Whether to stop gradient from pitch
-                predictor to encoder.
-            energy_predictor_layers (int): Number of energy predictor layers.
-            energy_predictor_chans (int): Number of energy predictor channels.
-            energy_predictor_kernel_size (int): Kernel size of energy predictor.
-            energy_predictor_dropout_rate (float): Dropout rate in energy predictor.
-            energy_embed_kernel_size (float): Kernel size of energy embedding.
-            energy_embed_dropout_rate (float): Dropout rate for energy embedding.
-            stop_gradient_from_energy_predictor: Whether to stop gradient from energy
-                predictor to encoder.
+            vocabs (int): Input vocabulary size.
+            aux_channels (int): Number of acoustic feature channels.
+            hidden_channels (int): Number of hidden channels.
             spks (Optional[int]): Number of speakers. If set to > 1, assume that the
                 sids will be provided as the input and use sid embedding layer.
             langs (Optional[int]): Number of languages. If set to > 1, assume that the
                 lids will be provided as the input and use sid embedding layer.
             spk_embed_dim (Optional[int]): Speaker embedding dimension. If set to > 0,
                 assume that spembs will be provided as the input.
-            spk_embed_integration_type: How to integrate speaker embedding.
-            use_gst (str): Whether to use global style token.
-            gst_tokens (int): The number of GST embeddings.
-            gst_heads (int): The number of heads in GST multihead attention.
-            gst_conv_layers (int): The number of conv layers in GST.
-            gst_conv_chans_list: (Sequence[int]):
-                List of the number of channels of conv layers in GST.
-            gst_conv_kernel_size (int): Kernel size of conv layers in GST.
-            gst_conv_stride (int): Stride size of conv layers in GST.
-            gst_gru_layers (int): The number of GRU layers in GST.
-            gst_gru_units (int): The number of GRU units in GST.
-            init_type (str): How to initialize transformer parameters.
-            init_enc_alpha (float): Initial value of alpha in scaled pos encoding of the
-                encoder.
-            init_dec_alpha (float): Initial value of alpha in scaled pos encoding of the
+            global_channels (int): Number of global conditioning channels.
+            segment_size (int): Segment size for decoder.
+            text_encoder_attention_heads (int): Number of heads in conformer block
+                of text encoder.
+            text_encoder_ffn_expand (int): Expansion ratio of FFN in conformer block
+                of text encoder.
+            text_encoder_blocks (int): Number of conformer blocks in text encoder.
+            text_encoder_positionwise_layer_type (str): Position-wise layer type in
+                conformer block of text encoder.
+            text_encoder_positionwise_conv_kernel_size (int): Position-wise convolution
+                kernel size in conformer block of text encoder. Only used when the
+                above layer type is conv1d or conv1d-linear.
+            text_encoder_positional_encoding_layer_type (str): Positional encoding layer
+                type in conformer block of text encoder.
+            text_encoder_self_attention_layer_type (str): Self-attention layer type in
+                conformer block of text encoder.
+            text_encoder_activation_type (str): Activation function type in conformer
+                block of text encoder.
+            text_encoder_normalize_before (bool): Whether to apply layer norm before
+                self-attention in conformer block of text encoder.
+            text_encoder_dropout_rate (float): Dropout rate in conformer block of
+                text encoder.
+            text_encoder_positional_dropout_rate (float): Dropout rate for positional
+                encoding in conformer block of text encoder.
+            text_encoder_attention_dropout_rate (float): Dropout rate for attention in
+                conformer block of text encoder.
+            text_encoder_conformer_kernel_size (int): Conformer conv kernel size. It
+                will be used when only use_conformer_conv_in_text_encoder = True.
+            use_macaron_style_in_text_encoder (bool): Whether to use macaron style FFN
+                in conformer block of text encoder.
+            use_conformer_conv_in_text_encoder (bool): Whether to use covolution in
+                conformer block of text encoder.
+            decoder_kernel_size (int): Decoder kernel size.
+            decoder_channels (int): Number of decoder initial channels.
+            decoder_downsample_scales (List[int]): List of downsampling scales in
                 decoder.
-            use_masking (bool): Whether to apply masking for padded part in loss
-                calculation.
-            use_weighted_masking (bool): Whether to apply weighted masking in loss
-                calculation.
-
+            decoder_downsample_kernel_sizes (List[int]): List of kernel sizes for
+                downsampling layers in decoder.
+            decoder_upsample_scales (List[int]): List of upsampling scales in decoder.
+            decoder_upsample_kernel_sizes (List[int]): List of kernel sizes for
+                upsampling layers in decoder.
+            decoder_resblock_kernel_sizes (List[int]): List of kernel sizes for
+                resblocks in decoder.
+            decoder_resblock_dilations (List[List[int]]): List of list of dilations for
+                resblocks in decoder.
+            use_avocodo (bool): Whether to use Avocodo model in the generator.
+            projection_filters (List[int]): List of projection filter sizes.
+            projection_kernels (List[int]): List of projection kernel sizes.
+            n_harmonic (int): Number of harmonic components.
+            use_weight_norm_in_decoder (bool): Whether to apply weight normalization in
+                decoder.
+            posterior_encoder_kernel_size (int): Posterior encoder kernel size.
+            posterior_encoder_layers (int): Number of layers of posterior encoder.
+            posterior_encoder_stacks (int): Number of stacks of posterior encoder.
+            posterior_encoder_base_dilation (int): Base dilation of posterior encoder.
+            posterior_encoder_dropout_rate (float): Dropout rate for posterior encoder.
+            use_weight_norm_in_posterior_encoder (bool): Whether to apply weight
+                normalization in posterior encoder.
+            flow_flows (int): Number of flows in flow.
+            flow_kernel_size (int): Kernel size in flow.
+            flow_base_dilation (int): Base dilation in flow.
+            flow_layers (int): Number of layers in flow.
+            flow_dropout_rate (float): Dropout rate in flow
+            use_weight_norm_in_flow (bool): Whether to apply weight normalization in
+                flow.
+            use_only_mean_in_flow (bool): Whether to use only mean in flow.
+            generator_type (str): Type of generator to use for the model.
+            vocoder_generator_type (str): Type of vocoder generator to use for the
+                model.
+            fs (int): Sample rate of the audio.
+            hop_length (int): Number of samples between successive frames in STFT.
+            win_length (int): Window size of the STFT.
+            n_fft (int): Length of the FFT window to be used.
+            use_phoneme_predictor (bool): Whether to use phoneme predictor in the model.
+            expand_f0_method (str): The method used to expand F0. Use "repeat" or
+                "interpolation".
         """
-        assert check_argument_types()
         super().__init__()
+        self.aux_channels = aux_channels
+        self.hidden_channels = hidden_channels
+        self.generator_type = generator_type
+        self.segment_size = segment_size
+        self.sample_rate = fs
+        self.hop_length = hop_length
+        self.use_avocodo = use_avocodo
+        self.use_flow = True if flow_flows > 0 else False
+        self.use_phoneme_predictor = use_phoneme_predictor
+        self.text_encoder = TextEncoder(
+            vocabs=vocabs,
+            attention_dim=hidden_channels,
+            attention_heads=text_encoder_attention_heads,
+            linear_units=hidden_channels * text_encoder_ffn_expand,
+            blocks=text_encoder_blocks,
+            positionwise_layer_type=text_encoder_positionwise_layer_type,
+            positionwise_conv_kernel_size=text_encoder_positionwise_conv_kernel_size,
+            positional_encoding_layer_type=text_encoder_positional_encoding_layer_type,
+            self_attention_layer_type=text_encoder_self_attention_layer_type,
+            activation_type=text_encoder_activation_type,
+            normalize_before=text_encoder_normalize_before,
+            dropout_rate=text_encoder_dropout_rate,
+            positional_dropout_rate=text_encoder_positional_dropout_rate,
+            attention_dropout_rate=text_encoder_attention_dropout_rate,
+            conformer_kernel_size=text_encoder_conformer_kernel_size,
+            use_macaron_style=use_macaron_style_in_text_encoder,
+            use_conformer_conv=use_conformer_conv_in_text_encoder,
+        )
+        if vocoder_generator_type == "uhifigan":
+            self.decoder = UHiFiGANGenerator(
+                in_channels=hidden_channels,
+                out_channels=1,
+                channels=decoder_channels,
+                global_channels=global_channels,
+                kernel_size=decoder_kernel_size,
+                downsample_scales=decoder_downsample_scales,
+                downsample_kernel_sizes=decoder_downsample_kernel_sizes,
+                upsample_scales=decoder_upsample_scales,
+                upsample_kernel_sizes=decoder_upsample_kernel_sizes,
+                resblock_kernel_sizes=decoder_resblock_kernel_sizes,
+                resblock_dilations=decoder_resblock_dilations,
+                use_weight_norm=use_weight_norm_in_decoder,
+                use_avocodo=use_avocodo,
+            )
+            self.sine_generator = SineGen(
+                sample_rate=fs,
+            )
+        elif vocoder_generator_type == "hifigan":
+            self.decoder = HiFiGANGenerator(
+                in_channels=hidden_channels,
+                out_channels=1,
+                channels=decoder_channels,
+                global_channels=global_channels,
+                kernel_size=decoder_kernel_size,
+                upsample_scales=decoder_upsample_scales,
+                upsample_kernel_sizes=decoder_upsample_kernel_sizes,
+                resblock_kernel_sizes=decoder_resblock_kernel_sizes,
+                resblock_dilations=decoder_resblock_dilations,
+                use_weight_norm=use_weight_norm_in_decoder,
+            )
+        elif vocoder_generator_type == "avocodo":
+            self.decoder = AvocodoGenerator(
+                in_channels=hidden_channels,
+                out_channels=1,
+                channels=decoder_channels,
+                global_channels=global_channels,
+                kernel_size=decoder_kernel_size,
+                upsample_scales=decoder_upsample_scales,
+                upsample_kernel_sizes=decoder_upsample_kernel_sizes,
+                resblock_kernel_sizes=decoder_resblock_kernel_sizes,
+                resblock_dilations=decoder_resblock_dilations,
+                projection_filters=projection_filters,
+                projection_kernels=projection_kernels,
+                use_weight_norm=use_weight_norm_in_decoder,
+            )
+        elif vocoder_generator_type == "visinger2":
+            self.decoder = VISinger2VocoderGenerator(
+                in_channels=hidden_channels,
+                out_channels=1,
+                channels=decoder_channels,
+                global_channels=global_channels,
+                kernel_size=decoder_kernel_size,
+                upsample_scales=decoder_upsample_scales,
+                upsample_kernel_sizes=decoder_upsample_kernel_sizes,
+                resblock_kernel_sizes=decoder_resblock_kernel_sizes,
+                resblock_dilations=decoder_resblock_dilations,
+                use_weight_norm=use_weight_norm_in_decoder,
+                n_harmonic=n_harmonic,
+            )
+            self.dec_harm = Generator_Harm(
+                hidden_channels=hidden_channels,
+                n_harmonic=n_harmonic,
+                kernel_size=3,
+                padding=1,
+                dropout_rate=0.1,
+                sample_rate=fs,
+                hop_size=hop_length,
+            )
+            self.dec_noise = Generator_Noise(
+                win_length=win_length,
+                hop_length=hop_length,
+                n_fft=n_fft,
+                hidden_channels=hidden_channels,
+                kernel_size=3,
+                padding=1,
+                dropout_rate=0.1,
+            )
+            self.sin_prenet = torch.nn.Conv1d(1, n_harmonic + 2, 3, padding=1)
+        else:
+            raise ValueError(
+                f"Not supported vocoder generator type: {vocoder_generator_type}"
+            )
+        self.posterior_encoder = PosteriorEncoder(
+            in_channels=aux_channels,
+            out_channels=hidden_channels,
+            hidden_channels=hidden_channels,
+            kernel_size=posterior_encoder_kernel_size,
+            layers=posterior_encoder_layers,
+            stacks=posterior_encoder_stacks,
+            base_dilation=posterior_encoder_base_dilation,
+            global_channels=global_channels,
+            dropout_rate=posterior_encoder_dropout_rate,
+            use_weight_norm=use_weight_norm_in_posterior_encoder,
+        )
+        if self.use_flow:
+            self.flow = ResidualAffineCouplingBlock(
+                in_channels=hidden_channels,
+                hidden_channels=hidden_channels,
+                flows=flow_flows,
+                kernel_size=flow_kernel_size,
+                base_dilation=flow_base_dilation,
+                layers=flow_layers,
+                global_channels=global_channels,
+                dropout_rate=flow_dropout_rate,
+                use_weight_norm=use_weight_norm_in_flow,
+                use_only_mean=use_only_mean_in_flow,
+            )
 
-        # store hyperparameters
-        self.idim = idim
-        self.odim = odim
-        self.eos = idim - 1
-        self.reduction_factor = reduction_factor
-        self.encoder_type = encoder_type
-        self.decoder_type = decoder_type
-        self.stop_gradient_from_pitch_predictor = stop_gradient_from_pitch_predictor
-        self.stop_gradient_from_energy_predictor = stop_gradient_from_energy_predictor
-        self.use_scaled_pos_enc = use_scaled_pos_enc
-        self.use_gst = use_gst
-
-        # use idx 0 as padding idx
-        self.padding_idx = 0
-
-        # get positional encoding class
-        pos_enc_class = (
-            ScaledPositionalEncoding if self.use_scaled_pos_enc else PositionalEncoding
-        )
-
-        # check relative positional encoding compatibility
-        if "conformer" in [encoder_type, decoder_type]:
-            if conformer_rel_pos_type == "legacy":
-                if conformer_pos_enc_layer_type == "rel_pos":
-                    conformer_pos_enc_layer_type = "legacy_rel_pos"
-                    logging.warning(
-                        "Fallback to conformer_pos_enc_layer_type = 'legacy_rel_pos' "
-                        "due to the compatibility. If you want to use the new one, "
-                        "please use conformer_pos_enc_layer_type = 'latest'."
-                    )
-                if conformer_self_attn_layer_type == "rel_selfattn":
-                    conformer_self_attn_layer_type = "legacy_rel_selfattn"
-                    logging.warning(
-                        "Fallback to "
-                        "conformer_self_attn_layer_type = 'legacy_rel_selfattn' "
-                        "due to the compatibility. If you want to use the new one, "
-                        "please use conformer_pos_enc_layer_type = 'latest'."
-                    )
-            elif conformer_rel_pos_type == "latest":
-                assert conformer_pos_enc_layer_type != "legacy_rel_pos"
-                assert conformer_self_attn_layer_type != "legacy_rel_selfattn"
-            else:
-                raise ValueError(f"Unknown rel_pos_type: {conformer_rel_pos_type}")
+        self.f0_prenet = torch.nn.Conv1d(1, hidden_channels + 2, 3, padding=1)
 
-        # define encoder
-        encoder_input_layer = torch.nn.Embedding(
-            num_embeddings=idim, embedding_dim=adim, padding_idx=self.padding_idx
-        )
-        if encoder_type == "transformer":
-            self.encoder = TransformerEncoder(
-                idim=idim,
-                attention_dim=adim,
-                attention_heads=aheads,
-                linear_units=eunits,
-                num_blocks=elayers,
-                input_layer=encoder_input_layer,
-                dropout_rate=transformer_enc_dropout_rate,
-                positional_dropout_rate=transformer_enc_positional_dropout_rate,
-                attention_dropout_rate=transformer_enc_attn_dropout_rate,
-                pos_enc_class=pos_enc_class,
-                normalize_before=encoder_normalize_before,
-                concat_after=encoder_concat_after,
-                positionwise_layer_type=positionwise_layer_type,
-                positionwise_conv_kernel_size=positionwise_conv_kernel_size,
-            )
-        elif encoder_type == "conformer":
-            self.encoder = ConformerEncoder(
-                idim=idim,
-                attention_dim=adim,
-                attention_heads=aheads,
-                linear_units=eunits,
-                num_blocks=elayers,
-                input_layer=encoder_input_layer,
-                dropout_rate=transformer_enc_dropout_rate,
-                positional_dropout_rate=transformer_enc_positional_dropout_rate,
-                attention_dropout_rate=transformer_enc_attn_dropout_rate,
-                normalize_before=encoder_normalize_before,
-                concat_after=encoder_concat_after,
-                positionwise_layer_type=positionwise_layer_type,
-                positionwise_conv_kernel_size=positionwise_conv_kernel_size,
-                macaron_style=use_macaron_style_in_conformer,
-                pos_enc_layer_type=conformer_pos_enc_layer_type,
-                selfattention_layer_type=conformer_self_attn_layer_type,
-                activation_type=conformer_activation_type,
-                use_cnn_module=use_cnn_in_conformer,
-                cnn_module_kernel=conformer_enc_kernel_size,
-                zero_triu=zero_triu,
+        if generator_type == "visinger2":
+            self.energy_prenet = torch.nn.Conv1d(1, hidden_channels + 2, 3, padding=1)
+            self.mel_prenet = torch.nn.Conv1d(
+                aux_channels, hidden_channels + 2, 3, padding=1
             )
-        else:
-            raise ValueError(f"{encoder_type} is not supported.")
 
-        # define GST
-        if self.use_gst:
-            self.gst = StyleEncoder(
-                idim=odim,  # the input is mel-spectrogram
-                gst_tokens=gst_tokens,
-                gst_token_dim=adim,
-                gst_heads=gst_heads,
-                conv_layers=gst_conv_layers,
-                conv_chans_list=gst_conv_chans_list,
-                conv_kernel_size=gst_conv_kernel_size,
-                conv_stride=gst_conv_stride,
-                gru_layers=gst_gru_layers,
-                gru_units=gst_gru_units,
+        # TODO(kan-bayashi): Add deterministic version as an option
+
+        self.duration_predictor = DurationPredictor(
+            channels=hidden_channels,
+            filter_channels=256,
+            kernel_size=3,
+            dropout_rate=0.5,
+            global_channels=global_channels,
+        )
+
+        self.lr = LengthRegulator()
+
+        if self.use_phoneme_predictor:
+            self.phoneme_predictor = PhonemePredictor(
+                vocabs=vocabs,
+                hidden_channels=hidden_channels,
+                attention_dim=hidden_channels,
+                blocks=2,
             )
 
-        # define spk and lang embedding
+        self.f0_decoder = Decoder(
+            1,
+            attention_dim=hidden_channels,
+            attention_heads=text_encoder_attention_heads,
+            linear_units=hidden_channels * text_encoder_ffn_expand,
+            blocks=text_encoder_blocks,
+            pw_layer_type=text_encoder_positionwise_layer_type,
+            pw_conv_kernel_size=text_encoder_positionwise_conv_kernel_size,
+            pos_enc_layer_type=text_encoder_positional_encoding_layer_type,
+            self_attention_layer_type=text_encoder_self_attention_layer_type,
+            activation_type=text_encoder_activation_type,
+            normalize_before=text_encoder_normalize_before,
+            dropout_rate=text_encoder_dropout_rate,
+            positional_dropout_rate=text_encoder_positional_dropout_rate,
+            attention_dropout_rate=text_encoder_attention_dropout_rate,
+            conformer_kernel_size=text_encoder_conformer_kernel_size,
+            use_macaron_style=use_macaron_style_in_text_encoder,
+            use_conformer_conv=use_conformer_conv_in_text_encoder,
+            global_channels=global_channels,
+        )
+
+        if self.generator_type == "visinger2":
+            self.mel_decoder = Decoder(
+                out_channels=aux_channels,
+                attention_dim=hidden_channels,
+                attention_heads=text_encoder_attention_heads,
+                linear_units=hidden_channels * text_encoder_ffn_expand,
+                blocks=text_encoder_blocks,
+                pw_layer_type=text_encoder_positionwise_layer_type,
+                pw_conv_kernel_size=text_encoder_positionwise_conv_kernel_size,
+                pos_enc_layer_type=text_encoder_positional_encoding_layer_type,
+                self_attention_layer_type=text_encoder_self_attention_layer_type,
+                activation_type=text_encoder_activation_type,
+                normalize_before=text_encoder_normalize_before,
+                dropout_rate=text_encoder_dropout_rate,
+                positional_dropout_rate=text_encoder_positional_dropout_rate,
+                attention_dropout_rate=text_encoder_attention_dropout_rate,
+                conformer_kernel_size=text_encoder_conformer_kernel_size,
+                use_macaron_style=use_macaron_style_in_text_encoder,
+                use_conformer_conv=use_conformer_conv_in_text_encoder,
+                global_channels=global_channels,
+            )
+
+        self.prior_decoder = PriorDecoder(
+            out_channels=hidden_channels * 2,
+            attention_dim=hidden_channels,
+            attention_heads=text_encoder_attention_heads,
+            linear_units=hidden_channels * text_encoder_ffn_expand,
+            blocks=text_encoder_blocks,
+            positionwise_layer_type=text_encoder_positionwise_layer_type,
+            positionwise_conv_kernel_size=text_encoder_positionwise_conv_kernel_size,
+            positional_encoding_layer_type=text_encoder_positional_encoding_layer_type,
+            self_attention_layer_type=text_encoder_self_attention_layer_type,
+            activation_type=text_encoder_activation_type,
+            normalize_before=text_encoder_normalize_before,
+            dropout_rate=text_encoder_dropout_rate,
+            positional_dropout_rate=text_encoder_positional_dropout_rate,
+            attention_dropout_rate=text_encoder_attention_dropout_rate,
+            conformer_kernel_size=text_encoder_conformer_kernel_size,
+            use_macaron_style=use_macaron_style_in_text_encoder,
+            use_conformer_conv=use_conformer_conv_in_text_encoder,
+            global_channels=global_channels,
+        )
+
+        self.upsample_factor = int(np.prod(decoder_upsample_scales))
         self.spks = None
+
         if spks is not None and spks > 1:
+            assert global_channels > 0
             self.spks = spks
-            self.sid_emb = torch.nn.Embedding(spks, adim)
-        self.langs = None
-        if langs is not None and langs > 1:
-            self.langs = langs
-            self.lid_emb = torch.nn.Embedding(langs, adim)
-
-        # define additional projection for speaker embedding
+            self.global_emb = torch.nn.Embedding(spks, global_channels)
         self.spk_embed_dim = None
         if spk_embed_dim is not None and spk_embed_dim > 0:
+            assert global_channels > 0
             self.spk_embed_dim = spk_embed_dim
-            self.spk_embed_integration_type = spk_embed_integration_type
-        if self.spk_embed_dim is not None:
-            if self.spk_embed_integration_type == "add":
-                self.projection = torch.nn.Linear(self.spk_embed_dim, adim)
-            else:
-                self.projection = torch.nn.Linear(adim + self.spk_embed_dim, adim)
-
-        # define duration predictor
-        self.duration_predictor = DurationPredictor(
-            idim=adim,
-            n_layers=duration_predictor_layers,
-            n_chans=duration_predictor_chans,
-            kernel_size=duration_predictor_kernel_size,
-            dropout_rate=duration_predictor_dropout_rate,
-        )
-
-        # define pitch predictor
-        self.pitch_predictor = VariancePredictor(
-            idim=adim,
-            n_layers=pitch_predictor_layers,
-            n_chans=pitch_predictor_chans,
-            kernel_size=pitch_predictor_kernel_size,
-            dropout_rate=pitch_predictor_dropout,
-        )
-        # NOTE(kan-bayashi): We use continuous pitch + FastPitch style avg
-        self.pitch_embed = torch.nn.Sequential(
-            torch.nn.Conv1d(
-                in_channels=1,
-                out_channels=adim,
-                kernel_size=pitch_embed_kernel_size,
-                padding=(pitch_embed_kernel_size - 1) // 2,
-            ),
-            torch.nn.Dropout(pitch_embed_dropout),
-        )
-
-        # define energy predictor
-        self.energy_predictor = VariancePredictor(
-            idim=adim,
-            n_layers=energy_predictor_layers,
-            n_chans=energy_predictor_chans,
-            kernel_size=energy_predictor_kernel_size,
-            dropout_rate=energy_predictor_dropout,
-        )
-        # NOTE(kan-bayashi): We use continuous enegy + FastPitch style avg
-        self.energy_embed = torch.nn.Sequential(
-            torch.nn.Conv1d(
-                in_channels=1,
-                out_channels=adim,
-                kernel_size=energy_embed_kernel_size,
-                padding=(energy_embed_kernel_size - 1) // 2,
-            ),
-            torch.nn.Dropout(energy_embed_dropout),
-        )
-
-        # define length regulator
-        self.length_regulator = LengthRegulator()
-
-        # define decoder
-        if decoder_type == "diffusion":
-            self.decoder = SpectogramDenoiser(
-                odim,
-                adim=adim,
-                layers=denoiser_layers,
-                channels=denoiser_channels,
-                timesteps=diffusion_steps,
-                timescale=diffusion_timescale,
-                max_beta=diffusion_beta,
-                scheduler=diffusion_scheduler,
-                cycle_length=diffusion_cycle_ln,
-            )
-        else:
-            raise NotImplementedError(decoder_type)
-
-        # define final projection
-        if decoder_type != "diffusion":
-            self.feat_out = torch.nn.Linear(adim, odim * reduction_factor)
-        if reduction_factor > 1:
-            raise NotImplementedError()
-
-        # define postnet
-        self.postnet = (
-            None
-            if postnet_layers == 0
-            else Postnet(
-                idim=idim,
-                odim=odim,
-                n_layers=postnet_layers,
-                n_chans=postnet_chans,
-                n_filts=postnet_filts,
-                use_batch_norm=use_batch_norm,
-                dropout_rate=postnet_dropout_rate,
-            )
-        )
+            self.spemb_proj = torch.nn.Linear(spk_embed_dim, global_channels)
+        self.langs = None
+        if langs is not None and langs > 1:
+            assert global_channels > 0
+            self.langs = langs
+            self.lang_emb = torch.nn.Embedding(langs, global_channels)
 
-        # initialize parameters
-        self._reset_parameters(
-            init_type=init_type,
-            init_enc_alpha=init_enc_alpha,
-            init_dec_alpha=init_dec_alpha,
-        )
-
-        # define criterions
-        self.criterion = ProDiffLoss(
-            use_masking=use_masking, use_weighted_masking=use_weighted_masking
-        )
+        self.vocoder_generator_type = vocoder_generator_type
+        self.dropout = torch.nn.Dropout(0.2)
+        self.expand_f0_method = expand_f0_method
 
     def forward(
         self,
         text: torch.Tensor,
         text_lengths: torch.Tensor,
         feats: torch.Tensor,
         feats_lengths: torch.Tensor,
-        durations: torch.Tensor,
-        durations_lengths: torch.Tensor,
-        pitch: torch.Tensor,
-        pitch_lengths: torch.Tensor,
-        energy: torch.Tensor,
-        energy_lengths: torch.Tensor,
-        spembs: Optional[torch.Tensor] = None,
+        label: torch.Tensor = None,
+        label_lengths: torch.Tensor = None,
+        melody: torch.Tensor = None,
+        gt_dur: torch.Tensor = None,
+        score_dur: torch.Tensor = None,
+        slur: torch.Tensor = None,
+        pitch: torch.Tensor = None,
+        ying: Optional[torch.Tensor] = None,
         sids: Optional[torch.Tensor] = None,
+        spembs: Optional[torch.Tensor] = None,
         lids: Optional[torch.Tensor] = None,
-        joint_training: bool = False,
-    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor]:
+    ) -> Tuple[
+        torch.Tensor,
+        torch.Tensor,
+        torch.Tensor,
+        torch.Tensor,
+        torch.Tensor,
+        torch.Tensor,
+        Tuple[
+            torch.Tensor,
+            torch.Tensor,
+            torch.Tensor,
+            torch.Tensor,
+            torch.Tensor,
+            torch.Tensor,
+        ],
+    ]:
         """Calculate forward propagation.
 
         Args:
-            text (LongTensor): Batch of padded token ids (B, T_text).
-            text_lengths (LongTensor): Batch of lengths of each input (B,).
-            feats (Tensor): Batch of padded target features (B, T_feats, odim).
+            text (LongTensor): Batch of padded character ids (B, Tmax).
+            text_lengths (LongTensor): Batch of lengths of each input batch (B,).
+            feats (Tensor): Batch of padded target features (B, Lmax, odim).
             feats_lengths (LongTensor): Batch of the lengths of each target (B,).
-            durations (LongTensor): Batch of padded durations (B, T_text + 1).
-            durations_lengths (LongTensor): Batch of duration lengths (B, T_text + 1).
-            pitch (Tensor): Batch of padded token-averaged pitch (B, T_text + 1, 1).
-            pitch_lengths (LongTensor): Batch of pitch lengths (B, T_text + 1).
-            energy (Tensor): Batch of padded token-averaged energy (B, T_text + 1, 1).
-            energy_lengths (LongTensor): Batch of energy lengths (B, T_text + 1).
+            label (LongTensor): Batch of padded label ids (B, Tmax).
+            label_lengths (LongTensor): Batch of the lengths of padded label ids (B, ).
+            melody (LongTensor): Batch of padded midi (B, Tmax).
+            gt_dur (LongTensor): Batch of padded ground truth duration (B, Tmax).
+            score_dur (LongTensor): Batch of padded score duration (B, Tmax).
+            pitch (FloatTensor): Batch of padded f0 (B, Tmax).
+            ying (Optional[Tensor]): Batch of padded ying (B, Tmax).
             spembs (Optional[Tensor]): Batch of speaker embeddings (B, spk_embed_dim).
             sids (Optional[Tensor]): Batch of speaker IDs (B, 1).
             lids (Optional[Tensor]): Batch of language IDs (B, 1).
-            joint_training (bool): Whether to perform joint training with vocoder.
 
         Returns:
-            Tensor: Loss scalar value.
-            Dict: Statistics to be monitored.
-            Tensor: Weight value if not joint training else model outputs.
+            Tensor: Waveform tensor (B, 1, segment_size * upsample_factor).
+            Tensor: Duration negative log-likelihood (NLL) tensor (B,).
+            Tensor: Monotonic attention weight tensor (B, 1, T_feats, T_text).
+            Tensor: Segments start index tensor (B,).
+            Tensor: Text mask tensor (B, 1, T_text).
+            Tensor: Feature mask tensor (B, 1, T_feats).
+            tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]:
+                - Tensor: Posterior encoder hidden representation (B, H, T_feats).
+                - Tensor: Flow hidden representation (B, H, T_feats).
+                - Tensor: Expanded text encoder projected mean (B, H, T_feats).
+                - Tensor: Expanded text encoder projected scale (B, H, T_feats).
+                - Tensor: Posterior encoder projected mean (B, H, T_feats).
+                - Tensor: Posterior encoder projected scale (B, H, T_feats).
 
         """
-        text = text[:, : text_lengths.max()]  # for data-parallel
-        feats = feats[:, : feats_lengths.max()]  # for data-parallel
-        durations = durations[:, : durations_lengths.max()]  # for data-parallel
-        pitch = pitch[:, : pitch_lengths.max()]  # for data-parallel
-        energy = energy[:, : energy_lengths.max()]  # for data-parallel
-
-        batch_size = text.size(0)
-
-        # Add eos at the last of sequence
-        xs = F.pad(text, [0, 1], "constant", self.padding_idx)
-        for i, l in enumerate(text_lengths):
-            xs[i, l] = self.eos
-        ilens = text_lengths + 1
-
-        ys, ds, ps, es = feats, durations, pitch, energy
-        olens = feats_lengths
-
-        # forward propagation
-        before_outs, after_outs, d_outs, p_outs, e_outs = self._forward(
-            xs,
-            ilens,
-            ys,
-            olens,
-            ds,
-            ps,
-            es,
-            spembs=spembs,
-            sids=sids,
-            lids=lids,
-            is_inference=False,
-        )
-
-        # modify mod part of groundtruth
-        if self.reduction_factor > 1:
-            olens = olens.new([olen - olen % self.reduction_factor for olen in olens])
-            max_olen = max(olens)
-            ys = ys[:, :max_olen]
-
-        # calculate loss
-        if self.postnet is None:
-            after_outs = None
-
-        # calculate loss
-        l1_loss, ssim_loss, duration_loss, pitch_loss, energy_loss = self.criterion(
-            after_outs=after_outs,
-            before_outs=before_outs,
-            d_outs=d_outs,
-            p_outs=p_outs,
-            e_outs=e_outs,
-            ys=ys,
-            ds=ds,
-            ps=ps,
-            es=es,
-            ilens=ilens,
-            olens=olens,
-        )
-        loss = l1_loss + ssim_loss + duration_loss + pitch_loss + energy_loss
-
-        stats = dict(
-            l1_loss=l1_loss.item(),
-            ssim_loss=ssim_loss.item(),
-            duration_loss=duration_loss.item(),
-            pitch_loss=pitch_loss.item(),
-            energy_loss=energy_loss.item(),
-        )
-
-        # report extra information
-        if self.encoder_type == "transformer" and self.use_scaled_pos_enc:
-            stats.update(
-                encoder_alpha=self.encoder.embed[-1].alpha.data.item(),
-            )
-        if self.decoder_type == "transformer" and self.use_scaled_pos_enc:
-            stats.update(
-                decoder_alpha=self.decoder.embed[-1].alpha.data.item(),
-            )
-
-        if not joint_training:
-            stats.update(loss=loss.item())
-            loss, stats, weight = force_gatherable(
-                (loss, stats, batch_size), loss.device
-            )
-            return loss, stats, weight
-        else:
-            return loss, stats, after_outs if after_outs is not None else before_outs
+        # calculate global conditioning
+        g = None
+        if self.spks is not None:
+            # speaker one-hot vector embedding: (B, global_channels, 1)
+            g = self.global_emb(sids.view(-1)).unsqueeze(-1)
+        if self.spk_embed_dim is not None:
+            # pretreined speaker embedding, e.g., X-vector (B, global_channels, 1)
+            g_ = self.spemb_proj(F.normalize(spembs)).unsqueeze(-1)
+            if g is None:
+                g = g_
+            else:
+                g = g + g_
+        if self.langs is not None:
+            # language one-hot vector embedding: (B, global_channels, 1)
+            g_ = self.lang_emb(lids.view(-1)).unsqueeze(-1)
+            if g is None:
+                g = g_
+            else:
+                g = g + g_
 
-    def _forward(
-        self,
-        xs: torch.Tensor,
-        ilens: torch.Tensor,
-        ys: Optional[torch.Tensor] = None,
-        olens: Optional[torch.Tensor] = None,
-        ds: Optional[torch.Tensor] = None,
-        ps: Optional[torch.Tensor] = None,
-        es: Optional[torch.Tensor] = None,
-        spembs: Optional[torch.Tensor] = None,
-        sids: Optional[torch.Tensor] = None,
-        lids: Optional[torch.Tensor] = None,
-        is_inference: bool = False,
-        alpha: float = 1.0,
-    ) -> Sequence[torch.Tensor]:
-        """Calculate forward propagation without loss.
+        # forward text encoder
 
-        Args:
-            xs (Tensor): Batch of padded target features (B, T_feats, odim).
-            ilens (LongTensor): Batch of the lengths of each target (B,).
+        # Encoder
+        x, x_mask, dur_input, x_pitch = self.text_encoder(
+            label, label_lengths, melody, score_dur, slur
+        )
 
-        Returns:
-            Tensor: Weight value if not joint training else model outputs.
+        # dur
+        # Note this is different, we use frame level duration not time level
+        # but it has no big difference on performance
+        predict_dur = self.duration_predictor(dur_input, x_mask, g=g)
+        predict_dur = (torch.exp(predict_dur) - 1) * x_mask
+        predict_dur = predict_dur * self.sample_rate / self.hop_length
 
-        """
-        # forward encoder
-        x_masks = self._source_mask(ilens)
-        hs, _ = self.encoder(xs, x_masks)  # (B, T_text, adim)
-
-        # integrate with GST
-        if self.use_gst:
-            style_embs = self.gst(ys)
-            hs = hs + style_embs.unsqueeze(1)
+        # LR
+        decoder_input, mel_len = self.lr(x, gt_dur, use_state_info=True)
+        decoder_input_pitch, mel_len = self.lr(x_pitch, gt_dur, use_state_info=True)
 
-        # integrate with SID and LID embeddings
-        if self.spks is not None:
-            sid_embs = self.sid_emb(sids.view(-1))
-            hs = hs + sid_embs.unsqueeze(1)
-        if self.langs is not None:
-            lid_embs = self.lid_emb(lids.view(-1))
-            hs = hs + lid_embs.unsqueeze(1)
+        LF0 = 2595.0 * torch.log10(1.0 + pitch / 700.0)
+        LF0 = LF0 / 500
+        LF0 = LF0.transpose(1, 2)
 
-        # integrate speaker embedding
-        if self.spk_embed_dim is not None:
-            hs = self._integrate_with_spk_embed(hs, spembs)
+        predict_lf0, predict_bn_mask = self.f0_decoder(
+            decoder_input + decoder_input_pitch, feats_lengths, g=g
+        )
+        predict_lf0 = torch.max(
+            predict_lf0, torch.zeros_like(predict_lf0).to(predict_lf0)
+        )
 
-        # forward duration predictor and variance predictors
-        d_masks = make_pad_mask(ilens).to(xs.device)
+        if self.generator_type == "visinger2":
+            predict_mel, predict_bn_mask = self.mel_decoder(
+                decoder_input + self.f0_prenet(LF0), feats_lengths, g=g
+            )
 
-        if self.stop_gradient_from_pitch_predictor:
-            p_outs = self.pitch_predictor(hs.detach(), d_masks.unsqueeze(-1))
-        else:
-            p_outs = self.pitch_predictor(hs, d_masks.unsqueeze(-1))
-        if self.stop_gradient_from_energy_predictor:
-            e_outs = self.energy_predictor(hs.detach(), d_masks.unsqueeze(-1))
-        else:
-            e_outs = self.energy_predictor(hs, d_masks.unsqueeze(-1))
+            predict_energy = (
+                predict_mel.detach().sum(1).unsqueeze(1) / self.aux_channels
+            )
 
-        if is_inference:
-            d_outs = self.duration_predictor.inference(hs, d_masks)  # (B, T_text)
-            # use prediction in inference
-            p_embs = self.pitch_embed(p_outs.transpose(1, 2)).transpose(1, 2)
-            e_embs = self.energy_embed(e_outs.transpose(1, 2)).transpose(1, 2)
-            hs = hs + e_embs + p_embs
-            hs = self.length_regulator(hs, d_outs, alpha)  # (B, T_feats, adim)
-        else:
-            d_outs = self.duration_predictor(hs, d_masks)
-            # use groundtruth in training
-            p_embs = self.pitch_embed(ps.transpose(1, 2)).transpose(1, 2)
-            e_embs = self.energy_embed(es.transpose(1, 2)).transpose(1, 2)
-            hs = hs + e_embs + p_embs
-            hs = self.length_regulator(hs, ds)  # (B, T_feats, adim)
-
-        # forward decoder
-        if olens is not None and not is_inference:
-            if self.reduction_factor > 1:
-                olens_in = olens.new([olen // self.reduction_factor for olen in olens])
-            else:
-                olens_in = olens
-            h_masks = self._source_mask(olens_in)
+        decoder_input = decoder_input + self.f0_prenet(LF0)
+        if self.generator_type == "visinger2":
+            decoder_input = (
+                decoder_input
+                + self.energy_prenet(predict_energy)
+                + self.mel_prenet(predict_mel.detach())
+            )
+        decoder_output, predict_bn_mask = self.prior_decoder(
+            decoder_input, feats_lengths, g=g
+        )
+
+        prior_info = decoder_output
+        prior_mean = prior_info[:, : self.hidden_channels, :]
+        prior_logstd = prior_info[:, self.hidden_channels :, :]
+
+        # forward posterior encoder
+        posterior_z, posterior_mean, posterior_logstd, y_mask = self.posterior_encoder(
+            feats, feats_lengths, g=g
+        )
+
+        if self.use_flow:
+            z_flow = self.flow(posterior_z, y_mask, g=g)
         else:
-            h_masks = None
+            z_flow = None
 
-        if self.decoder_type == "diffusion":
-            before_outs = self.decoder(
-                hs, ys, h_masks, is_inference
-            )  # (B, T_feats, odim)
+        # phoneme predictor
+        if self.use_phoneme_predictor:
+            log_probs = self.phoneme_predictor(posterior_z, y_mask)
         else:
-            zs, _ = self.decoder(hs, h_masks)  # (B, T_feats, adim)
-            before_outs = self.feat_out(zs).view(
-                zs.size(0), -1, self.odim
-            )  # (B, T_feats, odim)
-
-        # postnet -> (B, T_feats//r * r, odim)
-        if self.postnet is None:
-            after_outs = before_outs
+            log_probs = None
+
+        p_z = posterior_z
+        p_z = self.dropout(p_z)
+
+        # get random segments
+        z_segments, z_start_idxs = get_random_segments(
+            p_z, feats_lengths, self.segment_size
+        )
+
+        if self.vocoder_generator_type == "uhifigan":
+            # get sine wave
+
+            # def plot_sine_waves(sine_waves, name):
+            #     import matplotlib.pyplot as plt
+
+            #     sine_waves_np = sine_waves[0].detach().cpu().numpy()
+            #     plt.plot(sine_waves_np)
+            #     plt.xlabel("Time (samples)")
+            #     plt.ylabel("Amplitude")
+            #     plt.title("Sine Wave")
+            #     plt.savefig(name + ".png")
+            #     plt.close()
+
+            # plot_sine_waves(pitch_segments[0], "pitch_segments")
+
+            pitch_segments = get_segments(
+                pitch.transpose(1, 2), z_start_idxs, self.segment_size
+            )
+            pitch_segments_expended = expand_f0(
+                pitch_segments, self.hop_length, method="repeat"
+            )
+
+            # plot_sine_waves(
+            #     pitch_segments_expended[0].unsqueeze(0), "pitch_segments_expended"
+            # )
+            pitch_segments_expended = pitch_segments_expended.reshape(
+                -1, pitch_segments_expended.shape[-1], 1
+            )
+
+            sine_waves, uv, noise = self.sine_generator(pitch_segments_expended)
+
+            sine_waves = sine_waves.transpose(1, 2)
+
+            wav = self.decoder(z_segments, excitation=sine_waves, g=g)
+        elif self.vocoder_generator_type == "visinger2":
+            pitch_ = upsample(pitch, self.hop_length)
+            omega = torch.cumsum(2 * math.pi * pitch_ / self.sample_rate, 1)
+            sin = torch.sin(omega).transpose(1, 2)
+
+            # dsp synthesize
+            pitch = pitch.transpose(1, 2)
+            noise_x = self.dec_noise(posterior_z, y_mask)
+            harm_x = self.dec_harm(pitch, posterior_z, y_mask)
+
+            # dsp waveform
+            dsp_o = torch.cat([harm_x, noise_x], axis=1)
+
+            # decoder_condition = torch.cat([harm_x, noise_x, sin], axis=1)
+            decoder_condition = self.sin_prenet(sin)
+
+            # dsp based HiFiGAN vocoder
+            F0_slice = get_segments(pitch, z_start_idxs, self.segment_size)
+            dsp_slice = get_segments(
+                dsp_o,
+                z_start_idxs * self.hop_length,
+                self.segment_size * self.hop_length,
+            )
+
+            condition_slice = get_segments(
+                decoder_condition,
+                z_start_idxs * self.hop_length,
+                self.segment_size * self.hop_length,
+            )
+            wav = self.decoder(z_segments, condition_slice, g=g)
         else:
-            after_outs = before_outs + self.postnet(
-                before_outs.transpose(1, 2)
-            ).transpose(1, 2)
+            wav = self.decoder(z_segments, g=g)
 
-        return before_outs, after_outs, d_outs, p_outs, e_outs
+            # wav = dsp_slice.sum(1, keepdim=True)
+
+        common_tuple = (
+            posterior_z,
+            z_flow,
+            prior_mean,
+            prior_logstd,
+            posterior_mean,
+            posterior_logstd,
+            predict_lf0,
+            LF0 * predict_bn_mask,
+            predict_dur,
+            gt_dur,
+            log_probs,
+        )
+
+        output = (wav, z_start_idxs, x_mask, y_mask, common_tuple)
+
+        if self.vocoder_generator_type == "visinger2":
+            output = output + (dsp_slice.sum(1),)
+        if self.generator_type == "visinger2":
+            output = output + (predict_mel,)
+        return output
 
-    @torch.no_grad()
     def inference(
         self,
         text: torch.Tensor,
+        text_lengths: torch.Tensor,
         feats: Optional[torch.Tensor] = None,
-        durations: Optional[torch.Tensor] = None,
-        spembs: Optional[torch.Tensor] = None,
+        feats_lengths: Optional[torch.Tensor] = None,
+        label: torch.Tensor = None,
+        label_lengths: torch.Tensor = None,
+        melody: torch.Tensor = None,
+        score_dur: torch.Tensor = None,
+        slur: torch.Tensor = None,
+        gt_dur: Optional[torch.Tensor] = None,
+        pitch: Optional[torch.Tensor] = None,
         sids: Optional[torch.Tensor] = None,
+        spembs: Optional[torch.Tensor] = None,
         lids: Optional[torch.Tensor] = None,
-        pitch: Optional[torch.Tensor] = None,
-        energy: Optional[torch.Tensor] = None,
+        noise_scale: float = 0.667,
+        noise_scale_dur: float = 0.8,
         alpha: float = 1.0,
+        max_len: Optional[int] = None,
         use_teacher_forcing: bool = False,
-    ) -> Dict[str, torch.Tensor]:
-        """Generate the sequence of features given the sequences of characters.
+    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
+        """Run inference.
 
         Args:
-            text (LongTensor): Input sequence of characters (T_text,).
-            feats (Optional[Tensor): Feature sequence to extract style (N, idim).
-            durations (Optional[Tensor): Groundtruth of duration (T_text + 1,).
-            spembs (Optional[Tensor): Speaker embedding vector (spk_embed_dim,).
-            sids (Optional[Tensor]): Speaker ID (1,).
-            lids (Optional[Tensor]): Language ID (1,).
-            pitch (Optional[Tensor]): Groundtruth of token-avg pitch (T_text + 1, 1).
-            energy (Optional[Tensor]): Groundtruth of token-avg energy (T_text + 1, 1).
-            alpha (float): Alpha to control the speed.
+            text (LongTensor): Batch of padded character ids (B, Tmax).
+            text_lengths (LongTensor): Batch of lengths of each input batch (B,).
+            feats (Tensor): Batch of padded target features (B, Lmax, odim).
+            feats_lengths (LongTensor): Batch of the lengths of each target (B,).
+            label (LongTensor): Batch of padded label ids (B, Tmax).
+            label_lengths (LongTensor): Batch of the lengths of padded label ids (B, ).
+            melody (LongTensor): Batch of padded midi (B, Tmax).
+            gt_dur (LongTensor): Batch of padded ground truth duration (B, Tmax).
+            score_dur (LongTensor): Batch of padded score duration (B, Tmax).
+            pitch (FloatTensor): Batch of padded f0 (B, Tmax).
+            ying (Optional[Tensor]): Batch of padded ying (B, Tmax).
+            spembs (Optional[Tensor]): Batch of speaker embeddings (B, spk_embed_dim).
+            sids (Optional[Tensor]): Batch of speaker IDs (B, 1).
+            lids (Optional[Tensor]): Batch of language IDs (B, 1).
+            noise_scale (float): Noise scale parameter for flow.
+            noise_scale_dur (float): Noise scale parameter for duration predictor.
+            alpha (float): Alpha parameter to control the speed of generated speech.
+            max_len (Optional[int]): Maximum length of acoustic feature sequence.
             use_teacher_forcing (bool): Whether to use teacher forcing.
-                If true, groundtruth of duration, pitch and energy will be used.
 
         Returns:
-            Dict[str, Tensor]: Output dict including the following items:
-                * feat_gen (Tensor): Output sequence of features (T_feats, odim).
-                * duration (Tensor): Duration sequence (T_text + 1,).
-                * pitch (Tensor): Pitch sequence (T_text + 1,).
-                * energy (Tensor): Energy sequence (T_text + 1,).
+            Tensor: Generated waveform tensor (B, T_wav).
 
         """
-        x, y = text, feats
-        spemb, d, p, e = spembs, durations, pitch, energy
-
-        # add eos at the last of sequence
-        x = F.pad(x, [0, 1], "constant", self.eos)
-
-        # setup batch axis
-        ilens = torch.tensor([x.shape[0]], dtype=torch.long, device=x.device)
-        xs, ys = x.unsqueeze(0), None
-        if y is not None:
-            ys = y.unsqueeze(0)
-        if spemb is not None:
-            spembs = spemb.unsqueeze(0)
+        # encoder
+        x, x_mask, dur_input, x_pitch = self.text_encoder(
+            label, label_lengths, melody, score_dur, slur
+        )
+        g = None
+        if self.spks is not None:
+            # (B, global_channels, 1)
+            g = self.global_emb(sids.view(-1)).unsqueeze(-1)
+        if self.spk_embed_dim is not None:
+            # (B, global_channels, 1)
+            g_ = self.spemb_proj(F.normalize(spembs.unsqueeze(0))).unsqueeze(-1)
+            if g is None:
+                g = g_
+            else:
+                g = g + g_
+        if self.langs is not None:
+            # (B, global_channels, 1)
+            g_ = self.lang_emb(lids.view(-1)).unsqueeze(-1)
+            if g is None:
+                g = g_
+            else:
+                g = g + g_
 
         if use_teacher_forcing:
-            # use groundtruth of duration, pitch, and energy
-            ds, ps, es = d.unsqueeze(0), p.unsqueeze(0), e.unsqueeze(0)
-            _, outs, d_outs, p_outs, e_outs = self._forward(
-                xs,
-                ilens,
-                ys,
-                ds=ds,
-                ps=ps,
-                es=es,
-                spembs=spembs,
-                sids=sids,
-                lids=lids,
-                is_inference=True,
-            )  # (1, T_feats, odim)
-        else:
-            _, outs, d_outs, p_outs, e_outs = self._forward(
-                xs,
-                ilens,
-                ys,
-                spembs=spembs,
-                sids=sids,
-                lids=lids,
-                is_inference=True,
-                alpha=alpha,
-            )  # (1, T_feats, odim)
-
-        return dict(
-            feat_gen=outs[0],
-            duration=d_outs[0],
-            pitch=p_outs[0],
-            energy=e_outs[0],
-        )
-
-    def _integrate_with_spk_embed(
-        self, hs: torch.Tensor, spembs: torch.Tensor
-    ) -> torch.Tensor:
-        """Integrate speaker embedding with hidden states.
+            # forward posterior encoder
+            z, m_q, logs_q, y_mask = self.posterior_encoder(feats, feats_lengths, g=g)
 
-        Args:
-            hs (Tensor): Batch of hidden state sequences (B, T_text, adim).
-            spembs (Tensor): Batch of speaker embeddings (B, spk_embed_dim).
-
-        Returns:
-            Tensor: Batch of integrated hidden state sequences (B, T_text, adim).
-
-        """
-        if self.spk_embed_integration_type == "add":
-            # apply projection and then add to hidden states
-            spembs = self.projection(F.normalize(spembs))
-            hs = hs + spembs.unsqueeze(1)
-        elif self.spk_embed_integration_type == "concat":
-            # concat hidden states with spk embeds and then apply projection
-            spembs = F.normalize(spembs).unsqueeze(1).expand(-1, hs.size(1), -1)
-            hs = self.projection(torch.cat([hs, spembs], dim=-1))
+            # forward flow
+            if self.use_flow:
+                z_p = self.flow(z, y_mask, g=g)  # (B, H, T_feats)
+
+            # decoder
+            pitch = pitch.transpose(0, 1).reshape(1, 1, -1)
+
+            if self.vocoder_generator_type == "uhifigan":
+                pitch_segments_expended = expand_f0(
+                    pitch, self.hop_length, method=self.expand_f0_method
+                )
+                pitch_segments_expended = pitch_segments_expended.reshape(
+                    -1, pitch_segments_expended.shape[-1], 1
+                )
+                sine_waves, uv, noise = self.sine_generator(pitch_segments_expended)
+                sine_waves = sine_waves.transpose(1, 2)
+                wav = self.decoder(
+                    (z * y_mask)[:, :, :max_len], excitation=sine_waves, g=g
+                )
+            elif self.vocoder_generator_type == "avocodo":
+                wav = self.decoder((z * y_mask)[:, :, :max_len], g=g)[-1]
+            elif self.vocoder_generator_type == "visinger2":
+                pitch_ = upsample(pitch.transpose(1, 2), self.hop_length)
+                omega = torch.cumsum(2 * math.pi * pitch_ / self.sample_rate, 1)
+                sin = torch.sin(omega).transpose(1, 2)
+
+                # dsp synthesize
+                noise_x = self.dec_noise(z, y_mask)
+                harm_x = self.dec_harm(pitch, z, y_mask)
+
+                # dsp waveform
+                dsp_o = torch.cat([harm_x, noise_x], axis=1)
+
+                # decoder_condition = torch.cat([harm_x, noise_x, sin], axis=1)
+                decoder_condition = self.sin_prenet(sin)
+
+                # dsp based HiFiGAN vocoder
+                wav = self.decoder((z * y_mask)[:, :, :max_len], decoder_condition, g=g)
+                # wav = dsp_o.sum(1)
+                # wav = noise_x
+                # wav = harm_x.sum(1)
+            else:
+                wav = self.decoder((z * y_mask)[:, :, :max_len], g=g)
         else:
-            raise NotImplementedError("support only add or concat.")
+            # dur
+            predict_dur = self.duration_predictor(dur_input, x_mask, g=g)
+            predict_dur = (torch.exp(predict_dur) - 1) * x_mask
+            predict_dur = predict_dur * self.sample_rate / self.hop_length
+
+            predict_dur = torch.max(predict_dur, torch.ones_like(predict_dur).to(x))
+            predict_dur = torch.ceil(predict_dur).long()
+            predict_dur = predict_dur[:, 0, :]
+            y_lengths = torch.clamp_min(torch.sum(predict_dur, [1]), 1).long()
+
+            # LR
+            decoder_input, mel_len = self.lr(x, predict_dur, use_state_info=True)
+            decoder_input_pitch, mel_len = self.lr(
+                x_pitch, predict_dur, use_state_info=True
+            )
 
-        return hs
+            # aam
+            predict_lf0, predict_bn_mask = self.f0_decoder(
+                decoder_input + decoder_input_pitch, y_lengths, g=g
+            )
 
-    def _source_mask(self, ilens: torch.Tensor) -> torch.Tensor:
-        """Make masks for self-attention.
+            if self.generator_type == "visinger2":
+                predict_mel, predict_bn_mask = self.mel_decoder(
+                    decoder_input + self.f0_prenet(predict_lf0),
+                    y_lengths,
+                    g=g,
+                )
+                predict_energy = predict_mel.sum(1).unsqueeze(1) / self.aux_channels
 
-        Args:
-            ilens (LongTensor): Batch of lengths (B,).
+            predict_lf0 = torch.max(
+                predict_lf0, torch.zeros_like(predict_lf0).to(predict_lf0)
+            )
 
-        Returns:
-            Tensor: Mask tensor for self-attention.
-                dtype=torch.uint8 in PyTorch 1.2-
-                dtype=torch.bool in PyTorch 1.2+ (including 1.2)
-
-        Examples:
-            >>> ilens = [5, 3]
-            >>> self._source_mask(ilens)
-            tensor([[[1, 1, 1, 1, 1],
-                     [1, 1, 1, 0, 0]]], dtype=torch.uint8)
+            decoder_input = decoder_input + self.f0_prenet(predict_lf0)
+            if self.generator_type == "visinger2":
+                decoder_input = (
+                    decoder_input
+                    + self.energy_prenet(predict_energy)
+                    + self.mel_prenet(predict_mel)
+                )
+
+            decoder_output, y_mask = self.prior_decoder(decoder_input, y_lengths, g=g)
+
+            prior_info = decoder_output
+            m_p = prior_info[:, : self.hidden_channels, :]
+            logs_p = prior_info[:, self.hidden_channels :, :]
 
-        """
-        x_masks = make_non_pad_mask(ilens).to(next(self.parameters()).device)
-        return x_masks.unsqueeze(-2)
+            # decoder
+            z_p = m_p + torch.randn_like(m_p) * torch.exp(logs_p) * noise_scale
 
-    def _reset_parameters(
-        self, init_type: str, init_enc_alpha: float, init_dec_alpha: float
-    ):
-        """Reset parameters of the model.
+            if self.use_flow:
+                z = self.flow(z_p, y_mask, g=g, inverse=True)
+            else:
+                z = z_p
 
-        Args:
-            init_type (str): Type of initialization.
-            init_enc_alpha (float): Value of the initialization for the encoder.
-            init_dec_alpha (float): Value of the initialization for the decoder.
+            F0_std = 500
+            F0 = predict_lf0 * F0_std
+            F0 = F0 / 2595
+            F0 = torch.pow(10, F0)
+            F0 = (F0 - 1) * 700.0
+
+            if self.vocoder_generator_type == "uhifigan":
+                pitch_segments_expended = expand_f0(
+                    F0, self.hop_length, method=self.expand_f0_method
+                )
+                pitch_segments_expended = pitch_segments_expended.reshape(
+                    -1, pitch_segments_expended.shape[-1], 1
+                )
+                sine_waves, uv, noise = self.sine_generator(pitch_segments_expended)
+                sine_waves = sine_waves.transpose(1, 2)
+                wav = self.decoder(
+                    (z * y_mask)[:, :, :max_len], excitation=sine_waves, g=g
+                )
+            elif self.vocoder_generator_type == "avocodo":
+                wav = self.decoder((z * y_mask)[:, :, :max_len], g=g)[-1]
+            elif self.vocoder_generator_type == "visinger2":
+                pitch_ = upsample(F0.transpose(1, 2), self.hop_length)
+                omega = torch.cumsum(2 * math.pi * pitch_ / self.sample_rate, 1)
+                sin = torch.sin(omega).transpose(1, 2)
+
+                # dsp synthesize
+                noise_x = self.dec_noise(z, y_mask)
+                harm_x = self.dec_harm(F0, z, y_mask)
+
+                # dsp waveform
+                dsp_o = torch.cat([harm_x, noise_x], axis=1)
+
+                # decoder_condition = torch.cat([harm_x, noise_x, sin], axis=1)
+                decoder_condition = self.sin_prenet(sin)
+
+                # dsp based HiFiGAN vocoder
+                wav = self.decoder((z * y_mask)[:, :, :max_len], decoder_condition, g=g)
+                # wav = dsp_o.sum(1)
+                # wav = noise_x
+                # wav = harm_x.sum(1)
+            else:
+                wav = self.decoder((z * y_mask)[:, :, :max_len], g=g)
 
-        """
-        # initialize parameters
-        if init_type != "pytorch":
-            initialize(self, init_type)
-
-        # initialize alpha in scaled positional encoding
-        if self.encoder_type == "transformer" and self.use_scaled_pos_enc:
-            self.encoder.embed[-1].alpha.data = torch.tensor(init_enc_alpha)
-        if self.decoder_type == "transformer" and self.use_scaled_pos_enc:
-            self.decoder.embed[-1].alpha.data = torch.tensor(init_dec_alpha)
+        return wav.squeeze(1)
```

### Comparing `espnet-202304/espnet2/tts/tacotron2/tacotron2.py` & `espnet-202308/espnet2/tts/tacotron2/tacotron2.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/tts/transformer/transformer.py` & `espnet-202308/espnet2/tts/transformer/transformer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/tts/utils/duration_calculator.py` & `espnet-202308/espnet2/tts/utils/duration_calculator.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/tts/utils/parallel_wavegan_pretrained_vocoder.py` & `espnet-202308/espnet2/tts/utils/parallel_wavegan_pretrained_vocoder.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/uasr/discriminator/conv_discriminator.py` & `espnet-202308/espnet2/uasr/discriminator/conv_discriminator.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/uasr/generator/conv_generator.py` & `espnet-202308/espnet2/uasr/generator/conv_generator.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/uasr/loss/discriminator_loss.py` & `espnet-202308/espnet2/uasr/loss/discriminator_loss.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/uasr/loss/gradient_penalty.py` & `espnet-202308/espnet2/uasr/loss/gradient_penalty.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/uasr/loss/phoneme_diversity_loss.py` & `espnet-202308/espnet2/uasr/loss/phoneme_diversity_loss.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/uasr/loss/pseudo_label_loss.py` & `espnet-202308/espnet2/uasr/loss/pseudo_label_loss.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/uasr/loss/smoothness_penalty.py` & `espnet-202308/espnet2/uasr/loss/smoothness_penalty.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/uasr/segmenter/abs_segmenter.py` & `espnet-202308/espnet2/uasr/segmenter/abs_segmenter.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/uasr/segmenter/join_segmenter.py` & `espnet-202308/espnet2/uasr/segmenter/join_segmenter.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/uasr/segmenter/random_segmenter.py` & `espnet-202308/espnet2/uasr/segmenter/random_segmenter.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/utils/build_dataclass.py` & `espnet-202308/espnet2/utils/build_dataclass.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/utils/config_argparse.py` & `espnet-202308/espnet2/utils/config_argparse.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/utils/get_default_kwargs.py` & `espnet-202308/espnet2/utils/get_default_kwargs.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/utils/griffin_lim.py` & `espnet-202308/espnet2/utils/griffin_lim.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/utils/nested_dict_action.py` & `espnet-202308/espnet2/utils/nested_dict_action.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/utils/sized_dict.py` & `espnet-202308/espnet2/utils/sized_dict.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/espnet2/utils/types.py` & `espnet-202308/espnet2/utils/types.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/setup.py` & `espnet-202308/setup.py`

 * *Files 4% similar despite different names*

```diff
@@ -20,52 +20,55 @@
         "PyYAML>=5.1.2",
         "soundfile>=0.10.2",
         "h5py>=2.10.0",
         "kaldiio>=2.18.0",
         "torch>=1.3.0",
         "torch_complex",
         "nltk>=3.4.5",
-        "numpy",
+        # fix CI error due to the use of deprecated aliases
+        "numpy<1.24",
         # https://github.com/espnet/espnet/runs/6646737793?check_suite_focus=true#step:8:7651
         "protobuf<=3.20.1",
         "hydra-core",
         "opt-einsum",
         # ASR
         "sentencepiece==0.1.97",
         "ctc-segmentation>=1.6.6",
         # TTS
-        "pyworld>=0.2.10",
+        "pyworld>=0.3.4",
         "pypinyin<=0.44.0",
         "espnet_tts_frontend",
         # ENH
         "ci_sdr",
         "pytorch_wpe",
         "fast-bss-eval==0.1.3",
+        # SPK
+        "asteroid_filterbanks==0.4.0",
         # UASR
         "editdistance",
         # fix CI error due to the use of deprecated functions
         # https://github.com/espnet/espnet/actions/runs/3174416926/jobs/5171182884#step:8:8419
         # https://importlib-metadata.readthedocs.io/en/latest/history.html#v5-0-0
         "importlib-metadata<5.0",
     ],
     # train: The modules invoked when training only.
     "train": [
         "matplotlib",
-        "pillow>=6.1.0",
+        "pillow==9.5.0",
         "editdistance==0.5.2",
         "wandb",
         "tensorboard>=1.14",
     ],
     # recipe: The modules actually are not invoked in the main module of espnet,
     #         but are invoked for the python scripts in each recipe
     "recipe": [
         "espnet_model_zoo",
         "gdown",
         "resampy",
-        "pysptk>=0.1.17",
+        "pysptk>=0.2.1",
         "morfessor",  # for zeroth-korean
         "youtube_dl",  # for laborotv
         "nnmnkwii",
         "museval>=0.2.1",
         "pystoi>=0.2.2",
         "mir-eval>=0.6",
         "fastdtw",
@@ -80,14 +83,15 @@
     "all": [
         # NOTE(kamo): Append modules requiring specific pytorch version or torch>1.3.0
         "torchaudio",
         "torch_optimizer",
         "fairscale",
         "transformers",
         "gtn==0.0.0",
+        "evaluate",
     ],
     "setup": [
         "pytest-runner",
     ],
     "test": [
         "pytest>=3.3.0",
         "pytest-timeouts>=1.2.1",
@@ -97,15 +101,14 @@
         "mock>=2.0.0",
         "pycodestyle",
         "jsondiff<2.0.0,>=1.2.0",
         "flake8>=3.7.8",
         "flake8-docstrings>=1.3.1",
         "black",
         "isort",
-        "music21",
     ],
     "doc": [
         "Jinja2<3.1",
         "Sphinx==2.1.2",
         "sphinx-rtd-theme>=0.2.4",
         "sphinx-argparse>=0.2.5",
         "commonmark==0.8.1",
```

### Comparing `espnet-202304/test/test_asr_init.py` & `espnet-202308/test/test_asr_init.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/test/test_asr_quantize.py` & `espnet-202308/test/test_asr_quantize.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/test/test_batch_beam_search.py` & `espnet-202308/test/test_batch_beam_search.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/test/test_beam_search.py` & `espnet-202308/test/test_beam_search.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/test/test_beam_search_timesync.py` & `espnet-202308/test/test_beam_search_timesync.py`

 * *Files 0% similar despite different names*

```diff
@@ -131,15 +131,15 @@
         for ctc_train in (0.0, 0.5, 1.0)
         for ctc_recog in (0.0, 0.5, 1.0)
         for lm in (0.5,)
         for bonus in (0.1,)
         for dtype in ("float16", "float32", "float64")
     ],
 )
-def test_beam_search_equal(
+def test_beam_search_timesync(
     model_class, args, mtlalpha, ctc_weight, lm_weight, bonus, device, dtype
 ):
     if device == "cuda" and not torch.cuda.is_available():
         pytest.skip("no cuda device is available")
     if device == "cpu" and dtype == "float16":
         pytest.skip("cpu float16 implementation is not available in pytorch yet")
     if mtlalpha == 0.0 or ctc_weight == 0:
```

### Comparing `espnet-202304/test/test_cli.py` & `espnet-202308/test/test_cli.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/test/test_custom_transducer.py` & `espnet-202308/test/test_custom_transducer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/test/test_distributed_launch.py` & `espnet-202308/test/test_distributed_launch.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/test/test_e2e_asr.py` & `espnet-202308/test/test_e2e_asr.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/test/test_e2e_asr_conformer.py` & `espnet-202308/test/test_e2e_asr_conformer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/test/test_e2e_asr_maskctc.py` & `espnet-202308/test/test_e2e_asr_maskctc.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/test/test_e2e_asr_mulenc.py` & `espnet-202308/test/test_e2e_asr_mulenc.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/test/test_e2e_asr_transducer.py` & `espnet-202308/test/test_e2e_asr_transducer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/test/test_e2e_asr_transformer.py` & `espnet-202308/test/test_e2e_asr_transformer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/test/test_e2e_compatibility.py` & `espnet-202308/test/test_e2e_compatibility.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/test/test_e2e_mt.py` & `espnet-202308/test/test_e2e_mt.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/test/test_e2e_mt_transformer.py` & `espnet-202308/test/test_e2e_mt_transformer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/test/test_e2e_st.py` & `espnet-202308/test/test_e2e_st.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/test/test_e2e_st_conformer.py` & `espnet-202308/test/test_e2e_st_conformer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/test/test_e2e_st_transformer.py` & `espnet-202308/test/test_e2e_st_transformer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/test/test_e2e_tts_fastspeech.py` & `espnet-202308/test/test_e2e_tts_fastspeech.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/test/test_e2e_tts_tacotron2.py` & `espnet-202308/test/test_e2e_tts_tacotron2.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/test/test_e2e_tts_transformer.py` & `espnet-202308/test/test_e2e_tts_transformer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/test/test_e2e_vc_tacotron2.py` & `espnet-202308/test/test_e2e_vc_tacotron2.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/test/test_e2e_vc_transformer.py` & `espnet-202308/test/test_e2e_vc_transformer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/test/test_initialization.py` & `espnet-202308/test/test_initialization.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/test/test_io_voxforge.py` & `espnet-202308/test/test_io_voxforge.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/test/test_lm.py` & `espnet-202308/test/test_lm.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/test/test_loss.py` & `espnet-202308/test/test_loss.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/test/test_multi_spkrs.py` & `espnet-202308/test/test_multi_spkrs.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/test/test_optimizer.py` & `espnet-202308/test/test_optimizer.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/test/test_positional_encoding.py` & `espnet-202308/test/test_positional_encoding.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/test/test_recog.py` & `espnet-202308/test/test_recog.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/test/test_scheduler.py` & `espnet-202308/test/test_scheduler.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/test/test_sentencepiece.py` & `espnet-202308/test/test_sentencepiece.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/test/test_tensorboard.py` & `espnet-202308/test/test_tensorboard.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/test/test_torch.py` & `espnet-202308/test/test_torch.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/test/test_train_dtype.py` & `espnet-202308/test/test_train_dtype.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/test/test_transform.py` & `espnet-202308/test/test_transform.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/test/test_transformer_decode.py` & `espnet-202308/test/test_transformer_decode.py`

 * *Files identical despite different names*

### Comparing `espnet-202304/test/test_utils.py` & `espnet-202308/test/test_utils.py`

 * *Files identical despite different names*

