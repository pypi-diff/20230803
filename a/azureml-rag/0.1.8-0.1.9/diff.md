# Comparing `tmp/azureml_rag-0.1.8-py3-none-any.whl.zip` & `tmp/azureml_rag-0.1.9-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,61 +1,69 @@
-Zip file size: 173642 bytes, number of entries: 59
--rw-rw-rw-  2.0 fat      246 b- defN 23-Jun-22 18:43 azureml/rag/__init__.py
--rw-rw-rw-  2.0 fat    39881 b- defN 23-Jun-22 18:43 azureml/rag/documents.py
--rw-rw-rw-  2.0 fat    33664 b- defN 23-Jun-22 18:43 azureml/rag/embeddings.py
--rw-rw-rw-  2.0 fat     5058 b- defN 23-Jun-22 18:43 azureml/rag/mlindex.py
--rw-rw-rw-  2.0 fat     4149 b- defN 23-Jun-22 18:43 azureml/rag/models.py
--rw-rw-rw-  2.0 fat     4646 b- defN 23-Jun-22 18:45 azureml/rag/_asset_client/client.py
--rw-rw-rw-  2.0 fat      893 b- defN 23-Jun-22 18:46 azureml/rag/_asset_client/_restclient/__init__.py
--rw-rw-rw-  2.0 fat     4381 b- defN 23-Jun-22 18:46 azureml/rag/_asset_client/_restclient/_azure_machine_learning_workspaces.py
--rw-rw-rw-  2.0 fat     3538 b- defN 23-Jun-22 18:46 azureml/rag/_asset_client/_restclient/_configuration.py
--rw-rw-rw-  2.0 fat     1561 b- defN 23-Jun-22 18:46 azureml/rag/_asset_client/_restclient/_patch.py
--rw-rw-rw-  2.0 fat      495 b- defN 23-Jun-22 18:46 azureml/rag/_asset_client/_restclient/_version.py
--rw-rw-rw-  2.0 fat      399 b- defN 23-Jun-22 18:46 azureml/rag/_asset_client/_restclient/models.py
--rw-rw-rw-  2.0 fat      957 b- defN 23-Jun-22 18:46 azureml/rag/_asset_client/_restclient/dataset/__init__.py
--rw-rw-rw-  2.0 fat     3802 b- defN 23-Jun-22 18:46 azureml/rag/_asset_client/_restclient/dataset/_azure_machine_learning_workspaces.py
--rw-rw-rw-  2.0 fat     3144 b- defN 23-Jun-22 18:46 azureml/rag/_asset_client/_restclient/dataset/_configuration.py
--rw-rw-rw-  2.0 fat      694 b- defN 23-Jun-22 18:46 azureml/rag/_asset_client/_restclient/dataset/_patch.py
--rw-rw-rw-  2.0 fat    81019 b- defN 23-Jun-22 18:46 azureml/rag/_asset_client/_restclient/dataset/_serialization.py
--rw-rw-rw-  2.0 fat     1833 b- defN 23-Jun-22 18:46 azureml/rag/_asset_client/_restclient/dataset/_vendor.py
--rw-rw-rw-  2.0 fat      495 b- defN 23-Jun-22 18:46 azureml/rag/_asset_client/_restclient/dataset/_version.py
--rw-rw-rw-  2.0 fat     6787 b- defN 23-Jun-22 18:47 azureml/rag/_asset_client/_restclient/dataset/models/__init__.py
--rw-rw-rw-  2.0 fat     4635 b- defN 23-Jun-22 18:47 azureml/rag/_asset_client/_restclient/dataset/models/_azure_machine_learning_workspaces_enums.py
--rw-rw-rw-  2.0 fat   131448 b- defN 23-Jun-22 18:47 azureml/rag/_asset_client/_restclient/dataset/models/_models_py3.py
--rw-rw-rw-  2.0 fat      694 b- defN 23-Jun-22 18:47 azureml/rag/_asset_client/_restclient/dataset/models/_patch.py
--rw-rw-rw-  2.0 fat      585 b- defN 23-Jun-22 18:47 azureml/rag/_asset_client/_restclient/dataset/operations/__init__.py
--rw-rw-rw-  2.0 fat   104636 b- defN 23-Jun-22 18:47 azureml/rag/_asset_client/_restclient/dataset/operations/_data_version_operations.py
--rw-rw-rw-  2.0 fat    80785 b- defN 23-Jun-22 18:47 azureml/rag/_asset_client/_restclient/dataset/operations/_mlindex_operations.py
--rw-rw-rw-  2.0 fat      893 b- defN 23-Jun-22 18:46 azureml/rag/_asset_client/_restclient/runhistory/__init__.py
--rw-rw-rw-  2.0 fat     3902 b- defN 23-Jun-22 18:46 azureml/rag/_asset_client/_restclient/runhistory/_azure_machine_learning_workspaces.py
--rw-rw-rw-  2.0 fat     3226 b- defN 23-Jun-22 18:46 azureml/rag/_asset_client/_restclient/runhistory/_configuration.py
--rw-rw-rw-  2.0 fat     1561 b- defN 23-Jun-22 18:46 azureml/rag/_asset_client/_restclient/runhistory/_patch.py
--rw-rw-rw-  2.0 fat     1255 b- defN 23-Jun-22 18:46 azureml/rag/_asset_client/_restclient/runhistory/_vendor.py
--rw-rw-rw-  2.0 fat      495 b- defN 23-Jun-22 18:46 azureml/rag/_asset_client/_restclient/runhistory/_version.py
--rw-rw-rw-  2.0 fat    11705 b- defN 23-Jun-22 18:47 azureml/rag/_asset_client/_restclient/runhistory/models/__init__.py
--rw-rw-rw-  2.0 fat     2566 b- defN 23-Jun-22 18:47 azureml/rag/_asset_client/_restclient/runhistory/models/_azure_machine_learning_workspaces_enums.py
--rw-rw-rw-  2.0 fat   175170 b- defN 23-Jun-22 18:47 azureml/rag/_asset_client/_restclient/runhistory/models/_models.py
--rw-rw-rw-  2.0 fat   188776 b- defN 23-Jun-22 18:47 azureml/rag/_asset_client/_restclient/runhistory/models/_models_py3.py
--rw-rw-rw-  2.0 fat      563 b- defN 23-Jun-22 18:47 azureml/rag/_asset_client/_restclient/runhistory/operations/__init__.py
--rw-rw-rw-  2.0 fat   162796 b- defN 23-Jun-22 18:47 azureml/rag/_asset_client/_restclient/runhistory/operations/_runs_operations.py
--rw-rw-rw-  2.0 fat    10785 b- defN 23-Jun-22 18:45 azureml/rag/langchain/acs.py
--rw-rw-rw-  2.0 fat      183 b- defN 23-Jun-22 18:45 azureml/rag/tasks/__init__.py
--rw-rw-rw-  2.0 fat     2289 b- defN 23-Jun-22 18:45 azureml/rag/tasks/build_faiss.py
--rw-rw-rw-  2.0 fat     9822 b- defN 23-Jun-22 18:45 azureml/rag/tasks/crack_and_chunk.py
--rw-rw-rw-  2.0 fat     7502 b- defN 23-Jun-22 18:45 azureml/rag/tasks/embed.py
--rw-rw-rw-  2.0 fat     6121 b- defN 23-Jun-22 18:45 azureml/rag/tasks/embed_prs.py
--rw-rw-rw-  2.0 fat     3239 b- defN 23-Jun-22 18:45 azureml/rag/tasks/git_clone.py
--rw-rw-rw-  2.0 fat     3327 b- defN 23-Jun-22 18:45 azureml/rag/tasks/register_mlindex.py
--rw-rw-rw-  2.0 fat    21619 b- defN 23-Jun-22 18:45 azureml/rag/tasks/update_acs.py
--rw-rw-rw-  2.0 fat      208 b- defN 23-Jun-22 18:45 azureml/rag/utils/__init__.py
--rw-rw-rw-  2.0 fat       98 b- defN 23-Jun-22 18:45 azureml/rag/utils/_telemetry.json
--rw-rw-rw-  2.0 fat     1588 b- defN 23-Jun-22 18:45 azureml/rag/utils/azureml.py
--rw-rw-rw-  2.0 fat     7435 b- defN 23-Jun-22 18:45 azureml/rag/utils/connections.py
--rw-rw-rw-  2.0 fat     1506 b- defN 23-Jun-22 18:45 azureml/rag/utils/deployment.py
--rw-rw-rw-  2.0 fat     2634 b- defN 23-Jun-22 18:45 azureml/rag/utils/git.py
--rw-rw-rw-  2.0 fat    10082 b- defN 23-Jun-22 18:45 azureml/rag/utils/logging.py
--rw-rw-rw-  2.0 fat     1021 b- defN 23-Jun-22 18:51 azureml_rag-0.1.8.dist-info/LICENSE.txt
--rw-rw-rw-  2.0 fat     5744 b- defN 23-Jun-22 18:51 azureml_rag-0.1.8.dist-info/METADATA
--rw-rw-rw-  2.0 fat       97 b- defN 23-Jun-22 18:51 azureml_rag-0.1.8.dist-info/WHEEL
--rw-rw-rw-  2.0 fat        8 b- defN 23-Jun-22 18:51 azureml_rag-0.1.8.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     6240 b- defN 23-Jun-22 18:51 azureml_rag-0.1.8.dist-info/RECORD
-59 files, 1174881 bytes uncompressed, 163268 bytes compressed:  86.1%
+Zip file size: 189409 bytes, number of entries: 67
+-rw-rw-rw-  2.0 fat      246 b- defN 23-Jun-23 08:26 azureml/rag/__init__.py
+-rw-rw-rw-  2.0 fat    40590 b- defN 23-Jun-23 08:26 azureml/rag/documents.py
+-rw-rw-rw-  2.0 fat    32799 b- defN 23-Jun-23 08:26 azureml/rag/embeddings.py
+-rw-rw-rw-  2.0 fat     6356 b- defN 23-Jun-23 08:26 azureml/rag/mlindex.py
+-rw-rw-rw-  2.0 fat     7895 b- defN 23-Jun-23 08:26 azureml/rag/models.py
+-rw-rw-rw-  2.0 fat     4646 b- defN 23-Jun-23 08:27 azureml/rag/_asset_client/client.py
+-rw-rw-rw-  2.0 fat      893 b- defN 23-Jun-23 08:28 azureml/rag/_asset_client/_restclient/__init__.py
+-rw-rw-rw-  2.0 fat     4381 b- defN 23-Jun-23 08:28 azureml/rag/_asset_client/_restclient/_azure_machine_learning_workspaces.py
+-rw-rw-rw-  2.0 fat     3538 b- defN 23-Jun-23 08:28 azureml/rag/_asset_client/_restclient/_configuration.py
+-rw-rw-rw-  2.0 fat     1561 b- defN 23-Jun-23 08:28 azureml/rag/_asset_client/_restclient/_patch.py
+-rw-rw-rw-  2.0 fat      495 b- defN 23-Jun-23 08:28 azureml/rag/_asset_client/_restclient/_version.py
+-rw-rw-rw-  2.0 fat      399 b- defN 23-Jun-23 08:28 azureml/rag/_asset_client/_restclient/models.py
+-rw-rw-rw-  2.0 fat      957 b- defN 23-Jun-23 08:29 azureml/rag/_asset_client/_restclient/dataset/__init__.py
+-rw-rw-rw-  2.0 fat     3802 b- defN 23-Jun-23 08:29 azureml/rag/_asset_client/_restclient/dataset/_azure_machine_learning_workspaces.py
+-rw-rw-rw-  2.0 fat     3144 b- defN 23-Jun-23 08:29 azureml/rag/_asset_client/_restclient/dataset/_configuration.py
+-rw-rw-rw-  2.0 fat      694 b- defN 23-Jun-23 08:29 azureml/rag/_asset_client/_restclient/dataset/_patch.py
+-rw-rw-rw-  2.0 fat    81019 b- defN 23-Jun-23 08:29 azureml/rag/_asset_client/_restclient/dataset/_serialization.py
+-rw-rw-rw-  2.0 fat     1833 b- defN 23-Jun-23 08:29 azureml/rag/_asset_client/_restclient/dataset/_vendor.py
+-rw-rw-rw-  2.0 fat      495 b- defN 23-Jun-23 08:29 azureml/rag/_asset_client/_restclient/dataset/_version.py
+-rw-rw-rw-  2.0 fat     6787 b- defN 23-Jun-23 08:29 azureml/rag/_asset_client/_restclient/dataset/models/__init__.py
+-rw-rw-rw-  2.0 fat     4635 b- defN 23-Jun-23 08:29 azureml/rag/_asset_client/_restclient/dataset/models/_azure_machine_learning_workspaces_enums.py
+-rw-rw-rw-  2.0 fat   131448 b- defN 23-Jun-23 08:29 azureml/rag/_asset_client/_restclient/dataset/models/_models_py3.py
+-rw-rw-rw-  2.0 fat      694 b- defN 23-Jun-23 08:29 azureml/rag/_asset_client/_restclient/dataset/models/_patch.py
+-rw-rw-rw-  2.0 fat      585 b- defN 23-Jun-23 08:29 azureml/rag/_asset_client/_restclient/dataset/operations/__init__.py
+-rw-rw-rw-  2.0 fat   104636 b- defN 23-Jun-23 08:29 azureml/rag/_asset_client/_restclient/dataset/operations/_data_version_operations.py
+-rw-rw-rw-  2.0 fat    80785 b- defN 23-Jun-23 08:29 azureml/rag/_asset_client/_restclient/dataset/operations/_mlindex_operations.py
+-rw-rw-rw-  2.0 fat      893 b- defN 23-Jun-23 08:29 azureml/rag/_asset_client/_restclient/runhistory/__init__.py
+-rw-rw-rw-  2.0 fat     3902 b- defN 23-Jun-23 08:29 azureml/rag/_asset_client/_restclient/runhistory/_azure_machine_learning_workspaces.py
+-rw-rw-rw-  2.0 fat     3226 b- defN 23-Jun-23 08:29 azureml/rag/_asset_client/_restclient/runhistory/_configuration.py
+-rw-rw-rw-  2.0 fat     1561 b- defN 23-Jun-23 08:29 azureml/rag/_asset_client/_restclient/runhistory/_patch.py
+-rw-rw-rw-  2.0 fat     1255 b- defN 23-Jun-23 08:29 azureml/rag/_asset_client/_restclient/runhistory/_vendor.py
+-rw-rw-rw-  2.0 fat      495 b- defN 23-Jun-23 08:29 azureml/rag/_asset_client/_restclient/runhistory/_version.py
+-rw-rw-rw-  2.0 fat    11705 b- defN 23-Jun-23 08:29 azureml/rag/_asset_client/_restclient/runhistory/models/__init__.py
+-rw-rw-rw-  2.0 fat     2566 b- defN 23-Jun-23 08:29 azureml/rag/_asset_client/_restclient/runhistory/models/_azure_machine_learning_workspaces_enums.py
+-rw-rw-rw-  2.0 fat   175170 b- defN 23-Jun-23 08:29 azureml/rag/_asset_client/_restclient/runhistory/models/_models.py
+-rw-rw-rw-  2.0 fat   188776 b- defN 23-Jun-23 08:29 azureml/rag/_asset_client/_restclient/runhistory/models/_models_py3.py
+-rw-rw-rw-  2.0 fat      563 b- defN 23-Jun-23 08:29 azureml/rag/_asset_client/_restclient/runhistory/operations/__init__.py
+-rw-rw-rw-  2.0 fat   162796 b- defN 23-Jun-23 08:29 azureml/rag/_asset_client/_restclient/runhistory/operations/_runs_operations.py
+-rw-rw-rw-  2.0 fat    12423 b- defN 23-Jun-23 08:27 azureml/rag/data_generation/qa.py
+-rw-rw-rw-  2.0 fat     3081 b- defN 23-Jun-23 08:28 azureml/rag/data_generation/prompts/prompt_qa_boolean.txt
+-rw-rw-rw-  2.0 fat     3324 b- defN 23-Jun-23 08:28 azureml/rag/data_generation/prompts/prompt_qa_long_answer.txt
+-rw-rw-rw-  2.0 fat     3070 b- defN 23-Jun-23 08:28 azureml/rag/data_generation/prompts/prompt_qa_short_answer.txt
+-rw-rw-rw-  2.0 fat     2181 b- defN 23-Jun-23 08:28 azureml/rag/data_generation/prompts/prompt_qa_summary.txt
+-rw-rw-rw-  2.0 fat    10785 b- defN 23-Jun-23 08:27 azureml/rag/langchain/acs.py
+-rw-rw-rw-  2.0 fat      198 b- defN 23-Jun-23 08:27 azureml/rag/parsers/__init__.py
+-rw-rw-rw-  2.0 fat      646 b- defN 23-Jun-23 08:27 azureml/rag/parsers/markdown.py
+-rw-rw-rw-  2.0 fat      183 b- defN 23-Jun-23 08:27 azureml/rag/tasks/__init__.py
+-rw-rw-rw-  2.0 fat     3032 b- defN 23-Jun-23 08:27 azureml/rag/tasks/build_faiss.py
+-rw-rw-rw-  2.0 fat     7884 b- defN 23-Jun-23 08:27 azureml/rag/tasks/crack_and_chunk.py
+-rw-rw-rw-  2.0 fat    10408 b- defN 23-Jun-23 08:27 azureml/rag/tasks/embed.py
+-rw-rw-rw-  2.0 fat     6919 b- defN 23-Jun-23 08:27 azureml/rag/tasks/embed_prs.py
+-rw-rw-rw-  2.0 fat     9629 b- defN 23-Jun-23 08:27 azureml/rag/tasks/generate_qa.py
+-rw-rw-rw-  2.0 fat     3678 b- defN 23-Jun-23 08:27 azureml/rag/tasks/git_clone.py
+-rw-rw-rw-  2.0 fat     3842 b- defN 23-Jun-23 08:27 azureml/rag/tasks/register_mlindex.py
+-rw-rw-rw-  2.0 fat    21540 b- defN 23-Jun-23 08:27 azureml/rag/tasks/update_acs.py
+-rw-rw-rw-  2.0 fat      208 b- defN 23-Jun-23 08:27 azureml/rag/utils/__init__.py
+-rw-rw-rw-  2.0 fat       98 b- defN 23-Jun-23 08:27 azureml/rag/utils/_telemetry.json
+-rw-rw-rw-  2.0 fat     1588 b- defN 23-Jun-23 08:27 azureml/rag/utils/azureml.py
+-rw-rw-rw-  2.0 fat     7435 b- defN 23-Jun-23 08:27 azureml/rag/utils/connections.py
+-rw-rw-rw-  2.0 fat     1506 b- defN 23-Jun-23 08:27 azureml/rag/utils/deployment.py
+-rw-rw-rw-  2.0 fat     2634 b- defN 23-Jun-23 08:27 azureml/rag/utils/git.py
+-rw-rw-rw-  2.0 fat    10151 b- defN 23-Jun-23 08:27 azureml/rag/utils/logging.py
+-rw-rw-rw-  2.0 fat     1021 b- defN 23-Jun-23 08:33 azureml_rag-0.1.9.dist-info/LICENSE.txt
+-rw-rw-rw-  2.0 fat     6497 b- defN 23-Jun-23 08:33 azureml_rag-0.1.9.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       97 b- defN 23-Jun-23 08:33 azureml_rag-0.1.9.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat        8 b- defN 23-Jun-23 08:33 azureml_rag-0.1.9.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat     7060 b- defN 23-Jun-23 08:33 azureml_rag-0.1.9.dist-info/RECORD
+67 files, 1219347 bytes uncompressed, 177699 bytes compressed:  85.4%
```

## zipnote {}

```diff
@@ -108,17 +108,38 @@
 
 Filename: azureml/rag/_asset_client/_restclient/runhistory/operations/__init__.py
 Comment: 
 
 Filename: azureml/rag/_asset_client/_restclient/runhistory/operations/_runs_operations.py
 Comment: 
 
+Filename: azureml/rag/data_generation/qa.py
+Comment: 
+
+Filename: azureml/rag/data_generation/prompts/prompt_qa_boolean.txt
+Comment: 
+
+Filename: azureml/rag/data_generation/prompts/prompt_qa_long_answer.txt
+Comment: 
+
+Filename: azureml/rag/data_generation/prompts/prompt_qa_short_answer.txt
+Comment: 
+
+Filename: azureml/rag/data_generation/prompts/prompt_qa_summary.txt
+Comment: 
+
 Filename: azureml/rag/langchain/acs.py
 Comment: 
 
+Filename: azureml/rag/parsers/__init__.py
+Comment: 
+
+Filename: azureml/rag/parsers/markdown.py
+Comment: 
+
 Filename: azureml/rag/tasks/__init__.py
 Comment: 
 
 Filename: azureml/rag/tasks/build_faiss.py
 Comment: 
 
 Filename: azureml/rag/tasks/crack_and_chunk.py
@@ -126,14 +147,17 @@
 
 Filename: azureml/rag/tasks/embed.py
 Comment: 
 
 Filename: azureml/rag/tasks/embed_prs.py
 Comment: 
 
+Filename: azureml/rag/tasks/generate_qa.py
+Comment: 
+
 Filename: azureml/rag/tasks/git_clone.py
 Comment: 
 
 Filename: azureml/rag/tasks/register_mlindex.py
 Comment: 
 
 Filename: azureml/rag/tasks/update_acs.py
@@ -156,23 +180,23 @@
 
 Filename: azureml/rag/utils/git.py
 Comment: 
 
 Filename: azureml/rag/utils/logging.py
 Comment: 
 
-Filename: azureml_rag-0.1.8.dist-info/LICENSE.txt
+Filename: azureml_rag-0.1.9.dist-info/LICENSE.txt
 Comment: 
 
-Filename: azureml_rag-0.1.8.dist-info/METADATA
+Filename: azureml_rag-0.1.9.dist-info/METADATA
 Comment: 
 
-Filename: azureml_rag-0.1.8.dist-info/WHEEL
+Filename: azureml_rag-0.1.9.dist-info/WHEEL
 Comment: 
 
-Filename: azureml_rag-0.1.8.dist-info/top_level.txt
+Filename: azureml_rag-0.1.9.dist-info/top_level.txt
 Comment: 
 
-Filename: azureml_rag-0.1.8.dist-info/RECORD
+Filename: azureml_rag-0.1.9.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## azureml/rag/documents.py

```diff
@@ -6,27 +6,27 @@
 from collections import defaultdict
 import copy
 from dataclasses import dataclass
 from functools import lru_cache
 import json
 import mlflow
 from pathlib import Path
-from pydantic import BaseModel
 import tiktoken
 import time
-from typing import Any, IO, Iterator, List, Optional, Tuple, Union, Callable
+from typing import Any, IO, Iterable, Iterator, List, Optional, Tuple, Union, Callable
 import re
 
-from langchain.docstore.document import Document
+from langchain.docstore.document import Document as LangChainDocument
 from langchain.document_loaders.base import BaseLoader
 from langchain.document_loaders import UnstructuredFileIOLoader
 from langchain.text_splitter import TextSplitter
 
 from azureml.rag.models import init_llm
-from azureml.rag.utils.logging import get_logger
+from azureml.rag.parsers.markdown import MarkdownBlock
+from azureml.rag.utils.logging import get_logger, mlflow_enabled
 
 
 logger = get_logger(__name__)
 
 
 def merge_dicts(dict1, dict2):
     """Merge two dictionaries recursively."""
@@ -38,20 +38,20 @@
                 result[key] = merge_dicts(result[key], value)
             else:
                 result[key] = value
 
     return dict(result)
 
 
-class LazyDocument(ABC):
-    """Lazy Document"""
+class Document(ABC):
+    """Document"""
     document_id: str
 
     def __init__(self, document_id: str):
-        """Initialize Lazy Document"""
+        """Initialize Document"""
         self.document_id = document_id
 
     @abstractmethod
     def modified_time(self) -> Any:
         """Get the modified time of the document"""
         pass
 
@@ -61,32 +61,61 @@
         pass
 
     @abstractmethod
     def get_metadata(self) -> dict:
         """Get the metadata of the document"""
         pass
 
+    @abstractmethod
+    def set_metadata(self, metadata: dict):
+        """Set the metadata of the document"""
+        pass
+
     @property
     def page_content(self) -> str:
         """Get the page content of the document"""
         return self.load_data()
 
     @property
     def metadata(self) -> dict:
         """Get the metadata of the document"""
         return self.get_metadata()
 
+    @metadata.setter
+    def metadata(self, value: dict):
+        """Set the metadata of the document"""
+        self.set_metadata(value)
+
+    @abstractmethod
+    def dumps(self) -> str:
+        """Dump the document to a json string"""
+        pass
+
+    @classmethod
+    @abstractmethod
+    def loads(cls, data: str) -> "Document":
+        """Load the document from a json string"""
+        pass
+
 
-class StaticDocument(LazyDocument):
+class StaticDocument(Document):
     """Static Document holds data in-memory"""
     data: str
     _metadata: dict
 
-    def __init__(self, document_id: str, data: str, metadata: dict, mtime=None):
-        """Initialize Static Document"""
+    def __init__(self, data: str, metadata: dict, document_id: Optional[str] = None, mtime=None):
+        """Initialize StaticDocument"""
+        if document_id is None:
+            filename = metadata.get('source', {}).get('filename', None)
+            if filename is not None:
+                document_id = f"{filename}{metadata.get('source', {}).get('chunk_id', '')}"
+            else:
+                import mmh3
+                document_id = str(mmh3.hash128(data))
+
         super().__init__(document_id)
         self.data = data
         self._metadata = metadata
         self.mtime = mtime
 
     def modified_time(self) -> Any:
         """Get the modified time of the document"""
@@ -94,33 +123,49 @@
 
     def load_data(self) -> str:
         """Load the data of the document"""
         return self.data
 
     def get_metadata(self) -> dict:
         """Get the metadata of the document"""
-        return {**self._metadata, 'stats': self.document_stats()}
+        if 'stats' not in self._metadata:
+            self._metadata = {**self._metadata, 'stats': self.document_stats()}
+        return self._metadata
+
+    def set_metadata(self, metadata: dict):
+        """Set the metadata of the document"""
+        self._metadata = metadata
 
     def document_stats(self) -> dict:
         """Get the stats of the document"""
         return {
-            'tiktokens': _tiktoken_len(self.data),
+            'tiktokens': token_length_function()(self.data),
             'chars': len(self.data),
             'lines': len(self.data.splitlines()),
         }
 
     def __repr__(self):
         """Get the representation of the document"""
         return f"StaticDocument(id={self.document_id}, mtime={self.mtime}, metadata={self._metadata})"
 
+    def dumps(self) -> str:
+        """Dump the document to a json string"""
+        return json.dumps({'content': self.data, 'metadata': self._metadata, 'document_id': self.document_id})
+
+    @classmethod
+    def loads(cls, data: str) -> "Document":
+        """Load the document from a json string"""
+        data_dict = json.loads(data)
+        return cls(data_dict['content'], data_dict['metadata'], data_dict['document_id'])
+
 
 @dataclass
 class DocumentSource:
     """Document Source"""
-    path: Path
+    path: Path  # TODO:, should be full_url or something to be compat with not local Path
     filename: str
     url: str
     mtime: float
 
     def get_metadata(self) -> dict:
         """Get the metadata of the document source"""
         return {
@@ -142,43 +187,50 @@
         """Get the page content of the chunked document"""
         return "\n\n".join([chunk.page_content for chunk in self.chunks])
 
     def get_metadata(self):
         """Get the metadata of the chunked document"""
         return merge_dicts(self.metadata, {'source': self.source.get_metadata()})
 
-
-# TODO: This should be embeddings model dependent
-enc = tiktoken.get_encoding("gpt2")
+    def flatten(self) -> List[Document]:
+        """Flatten the chunked document"""
+        chunks = []
+        for i, chunk in enumerate(self.chunks):
+            chunk.metadata['source']['chunk_id'] = str(i)
+            chunks.append(chunk)
+        return chunks
+
+
+class TokenEstimator:
+    """Token Estimator"""
+    def __init__(self, encoding: str = "gpt2"):
+        """Initialize TokenEstimator"""
+        self.encoder = tiktoken.get_encoding(encoding)
+
+    def estimate(self, text: str) -> int:
+        """Estimate the number of tokens in the text"""
+        return len(self.encoder.encode(text, disallowed_special=(), allowed_special="all"))
+
+    def truncate(self, text: str, max_tokens: int) -> str:
+        """Truncate the text to the max number of tokens"""
+        return self.encoder.decode(self.encoder.encode(text, disallowed_special=(), allowed_special="all")[:max_tokens])
 
 
-def _tiktoken_len(text: str) -> int:
-    return len(enc.encode(text=text, disallowed_special=(), allowed_special="all"))
+@lru_cache(maxsize=1)
+def token_length_function(encoding: str = "gpt2") -> Callable[[str], int]:
+    """Get the token length function"""
+    return TokenEstimator(encoding).estimate
 
 
 @lru_cache(maxsize=1)
 def _init_nltk():
     import nltk
     nltk.download("punkt")
 
 
-class MarkdownBlock(BaseModel):
-    """Markdown Block"""
-
-    header: Optional[str]
-    content: str
-
-    @property
-    def header_level(self) -> int:
-        """Get the header level of the block"""
-        if self.header is None:
-            return 0
-        return self.header.count("#", 0, self.header.find(' '))
-
-
 class MarkdownHeaderSplitter(TextSplitter):
     """Split text by markdown headers."""
 
     def __init__(self, remove_hyperlinks: bool = True, remove_images: bool = True, **kwargs: Any):
         """Initialize Markdown Header Splitter"""
         from langchain.text_splitter import TokenTextSplitter
         self._remove_hyperlinks = remove_hyperlinks
@@ -193,78 +245,81 @@
 
     def create_documents(
         self, texts: List[str], metadatas: Optional[List[dict]] = None
     ) -> List[Document]:
         """Create documents from a list of texts."""
         _metadatas = metadatas or [{}] * len(texts)
         documents = []
+
+        def get_nested_heading_string(md_block):
+            nested_headings = []
+            current_block = md_block
+            while current_block is not None:
+                if current_block.header is not None:
+                    nested_headings.append(current_block.header)
+                current_block = current_block.parent
+            return "\n".join(nested_headings[::-1]) if len(nested_headings) > 0 else ""
+
         for i, text in enumerate(texts):
             for md_block in self.get_blocks(text):
                 # TODO: Handle chunk being much smaller than ideal
                 # Add to list for concat with other chunk? Make deep linking much harder,
                 # could concat sections but still chunk other sections separately if large enough?
-                if self._length_function(md_block.content) > self._chunk_size:
+                block_nested_headings = get_nested_heading_string(md_block)
+                if self._length_function(block_nested_headings + md_block.content) > self._chunk_size:
                     logger.info(f"Splitting section in chunks: {md_block.header}")
-                    chunks = self._sub_splitter.split_text(md_block.content)
+                    chunks = [f"{block_nested_headings}\n{chunk}" for chunk in self._sub_splitter.split_text(md_block.content)]
                 else:
-                    chunks = [md_block.content]
+                    chunks = [f"{block_nested_headings}\n{md_block.content}"]
 
                 metadata = _metadatas[i]
                 metadata['markdown_heading'] = {
                     'heading': re.sub(
                         r"#",
                         "",
                         md_block.header if md_block.header is not None else metadata['source']['filename']
                     ).strip(),
                     'level': md_block.header_level
                 }
                 if len(chunks) > 0:
-                    for c in chunks:
-                        new_doc = Document(
-                            page_content=c, metadata=copy.deepcopy(metadata)
+                    for chunk in chunks:
+                        new_doc = StaticDocument(
+                            chunk, metadata=copy.deepcopy(metadata)
                         )
                         documents.append(new_doc)
         return documents
 
     def get_blocks(self, markdown_text: str) -> List[MarkdownBlock]:
         """Parse blocks from markdown text."""
-        lines = markdown_text.split("\n")
+        blocks = re.split(r"(^#+\s.*)", markdown_text, flags=re.MULTILINE)
+        blocks = [b for b in blocks if b.strip()]
 
-        block_header = None
-        block_text = ""
+        markdown_blocks = []
+        header_stack = []
 
-        blocks = []
-        for line in lines:
-            header_match = re.match(r"^#+\s", line)
-            if header_match:
-                if block_header is not None:
-                    if block_text == "":
-                        continue
-                    blocks.append((block_header, block_text))
+        if not blocks[0].startswith("#"):
+            markdown_blocks.append(MarkdownBlock(header=None, content=blocks[0]))
+            blocks = blocks[1:]
 
-                block_header = line
-                block_text = ""
-            else:
-                block_text += line + "\n"
-        blocks.append((block_header, block_text))
+        for i in range(0, len(blocks), 2):
+            header = blocks[i].strip()
+            content = blocks[i + 1].strip() if i + 1 < len(blocks) else ""
+            current_block = MarkdownBlock(header=header, content=content)
+            header_level = current_block.header_level
 
-        if block_header is not None:
-            # TODO: Add all parent headers to chunk_prefix to provide more context?
-            # Inspired by: openai-cookbook - Question_answering_using_embeddings.ipynb
-            markdown_tups = [
-                MarkdownBlock(header=heading, content=heading + '\n' + MarkdownHeaderSplitter._clean_markdown(content))
-                for heading, content in blocks
-            ]
-        else:
-            markdown_tups = [
-                MarkdownBlock(header=key, content=re.sub("\n", "", value))
-                for key, value in blocks
-            ]
+            while len(header_stack) > 0 and header_stack[-1][0] >= header_level:
+                header_stack.pop()
+
+            parent_block = header_stack[-1][1] if len(header_stack) > 0 else None
+            current_block.parent = parent_block
+
+            header_stack.append((header_level, current_block))
+            markdown_blocks.append(current_block)
 
-        return markdown_tups
+        return markdown_blocks
 
     @staticmethod
     def _clean_markdown(text: str) -> str:
         # Remove html tags
         # If there's a <!-- comment -->, remove it, otherwise remove each <> pairing
         # TODO: Consider keeping some info from `<img src="img/img_name.PNG" alt="my img desc"/>`?`
         # Finding the image and doing img2text could be useful for linking back to the image,
@@ -287,23 +342,23 @@
     if "use_rcts" in arguments:
         use_rcts = arguments['use_rcts'] is True
         del arguments['use_rcts']
 
     # Handle non-natural language splitters
     if file_extension == ".py":
         from langchain.text_splitter import PythonCodeTextSplitter
-        return PythonCodeTextSplitter.from_tiktoken_encoder(**arguments)
+        return PythonCodeTextSplitter.from_tiktoken_encoder(**{**arguments, 'disallowed_special': ()})
 
     # If configured to use NLTK for splitting on sentence boundaries use that for non-code text formats
     if use_nltk:
         _init_nltk()
         from langchain.text_splitter import NLTKTextSplitter
 
         return NLTKTextSplitter(
-            length_function=_tiktoken_len,
+            length_function=token_length_function(),
             **arguments
         )
 
     # TODO: Support NLTK for splitting text as default?
     # Though want to keep MD specific splitting, only using NLTK on large chunks of plain text.
 
     # Finally use any text format specific splitters
@@ -313,17 +368,17 @@
         return TokenTextSplitter(**arguments)
     elif file_extension == ".html" or file_extension == ".htm":
         from langchain.text_splitter import TokenTextSplitter
         return TokenTextSplitter(**arguments)
     elif file_extension == ".md":
         if use_rcts:
             from langchain.text_splitter import MarkdownTextSplitter
-            return MarkdownTextSplitter.from_tiktoken_encoder(**arguments)
+            return MarkdownTextSplitter.from_tiktoken_encoder(**{**arguments, 'disallowed_special': ()})
         else:
-            return MarkdownHeaderSplitter.from_tiktoken_encoder(remove_hyperlinks=True, remove_images=True, **arguments)
+            return MarkdownHeaderSplitter.from_tiktoken_encoder(remove_hyperlinks=True, remove_images=True, **{**arguments, 'disallowed_special': ()})
     else:
         raise ValueError(f"Invalid file_extension: {file_extension}")
 
 
 file_extension_splitters = {
     ".txt": lambda **kwargs: get_langchain_splitter(".txt", kwargs),
     ".md": lambda **kwargs: get_langchain_splitter(".md", kwargs),
@@ -341,17 +396,19 @@
 
 
 # TODO: Change to be classes referenced in a map?
 def extract_text_document_title(text: str, file_name: str) -> Tuple[str, str]:
     """Extract a title from a text document."""
     file_extension = Path(file_name).suffix
     if file_extension == ".md":
-        # TODO: More targeted parsing?
-        # text.find("title: ") or re.match(r"^#\s", text), If neither of these use file_name
-        # Also pull out titleSuffix when parsing 'title: '?
+        heading_0 = re.search(r"#\s.*", text)
+        if heading_0:
+            title = heading_0.group(0).strip()
+            return title, title[2:]
+
         from bs4 import BeautifulSoup
         import markdown
         html_content = markdown.markdown(text)
         soup = BeautifulSoup(html_content, 'html.parser')
         title = ""
         clean_title = ""
         try:
@@ -410,27 +467,34 @@
         self.file = file
         self.document_source = document_source
         self.summarize_config = summarize_config
         self.metadata = metadata
 
     def load_chunked_document(self) -> ChunkedDocument:
         """Load file contents into ChunkedDocument."""
-        text = self.file.read()
+        try:
+            text = self.file.read().decode()
+        except UnicodeDecodeError:
+            self.file.seek(0)
+            # Instead of trying to guess the correct text encoding if not 'utf-8', just ignore errors and log a warning
+            logger.warning(f"UnicodeDecodeError has been ignored when reading file: {self.document_source.filename}")
+            text = self.file.read().decode("utf-8", errors="ignore")
+
         title, clean_title = extract_text_document_title(text, self.document_source.filename)
         self.metadata = {**self.metadata, "source": {"title": clean_title}}
         chunk_prefix = title + "\n\n"
         if self.summarize_config:
             chunk_prefix += f"Summary: {TextFileIOLoader.summarize_text(text, self.summarize_config)}" + "\n\n"
         return ChunkedDocument(
-            chunks=[Document(page_content=text, metadata=self.metadata)],
+            chunks=[StaticDocument(text, metadata=self.metadata)],
             source=self.document_source,
             metadata={"chunk_prefix": chunk_prefix, **self.metadata}
         )
 
-    def load(self) -> List[Document]:
+    def load(self) -> List[LangChainDocument]:
         """Load file contents into Document."""
         chunked_doc = self.load_chunked_document()
         docs = []
         for chunk in chunked_doc.chunks:
             chunk.metadata["chunk_prefix"] = chunked_doc.metadata["chunk_prefix"]
             chunk.metadata["source"] = {**chunked_doc.source.get_metadata(), **chunk.metadata["source"]}
             docs.append(chunk)
@@ -454,18 +518,18 @@
         Make the summary objective and to the point. Do not start the summary with "the text describes" or "the text explains":
         {text}:
         """
         PROMPT = PromptTemplate(template=prompt_template, input_variables=["text"])
 
         summary_splitter = NLTKTextSplitter(
             chunk_size=2500,
-            length_function=_tiktoken_len
+            length_function=token_length_function()
         )
         texts = summary_splitter.split_text(text)
-        docs = [Document(page_content=t) for t in texts]
+        docs = [LangChainDocument(page_content=t) for t in texts]
         chain = load_summarize_chain(llm, chain_type="map_reduce", map_prompt=PROMPT)
         try:
             doc_summary = chain.run(docs)
             # take only first 3 sentences from summary
             doc_summary = ' '.join(re.split(r'(?<=[.!])\s', doc_summary)[:3])
         except APIError as e:
             if "The response was filtered due to the prompt triggering Azure OpenAIâ€™s content management policy" in str(e):
@@ -567,15 +631,15 @@
 
         docs: List[Document] = list()
         reader = PdfReader(self.file)
         for page in reader.pages:
             page_text = page.extract_text()
             if page_text is not None:
                 metadata = {"page_number": reader.get_page_number(page), **self.metadata}
-                docs.append(Document(page_content=page_text, metadata=metadata))
+                docs.append(StaticDocument(page_text, metadata=metadata))
         return docs
 
 
 class TikaLoader(BaseLoader):
     """Load various unstructured files formats using Apache Tika."""
 
     def __init__(self, file: IO, document_source: DocumentSource, metadata: dict, summarize_config: Optional[dict] = None):
@@ -603,15 +667,15 @@
         from tika import parser
 
         parsed = parser.from_file(self.file)
         content = parsed["content"]
         import re
         text = re.sub(r'\n{3,}', '\n\n', content)
 
-        return [Document(page_content=text, metadata=self.metadata)]
+        return [StaticDocument(text, metadata=self.metadata)]
 
 
 file_extension_loaders = {
     ".txt": TextFileIOLoader,
     ".md": TextFileIOLoader,
     ".html": UnstructuredHTMLFileIOLoader,
     ".htm": UnstructuredHTMLFileIOLoader,
@@ -625,15 +689,15 @@
     ".xls": TikaLoader,
     ".xlsx": TikaLoader,
 }
 
 SUPPORTED_EXTENSIONS = list(file_extension_loaders.keys())
 
 
-def filter_extensions(sources: Iterator[DocumentSource], allowed_extensions=SUPPORTED_EXTENSIONS) -> Iterator[DocumentSource]:
+def filter_extensions(sources: Iterable[DocumentSource], allowed_extensions=SUPPORTED_EXTENSIONS) -> Iterator[DocumentSource]:
     """Filter out sources with extensions not in allowed_extensions."""
     total_files = 0
     skipped_files = 0
     skipped_extensions = {}
     for source in sources:
         total_files += 1
         if allowed_extensions is not None:
@@ -643,46 +707,47 @@
                 skipped_extensions[source.path.suffix] = ext_skipped + 1
                 logger.debug(f'Filtering out extension "{source.path.suffix}" source: {source.filename}')
                 continue
         yield source
     logger.info(f"[DocumentChunksIterator::filter_extensions] Filtered {skipped_files} files out of {total_files}", extra={'print': True})
 
 
-def crack_documents(sources: Iterator[DocumentSource], summary_model_config: Optional[dict] = None, file_extension_loaders=file_extension_loaders) -> Iterator[ChunkedDocument]:
+def crack_documents(sources: Iterable[DocumentSource], summary_model_config: Optional[dict] = None, file_extension_loaders=file_extension_loaders) -> Iterator[ChunkedDocument]:
     """Crack documents into chunks."""
     total_time = 0
     files_by_extension = {
         str(ext): 0.0 for ext in file_extension_loaders.keys()
     }
     for source in sources:
         files_by_extension[source.path.suffix] += 1
         file_start_time = time.time()
 
         loader_cls = file_extension_loaders.get(source.path.suffix)
         mode = "r"
         if loader_cls is None:
             raise RuntimeError(f"Unsupported file extension '{source.path.suffix}': {source.filename}")
-        elif loader_cls is TikaLoader or loader_cls is PDFFileLoader:
+        elif loader_cls is TikaLoader or loader_cls is PDFFileLoader or loader_cls is TextFileIOLoader:
             mode = "rb"
 
         with open(source.path, mode=mode) as f:
             loader = loader_cls(**{
                 "file": f,
                 "document_source": source,
                 "summarize_config": summary_model_config,
                 "metadata": {}
             })
             file_pre_yield_time = time.time()
             total_time += file_pre_yield_time - file_start_time
             yield loader.load_chunked_document()
     logger.info(f"[DocumentChunksIterator::crack_documents] Total time to load files: {total_time}\n{json.dumps(files_by_extension, indent=2)}", extra={'print': True})
-    mlflow.log_metrics(files_by_extension)
+    if mlflow_enabled():
+        mlflow.log_metrics(files_by_extension)
 
 
-def split_documents(documents: Iterator[ChunkedDocument], splitter_args: dict, file_extension_splitters=file_extension_splitters) -> Iterator[ChunkedDocument]:
+def split_documents(documents: Iterable[ChunkedDocument], splitter_args: dict, file_extension_splitters=file_extension_splitters) -> Iterator[ChunkedDocument]:
     """Split documents into chunks."""
     total_time = 0
     total_documents = 0
     total_splits = 0
     for document in documents:
         if len(document.chunks) < 1:
             continue
@@ -691,15 +756,15 @@
 
         local_splitter_args = splitter_args.copy()
 
         document_metadata = document.get_metadata()
         chunk_prefix = document_metadata.get('chunk_prefix', '')
         if len(chunk_prefix) > 0:
             if 'chunk_size' in local_splitter_args:
-                prefix_token_length = _tiktoken_len(chunk_prefix)
+                prefix_token_length = token_length_function()(chunk_prefix)
                 if prefix_token_length > local_splitter_args['chunk_size'] // 2:
                     chunk_prefix = chunk_prefix[:local_splitter_args['chunk_size'] // 2]
                     # should we update local_splitter_args['chunk_size'] here?
                 else:
                     local_splitter_args['chunk_size'] = local_splitter_args['chunk_size'] - prefix_token_length
 
         if 'chunk_prefix' in document_metadata:
@@ -728,32 +793,34 @@
 
         i = -1
         file_chunks = []
         for chunk in split_docs:
             i += 1
             if 'chunk_prefix' in chunk.metadata:
                 del chunk.metadata['chunk_prefix']
-            file_chunks.append(StaticDocument(document.source.filename + str(i), chunk_prefix + chunk.page_content, merge_dicts(chunk.metadata, document_metadata), document.source.mtime))
+            file_chunks.append(StaticDocument(chunk_prefix + chunk.page_content, merge_dicts(chunk.metadata, document_metadata), document_id=document.source.filename + str(i), mtime=document.source.mtime))
 
         file_pre_yield_time = time.time()
         total_time += file_pre_yield_time - file_start_time
         if len(file_chunks) < 1:
             logger.info('No file_chunks to yield, continuing')
             continue
         total_splits += len(file_chunks)
         document.chunks = file_chunks
         yield document
 
     logger.info(f"[DocumentChunksIterator::split_documents] Total time to split {total_documents} documents into {total_splits} chunks: {total_time}", extra={'print': True})
-    mlflow.log_metrics({
-        'Total Source Documents': total_documents,
-        'Total Chunked Documents': total_splits,
-    })
+    if mlflow_enabled():
+        mlflow.log_metrics({
+            'Total Source Documents': total_documents,
+            'Total Chunked Documents': total_splits,
+        })
 
 
+# TODO: Should handle uris via fsspec/MLTable
 def files_to_document_source(
         files_source: Union[str, Path],
         glob: str = '**/*',
         base_url: Optional[str] = None,
         process_url: Optional[Callable[[str], str]] = None) -> Iterator[DocumentSource]:
     """Convert files to DocumentSource."""
     for file in Path(files_source).glob(glob):
@@ -769,210 +836,144 @@
             path=file,
             filename=str(relative_path),
             url=url,
             mtime=file.stat().st_mtime
         )
 
 
-class DocumentChunksIterator:
+class DocumentChunksIterator(Iterator):
     """Iterate over document chunks."""
     def __init__(
             self,
             files_source: Union[str, Path],
             glob: str,
-            base_url: str,
-            document_path_replacement_regex: Optional[str],
+            base_url: str = '',
+            document_path_replacement_regex: Optional[str] = None,
             # document_sources: Iterator[DocumentSource],
-            file_filter: Optional[Callable[[Iterator[DocumentSource]], Iterator[DocumentSource]]]=filter_extensions,
-            source_loader: Callable[[Iterator[DocumentSource]], Iterator[ChunkedDocument]]=crack_documents,
-            chunked_document_processors: Optional[List[Callable[[Iterator[ChunkedDocument]], Iterator[ChunkedDocument]]]] = [
+            file_filter: Optional[Callable[[Iterable[DocumentSource]], Iterator[DocumentSource]]]=None,
+            source_loader: Callable[[Iterable[DocumentSource]], Iterator[ChunkedDocument]]=crack_documents,
+            chunked_document_processors: Optional[List[Callable[[Iterable[ChunkedDocument]], Iterator[ChunkedDocument]]]] = [
                 lambda docs: split_documents(docs, splitter_args={'chunk_size': 1024, 'chunk_overlap': 0})
             ]):
         """Initialize a document chunks iterator."""
         self.files_source = files_source
         self.glob = glob
         self.base_url = base_url
         self.document_path_replacement_regex = document_path_replacement_regex
 
         # self.document_sources = document_sources
+        if file_filter is None:
+            file_filter = self._document_statistics
         self.file_filter = file_filter
         self.source_loader = source_loader
 
         self.chunked_document_processors = chunked_document_processors
+        self.document_chunks_iterator = None
+        self.__document_statistics = None
+        self.span = None
 
-    def __iter__(self):
+    def __iter__(self) -> Iterator[ChunkedDocument]:
         """Iterate over document chunks."""
         if self.document_path_replacement_regex:
             document_path_replacement = json.loads(self.document_path_replacement_regex)
             url_replacement_match = re.compile(document_path_replacement['match_pattern'])
 
             def process_url(url):
                 return url_replacement_match.sub(document_path_replacement['replacement_pattern'], url)
         else:
             def process_url(url):
                 return url
 
+        if self.base_url is None:
+            self.base_url = self._infer_base_url_from_git(self.files_source)
+
         source_documents = files_to_document_source(self.files_source, self.glob, self.base_url, process_url)
         if self.file_filter is not None:
             source_documents = self.file_filter(source_documents)
 
         document_chunks_iterator = self.source_loader(source_documents)
 
         if self.chunked_document_processors is not None:
             for chunked_document_processor in self.chunked_document_processors:
                 document_chunks_iterator = chunked_document_processor(document_chunks_iterator)
 
-        return document_chunks_iterator
+        self.document_chunks_iterator = document_chunks_iterator
 
+        return self
 
-def document_chunks_iterator(
-        files_source: Union[str, Path],
-        glob: str,
-        allowed_extensions: Optional[List[str]],
-        base_url: str,
-        document_path_replacement_regex: Optional[str],
-        splitter_args: dict = {},
-        summary_model_config: Optional[dict] = None) -> Iterator[List[StaticDocument]]:
-    """Iterate over files in a directory and yield chunks of documents."""
-    import time
-
-    # fs, uri = url_to_fs(files_source)
-    # files = fs.glob(f'{uri}/{glob}')
-
-    files = (f for f in Path(files_source).glob(glob) if f.is_file())
-
-    # Filter file list to only those in allowed_extensions, if provided, print files that are filtered out
-    if allowed_extensions is not None:
-        def filter_extensions(files):
-            total_files = 0
-            skipped_files = 0
-            skipped_extensions = {}
-            for file in files:
-                total_files += 1
-                if file.suffix in allowed_extensions:
-                    yield file
-                else:
-                    skipped_files += 1
-                    ext_skipped = skipped_extensions.get(file.suffix, 0)
-                    skipped_extensions[file.suffix] = ext_skipped + 1
-                    logger.debug(f'Filtering out extension "{file.suffix}" file: {file}')
-            logger.info(f"[document_chunks_iterator::filter_extensions] Filtered {skipped_files} files out of {total_files}", extra={'print': True})
-            mlflow.log_metrics(skipped_extensions)
-
-        files = filter_extensions(files)
-
-    if document_path_replacement_regex:
-        document_path_replacement = json.loads(document_path_replacement_regex)
-        url_replacement_match = re.compile(document_path_replacement['match_pattern'])
-
-        def process_url(url):
-            return url_replacement_match.sub(document_path_replacement['replacement_pattern'], url)
-    else:
-        def process_url(url):
-            return url
+    def __next__(self):
+        """Get the next document chunk."""""
+        if self.document_chunks_iterator is None:
+            raise StopIteration
+        # if self.span is None:
+        #     self.span = tracer.start_span('DocumentChunksIterator::__next__')
+        try:
+            return next(self.document_chunks_iterator)
+        except StopIteration:
+            self.document_chunks_iterator = None
+            if self.span is not None:
+                self.span.end()
+            raise StopIteration
 
-    # For each file find document loader based on extension
-    def crack_documents(files) -> Iterator[List[Document]]:
-        total_time = 0
-        files_by_extension = {
-            ext: 0.0 for ext in file_extension_loaders.keys()
-        }
-        for file in files:
-            files_by_extension[file.suffix] += 1
-            file_start_time = time.time()
-
-            relative_path = file.relative_to(files_source)
-            url = relative_path
-            if base_url:
-                url = f'{base_url}/{relative_path}'
-            if document_path_replacement_regex:
-                url = process_url(url)
-
-            loader_cls = file_extension_loaders.get(file.suffix)
-            mode = "r"
-            if loader_cls is None:
-                raise RuntimeError(f"Unsupported file extension '{file.suffix}': {file}")
-            elif loader_cls is TikaLoader or loader_cls is PDFFileLoader:
-                mode = "rb"
-
-            file_info = file.stat()
-            mtime = file_info.st_mtime
-            with open(file, mode=mode) as f:
-                loader = loader_cls(**{
-                    "file": f,
-                    "file_path": file,
-                    "metadata": {
-                        "source": {
-                            "filename": str(relative_path),
-                            "url": url
-                        },
-                        "extension": file.suffix,
-                        "mtime": mtime
-                    },
-                    "summarize_config": summary_model_config
-                })
-                file_pre_yield_time = time.time()
-                total_time += file_pre_yield_time - file_start_time
-                yield loader.load()
-        logger.info(f"[document_chunks_iterator::crack_documents] Total time to load files: {total_time}\n{json.dumps(files_by_extension, indent=2)}", extra={'print': True})
-        mlflow.log_metrics(files_by_extension)
+    def document_statistics(self):
+        """Provide current statistics about the documents processed by iDocumentChunkIterator.
 
-    cracked_documents = crack_documents(files)
+        **Note:** The statistics only include files which have already been pulled through the iterator, calling this before iterating will yield None.
+        """
+        return self.__document_statistics
 
-    # For each part of loaded document apply appropriate TextSplitter
-    def split_documents(documents_per_file: Iterator[List[Document]]) -> Iterator[List[StaticDocument]]:
-        total_time = 0
-        total_documents = 0
-        total_splits = 0
-        for file_documents in documents_per_file:
-            if len(file_documents) < 1:
-                continue
-            file_start_time = time.time()
-            total_documents += len(file_documents)
+    def _document_statistics(self, sources: Iterable[DocumentSource], allowed_extensions=SUPPORTED_EXTENSIONS) -> Iterator[DocumentSource]:
+        """Filter out sources with extensions not in allowed_extensions."""
+        if self.__document_statistics is None:
+            self.__document_statistics = {
+                'total_files': 0,
+                'skipped_files': 0,
+                'skipped_extensions': {},
+                'kept_extensions': {}
+            }
+        for source in sources:
+            self.__document_statistics['total_files'] += 1
+            if allowed_extensions is not None:
+                if source.path.suffix not in allowed_extensions:
+                    self.__document_statistics['skipped_files'] += 1
+                    ext_skipped = self.__document_statistics['skipped_extensions'].get(source.path.suffix, 0)
+                    self.__document_statistics['skipped_extensions'][source.path.suffix] = ext_skipped + 1
+                    logger.debug(f'Filtering out extension "{source.path.suffix}" source: {source.filename}')
+                    continue
+            ext_kept = self.__document_statistics['kept_extensions'].get(source.path.suffix, 0)
+            self.__document_statistics['kept_extensions'][source.path.suffix] = ext_kept + 1
+            yield source
+        logger.info(f"[DocumentChunksIterator::filter_extensions] Filtered {self.__document_statistics['skipped_files']} files out of {self.__document_statistics['total_files']}")
+        if self.span is not None:
+            self.span.set_attributes({
+                f'document_statistics.{k}': v for k, v in self.__document_statistics.items()
+            })
 
-            local_splitter_args = splitter_args.copy()
+    @staticmethod
+    def _infer_base_url_from_git(files_source: Union[str, Path]) -> Optional[str]:
+        """Try and infer base_url from git repo remote info if source is in a git repo."""
+        try:
+            import git
 
-            chunk_prefix = file_documents[0].metadata.get('chunk_prefix', '')
-            if len(chunk_prefix) > 0:
-                if 'chunk_size' in local_splitter_args:
-                    prefix_token_length = _tiktoken_len(chunk_prefix)
-                    if prefix_token_length > local_splitter_args['chunk_size'] // 2:
-                        chunk_prefix = chunk_prefix[:local_splitter_args['chunk_size'] // 2]
-                    else:
-                        local_splitter_args['chunk_size'] = local_splitter_args['chunk_size'] - _tiktoken_len(chunk_prefix)
-
-            chunk_overlap = 0
-            if 'chunk_overlap' in local_splitter_args:
-                chunk_overlap = local_splitter_args['chunk_overlap']
-
-            def filter_short_docs(documents):
-                for doc in documents:
-                    doc_len = len(doc.page_content)
-                    if doc_len < chunk_overlap:
-                        logger.info(f"Filtering out doc_chunk shorter than {chunk_overlap}: {doc.metadata['source']['filename']}")
-                        continue
-                    yield doc
-
-            splitter = get_langchain_splitter(file_documents[0].metadata['extension'], local_splitter_args)
-            split_docs = splitter.split_documents(list(filter_short_docs(file_documents)))
-
-            i = -1
-            file_chunks = []
-            for chunk in split_docs:
-                i += 1
-                if 'chunk_prefix' in chunk.metadata:
-                    del chunk.metadata['chunk_prefix']
-                file_chunks.append(StaticDocument(chunk.metadata['source']['filename'] + str(i), chunk_prefix + chunk.page_content, chunk.metadata, chunk.metadata.get('mtime')))
-            file_pre_yield_time = time.time()
-            total_time += file_pre_yield_time - file_start_time
-            if len(file_chunks) < 1:
-                logger.info('No file_chunks to yield, continuing')
-                continue
-            total_splits += len(file_chunks)
-            yield file_chunks
-        logger.info(f"[document_chunks_iterator::split_documents] Total time to split {total_documents} documents into {total_splits} chunks: {total_time}", extra={'print': True})
-        mlflow.log_metrics({
-            'Total Source Documents': total_documents,
-            'Total Chunked Documents': total_splits,
-        })
+            repo = git.Repo(str(files_source), search_parent_directories=True)
+            remote_url = repo.remote().url
+            if remote_url.endswith('.git'):
+                remote_url = remote_url[:-4]
+            if remote_url.startswith('git@'):
+                remote_url = remote_url.replace(':', '/')
+                remote_url = remote_url.replace('git@', 'https://')
+
+            if 'dev.azure.com' in remote_url:
+                remote_url = remote_url.replace('https://ssh.dev.azure.com/v3/', '')
+                try:
+                    org, project, repo = remote_url.split('/')
+                    remote_url = f'https://{org}.visualstudio.com/DefaultCollection/{project}/_git/{repo}?version=GB{repo.active_branch.name}&path='
+                except Exception:
+                    logger.warning(f"Failed to parse org, project and repo from Azure DevOps remote url: {remote_url}")
+                    pass
+            else:
+                # Infer branch from repo
+                remote_url = f'{remote_url}/blob/{repo.active_branch.name}'
 
-    return split_documents(cracked_documents)
+            return remote_url
+        except Exception:
+            pass
```

## azureml/rag/embeddings.py

```diff
@@ -12,27 +12,27 @@
 import uuid
 
 from abc import ABC, abstractmethod
 from typing import Any, Callable, Iterator, List, Optional, Union, Tuple
 
 import cloudpickle
 from collections import OrderedDict
-from langchain.docstore.document import Document
+from langchain.docstore.document import Document as LangChainDocument
 from langchain.document_loaders.base import BaseLoader
 from langchain.embeddings.base import Embeddings as Embedder
+from langchain.embeddings.openai import OpenAIEmbeddings
 from pathlib import Path
 import pyarrow as pa
 import pyarrow.parquet as pq
 import time
 import yaml
 
-from azureml.rag.documents import LazyDocument
-from azureml.rag.models import parse_model_uri
-from azureml.rag.utils.connections import get_connection_credential, get_connection_by_id_v2, workspace_connection_to_credential
-from azureml.rag.utils.logging import get_logger, track_activity
+from azureml.rag.documents import Document, DocumentChunksIterator
+from azureml.rag.models import parse_model_uri, init_open_ai_from_config
+from azureml.rag.utils.logging import get_logger, track_activity, mlflow_enabled
 
 
 logger = get_logger(__name__)
 
 
 def patch_openai_embedding_retries(logger, activity_logger, max_seconds_retrying=540):
     """Patch the openai embedding to retry on failure."""""
@@ -77,15 +77,15 @@
             self.activity_logger = activity_logger
 
         def __call__(self, retry_state) -> bool:
             first_retry = self.activity_logger.activity_info.get('first_retry', None)
             if first_retry:
                 return (datetime.utcnow() - first_retry).seconds >= self.max_delay
             else:
-                False
+                return False
 
     # Copied from https://github.com/hwchase17/langchain/blob/511c12dd3985ce682226371c12f8fa70d8c9a8e1/langchain/embeddings/openai.py#L34
     def _create_retry_decorator(embeddings):
         import openai
 
         min_seconds = 4
         max_seconds = 10
@@ -105,65 +105,25 @@
             ),
             before_sleep=_log_it,
         )
 
     langchain_openai._create_retry_decorator = _create_retry_decorator
 
 
-def _parse_open_ai_args(arguments: dict):
-    from langchain.embeddings.openai import OpenAIEmbeddings
+def _parse_open_ai_args(arguments: dict) -> OpenAIEmbeddings:
     import openai
 
-    logger.info('OpenAI arguments: \n')
-    logger.info('\n'.join(f'{k}={v}' for k, v in arguments.items()))
-
-    if "api_type" in arguments:
-        openai.api_type = arguments["api_type"]
-
-    if "azure" in openai.api_type:
-        openai.api_version = arguments.get("api_version", "2023-03-15-preview")
-
-    if "endpoint" in arguments:
-        openai.api_base = arguments["endpoint"]
-
-    if "api_base" in arguments:
-        openai.api_base = arguments["api_base"]
-
-    if "connection_type" not in arguments:
-        openai.api_key = os.environ["OPENAI_API_KEY"]
-    else:
-        if arguments["connection_type"] == "workspace_connection":
-            connection_id = arguments.get('connection', {}).get('id', '')
-            connection = get_connection_by_id_v2(connection_id)
-            openai.api_base = connection.get('properties', {}).get('target')
-            openai.api_version = connection.get('properties', {}).get('metadata', {}).get('apiVersion', '2023-03-15-preview')
-            openai.api_type = connection.get('properties', {}).get('metadata', {}).get('apiType', openai.api_type)
-            if openai.api_type == 'azure_ad' or openai.api_type == 'azuread':
-                from azure.identity import DefaultAzureCredential
-                credential = DefaultAzureCredential()
-            else:
-                credential = workspace_connection_to_credential(connection)
-        else:
-            credential = get_connection_credential(arguments)
-        if hasattr(credential, 'key'):
-            openai.api_key = credential.key
-        else:
-            # Add hack to check for "BAKER-OPENAI-API-KEY"
-            if arguments.get("connection_type", "workspace_keyvault") == "workspace_keyvault":
-                new_args = copy.deepcopy(arguments)
-                new_args["connection"]["key"] = "BAKER-OPENAI-API-KEY"
-                credential = get_connection_credential(new_args)
-                if hasattr(credential, 'key'):
-                    openai.api_key = credential.key
-                else:
-                    openai.api_key = credential.get_token('https://cognitiveservices.azure.com/.default').token
+    arguments = init_open_ai_from_config(arguments)
 
     embedder = OpenAIEmbeddings(
-        openai_api_key=openai.api_key,
-        max_retries=100,
+        openai_api_base=arguments.get("api_base", openai.api_base),
+        openai_api_type=arguments.get("api_type", openai.api_type),
+        openai_api_version=arguments.get("api_version", openai.api_version),
+        openai_api_key=arguments.get("api_key", openai.api_key),
+        max_retries=100,  # TODO: Make this configurable
     )
 
     if "model_name" in arguments:
         embedder.model = arguments["model_name"]
 
     if "model" in arguments:
         embedder.model = arguments["model"]
@@ -221,14 +181,16 @@
 
             def embed_query(self, query: str) -> List[float]:
                 return []
 
         return NoneEmbeddings()
     elif embedding_kind == "custom":
         raise NotImplementedError("Custom embeddings are not supported yet.")
+    else:
+        raise ValueError(f"Unknown embedding kind: {embedding_kind}")
 
 
 def get_embed_fn(embedding_kind: str, arguments: dict) -> Callable[[List[str]], List[List[float]]]:
     """Get an embedding function from the given arguments."""
     if "open_ai" in embedding_kind:
         embedder = _parse_open_ai_args(arguments)
 
@@ -363,19 +325,19 @@
             logger.debug(f'caching embeddings file: \n{path}\n   previous path cached was: \n{cls._last_opened_embeddings}')
             table = pq.read_table(path)
             cls._last_opened_embeddings = (path, table)
 
         return cls._last_opened_embeddings[1]
 
 
-class LangChainDocument(LazyDocument):
+class WrappedLangChainDocument(Document):
     """A document with an embedding and a reference to the data."""
-    document: Document
+    document: LangChainDocument
 
-    def __init__(self, document: Document):
+    def __init__(self, document: LangChainDocument):
         """Initialize the document."""
         super().__init__(str(uuid.uuid4()))
         self.document = document
 
     def modified_time(self) -> Any:
         """Get the modified time of the document."""
         self.document.metadata.get("mtime", None)
@@ -384,16 +346,20 @@
         """Load the data of the document."""
         return self.document.page_content
 
     def get_metadata(self) -> dict:
         """Get the metadata of the document."""
         return self.document.metadata
 
+    def set_metadata(self, metadata: dict):
+        """Set the metadata of the document"""
+        self.document.metadata = metadata
 
-class Embeddings:
+
+class EmbeddingsContainer:
     """A class for generating embeddings."""
     _embeddings_schema = ["doc_id", "mtime", "hash", "metadata", "path_to_data", "index", "is_local"]
     _data_schema = ["data", "embeddings"]
     _model_context_lengths = {
         "text-embedding-ada-002": 2047
     }
     kind: str
@@ -415,18 +381,18 @@
         self.kind = kind
         self.arguments = kwargs
         self._embed_fn = get_embed_fn(kind, kwargs)
         self._document_embeddings = OrderedDict()
         self.dimension = kwargs.get('dimension', None)
 
     @staticmethod
-    def from_uri(uri: str, **kwargs) -> 'Embeddings':
+    def from_uri(uri: str, **kwargs) -> 'EmbeddingsContainer':
         """Create an embeddings object from a URI."""
         config = parse_model_uri(uri, **kwargs)
-        return Embeddings(**{**config, **kwargs})
+        return EmbeddingsContainer(**{**config, **kwargs})
 
     def get_metadata(self):
         """Get the metadata of the embeddings."""
         if self.kind == "custom":
             arguments = copy.deepcopy(self.arguments)
             arguments["pickled_embedding_fn"] = gzip.compress(
                 cloudpickle.dumps(arguments["embedding_fn"]))
@@ -444,29 +410,29 @@
             "dimension": self.get_embedding_dimensions(),
             **self.arguments
         }
 
         return metadata
 
     @staticmethod
-    def from_metadata(metadata: dict) -> 'Embeddings':
+    def from_metadata(metadata: dict) -> 'EmbeddingsContainer':
         """Create an embeddings object from metadata."""
         schema_version = metadata.get('schema_version', "1")
         if schema_version == "1":
-            embeddings = Embeddings(metadata['kind'], **metadata['arguments'])
+            embeddings = EmbeddingsContainer(metadata['kind'], **metadata['arguments'])
             return embeddings
         elif schema_version == "2":
             kind = metadata['kind']
             del metadata['kind']
             if kind == "custom":
                 metadata['embedding_fn'] = cloudpickle.loads(
                     gzip.decompress(metadata['pickled_embedding_fn']))
                 del metadata['pickled_embedding_fn']
 
-            embeddings = Embeddings(kind, **metadata)
+            embeddings = EmbeddingsContainer(kind, **metadata)
             return embeddings
         else:
             raise ValueError(f"Schema version {schema_version} is not supported")
 
     @staticmethod
     def load(dir_name: str, embeddings_container_path, metadata_only=False):
         """Load embeddings from a directory."""
@@ -475,22 +441,22 @@
         logger.info(f'loading embeddings from : {path}')
         with open(f"{path}/embeddings_metadata.yaml", "r") as f:
             metadata = yaml.safe_load(f)
 
         if metadata is None:
             raise ValueError("Metadata file is empty.")
 
-        embeddings = Embeddings.from_metadata(metadata)
+        embeddings = EmbeddingsContainer.from_metadata(metadata)
         if not metadata_only:
             embedding_partitions_files = list(pathlib.Path(path).glob('embeddings*.parquet'))
             logger.info(f'found following embedding partitions: {embedding_partitions_files}')
             for partition in embedding_partitions_files:
                 logger.info(f'processing partition: {partition}')
                 table = pq.read_table(partition)
-                for column in Embeddings._embeddings_schema:
+                for column in EmbeddingsContainer._embeddings_schema:
                     if column not in table.column_names:
                         raise ValueError(f'Format of provided embedding file ({partition}) is not supported.  Missing column {column}')
                 # TODO: Keep pyarrow partition in Embeddings instance and give out `ReferenceEmbeddedDocument` when iterated over/indexed into with doc_id.
                 # Allows each partition of embeddings to be stored in one array and users to retrieve it as one array with cow properties.
                 for i in range(table.num_rows):
                     doc_id = table.column('doc_id')[i].as_py()
                     mtime = table.column('mtime')[i].as_py()
@@ -554,15 +520,15 @@
                 pa.array(doc_ids),
                 pa.array(mtimes),
                 pa.array(hashes),
                 pa.array(metadata),
                 pa.array(paths),
                 pa.array(indexes),
                 pa.array(is_local)
-            ], names=Embeddings._embeddings_schema)
+            ], names=EmbeddingsContainer._embeddings_schema)
         pq.write_table(table, os.path.join(path, embeddings_file_name))
 
         # write data.paruet with data embedded in this run
         data_table = pa.Table.from_arrays(
             [
                 pa.array(local_data),
                 pa.array(embeddings)
@@ -602,64 +568,66 @@
         """Returns a function that truncates text to the embedding context length if it's set."""
         model = ''
         if 'model' in self.arguments:
             model = self.arguments['model']
         elif 'model_name' in self.arguments:
             model = self.arguments['model_name']
 
-        ctx_length = Embeddings._model_context_lengths.get(model, None)
+        ctx_length = EmbeddingsContainer._model_context_lengths.get(model, None)
 
         if ctx_length:
             import tiktoken
 
             def truncate_by_tokens(text):
                 enc = tiktoken.encoding_for_model(model)
                 # Some chunks still managed to tokenize above the limit so leaving 20 tokens buffer.
                 tokens = enc.encode(text=text)[:ctx_length - 20]
                 return enc.decode(tokens)
             return truncate_by_tokens
         else:
             return lambda text: text
 
-    def embed(self, input_documents: Union[Iterator[LazyDocument], BaseLoader]):
+    def embed(self, input_documents: Union[Iterator[Document], BaseLoader, DocumentChunksIterator]):
         """
         Embeds inout documents if they are new or changed and mutates current instance to drop embeddings for documents that are no longer present.
         """
         self._document_embeddings = self._get_embeddings_internal(input_documents)
         return self
 
-    def embed_and_create_new_instance(self, input_documents: Union[Iterator[LazyDocument], BaseLoader]) -> 'Embeddings':
+    def embed_and_create_new_instance(self, input_documents: Union[Iterator[Document], BaseLoader, DocumentChunksIterator]) -> 'EmbeddingsContainer':
         """
         Embeds input documents if they are new or changed and returns a new instance with the new embeddings. Current instance is not mutated.
         """
         document_embeddings = self._get_embeddings_internal(input_documents)
-        new_embeddings = Embeddings(self.kind, **self.arguments)
+        new_embeddings = EmbeddingsContainer(self.kind, **self.arguments)
         new_embeddings._document_embeddings = document_embeddings
         return new_embeddings
 
-    def _get_embeddings_internal(self, input_documents: Union[Iterator[LazyDocument], BaseLoader]) -> OrderedDict:
+    def _get_embeddings_internal(self, input_documents: Union[Iterator[Document], BaseLoader, DocumentChunksIterator]) -> OrderedDict:
         if self._embed_fn is None:
             raise ValueError("No embed function provided.")
 
-        # TODO: mlflow.set_tracking_uri if not set already?
-
         if isinstance(input_documents, BaseLoader):
-            input_documents = iter([LangChainDocument(d)
+            input_documents = iter([WrappedLangChainDocument(d)
                                    for d in input_documents.load()])
+        elif isinstance(input_documents, DocumentChunksIterator):
+            flattened_docs = []
+            for chunked_doc in input_documents:
+                flattened_docs.extend(chunked_doc.flatten())
+            input_documents = iter(flattened_docs)
 
         documents_to_embed = []
         documents_embedded = OrderedDict()
         for document in input_documents:
-            if isinstance(document, Document):
-                document = LangChainDocument(document)
+            if isinstance(document, LangChainDocument):
+                document = WrappedLangChainDocument(document)
 
             logger.info(f'Processing document: {document.document_id}')
             mtime = document.modified_time()
-            current_embedded_document = self._document_embeddings.get(
-                document.document_id)
+            current_embedded_document = self._document_embeddings.get(document.document_id)
             if mtime \
                     and current_embedded_document \
                     and current_embedded_document.mtime \
                     and current_embedded_document.mtime == mtime:
                 documents_embedded[document.document_id] = current_embedded_document
 
                 try:
@@ -683,18 +651,19 @@
 
             document_metadata = document.get_metadata()
             documents_to_embed.append(
                 (document.document_id, mtime, document_data, document_hash, document_metadata))
 
         logger.info(f'Documents to embed: {len(documents_to_embed)}'
                     f'\nDocuments reused: {len(documents_embedded.keys())}')
-        mlflow.log_metrics({
-            'Embedded Documents': len(documents_to_embed),
-            'Reused Documents': len(documents_embedded.keys())
-        })
+        if mlflow_enabled():
+            mlflow.log_metrics({
+                'Embedded Documents': len(documents_to_embed),
+                'Reused Documents': len(documents_embedded.keys())
+            })
 
         truncate_func = self.get_embedding_ctx_length_truncate_func()
 
         data_to_embed = [truncate_func(t) for (_, _, t, _, _) in documents_to_embed]
 
         embeddings = []
         try:
@@ -732,28 +701,31 @@
         num_source_docs = 0
         documents = []
         embeddings = []
         for doc_id, emb_doc in self._document_embeddings.items():
             logger.info(f'Adding document: {doc_id}')
             logger.debug(f'{doc_id},{emb_doc.document_hash},{emb_doc.get_embeddings()[0:20]}')
             embeddings.append(emb_doc.get_embeddings())
-            # TODO: LazyDocument/RefDocument gets uri to page_content
+            # TODO: Document/RefDocument gets uri to page_content
             documents.append(
-                Document(
+                LangChainDocument(
                     page_content=emb_doc.get_data(),
                     metadata={
                         "source_doc_id": doc_id,
                         "chunk_hash": emb_doc.document_hash,
                         "mtime": emb_doc.mtime,
                         **emb_doc.metadata
                     }
                 )
             )
             num_source_docs += 1
 
+        if len(embeddings) == 0:
+            raise ValueError("No embeddings to index")
+
         index_to_id = {i: doc.metadata["source_doc_id"] for i, doc in enumerate(documents)}
         docstore = InMemoryDocstore(
             {index_to_id[i]: doc for i, doc in enumerate(documents)}
         )
 
         faiss = dependable_faiss_import()
         index = faiss.IndexFlatL2(len(embeddings[0]))
@@ -776,7 +748,12 @@
         mlindex_config["index"] = {
             "kind": "faiss",
             "engine": "langchain.vectorstores.FAISS",
             "method": "FlatL2"
         }
         with open(output_path / "MLIndex", "w") as f:
             yaml.dump(mlindex_config, f)
+
+    # def as_acs_index(endpoint: str, index_name: str, field_mapping: dict, credential: Optional[object] = None):
+    #     from azureml.rag.tasks.update_acs import create_index_from_raw_embeddings
+
+    #     return create_index_from_raw_embeddings()
```

## azureml/rag/mlindex.py

```diff
@@ -1,71 +1,101 @@
 # ---------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
 """MLIndex class for interacting with MLIndex assets."""
-from azureml.rag.embeddings import Embeddings
+from azureml.rag.documents import Document, DocumentChunksIterator
+from azureml.rag.embeddings import EmbeddingsContainer
 from azureml.rag.utils.connections import get_connection_credential
 from azureml.rag.utils.logging import get_logger
+from enum import Enum
+from langchain.document_loaders.base import BaseLoader
+from langchain.schema import Document as LangChainDocument
+from pathlib import Path
 import tempfile
-from typing import Union
+from typing import Any, Dict, Iterable, Iterator, List, Optional, Union
+import yaml
 
 
 logger = get_logger('mlindex')
 
 
 class MLIndex:
     """MLIndex class for interacting with MLIndex assets."""
     base_uri: str
     index_config: dict
     embeddings_config: dict
 
-    def __init__(self, uri: Union[str, object]):
-        """Initialize MLIndex from a URI or AzureML Data Asset"""
-        if isinstance(uri, str):
-            uri = str(uri)
-        else:
-            # Assume given AzureML Data Asset
-            uri = uri.path
-        import yaml
-        try:
-            import fsspec
-        except ImportError:
-            raise ValueError(
-                "Could not import fsspec python package. "
-                "Please install it with `pip install fsspec`."
-            )
-        try:
-            import azureml.fsspec
-        except ImportError:
-            raise ValueError(
-                "Could not import azureml-fsspec python package. "
-                "Please install it with `pip install azureml-fsspec`."
-            )
+    _underlying_index: Any = None
 
-        self.base_uri = uri
-
-        mlindex_yaml = None
-        try:
-            mlindex_file = fsspec.open(f"{uri.rstrip('/')}/MLIndex", 'r')
-            # parse yaml to dict
-            with mlindex_file as f:
-                mlindex_yaml = yaml.safe_load(f)
-        except Exception as e:
-            raise ValueError(f"Could not find MLIndex: {e}") from e
+    def __init__(self, uri: Optional[Union[str, object]] = None, mlindex_config: Optional[dict] = None):
+        """Initialize MLIndex from a URI or AzureML Data Asset"""
+        if uri is not None:
+            if isinstance(uri, str):
+                uri = str(uri)
+            else:
+                # Assume given AzureML Data Asset
+                uri = uri.path
+            try:
+                import fsspec
+            except ImportError:
+                raise ValueError(
+                    "Could not import fsspec python package. "
+                    "Please install it with `pip install fsspec`."
+                )
+            try:
+                import azureml.fsspec
+            except ImportError:
+                raise ValueError(
+                    "Could not import azureml-fsspec python package. "
+                    "Please install it with `pip install azureml-fsspec`."
+                )
+
+            self.base_uri = uri
+
+            mlindex_config = None
+            try:
+                mlindex_file = fsspec.open(f"{uri.rstrip('/')}/MLIndex", 'r')
+                # parse yaml to dict
+                with mlindex_file as f:
+                    mlindex_config = yaml.safe_load(f)
+            except Exception as e:
+                raise ValueError(f"Could not find MLIndex: {e}") from e
+        elif mlindex_config is None:
+            raise ValueError("Must provide either uri or mlindex_config")
 
-        self.index_config = mlindex_yaml.get('index', {})
+        self.index_config = mlindex_config.get('index', {})
         if self.index_config is None:
             raise ValueError("Could not find index config in MLIndex yaml")
-        self.embeddings_config = mlindex_yaml.get('embeddings', {})
+        self.embeddings_config = mlindex_config.get('embeddings', {})
         if self.embeddings_config is None:
             raise ValueError("Could not find embeddings config in MLIndex yaml")
 
+    @property
+    def name(self) -> str:
+        """Returns the name of the MLIndex."""
+        return self.index_config.get('name', '')
+
+    @name.setter
+    def name(self, value: str):
+        """Sets the name of the MLIndex."""
+        self.index_config['name'] = value
+
+    @property
+    def description(self) -> str:
+        """Returns the description of the MLIndex."""
+        return self.index_config.get('description', '')
+
+    @description.setter
+    def description(self, value: str):
+        """Sets the description of the MLIndex."""
+        self.index_config['description'] = value
+
     def get_langchain_embeddings(self):
         """Get the LangChainEmbeddings from the MLIndex"""
-        embeddings = Embeddings.from_metadata(self.embeddings_config)
+        embeddings = EmbeddingsContainer.from_metadata(self.embeddings_config)
 
         return embeddings.as_langchain_embeddings()
 
     def as_langchain_vectorstore(self):
         """Converts MLIndex to a retriever object that can be used with langchain, may download files."""
         index_kind = self.index_config.get('kind', None)
         if index_kind == 'acs':
@@ -80,15 +110,15 @@
                 field_mapping=self.index_config.get('field_mapping', {}),
                 credential=credential,
             )
         elif index_kind == 'faiss':
             from fsspec.core import url_to_fs
             from langchain.vectorstores.faiss import FAISS
 
-            embeddings = Embeddings.from_metadata(self.embeddings_config).as_langchain_embeddings()
+            embeddings = EmbeddingsContainer.from_metadata(self.embeddings_config).as_langchain_embeddings()
 
             fs, uri = url_to_fs(self.base_uri)
 
             with tempfile.TemporaryDirectory() as tmpdir:
                 fs.download(f"{uri.rstrip('/')}/index.pkl", f"{str(tmpdir)}")
                 fs.download(f"{uri.rstrip('/')}/index.faiss", f"{str(tmpdir)}")
                 langchain_faiss = FAISS.load_local(str(tmpdir), embeddings)
@@ -115,8 +145,11 @@
         elif index_kind == 'faiss':
             return self.as_langchain_vectorstore().as_retriever()
         else:
             raise ValueError(f"Unknown index kind: {index_kind}")
 
     def __repr__(self):
         """Returns a string representation of the MLIndex object."""
-        return f"MLIndex(index_config={self.index_config}, embeddings_config={self.embeddings_config})"
+        return yaml.dump({
+            'index': self.index_config,
+            'embeddings': self.embeddings_config,
+        })
```

## azureml/rag/models.py

```diff
@@ -1,20 +1,24 @@
 # ---------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
 """Language model classes."""
+import copy
 import json
+import os
 from langchain.llms import AzureOpenAI
 try:
     # Some after 0.0.149 this was moved.
     from langchain.schema import BaseLanguageModel
 except ImportError:
     from langchain.base_language import BaseLanguageModel
 from langchain.chat_models.azure_openai import AzureChatOpenAI
+from langchain.chat_models.openai import ChatOpenAI
 
+from azureml.rag.utils.connections import get_connection_credential, get_connection_by_id_v2, workspace_connection_to_credential
 from azureml.rag.utils.logging import get_logger
 
 
 logger = get_logger(__name__)
 
 
 def parse_model_uri(uri: str, **kwargs) -> dict:
@@ -33,15 +37,15 @@
         config = {**split_details(details), **config}
         config['kind'] = 'open_ai'
         if 'endpoint' in config:
             config['api_base'] = f"https://{config['endpoint']}.openai.azure.com"
         elif 'endpoint' in kwargs:
             config['api_base'] = f"https://{kwargs['endpoint']}.openai.azure.com"
         config['api_type'] = 'azure'
-        config['api_version'] = kwargs.get('api_version') if kwargs.get('api_version') is not None else '2023-03-15-preview'
+        config['api_version'] = kwargs.get('api_version') if kwargs.get('api_version') is not None else '2023-05-15'
         # Azure OpenAI has a batch_size limit of 1
         config['batch_size'] = '1'
     elif scheme == 'open_ai':
         config['kind'] = 'open_ai'
         config = {**split_details(details), **config}
         config['api_type'] = 'open_ai'
     elif scheme == 'hugging_face':
@@ -51,45 +55,118 @@
         config['kind'] = 'none'
     else:
         raise ValueError(f'Unknown model kind: {scheme}')
 
     return config
 
 
-def init_llm(model_config: dict) -> BaseLanguageModel:
+def init_open_ai_from_config(config: dict) -> dict:
+    """Initialize an OpenAI model from a configuration dictionary."""
+    import openai
+
+    logger.info('OpenAI arguments: \n')
+    logger.info('\n'.join(f'{k}={v}' if k != 'key' or k != 'api_key' else f'{k}=[REDACTED]' for k, v in config.items()))
+
+    if config.get('key') is not None:
+        config['api_key'] = config.get('key')
+    elif "connection_type" not in config:
+        config['api_key'] = os.environ.get("OPENAI_API_KEY", None)
+        if config['api_key'] is None and 'azure' in config['api_type']:
+            from azure.identity import DefaultAzureCredential
+
+            credential = DefaultAzureCredential()
+            config['api_key'] = credential.get_token('https://cognitiveservices.azure.com/.default').token
+            config['api_type'] = 'azure_ad'
+    else:
+        if config["connection_type"] == "workspace_connection":
+            connection_id = config.get('connection', {}).get('id', '')
+            connection = get_connection_by_id_v2(connection_id)
+            config['api_base'] = connection.get('properties', {}).get('target')
+            config['api_version'] = connection.get('properties', {}).get('metadata', {}).get('apiVersion', '2023-05-15')
+            config['api_type'] = connection.get('properties', {}).get('metadata', {}).get('apiType', openai.api_type)
+            if openai.api_type == 'azure_ad' or openai.api_type == 'azuread':
+                from azure.identity import DefaultAzureCredential
+
+                credential = DefaultAzureCredential()
+            else:
+                credential = workspace_connection_to_credential(connection)
+        else:
+            credential = get_connection_credential(config)
+
+        if not hasattr(credential, 'key'):
+            # Add hack to check for "BAKER-OPENAI-API-KEY"
+            if config.get("connection_type", "workspace_keyvault") == "workspace_keyvault":
+                new_args = copy.deepcopy(config)
+                new_args["connection"]["key"] = "BAKER-OPENAI-API-KEY"
+                credential = get_connection_credential(new_args)
+
+        if hasattr(credential, 'key'):
+            config['api_key'] = credential.key
+        else:
+            config['api_key'] = credential.get_token('https://cognitiveservices.azure.com/.default').token
+            config['api_type'] = 'azure_ad'
+
+    if "azure" in openai.api_type:
+        config["api_version"] = config.get("api_version", "2023-05-15")
+
+    return config
+
+
+def init_llm(model_config: dict, **kwargs) -> BaseLanguageModel:
     """Initialize a language model from a model configuration."""
     llm = None
     logger.debug(f"model_config: {json.dumps(model_config, indent=2)}")
+    model_kwargs = {
+        "frequency_penalty": model_config.get('frequency_penalty', 0),
+        "presence_penalty": model_config.get('presence_penalty', 0),
+    }
+    if model_config.get('stop') is not None:
+        model_kwargs["stop"] = model_config.get('stop')
     if model_config.get('kind') == 'open_ai' and model_config.get('api_type') == 'azure':
         if model_config['model'].startswith("gpt-3.5-turbo") or model_config['model'].startswith("gpt-35-turbo") or model_config['model'].startswith("gpt-4"):
-            logger.info(f"Initializing AzureChatOpenAI with model {model_config['model']}")
-            model_kwargs = {
-                "engine": model_config['deployment'],
-                "frequency_penalty": model_config.get('frequency_penalty', 0),
-                "presence_penalty": model_config.get('presence_penalty', 0),
-                "stop": model_config.get('stop'),
-            }
+            logger.info(f"Initializing AzureChatOpenAI with model {model_config['model']} with kwargs: {model_kwargs}")
+
+            model_config = init_open_ai_from_config(model_config)
             llm = AzureChatOpenAI(
                 deployment_name=model_config['deployment'],
-                model_name=model_config['model'],
-                temperature=model_config.get('temperature'),
+                model=model_config['model'],
                 max_tokens=model_config.get('max_tokens'),
                 model_kwargs=model_kwargs,
-                openai_api_key=model_config.get('key'),
+                openai_api_key=model_config.get('api_key'),
                 openai_api_base=model_config.get('api_base'),
                 openai_api_type=model_config.get('api_type'),
-                openai_api_version=model_config.get('api_version')
+                openai_api_version=model_config.get('api_version'),
+                max_retries=model_config.get('max_retries'),
+                **kwargs
             )  # type: ignore
+            if model_config.get('temperature', None) is not None:
+                llm.temperature = model_config.get('temperature')
         else:
-            print(f"Initializing AzureOpenAI with model {model_config['model']}")
+            logger.info(f"Initializing AzureOpenAI with model {model_config['model']} with kwargs: {model_kwargs}")
+
             llm = AzureOpenAI(
                 deployment_name=model_config['deployment'],
-                model_name=model_config['model'],
-                temperature=model_config.get('temperature'),
+                model=model_config['model'],
                 max_tokens=model_config.get('max_tokens'),
-                model_kwargs={"stop": model_config.get('stop')},
-                openai_api_key=model_config.get('key')
+                model_kwargs=model_kwargs,
+                openai_api_key=model_config.get('api_key'),
+                max_retries=model_config.get('max_retries'),
+                **kwargs
             )  # type: ignore
+            if model_config.get('temperature', None) is not None:
+                llm.temperature = model_config.get('temperature')
+    elif model_config.get('kind') == 'open_ai' and model_config.get('api_type') == 'open_ai':
+        logger.info(f"Initializing OpenAI with model {model_config['model']} with kwargs: {model_kwargs}")
+
+        llm = ChatOpenAI(
+            model=model_config['model'],
+            max_tokens=model_config.get('max_tokens'),
+            model_kwargs=model_kwargs,
+            openai_api_key=model_config.get('api_key'),
+            **kwargs
+        )  # type: ignore
+        if model_config.get('temperature', None) is not None:
+            llm.temperature = model_config.get('temperature')
     else:
         raise ValueError(f"Unsupported llm kind: {model_config.get('kind')}")
 
     return llm
```

## azureml/rag/tasks/build_faiss.py

```diff
@@ -1,50 +1,73 @@
 # ---------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
 """File for building FAISS Indexes."""
 import os
-from pathlib import Path
+import time
+import traceback
 
-from azureml.rag.embeddings import Embeddings
-from azureml.rag.utils.logging import get_logger, enable_stdout_logging, enable_appinsights_logging, track_activity
+from azureml.rag.embeddings import EmbeddingsContainer
+from azureml.rag.utils.logging import get_logger, enable_stdout_logging, enable_appinsights_logging, track_activity, _logger_factory
+from pathlib import Path
 
 
 logger = get_logger('build_faiss')
 
 
+def main(args, logger, activity_logger):
+    raw_embeddings_uri = args.embeddings
+    logger.info(f'got embeddings uri as input: {raw_embeddings_uri}')
+    splits = raw_embeddings_uri.split('/')
+    embeddings_dir_name = splits.pop(len(splits)-2)
+    logger.info(f'extracted embeddings directory name: {embeddings_dir_name}')
+    parent = '/'.join(splits)
+    logger.info(f'extracted embeddings container path: {parent}')
+
+    # Mock OPENAI_API_KEY being set so that loading Embeddings doesn't fail, we don't need to do any embedding so should be fine
+    os.environ['OPENAI_API_KEY'] = 'nope'
+
+    from azureml.dataprep.fuse.dprepfuse import (MountOptions, rslex_uri_volume_mount)
+    mnt_options = MountOptions(
+        default_permission=0o555, allow_other=False, read_only=True)
+    logger.info(f'mounting embeddings container from: \n{parent} \n   to: \n{os.getcwd()}/raw_embeddings', extra={'print': True})
+    activity_logger.info("Mounting embeddings container")
+    try:
+        with rslex_uri_volume_mount(parent, f'{os.getcwd()}/raw_embeddings', options=mnt_options) as mount_context:
+            logger.info("Loading Embeddings")
+            emb = EmbeddingsContainer.load(embeddings_dir_name, mount_context.mount_point)
+            activity_logger.activity_info["num_documents"] = len(emb._document_embeddings)
+            emb.write_as_faiss_mlindex(Path(args.output))
+    except Exception as e:
+        activity_logger.activity_info["error"] = str(e)
+        logger.error(f'Failed to load embeddings: {e}')
+        raise e
+    logger.info('Generated FAISS index')
+
+
+def main_wrapper(args, logger):
+    with track_activity(logger, "build_faiss") as activity_logger:
+        try:
+            main(args, logger, activity_logger)
+        except Exception:
+            activity_logger.error(f"build_faiss failed with exception: {traceback.format_exc()}")  # activity_logger doesn't log traceback
+            raise
+
 if __name__ == '__main__':
     from argparse import ArgumentParser
 
     parser = ArgumentParser()
     parser.add_argument("--embeddings", type=str)
     parser.add_argument("--output", type=str)
     args = parser.parse_args()
 
     print('\n'.join(f'{k}={v}' for k, v in vars(args).items()))
 
     enable_stdout_logging()
     enable_appinsights_logging()
 
-    with track_activity(logger, 'build_faiss') as activity_logger:
-        raw_embeddings_uri = args.embeddings
-        logger.info(f'got embeddings uri as input: {raw_embeddings_uri}')
-        splits = raw_embeddings_uri.split('/')
-        embeddings_dir_name = splits.pop(len(splits)-2)
-        logger.info(f'extracted embeddings directory name: {embeddings_dir_name}')
-        parent = '/'.join(splits)
-        logger.info(f'extracted embeddings container path: {parent}')
-
-        # Mock OPENAI_API_KEY being set so that loading Embeddings doesn't fail, we don't need to do any embedding so should be fine
-        os.environ['OPENAI_API_KEY'] = 'nope'
-
-        from azureml.dataprep.fuse.dprepfuse import (MountOptions, rslex_uri_volume_mount)
-        mnt_options = MountOptions(
-            default_permission=0o555, allow_other=False, read_only=True)
-        logger.info(f'mounting embeddings container from: \n{parent} \n   to: \n{os.getcwd()}/raw_embeddings', extra={'print': True})
-        with rslex_uri_volume_mount(parent, f'{os.getcwd()}/raw_embeddings', options=mnt_options) as mount_context:
-            logger.info("Loading Embeddings")
-            emb = Embeddings.load(embeddings_dir_name, mount_context.mount_point)
-            activity_logger.activity_info["num_documents"] = len(emb._document_embeddings)
-            emb.write_as_faiss_mlindex(Path(args.output))
-
-        logger.info('Generated FAISS index')
+    try:
+        main_wrapper(args, logger)
+    finally:
+        if _logger_factory.appinsights:
+            _logger_factory.appinsights.flush()
+            time.sleep(5)
```

## azureml/rag/tasks/crack_and_chunk.py

```diff
@@ -1,22 +1,21 @@
 # ---------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
 import json
-import openai
 import pandas as pd
 from pathlib import Path
 import time
-from typing import Iterator
+from typing import Iterator, List
 import re
+import time
+import traceback
 
-from azureml.rag.utils.logging import get_logger, enable_stdout_logging, enable_appinsights_logging, track_activity
-from azureml.rag.documents import SUPPORTED_EXTENSIONS, DocumentChunksIterator, DocumentSource, split_documents, crack_documents
-from azureml.rag.models import parse_model_uri
-from azureml.rag.utils.azureml import get_secret_from_workspace
+from azureml.rag.utils.logging import get_logger, enable_stdout_logging, enable_appinsights_logging, track_activity, _logger_factory
+from azureml.rag.documents import SUPPORTED_EXTENSIONS, DocumentChunksIterator, DocumentSource, Document, split_documents
 
 
 logger = get_logger('crack_and_chunk')
 
 
 def chunks_to_dataframe(chunks) -> pd.DataFrame:
     metadata = []
@@ -36,23 +35,108 @@
 def write_chunks_to_csv(chunks_df, output_path):
     output_dir = Path(output_path).parent
     output_dir.mkdir(parents=True, exist_ok=True)
 
     chunks_df.to_csv(output_path, index=False)
 
 
+def write_chunks_to_jsonl(chunks: List[Document], output_path):
+    output_dir = Path(output_path).parent
+    output_dir.mkdir(parents=True, exist_ok=True)
+
+    with open(output_path, 'w') as f:
+        for chunk in chunks:
+            f.write(chunk.dumps())
+            f.write('\n')
+
+
 def str2bool(v):
     if isinstance(v, bool):
         return v
     if v.lower() in ('yes', 'true', 't', 'y', '1'):
         return True
     elif v.lower() in ('no', 'false', 'f', 'n', '0'):
         return False
 
 
+def main(args, logger, activity_logger):
+    splitter_args = {'chunk_size': args.chunk_size, 'chunk_overlap': args.chunk_overlap, 'use_rcts': args.use_rcts}
+
+    def filter_and_log_extensions(sources: Iterator[DocumentSource], allowed_extensions=SUPPORTED_EXTENSIONS) -> Iterator[DocumentSource]:
+        """Filter out sources with extensions not in allowed_extensions."""
+        total_files = 0
+        skipped_files = 0
+        skipped_extensions = {}
+        kept_extension = {}
+        for source in sources:
+            total_files += 1
+            if allowed_extensions is not None:
+                if source.path.suffix not in allowed_extensions:
+                    skipped_files += 1
+                    ext_skipped = skipped_extensions.get(source.path.suffix, 0)
+                    skipped_extensions[source.path.suffix] = ext_skipped + 1
+                    logger.debug(f'Filtering out extension "{source.path.suffix}" source: {source.filename}')
+                    continue
+            ext_kept = kept_extension.get(source.path.suffix, 0)
+            kept_extension[source.path.suffix] = ext_kept + 1
+            logger.info(f'Processing file: {source.filename}')
+            yield source
+        logger.info(f"[DocumentChunksIterator::filter_extensions] Filtered {skipped_files} files out of {total_files}")
+        logger.info(f"[DocumentChunksIterator::filter_extensions] Skipped extensions: {json.dumps(skipped_extensions, indent=2)}")
+        logger.info(f"[DocumentChunksIterator::filter_extensions] Kept extensions: {json.dumps(kept_extension, indent=2)}")
+        activity_logger.activity_info['total_files'] = total_files
+        activity_logger.activity_info['skipped_files'] = skipped_files
+        activity_logger.activity_info['skipped_extensions'] = json.dumps(skipped_extensions)
+        activity_logger.activity_info['kept_extensions'] = json.dumps(kept_extension)
+
+    chunked_documents = DocumentChunksIterator(
+        files_source=args.input_data,
+        glob=args.input_glob,
+        base_url=args.data_source_url,
+        document_path_replacement_regex=args.document_path_replacement_regex,
+        file_filter=filter_and_log_extensions,
+        chunked_document_processors = [lambda docs: split_documents(docs, splitter_args=splitter_args)],
+    )
+    file_count = 0
+    total_time = 0
+    for chunked_document in chunked_documents:
+        file_start_time = time.time()
+        file_count += 1
+        # TODO: Ideally make it easy to limit number of files with a `- take: n` operation on input URI in MLTable
+        if (args.max_sample_files != -1 and file_count >= args.max_sample_files):
+            logger.info(f"file count: {file_count} - reached max sample file count: {args.max_sample_files}", extra={'print': True})
+            break
+        if args.output_format == "csv":
+            write_chunks_to_csv(chunks_to_dataframe(chunked_document.chunks), Path(args.output_title_chunk) / f"Chunks_{Path(chunked_document.source.filename).name}.csv")
+        elif args.output_format == "jsonl":
+            write_chunks_to_jsonl(chunked_document.chunks, Path(args.output_title_chunk) / f"Chunks_{Path(chunked_document.source.filename).name}.jsonl")
+        file_end_time = time.time()
+        total_time += file_end_time - file_start_time
+
+    logger.info(f"Processed {file_count} files",)
+    activity_logger.activity_info["file_count"] = str(file_count)
+
+    if file_count == 0:
+        logger.info(f"No chunked documents found in {args.input_data} with glob {args.input_glob}")
+        activity_logger.activity_info["error"] = "No chunks found"
+        activity_logger.activity_info["glob"] = args.input_glob if re.match("^[*/\\\"']+$", args.input_glob) is not None else "[REDACTED]"
+        raise ValueError(f"No chunked documents found in {args.input_data} with glob {args.input_glob}.")
+
+    logger.info(f"Wrote chunks to {file_count} files in {total_time} seconds (chunk generation time excluded)")
+    activity_logger.activity_info["file_count"] = file_count
+
+
+def main_wrapper(args, logger):
+    with track_activity(logger, "crack_and_chunk") as activity_logger:
+        try:
+            main(args, logger, activity_logger)
+        except Exception:
+            activity_logger.error(f"crack_and_chunk failed with exception: {traceback.format_exc()}")  # activity_logger doesn't log traceback
+            raise
+
 if __name__ == '__main__':
     import argparse
 
     parser = argparse.ArgumentParser()
     parser.add_argument("--input_data", type=str)
     parser.add_argument("--input_glob", type=str, default="**/*")
     parser.add_argument("--allowed_extensions", required=False, type=str, default=",".join(SUPPORTED_EXTENSIONS))
@@ -61,120 +145,24 @@
     parser.add_argument("--output_title_chunk", type=str)
     parser.add_argument("--output_summary_chunk", type=str, default=None)
     parser.add_argument("--data_source_url", type=str, required=False)
     parser.add_argument("--document_path_replacement_regex", type=str, required=False)
     parser.add_argument("--max_sample_files", type=int, default=-1)
     parser.add_argument("--include_summary", type=str, default="False")
     parser.add_argument("--summary_model_config", type=str, default='{"type": "azure_open_ai", "model_name": "gpt-35-turbo", "deployment_name": "gpt-35-turbo"}')
-    parser.add_argument("--openai_api_version", type=str, default='2023-03-15-preview')
+    parser.add_argument("--openai_api_version", type=str, default='2023-05-15')
     parser.add_argument("--openai_api_type", type=str, default=None)
     parser.add_argument("--use_rcts", type=str2bool, default=True)
+    parser.add_argument("--output_format", type=str, default="csv")
 
     args = parser.parse_args()
     print('\n'.join(f'{k}={v}' for k, v in vars(args).items()))
 
     enable_stdout_logging()
     enable_appinsights_logging()
 
-    summary_model_config = None
-    include_summary = (args.include_summary == "True" or args.include_summary == "true")
-    if include_summary:
-        try:
-            summary_model_config = json.loads(args.summary_model_config)
-            # TODO: For back compat, remove in a few weeks.
-            summary_model_config['kind'] = summary_model_config['type']
-            del summary_model_config['type']
-            summary_model_config['model'] = summary_model_config['model_name']
-            del summary_model_config['model_name']
-            if 'deployment_name' in summary_model_config:
-                summary_model_config['deployment'] = summary_model_config['deployment_name']
-                del summary_model_config['deployment_name']
-        except json.decoder.JSONDecodeError:
-            # Try parse as uri
-            summary_model_config = parse_model_uri(args.summary_model)
-
-        logger.info(f"Using summary_model: {json.dumps(summary_model_config, indent=2)}")
-        if summary_model_config.get("kind") == "azure_open_ai" or summary_model_config.get("api_type") == "azure":
-            summary_model_config["key"] = get_secret_from_workspace("OPENAI-API-KEY")
-            summary_model_config["kind"] = "open_ai"
-            summary_model_config["api_type"] = "azure"
-            summary_model_config["api_version"] = args.openai_api_version
-            summary_model_config["api_base"] = summary_model_config.get('endpoint') if summary_model_config.get('endpoint') is not None else get_secret_from_workspace("OPENAI-API-BASE")
-            openai.api_version = summary_model_config["api_version"]
-            openai.api_type = summary_model_config["api_type"]
-            openai.api_base = summary_model_config["api_base"]
-            openai.api_key = summary_model_config["key"]
-
-    splitter_args = {'chunk_size': args.chunk_size, 'chunk_overlap': args.chunk_overlap, 'use_rcts': args.use_rcts}
-
-    with track_activity(logger, 'crack_and_chunk', custom_dimensions={**splitter_args}) as activity_logger:
-
-        def filter_and_log_extensions(sources: Iterator[DocumentSource], allowed_extensions=SUPPORTED_EXTENSIONS) -> Iterator[DocumentSource]:
-            """Filter out sources with extensions not in allowed_extensions."""
-            total_files = 0
-            skipped_files = 0
-            skipped_extensions = {}
-            kept_extension = {}
-            for source in sources:
-                total_files += 1
-                if allowed_extensions is not None:
-                    if source.path.suffix not in allowed_extensions:
-                        skipped_files += 1
-                        ext_skipped = skipped_extensions.get(source.path.suffix, 0)
-                        skipped_extensions[source.path.suffix] = ext_skipped + 1
-                        logger.debug(f'Filtering out extension "{source.path.suffix}" source: {source.filename}')
-                        continue
-                ext_kept = kept_extension.get(source.path.suffix, 0)
-                kept_extension[source.path.suffix] = ext_kept + 1
-                yield source
-            logger.info(f"[DocumentChunksIterator::filter_extensions] Filtered {skipped_files} files out of {total_files}")
-            activity_logger.activity_info['total_files'] = total_files
-            activity_logger.activity_info['skipped_files'] = skipped_files
-            activity_logger.activity_info['skipped_extensions'] = json.dumps(skipped_extensions)
-            activity_logger.activity_info['kept_extensions'] = json.dumps(kept_extension)
-
-        chunked_documents = DocumentChunksIterator(
-            files_source=args.input_data,
-            glob=args.input_glob,
-            base_url=args.data_source_url,
-            document_path_replacement_regex=args.document_path_replacement_regex,
-            file_filter=filter_and_log_extensions,
-            chunked_document_processors = [lambda docs: split_documents(docs, splitter_args=splitter_args)],
-        )
-        file_count = 0
-        for document in chunked_documents:
-            file_count += 1
-            logger.info(f'Processing file: {document.source.filename}', extra={'print': True})
-            # TODO: Ideally make it easy to limit number of files with a `- take: n` operation on input URI in MLTable
-            if (args.max_sample_files != -1 and file_count >= args.max_sample_files):
-                logger.info(f"file count: {file_count} - reached max sample file count: {args.max_sample_files}", extra={'print': True})
-                break
-            write_chunks_to_csv(chunks_to_dataframe(document.chunks), Path(args.output_title_chunk) / f"Chunks_{Path(document.source.filename).name}.csv")
-        logger.info(f"Processed {file_count} files", extra={'print': True})
-        activity_logger.activity_info["file_count"] = file_count
-
-        if file_count == 0:
-            logger.info(f"No files found in {args.input_data} with glob {args.input_glob}", extra={'print': True})
-            activity_logger.activity_info["error"] = "No files found"
-            activity_logger.activity_info["glob"] = args.input_glob if re.match("^[*/\\\"']+$", args.input_glob) is not None else "[REDACTED]"
-            raise ValueError(f"No files found in {args.input_data} with glob {args.input_glob}, no chunks produced.")
-
-        file_count = 0
-        if include_summary:
-            chunked_documents = DocumentChunksIterator(
-                files_source=args.input_data,
-                glob=args.input_glob,
-                base_url=args.data_source_url,
-                document_path_replacement_regex=args.document_path_replacement_regex,
-                source_loader=lambda sources: crack_documents(sources, summary_model_config=summary_model_config),
-                chunked_document_processors = [lambda docs: split_documents(docs, splitter_args={'chunk_size': args.chunk_size, 'chunk_overlap': args.chunk_overlap, 'use_rcts': args.use_rcts})]
-            )
-            total_time = 0
-            for document in chunked_documents:
-                file_start_time = time.time()
-                if (args.max_sample_files != -1 and file_count >= args.max_sample_files):
-                    logger.info(f"file count: {file_count} - reached max sample file count: {args.max_sample_files}", extra={'print': True})
-                    break
-                write_chunks_to_csv(chunks_to_dataframe(document.chunks), Path(args.output_summary_chunk) / f"Chunks_{Path(document.source.filename).name}.csv")
-                file_end_time = time.time()
-                file_time = file_end_time - file_start_time
-            logger.info(f"Write chunks to {file_count} files in {total_time} seconds (chunk generation time excluded)", extra={'print': True})
+    try:
+        main_wrapper(args, logger)
+    finally:
+        if _logger_factory.appinsights:
+            _logger_factory.appinsights.flush()
+            time.sleep(5)
```

## azureml/rag/tasks/embed.py

```diff
@@ -1,99 +1,85 @@
 # ---------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
 import json
 import os
 import pandas as pd
 import pathlib
-from typing import Optional, Iterator
 import time
+import traceback
 
-
-from azureml.rag.documents import document_chunks_iterator, LazyDocument, StaticDocument, SUPPORTED_EXTENSIONS
-from azureml.rag.embeddings import Embeddings
+from azureml.rag.documents import Document, SUPPORTED_EXTENSIONS, DocumentChunksIterator, DocumentSource, split_documents, StaticDocument
+from azureml.rag.embeddings import EmbeddingsContainer
 from azureml.rag.utils.azureml import get_secret_from_workspace
-from azureml.rag.utils.logging import get_logger, enable_stdout_logging
+from azureml.rag.utils.logging import get_logger, enable_stdout_logging, track_activity, _logger_factory, enable_appinsights_logging
+from typing import Optional, Iterator
 
 logger = get_logger('embed')
 
 
-def create_embeddings(chunks: Iterator[LazyDocument],
-                      previous_embeddings: Optional[Embeddings],
+def create_embeddings(chunks: Iterator[Document],
+                      previous_embeddings: Optional[EmbeddingsContainer],
                       output: str,
                       embeddings_model: str,
                       batch_size: Optional[int]):
 
     extra_args = {}
     if batch_size is not None:
         extra_args["batch_size"] = batch_size
 
-    embeddings = previous_embeddings if previous_embeddings is not None else Embeddings.from_uri(embeddings_model, **extra_args)
+    embeddings = previous_embeddings if previous_embeddings is not None else EmbeddingsContainer.from_uri(embeddings_model, **extra_args)
 
     pre_embed = time.time()
     embeddings.embed(chunks)
     post_embed = time.time()
     logger.info(f"Embedding took {post_embed - pre_embed} seconds", extra={'print': True})
 
     embeddings.save(output)
 
 
-def read_chunks_into_documents(files: Iterator[pathlib.Path]):
+def read_chunks_into_documents(files: Iterator[pathlib.Path], chunk_format: str = 'csv') -> Iterator[Document]:
     # Append to list of texts and corresponding metadata
     file_max_chunk_len = 0
     for chunk_file in files:
         file_name = chunk_file.name
         logger.info(f'processing chunks for: {file_name}', extra={'print': True})
-        # Ensure Chunk data is read as string even if it looks like another datatype.
-        dtype = {'Chunk': str, 'Metadata': str}
-        chunks_df = pd.read_csv(chunk_file, dtype=dtype, keep_default_na=False)
-        chunks_dict = chunks_df.to_dict()
         max_chunk_len = 0
-        for chunk_idx, chunk in chunks_dict["Chunk"].items():
-            metadata = chunks_dict["Metadata"][chunk_idx]
-            metadata_dict = json.loads(metadata)
-            max_chunk_len = max(max_chunk_len, len(chunk))
-            yield StaticDocument(metadata_dict['source']['filename'] + str(chunk_idx), chunk, metadata_dict, metadata_dict['source'].get('mtime'))
-        logger.info(f'processed {len(chunks_dict["Chunk"])} chunks from {file_name}, max_chunk_len = {max_chunk_len}', extra={'print': True})
+        num_chunks = 0
+        if chunk_format == 'csv':
+            # Ensure Chunk data is read as string even if it looks like another datatype.
+            dtype = {'Chunk': str, 'Metadata': str}
+            chunks_df = pd.read_csv(chunk_file, dtype=dtype, keep_default_na=False)
+            chunks_dict = chunks_df.to_dict()
+            for chunk_idx, chunk in chunks_dict["Chunk"].items():
+                metadata = chunks_dict["Metadata"][chunk_idx]
+                metadata_dict = json.loads(metadata)
+                max_chunk_len = max(max_chunk_len, len(chunk))
+                num_chunks += 1
+                yield StaticDocument(data=chunk, metadata=metadata_dict, document_id=metadata_dict['source']['filename'] + str(chunk_idx), mtime=metadata_dict['source'].get('mtime'))
+
+        elif chunk_format == 'jsonl':
+            with open(chunk_file, 'r') as f:
+                for line in f:
+                    doc = StaticDocument.loads(line.strip())
+                    max_chunk_len = max(max_chunk_len, len(doc.data))
+                    num_chunks += 1
+                    yield doc
+
+        logger.info(f'processed {num_chunks} chunks from {file_name}, max_chunk_len = {max_chunk_len}', extra={'print': True})
         file_max_chunk_len = max(file_max_chunk_len, max_chunk_len)
     logger.info(f'longest chunk seen was {file_max_chunk_len}', extra={'print': True})
 
 
-if __name__ == '__main__':
-    from argparse import ArgumentParser
-
-    parser = ArgumentParser()
-    # If chunking done inline
-    parser.add_argument("--documents_source", required=False, type=str)
-    parser.add_argument("--source_glob",
-                        type=str, default="**/*")
-    parser.add_argument("--allowed_extensions", type=str, default=",".join(SUPPORTED_EXTENSIONS))
-    parser.add_argument("--documents_source_base_url", type=str, default="")
-    parser.add_argument("--document_path_replacement_regex", type=str, required=False)
-    parser.add_argument("--chunk_size", type=int, default=512)
-    parser.add_argument("--chunk_overlap", type=int, default=None)
-    # If chunking was done separately
-    parser.add_argument("--chunks_source", required=False, type=str)
-    # If adding to previously generated Embeddings
-    parser.add_argument("--previous_embeddings", required=False, type=str, default=None)
-    parser.add_argument("--output", type=str)
-    # Embeddings settings
-    parser.add_argument("--embeddings_model", type=str, default="text-embedding-ada-002")
-    parser.add_argument("--batch_size", type=int, default=1)
-    args = parser.parse_args()
-
-    print('\n'.join(f'{k}={v}' for k, v in vars(args).items()))
-
-    enable_stdout_logging()
-
-    os.environ["OPENAI_API_KEY"] = get_secret_from_workspace("OPENAI-API-KEY")
-
+def main(args, logger, activity_logger):
     if args.chunks_source and args.documents_source:
+        activity_logger.activity_info['error'] = 'chunks_source and documents_source were both specified'
         raise ValueError("Cannot specify both --chunks_source and --documents_source")
     elif args.chunks_source is None and args.documents_source is None:
+        activity_logger.activity_info['error'] = 'Neither chunks_source nor documents_source were specified'
         raise ValueError("Must specify either --chunks_source or --documents_source")
 
     splitter_args = {
         "chunk_size": args.chunk_size,
     }
     if args.chunk_overlap:
         splitter_args['chunk_overlap'] = args.chunk_overlap
@@ -116,40 +102,115 @@
                         f'failed to get latest folder from {mount_context.mount_point} with {e}.', extra={'print': True})
                     pass
 
                 if previous_embeddings_dir_name is not None:
                     logger.info(
                         f'loading from previous embeddings from {previous_embeddings_dir_name} in {mount_context.mount_point}', extra={'print': True})
                     try:
-                        previous_embeddings = Embeddings.load(
+                        previous_embeddings = EmbeddingsContainer.load(
                             previous_embeddings_dir_name, mount_context.mount_point)
                     except Exception as e:
                         logger.warn(
                             f'Failed to load from previous embeddings with {e}.\nCreating new Embeddings.', extra={'print': True})
         except Exception as e:
             logger.warning(f'Failed to load previous embeddings from mount with {e}, proceeding to create new embeddings.', extra={'print': True})
 
     # Load chunks to embed
     if args.documents_source is not None:
         logger.info("Getting chunks from documents_source", extra={'print': True})
-        per_document_chunks = document_chunks_iterator(args.documents_source, args.source_glob, args.allowed_extensions, args.documents_source_base_url, args.document_path_replacement_regex, splitter_args)
+
+        def filter_and_log_extensions(sources: Iterator[DocumentSource], allowed_extensions=args.allowed_extensions) -> Iterator[DocumentSource]:
+            """Filter out sources with extensions not in allowed_extensions."""
+            total_files = 0
+            skipped_files = 0
+            skipped_extensions = {}
+            kept_extension = {}
+            for source in sources:
+                total_files += 1
+                if allowed_extensions is not None:
+                    if source.path.suffix not in allowed_extensions:
+                        skipped_files += 1
+                        ext_skipped = skipped_extensions.get(source.path.suffix, 0)
+                        skipped_extensions[source.path.suffix] = ext_skipped + 1
+                        logger.debug(f'Filtering out extension "{source.path.suffix}" source: {source.filename}')
+                        continue
+                ext_kept = kept_extension.get(source.path.suffix, 0)
+                kept_extension[source.path.suffix] = ext_kept + 1
+                yield source
+            logger.info(f"[DocumentChunksIterator::filter_extensions] Filtered {skipped_files} files out of {total_files}")
+
+        chunked_documents = DocumentChunksIterator(
+            files_source=args.input_data,
+            glob=args.input_glob,
+            base_url=args.documents_source_base_url,
+            document_path_replacement_regex=args.document_path_replacement_regex,
+            file_filter=filter_and_log_extensions,
+            chunked_document_processors = [lambda docs: split_documents(docs, splitter_args=splitter_args)],
+        )
 
         def flatten_iterator(iterable):
             for i in iterable:
                 for j in i:
                     yield j
 
-        chunks = flatten_iterator(per_document_chunks)
+        chunks = flatten_iterator(chunked_documents)
     elif args.chunks_source is not None:
         logger.info("Reading chunks from the chunks_source", extra={'print': True})
 
         files = pathlib.Path(args.chunks_source).rglob("**/*")
 
         chunks = read_chunks_into_documents(files)
     else:
         raise ValueError("Must specify either --chunks_source or --documents_source")
 
     create_embeddings(chunks,
                       previous_embeddings,
                       args.output,
                       args.embeddings_model,
                       args.batch_size)
+
+
+def main_wrapper(args, logger):
+    with track_activity(logger, "embed") as activity_logger:
+        try:
+            main(args, logger, activity_logger)
+        except Exception:
+            activity_logger.error(f"embed failed with exception: {traceback.format_exc()}")  # activity_logger doesn't log traceback
+            raise
+
+
+if __name__ == '__main__':
+    from argparse import ArgumentParser
+
+    parser = ArgumentParser()
+    # If chunking done inline
+    parser.add_argument("--documents_source", required=False, type=str)
+    parser.add_argument("--source_glob",
+                        type=str, default="**/*")
+    parser.add_argument("--allowed_extensions", type=str, default=",".join(SUPPORTED_EXTENSIONS))
+    parser.add_argument("--documents_source_base_url", type=str, default="")
+    parser.add_argument("--document_path_replacement_regex", type=str, required=False)
+    parser.add_argument("--chunk_size", type=int, default=512)
+    parser.add_argument("--chunk_overlap", type=int, default=None)
+    # If chunking was done separately
+    parser.add_argument("--chunks_source", required=False, type=str)
+    # If adding to previously generated Embeddings
+    parser.add_argument("--previous_embeddings", required=False, type=str, default=None)
+    parser.add_argument("--output", type=str)
+    # Embeddings settings
+    parser.add_argument("--embeddings_model", type=str, default="text-embedding-ada-002")
+    parser.add_argument("--batch_size", type=int, default=1)
+    args = parser.parse_args()
+
+    print('\n'.join(f'{k}={v}' for k, v in vars(args).items()))
+
+    enable_stdout_logging()
+    enable_appinsights_logging()
+
+    os.environ["OPENAI_API_KEY"] = get_secret_from_workspace("OPENAI-API-KEY")
+
+    try:
+        main_wrapper(args, logger)
+    finally:
+        if _logger_factory.appinsights:
+            _logger_factory.appinsights.flush()
+            time.sleep(5)
```

## azureml/rag/tasks/embed_prs.py

```diff
@@ -3,39 +3,31 @@
 # ---------------------------------------------------------
 """ParallelRunStep entrypoint for embedding data."""
 import argparse
 import os
 import pandas as pd
 import pathlib
 import time
+import traceback
 
-from azureml.rag.embeddings import Embeddings
+from azureml.rag.embeddings import EmbeddingsContainer
 from azureml.rag.tasks.embed import read_chunks_into_documents
 from azureml.rag.utils.azureml import get_workspace_from_environment
-from azureml.rag.utils.logging import get_logger, enable_stdout_logging, enable_appinsights_logging, track_activity
+from azureml.rag.utils.logging import get_logger, enable_stdout_logging, enable_appinsights_logging, track_activity, _logger_factory
 
-logger = get_logger('embed')
 
+logger = get_logger('embed_prs')
 
-def init():
-    """Load previous embeddings if provided."""
+
+def main(args, logger, activity_logger):
     global output_data
     global embeddings_container
-    parser = argparse.ArgumentParser(allow_abbrev=False, description="ParallelRunStep Agent")
-    parser.add_argument("--output_data", type=str)
-    parser.add_argument("--embeddings_model", type=str)
-    parser.add_argument("--embeddings_container", required=False, type=str, default=None)
-    args, _ = parser.parse_known_args()
-
-    print('\n'.join(f'{k}={v}' for k, v in vars(args).items()))
-
-    enable_stdout_logging()
-    enable_appinsights_logging()
-
+    global chunk_format
     output_data = args.output_data
+    chunk_format = args.chunk_format
 
     embeddings_container = None
     if args.embeddings_container is not None:
         with track_activity(logger, 'init.load_embeddings_container') as activity_logger:
             if hasattr(activity_logger, 'activity_info'):
                 activity_logger.activity_info["completionStatus"] = "Failure"
             from azureml.dataprep.fuse.dprepfuse import MountOptions, rslex_uri_volume_mount
@@ -54,15 +46,15 @@
                             f'failed to get latest folder from {mount_context.mount_point} with {e}.')
                         pass
 
                     if embeddings_container_dir_name is not None:
                         logger.info(
                             f'loading from previous embeddings from {embeddings_container_dir_name} in {mount_context.mount_point}')
                         try:
-                            embeddings_container = Embeddings.load(
+                            embeddings_container = EmbeddingsContainer.load(
                                 embeddings_container_dir_name, mount_context.mount_point)
                             if hasattr(activity_logger, 'activity_info'):
                                 activity_logger.activity_info["completionStatus"] = "Success"
                         except Exception as e:
                             activity_logger.warn('Failed to load from embeddings_container_dir_name. Creating new Embeddings.')
                             logger.warn(
                                 f'Failed to load from previous embeddings with {e}.\nCreating new Embeddings.')
@@ -82,40 +74,74 @@
             connection_args["connection"] = {
                 "subscription": ws.subscription_id if ws is not None else "",
                 "resource_group": ws.resource_group if ws is not None else "",
                 "workspace": ws.name if ws is not None else "",
                 "key": "OPENAI-API-KEY"
             }
 
-    embeddings_container = embeddings_container if embeddings_container is not None else Embeddings.from_uri(args.embeddings_model, **connection_args)
+    embeddings_container = embeddings_container if embeddings_container is not None else EmbeddingsContainer.from_uri(args.embeddings_model, **connection_args)
+
+
+def main_wrapper(args, logger):
+    with track_activity(logger, "embed_prs") as activity_logger:
+        try:
+            main(args, logger, activity_logger)
+        except Exception:
+            activity_logger.error(f"embed_prs failed with exception: {traceback.format_exc()}")  # activity_logger doesn't log traceback
+            raise
+
+
+def init():
+    """Load previous embeddings if provided."""
+    parser = argparse.ArgumentParser(allow_abbrev=False, description="ParallelRunStep Agent")
+    parser.add_argument("--output_data", type=str)
+    parser.add_argument("--embeddings_model", type=str)
+    parser.add_argument("--embeddings_container", required=False, type=str, default=None)
+    parser.add_argument("--chunk_format", type=str, default="csv")
+    args, _ = parser.parse_known_args()
+
+    print('\n'.join(f'{k}={v}' for k, v in vars(args).items()))
+
+    enable_stdout_logging()
+    enable_appinsights_logging()
+
+    try:
+        main_wrapper(args, logger)
+    finally:
+        if _logger_factory.appinsights:
+            _logger_factory.appinsights.flush()
+            time.sleep(5)
 
 
-# TODO: Not handling throttling from openai api, need to back off more
 def _run_internal(mini_batch, output_data, embeddings):
     """
     Embed minibatch of chunks.
 
     :param mini_batch: The list of files to be processed.
     :param output_data: The output folder to save data to.
     :param embeddings: The Embeddings object that should be used to embed new data.
     """
+    global chunk_format
     logger.info(f'run method start: {__file__}, run({mini_batch})')
     logger.info(f'Task id: {mini_batch.task_id}')
 
     # read chunks
     pre_embed = time.time()
-    embeddings = embeddings.embed_and_create_new_instance(read_chunks_into_documents((pathlib.Path(p) for p in mini_batch)))
+    embeddings = embeddings.embed_and_create_new_instance(read_chunks_into_documents((pathlib.Path(p) for p in mini_batch), chunk_format))
     post_embed = time.time()
     logger.info(f"Embedding took {post_embed - pre_embed} seconds")
 
     save_metadata = str(mini_batch.task_id) == '0'
     if save_metadata:
         logger.info('Metadata will be saved')
     else:
         logger.info('Only data will be saved')
     embeddings.save(output_data, with_metadata=save_metadata, suffix=mini_batch.task_id)
 
 
 def run(mini_batch):
     """Embed minibatch of chunks."""
+    global output_data
+    global embeddings_container
+
     _run_internal(mini_batch, output_data, embeddings_container)
     return pd.DataFrame({"Files": [os.path.split(file)[-1] for file in mini_batch]})
```

## azureml/rag/tasks/git_clone.py

```diff
@@ -1,59 +1,72 @@
 # ---------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
 import argparse
 import git
 import os
+import time
+import traceback
 
 from azureml.rag.utils.git import clone_repo, get_keyvault_authentication
-from azureml.rag.utils.logging import get_logger, enable_stdout_logging, enable_appinsights_logging, track_activity
+from azureml.rag.utils.logging import get_logger, enable_stdout_logging, enable_appinsights_logging, track_activity, _logger_factory
 
 logger = get_logger('git_clone')
 
+def main(args, logger, activity_logger):
+    try:
+        connection_id = os.environ.get('AZUREML_WORKSPACE_CONNECTION_ID_GIT')
+        if connection_id is not None and connection_id != '':
+            from azureml.rag.utils.connections import get_connection_by_id_v2
+
+            connection = get_connection_by_id_v2(connection_id)
+            if args.git_repository != connection['properties']['target']:
+                logger.warning(f"Given git repository '{args.git_repository}' does not match the git repository '{connection['properties']['target']}' specified in the Workspace Connection '{connection_id}'. Using the Workspace Connection git repository.")
+            args.git_repository = connection['properties']['target']
+            authentication = {'username': connection['properties']['metadata']['username'], 'password': connection['properties']['credentials']['pat']}
+        elif args.authentication_key_prefix is not None:
+            authentication = get_keyvault_authentication(args.authentication_key_prefix)
+        else:
+            authentication = None
+    except Exception as e:
+        logger.error(f"Failed to get authentication information from the Workspace Connection '{connection_id}'.")
+        activity_logger.activity_info['error'] = f"{e.__class__.__name__}: Failed to get authentication information from the Workspace Connection."
+        raise e
+
+    activity_logger.activity_info['authentication_used'] = str(authentication is not None)
+
+    try:
+        clone_repo(args.git_repository, args.output_data, args.branch_name, authentication)
+    except git.exc.GitError as e:
+        activity_logger.activity_info['error'] = f"{e.__class__.__name__}: Failed with GitError."
+        raise e
+    except Exception as e:
+        activity_logger.activity_info['error'] = f"{e.__class__.__name__}: Failed to clone git repository."
+        raise e
+
+def main_wrapper(args, logger):
+    with track_activity(logger, "git_clone") as activity_logger:
+        try:
+            main(args, logger, activity_logger)
+        except Exception:
+            activity_logger.error(f"git_clone failed with exception: {traceback.format_exc()}")  # activity_logger doesn't log traceback
+            raise
 
 if __name__ == "__main__":
     parser = argparse.ArgumentParser()
     parser.add_argument("--git-repository", type=str, required=True, dest='git_repository')
     parser.add_argument("--branch-name", type=str, required=False, default=None)
     parser.add_argument("--authentication-key-prefix", type=str, required=False, default=None, help="<PREFIX>-USER and <PREFIX>-PASS are the expected names of two Secrets in the Workspace Key Vault which will be used for authenticated when pulling the given git repo.")
     parser.add_argument("--output-data", type=str, required=True, dest='output_data')
     args = parser.parse_args()
 
     enable_stdout_logging()
     enable_appinsights_logging()
 
-    with track_activity(logger, 'git_clone') as activity_logger:
-        try:
-            connection_id = os.environ.get('AZUREML_WORKSPACE_CONNECTION_ID_GIT')
-            if connection_id is not None and connection_id != '':
-                from azureml.rag.utils.connections import get_connection_by_id_v2
-
-                connection = get_connection_by_id_v2(connection_id)
-                if args.git_repository != connection['properties']['target']:
-                    logger.warning(f"Given git repository '{args.git_repository}' does not match the git repository '{connection['properties']['target']}' specified in the Workspace Connection '{connection_id}'. Using the Workspace Connection git repository.")
-                args.git_repository = connection['properties']['target']
-                authentication = {'username': connection['properties']['metadata']['username'], 'password': connection['properties']['credentials']['pat']}
-            elif args.authentication_key_prefix is not None:
-                authentication = get_keyvault_authentication(args.authentication_key_prefix)
-            else:
-                authentication = None
-        except Exception as e:
-            logger.error(f"Failed to get authentication information from the Workspace Connection '{connection_id}'.")
-            activity_logger.activity_info['error'] = f"{e.__class__.__name__}: Failed to get authentication information from the Workspace Connection."
-
-            raise e
-
-        activity_logger.activity_info['authentication_used'] = str(authentication is not None)
-
-        try:
-            clone_repo(args.git_repository, args.output_data, args.branch_name, authentication)
-        except git.exc.GitError as e:
-            activity_logger.activity_info['error'] = f"{e.__class__.__name__}: Failed with GitError."
-
-            raise e
-        except Exception as e:
-            activity_logger.activity_info['error'] = f"{e.__class__.__name__}: Failed to clone git repository."
-
-            raise e
+    try:
+        main_wrapper(args, logger)
+    finally:
+        if _logger_factory.appinsights:
+            _logger_factory.appinsights.flush()
+            time.sleep(5)
 
     logger.info('Finished cloning.')
```

## azureml/rag/tasks/register_mlindex.py

```diff
@@ -2,73 +2,87 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
 """File for registering ML Indexes."""
 import argparse
 from azureml.core import Run
 import fsspec
 import re
+import time
+import traceback
 import yaml
 
-from azureml.rag.utils.logging import get_logger, enable_stdout_logging, enable_appinsights_logging, track_activity
+from azureml.rag.utils.logging import get_logger, enable_stdout_logging, enable_appinsights_logging, track_activity, _logger_factory
 from azureml.rag._asset_client.client import get_rest_client, register_new_data_asset_version
 
 logger = get_logger('register_mlindex')
 
+def main(args, run, logger, activity_logger):
+    ws = run.experiment.workspace
+
+    logger.info(f'Checking for MLIndex at: {args.storage_uri.strip("/")}/MLIndex')
+    index_kind = None
+    mlindex_yaml = None
+    try:
+        mlindex_file = fsspec.open(f"{args.storage_uri}/MLIndex", 'r')
+        # parse yaml to dict
+        with mlindex_file as f:
+            mlindex_yaml = yaml.safe_load(f)
+            index_kind = mlindex_yaml.get('index', {}).get('kind', None)
+    except Exception as e:
+        logger.error(f"Could not find MLIndex: {e}")
+        activity_logger.activity_info['error'] = 'Could not find MLIndex yaml'
+        raise e
+
+    if index_kind is None:
+        logger.error(f"Could not find index.kind in MLIndex: {mlindex_yaml}")
+        activity_logger.activity_info['error'] = 'Could not find index.kind in MLIndex yaml'
+        raise ValueError(f"Could not find index.kind in MLIndex: {mlindex_yaml}")
+    activity_logger.activity_info['kind'] = index_kind
+
+    client = get_rest_client(ws)
+    data_version = register_new_data_asset_version(
+        client,
+        run,
+        args.asset_name,
+        args.storage_uri,
+        properties={
+            'azureml.mlIndexAssetKind': index_kind,
+            'azureml.mlIndexAsset': 'true',
+            'azureml.mlIndexAssetSource': run.properties.get('azureml.mlIndexAssetSource', 'Unknown'),
+            'azureml.mlIndexAssetPipelineRunId': run.properties.get('azureml.pipelinerunid', 'Unknown')
+        })
+
+    asset_id = re.sub('azureml://locations/(.*)/workspaces/(.*)/data', f'azureml://subscriptions/{ws._subscription_id}/resourcegroups/{ws._resource_group}/providers/Microsoft.MachineLearningServices/workspaces/{ws._workspace_name}/data', data_version.asset_id)
+    with open(args.output_asset_id, 'w') as f:
+        f.write(asset_id)
+
+    logger.info(f"Finished Registering MLIndex Asset '{args.asset_name}', version = {data_version.version_id}")
+
+def main_wrapper(args, run, logger):
+    with track_activity(logger, 'register_mlindex', custom_dimensions={'source': run.properties.get('azureml.mlIndexAssetSource', 'Unknown')}) as activity_logger:
+        try:
+            main(args, run, logger, activity_logger)
+        except Exception:
+            activity_logger.error(f"register_mlindex failed with exception: {traceback.format_exc()}")  # activity_logger doesn't log traceback
+            raise
 
 if __name__ == "__main__":
     parser = argparse.ArgumentParser()
     parser.add_argument("--storage-uri", type=str, required=True, dest='storage_uri')
     parser.add_argument("--asset-name", type=str, required=False, dest='asset_name', default='MLIndexAsset')
     parser.add_argument("--output-asset-id", type=str, dest='output_asset_id')
     args = parser.parse_args()
 
     print('\n'.join(f'{k}={v}' for k, v in vars(args).items()))
 
     enable_stdout_logging()
     enable_appinsights_logging()
 
     run: Run = Run.get_context()
-    with track_activity(logger, 'register_mlindex', custom_dimensions={'source': run.properties.get('azureml.mlIndexAssetSource', 'Unknown')}) as activity_logger:
-        ws = run.experiment.workspace
-
-        logger.info(f'Checking for MLIndex at: {args.storage_uri.strip("/")}/MLIndex')
-        index_kind = None
-        mlindex_yaml = None
-        try:
-            mlindex_file = fsspec.open(f"{args.storage_uri}/MLIndex", 'r')
-            # parse yaml to dict
-            with mlindex_file as f:
-                mlindex_yaml = yaml.safe_load(f)
-                index_kind = mlindex_yaml.get('index', {}).get('kind', None)
-        except Exception as e:
-            logger.error(f"Could not find MLIndex: {e}")
-            raise e
-
-        if index_kind is None:
-            logger.error(f"Could not find index.kind in MLIndex: {mlindex_yaml}")
-            activity_logger.activity_info['error'] = 'Could not find index.kind in MLIndex yaml'
-            raise ValueError(f"Could not find index.kind in MLIndex: {mlindex_yaml}")
-        activity_logger.activity_info['kind'] = index_kind
-
-        client = get_rest_client(ws)
-        data_version = register_new_data_asset_version(
-            client,
-            run,
-            args.asset_name,
-            args.storage_uri,
-            properties={
-                'azureml.mlIndexAssetKind': index_kind,
-                'azureml.mlIndexAsset': 'true',
-                'azureml.mlIndexAssetSource': run.properties.get('azureml.mlIndexAssetSource', 'Unknown'),
-                'azureml.mlIndexAssetPipelineRunId': run.properties.get('azureml.pipelinerunid', 'Unknown')
-            })
-
-        print(data_version.asset_id)
-
-        asset_id = re.sub('azureml://locations/(.*)/workspaces/(.*)/data', f'azureml://subscriptions/{ws._subscription_id}/resourcegroups/{ws._resource_group}/providers/Microsoft.MachineLearningServices/workspaces/{ws._workspace_name}/data', data_version.asset_id)
-
-        print(asset_id)
-
-        with open(args.output_asset_id, 'w') as f:
-            f.write(asset_id)
 
-        logger.info(f"Finished Registering MLIndex Asset '{args.asset_name}', version = {data_version.version_id}")
+    try:
+        main_wrapper(args, run, logger)
+    finally:
+        if _logger_factory.appinsights:
+            _logger_factory.appinsights.flush()
+            time.sleep(5)  # wait for appinsights to send telemetry
+    run.complete()
```

## azureml/rag/tasks/update_acs.py

```diff
@@ -1,30 +1,31 @@
 # ---------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
 import base64
 import json
 import logging
 import os
-from pathlib import Path
 import requests
 import tenacity
 import time
-from typing import Optional
+import traceback
 import yaml
 
 from azure.identity import DefaultAzureCredential
 from azure.core.credentials import AzureKeyCredential
 from azure.search.documents.indexes import SearchIndexClient
 from azure.search.documents.indexes.models import SearchIndex, SimpleField, SearchableField, SemanticSettings, SemanticConfiguration, PrioritizedFields, SemanticField, SearchField, ComplexField
 from azure.search.documents import SearchClient
-
-from azureml.rag.embeddings import Embeddings
+from azureml.rag.embeddings import EmbeddingsContainer
+from azureml.rag.mlindex import MLIndex
 from azureml.rag.utils.connections import get_connection_credential
-from azureml.rag.utils.logging import get_logger, enable_stdout_logging, enable_appinsights_logging, track_activity
+from azureml.rag.utils.logging import get_logger, enable_stdout_logging, enable_appinsights_logging, track_activity, _logger_factory
+from pathlib import Path
+from typing import Optional
 
 
 logger = get_logger('update_acs')
 
 _azure_logger = logging.getLogger('azure.core.pipeline')
 _azure_logger.setLevel(logging.WARNING)
 
@@ -33,15 +34,15 @@
     return SearchClient(endpoint=acs_config['endpoint'],
                         index_name=acs_config['index_name'],
                         credential=credential,
                         api_version=acs_config['api_version'])
 
 
 def create_search_index_sdk(acs_config: dict, credential):
-    logger.info(f"Ensuring search index {acs_config['index_name']} exists", extra={'print': True})
+    logger.info(f"Ensuring search index {acs_config['index_name']} exists")
     index_client = SearchIndexClient(endpoint=acs_config['endpoint'],
                                     credential=credential)
     if acs_config['index_name'] not in index_client.list_index_names():
         index = SearchIndex(
             name=acs_config['index_name'],
             fields=[
                 SimpleField(name="id", type="Edm.String", key=True),
@@ -59,18 +60,18 @@
             ],
             semantic_settings=SemanticSettings(
                 configurations=[SemanticConfiguration(
                     name='default',
                     prioritized_fields=PrioritizedFields(
                         title_field=None, prioritized_content_fields=[SemanticField(field_name='content')]))])
         )
-        logger.info(f"Creating {acs_config['index_name']} search index", extra={'print': True})
+        logger.info(f"Creating {acs_config['index_name']} search index")
         index_client.create_index(index)
     else:
-        logger.info(f"Search index {acs_config['index_name']} already exists", extra={'print': True})
+        logger.info(f"Search index {acs_config['index_name']} already exists")
 
 
 @tenacity.retry(
     wait=tenacity.wait_fixed(5),  # wait 5 seconds between retries
     stop=tenacity.stop_after_attempt(3),  # stop after 3 attempts
     reraise=True,  # re-raise the exception after the last retry attempt
 )
@@ -78,17 +79,17 @@
     response = requests.put(url, data=json.dumps(payload), headers=headers)
 
     # Raise an exception if the response contains an HTTP error status code
     response.raise_for_status()
     return response
 
 
-def create_search_index_rest(acs_config: dict, credential, embeddings: Optional[Embeddings] = None):
+def create_search_index_rest(acs_config: dict, credential, embeddings: Optional[EmbeddingsContainer] = None):
     # TODO: Ask users in private preview to provide the new api_version? 2023-07-01-Preview
-    logger.info(f"Ensuring search index {acs_config['index_name']} exists", extra={'print': True})
+    logger.info(f"Ensuring search index {acs_config['index_name']} exists")
     if 'api_version' not in acs_config:
         acs_config['api_version'] = "2023-07-01-preview"
     index_client = SearchIndexClient(endpoint=acs_config['endpoint'],
                                     credential=credential,
                                     api_version=acs_config['api_version'])
     if acs_config['index_name'] not in index_client.list_index_names():
         base_url = f"{acs_config['endpoint']}/indexes/{acs_config['index_name']}?api-version={acs_config['api_version']}"
@@ -147,23 +148,23 @@
                             "metric": "cosine",
                             "efSearch": 500
                         }
                     }
                 ]
             }
 
-        logger.info(f"Creating {acs_config['index_name']} search index with embeddings", extra={'print': True})
+        logger.info(f"Creating {acs_config['index_name']} search index with embeddings")
         try:
             response = send_put_request(base_url, headers, payload)
             logger.info(response.text)
         except requests.exceptions.RequestException as e:
             logger.error(f"Request failed: {e}\nResponse: {e.response.text}")
             raise
     else:
-        logger.info(f"Search index {acs_config['index_name']} already exists", extra={'print': True})
+        logger.info(f"Search index {acs_config['index_name']} already exists")
 
 
 @tenacity.retry(
     wait=tenacity.wait_fixed(5),  # wait 5 seconds between retries
     stop=tenacity.stop_after_attempt(3),  # stop after 3 attempts
     reraise=True,  # re-raise the exception after the last retry attempt
 )
@@ -197,17 +198,17 @@
     except requests.exceptions.RequestException as e:
         logger.error(f"Request failed: {e}\nResponse: {e.response.text}")
         raise
 
     return response.json()['value']
 
 
-def create_index_from_raw_embeddings(emb: Embeddings, acs_config={}, connection={}, output_path: Optional[str] = None):
+def create_index_from_raw_embeddings(emb: EmbeddingsContainer, acs_config={}, connection={}, output_path: Optional[str] = None) -> MLIndex:
     with track_activity(logger, 'update_acs', custom_dimensions={'num_documents': len(emb._document_embeddings)}) as activity_logger:
-        logger.info("Updating ACS index", extra={'print': True})
+        logger.info("Updating ACS index")
 
         credential = get_connection_credential(connection)
 
         if str(acs_config.get('push_embeddings')).lower() == "false":
             create_search_index_sdk(acs_config, credential)
             search_client = search_client_from_config(acs_config, credential)
 
@@ -232,31 +233,31 @@
                         succeeded.append(r)
                 else:
                     if r.succeeded:
                         succeeded.append(r)
                     else:
                         failed.append(r)
             duration = time.time() - start_time
-            logger.info(f"Uploaded {len(succeeded)} documents to ACS in {duration:.4f} seconds, {len(failed)} failed", extra={'print': True})
+            logger.info(f"Uploaded {len(succeeded)} documents to ACS in {duration:.4f} seconds, {len(failed)} failed")
             activity_logger.info("Uploaded documents", extra={'properties': {'succeeded': len(succeeded), 'failed': len(failed), 'duration': duration}})
             if len(failed) > 0:
                 for r in failed:
                     if isinstance(r, dict):
                         error = r['errorMessage']
                     else:
                         error = r.error_message
-                    logger.error(f"Failed document reason: {error}", extra={'print': True})
+                    logger.error(f"Failed document reason: {error}")
                 return failed
             return []
 
         t1 = time.time()
         num_source_docs = 0
         batch = []
         for doc_id, emb_doc in emb._document_embeddings.items():
-            logger.info(f'Adding document: {doc_id}', extra={'print': True})
+            logger.info(f'Adding document: {doc_id}')
             acs_doc = {
                 "@search.action": "mergeOrUpload",
                 "id": base64.urlsafe_b64encode(doc_id.encode('utf-8')).decode('utf-8'),
                 "content": emb_doc.get_data(),
                 "category": "document",
                 "sourcepage": emb_doc.metadata.get("source", {}).get("url"),
                 "sourcefile": emb_doc.metadata.get("source", {}).get("filename"),
@@ -268,80 +269,162 @@
             if str(acs_config.get('push_embeddings')).lower() == "false":
                 pass
             else:
                 acs_doc[f"content_vector_{emb.kind}"] = emb_doc.get_embeddings()
 
             batch.append(acs_doc)
             if len(batch) % batch_size == 0:
-                logger.info(f"Sending {len(batch)} documents to ACS", extra={'print': True})
+                logger.info(f"Sending {len(batch)} documents to ACS")
                 start_time = time.time()
                 results = upload_docs_batch(batch)
                 failed = process_upload_results(results, start_time)
                 if len(failed) > 0:
-                    logger.info(f"Retrying {len(failed)} documents", extra={'print': True})
+                    logger.info(f"Retrying {len(failed)} documents")
                     failed_ids = [fail['key'] for fail in failed]
                     results = upload_docs_batch([doc for doc in batch if doc['id'] in failed_ids])
                     failed = process_upload_results(results, start_time)
                     if len(failed) > 0:
                         raise RuntimeError(f"Failed to upload {len(failed)} documents.")
                 batch = []
                 num_source_docs += batch_size
 
         if len(batch) > 0:
-            logger.info(f"Sending {len(batch)} documents to ACS", extra={'print': True})
+            logger.info(f"Sending {len(batch)} documents to ACS")
             start_time = time.time()
             results = upload_docs_batch(batch)
             failed = process_upload_results(results, start_time)
             if len(failed) > 0:
-                logger.info(f"Retrying {len(failed)} documents", extra={'print': True})
+                logger.info(f"Retrying {len(failed)} documents")
                 failed_ids = [fail['key'] for fail in failed]
                 results = upload_docs_batch([doc for doc in batch if doc['id'] in failed_ids])
                 failed = process_upload_results(results, start_time)
                 if len(failed) > 0:
                     raise RuntimeError(f"Failed to upload {len(failed)} documents.")
 
             num_source_docs += len(batch)
 
         duration = time.time()-t1
-        logger.info(f"Built index from {num_source_docs} documents and {len(emb._document_embeddings)} chunks, took {duration:.4f} seconds", extra={'print': True})
+        logger.info(f"Built index from {num_source_docs} documents and {len(emb._document_embeddings)} chunks, took {duration:.4f} seconds")
         activity_logger.info("Built index", extra={'properties': {'num_source_docs': num_source_docs, 'duration': duration}})
 
-        if output_path is not None:
-            logger.info('Writing MLIndex yaml', extra={'print': True})
-            mlindex_config = {
-                "embeddings": emb.get_metadata()
-            }
-            mlindex_config["index"] = {
-                "kind": "acs",
-                "engine": "azure-sdk",
-                "index": acs_config['index_name'],
-                "api_version": acs_config['api_version'],
-                "field_mapping": {
-                    "content": "content",
-                    "url": "sourcepage",
-                    "filename": "sourcefile",
-                    "title": "title",
-                    "metadata": "meta_json_string",
-                }
+
+        logger.info('Writing MLIndex yaml')
+        mlindex_config = {
+            "embeddings": emb.get_metadata()
+        }
+        mlindex_config["index"] = {
+            "kind": "acs",
+            "engine": "azure-sdk",
+            "index": acs_config['index_name'],
+            "api_version": acs_config['api_version'],
+            "field_mapping": {
+                "content": "content",
+                "url": "sourcepage",
+                "filename": "sourcefile",
+                "title": "title",
+                "metadata": "meta_json_string",
             }
-            if str(acs_config.get('push_embeddings')).lower() == "false":
-                pass
-            else:
-                mlindex_config["index"]["field_mapping"]["embedding"] = f"content_vector_{emb.kind}"
+        }
+        if str(acs_config.get('push_embeddings')).lower() == "false":
+            pass
+        else:
+            mlindex_config["index"]["field_mapping"]["embedding"] = f"content_vector_{emb.kind}"
+
+        if not isinstance(connection, DefaultAzureCredential):
+            mlindex_config["index"] = {**mlindex_config["index"], **connection}
 
-            if not isinstance(connection, DefaultAzureCredential):
-                mlindex_config["index"] = {**mlindex_config["index"], **connection}
+        # Keyvault auth and Default ambient auth need the endpoint, Workspace Connection auth could get endpoint.
+        mlindex_config["index"]["endpoint"] = acs_config['endpoint']
 
-            # Keyvault auth and Default ambient auth need the endpoint, Workspace Connection auth could get endpoint.
-            mlindex_config["index"]["endpoint"] = acs_config['endpoint']
+        if output_path is not None:
             output = Path(output_path)
             output.mkdir(parents=True, exist_ok=True)
             with open(output / "MLIndex", "w") as f:
                 yaml.dump(mlindex_config, f)
 
+    mlindex = MLIndex(mlindex_config=mlindex_config)
+    return mlindex
+
+
+def main(args, logger, activity_logger):
+    try:
+        raw_embeddings_uri = args.embeddings
+        logger.info(f'got embeddings uri as input: {raw_embeddings_uri}')
+        splits = raw_embeddings_uri.split('/')
+        embeddings_dir_name = splits.pop(len(splits)-2)
+        logger.info(f'extracted embeddings directory name: {embeddings_dir_name}')
+        parent = '/'.join(splits)
+        logger.info(f'extracted embeddings container path: {parent}')
+
+        acs_config = json.loads(args.acs_config)
+
+        connection_args = {}
+        connection_id = os.environ.get('AZUREML_WORKSPACE_CONNECTION_ID_ACS')
+        if connection_id is not None:
+            connection_args['connection_type'] = 'workspace_connection'
+            connection_args['connection'] = {'id': connection_id}
+            from azureml.rag.utils.connections import get_connection_by_id_v2
+
+            connection = get_connection_by_id_v2(connection_id)
+            acs_config['endpoint'] = connection['properties']['target']
+            acs_config['api_version'] = connection['properties'].get('metadata', {}).get('apiVersion', "2023-07-01-preview")
+        elif 'endpoint_key_name' in acs_config:
+            connection_args['connection_type'] = 'workspace_keyvault'
+            from azureml.core import Run
+            run = Run.get_context()
+            ws = run.experiment.workspace
+            connection_args['connection'] = {
+                'key': acs_config['endpoint_key_name'],
+                "subscription": ws.subscription_id,
+                "resource_group": ws.resource_group,
+                "workspace": ws.name,
+            }
+
+        from azureml.dataprep.fuse.dprepfuse import (MountOptions, rslex_uri_volume_mount)
+        mnt_options = MountOptions(
+            default_permission=0o555, allow_other=False, read_only=True)
+        logger.info(f'mounting embeddings container from: \n{parent} \n   to: \n{os.getcwd()}/embeddings_mount')
+        with rslex_uri_volume_mount(parent, f'{os.getcwd()}/embeddings_mount', options=mnt_options) as mount_context:
+            emb = EmbeddingsContainer.load(embeddings_dir_name, mount_context.mount_point)
+            create_index_from_raw_embeddings(emb, acs_config=acs_config, connection=connection_args, output_path=args.output)
+    except Exception as e:
+        logger.error('Failed to update ACS index')
+        exception_str = str(e)
+        if isinstance(e, requests.exceptions.RequestException):
+            activity_logger.activity_info['error'] = 'Failed request to ACS'
+            activity_logger.activity_info['response_code'] = e.response.status_code
+            activity_logger.activity_info['error_classification'] = 'SystemError'
+            if 'Floats quota has been exceeded for this service.' in e.response.text:
+                logger.error('Floats quota exceeded on Azure Cognitive Search Service. For more information check these docs: https://github.com/Azure/cognitive-search-vector-pr#storage-and-vector-index-size-limits')
+                logger.error('The usage statistic of an index can be checked using this REST API: https://learn.microsoft.com/en-us/rest/api/searchservice/get-index-statistics ')
+                activity_logger.activity_info['error_classification'] = 'UserError'
+                activity_logger.activity_info['error'] += ": Floats quota has been exceeded for this service."
+            elif 'Cannot find nested property' in e.response.text:
+                logger.error(f'The vector index provided "{acs_config["index_name"]}" has a different schema than outlined in this components description. This can happen if a different embedding model was used previously when updating this index.')
+                activity_logger.activity_info['error_classification'] = 'UserError'
+                activity_logger.activity_info['error'] += ": Cannot find nested property"
+        elif 'Failed to upload' in exception_str:
+            activity_logger.activity_info['error'] = str(e)
+            activity_logger.activity_info['error_classification'] = 'SystemError'
+        else:
+            activity_logger.activity_info['error'] = str(e.__class__.__name__)
+            activity_logger.activity_info['error_classification'] = 'SystemError'
+        raise e
+
+    logger.info('Updated ACS index')
+
+
+def main_wrapper(args, logger):
+    with track_activity(logger, "update_acs") as activity_logger:
+        try:
+            main(args, logger, activity_logger)
+        except Exception:
+            activity_logger.error(f"update_acs failed with exception: {traceback.format_exc()}")  # activity_logger doesn't log traceback
+            raise
+
 
 if __name__ == '__main__':
     from argparse import ArgumentParser
 
     parser = ArgumentParser()
     parser.add_argument("--embeddings", type=str)
     parser.add_argument("--acs_config", type=str)
@@ -349,73 +432,13 @@
     args = parser.parse_args()
 
     print('\n'.join(f'{k}={v}' for k, v in vars(args).items()))
 
     enable_stdout_logging()
     enable_appinsights_logging()
 
-    with track_activity(logger, "main") as activity_logger:
-        try:
-            raw_embeddings_uri = args.embeddings
-            logger.info(f'got embeddings uri as input: {raw_embeddings_uri}', extra={'print': True})
-            splits = raw_embeddings_uri.split('/')
-            embeddings_dir_name = splits.pop(len(splits)-2)
-            logger.info(f'extracted embeddings directory name: {embeddings_dir_name}', extra={'print': True})
-            parent = '/'.join(splits)
-            logger.info(f'extracted embeddings container path: {parent}', extra={'print': True})
-
-            acs_config = json.loads(args.acs_config)
-
-            connection_args = {}
-            connection_id = os.environ.get('AZUREML_WORKSPACE_CONNECTION_ID_ACS')
-            if connection_id is not None:
-                connection_args['connection_type'] = 'workspace_connection'
-                connection_args['connection'] = {'id': connection_id}
-                from azureml.rag.utils.connections import get_connection_by_id_v2
-
-                connection = get_connection_by_id_v2(connection_id)
-                acs_config['endpoint'] = connection['properties']['target']
-                acs_config['api_version'] = connection['properties'].get('metadata', {}).get('apiVersion', "2023-07-01-preview")
-            elif 'endpoint_key_name' in acs_config:
-                connection_args['connection_type'] = 'workspace_keyvault'
-                from azureml.core import Run
-                run = Run.get_context()
-                ws = run.experiment.workspace
-                connection_args['connection'] = {
-                    'key': acs_config['endpoint_key_name'],
-                    "subscription": ws.subscription_id,
-                    "resource_group": ws.resource_group,
-                    "workspace": ws.name,
-                }
-
-            from azureml.dataprep.fuse.dprepfuse import (MountOptions, rslex_uri_volume_mount)
-            mnt_options = MountOptions(
-                default_permission=0o555, allow_other=False, read_only=True)
-            logger.info(f'mounting embeddings container from: \n{parent} \n   to: \n{os.getcwd()}/embeddings_mount', extra={'print': True})
-            with rslex_uri_volume_mount(parent, f'{os.getcwd()}/embeddings_mount', options=mnt_options) as mount_context:
-                emb = Embeddings.load(embeddings_dir_name, mount_context.mount_point)
-                create_index_from_raw_embeddings(emb, acs_config=acs_config, connection=connection_args, output_path=args.output)
-        except Exception as e:
-            logger.error('Failed to update ACS index')
-            exception_str = str(e)
-            if isinstance(e, requests.exceptions.RequestException):
-                activity_logger.activity_info['error'] = 'Failed request to ACS'
-                activity_logger.activity_info['response_code'] = e.response.status_code
-                activity_logger.activity_info['error_classification'] = 'SystemError'
-                if 'Floats quota has been exceeded for this service.' in e.response.text:
-                    logger.error('Floats quota exceeded on Azure Cognitive Search Service. For more information check these docs: https://github.com/Azure/cognitive-search-vector-pr#storage-and-vector-index-size-limits')
-                    logger.error('The usage statistic of an index can be checked using this REST API: https://learn.microsoft.com/en-us/rest/api/searchservice/get-index-statistics ')
-                    activity_logger.activity_info['error_classification'] = 'UserError'
-                    activity_logger.activity_info['error'] += ": Floats quota has been exceeded for this service."
-                elif 'Cannot find nested property' in e.response.text:
-                    logger.error(f'The vector index provided "{acs_config["index_name"]}" has a different schema than outlined in this components description. This can happen if a different embedding model was used previously when updating this index.')
-                    activity_logger.activity_info['error_classification'] = 'UserError'
-                    activity_logger.activity_info['error'] += ": Cannot find nested property"
-            elif 'Failed to upload' in exception_str:
-                activity_logger.activity_info['error'] = str(e)
-                activity_logger.activity_info['error_classification'] = 'SystemError'
-            else:
-                activity_logger.activity_info['error'] = str(e.__class__.__name__)
-                activity_logger.activity_info['error_classification'] = 'SystemError'
-            raise
-
-    logger.info('Updated ACS index')
+    try:
+        main_wrapper(args, logger)
+    finally:
+        if _logger_factory.appinsights:
+            _logger_factory.appinsights.flush()
+            time.sleep(5)
```

## azureml/rag/utils/logging.py

```diff
@@ -1,82 +1,70 @@
 # ---------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
 """Logging utilities."""
+from contextlib import contextmanager
 from functools import lru_cache
 import inspect
 import logging
 import pkg_resources
 import sys
 
 
-# class ExtraPrintHandler(logging.StreamHandler):
-#     LEVELS = {0: 'DBUG', 1: 'INFO', 2: 'WARN', 3: 'ERRR'}
-
-#     def emit(self, record):
-#         if hasattr(record, 'print') and record.print:
-#             print(f'[{datetime.utcnow()}] {record.levelname} - {record.msg}')
-#         super().emit(record)
-
-# handler = ExtraPrintHandler()
-
 COMPONENT_NAME = 'azureml.rag'
 instrumentation_key = ''
 try:
     version = pkg_resources.get_distribution("azureml-rag").version
     langchain_version = pkg_resources.get_distribution("langchain").version
 except Exception:
     version = ''
     langchain_version = ''
 default_custom_dimensions = {'source': COMPONENT_NAME, 'version': version, 'langchain_version': langchain_version}
 STACK_FMT = "%s, line %d in function %s."
 DEFAULT_ACTIVITY_TYPE = "InternalCall"
 
 try:
     from azureml.telemetry import get_telemetry_log_handler, INSTRUMENTATION_KEY
-    from azureml.telemetry.activity import log_activity as _log_activity, ActivityType, ActivityLoggerAdapter
+    from azureml.telemetry.activity import log_activity as _log_activity, ActivityLoggerAdapter
     from azureml._base_sdk_common import _ClientSessionId
     import os
 
     session_id = _ClientSessionId
     current_folder = os.path.dirname(os.path.realpath(__file__))
     telemetry_config_path = os.path.join(current_folder, '_telemetry.json')
 
     verbosity = logging.INFO
     instrumentation_key = INSTRUMENTATION_KEY
     telemetry_enabled = True
 except Exception:
-    from contextlib import contextmanager
-
     verbosity = None
     ActivityLoggerAdapter = None
     telemetry_enabled = False
 
     class ActivityLoggerAdapter(logging.LoggerAdapter):
         """Make logger look like Activity Logger"""
 
-        def __init__(self, logger):
-            """Initialize a new instance of the class
-            """
-            self._activity_info = {}
+        def __init__(self, logger, activity_info):
+            """Initialize a new instance of the class"""
+            self._activity_info = activity_info
             super(ActivityLoggerAdapter, self).__init__(logger, None)
 
         @property
         def activity_info(self):
             """Return current activity info."""
             return self._activity_info
 
         def process(self, msg, kwargs):
-            """Process the log message.
-            """
+            """Process the log message."""
             return msg, kwargs
 
-    @contextmanager
-    def _run_without_logging(logger, activity_name, activity_type, custom_dimensions):
-        yield ActivityLoggerAdapter(logger)
+
+@contextmanager
+def _run_without_logging(logger, activity_name, activity_type, custom_dimensions):
+    yield ActivityLoggerAdapter(logger, {})
 
 
 known_modules = [
     "crack_and_chunk",
     "create_faiss_index",
     "create_promptflow",
     "data_import_acs",
@@ -86,49 +74,59 @@
     "register_mlindex_asset",
     "update_acs_index"
 ]
 
 
 class LoggerFactory:
     """Factory for creating loggers"""
-    def __init__(self, stdout=False):
+    def __init__(self, stdout=False, mlflow=True, verbosity=logging.DEBUG):
         """Initialize the logger factory"""
         self.loggers = {}
-        self.stdout = stdout
+        self.verbosity = verbosity
+        self.stdout = None
+        self.with_stdout(stdout)
         self.appinsights = None
-        self.with_appinsights()
+        self.azuremonitor = None
+        self.mlflow = mlflow
+
+    def with_mlflow(self, mlflow=True):
+        """Set whether to log to mlflow"""
+        self.mlflow = mlflow
+        return self
 
-    def with_stdout(self, stdout=True):
+    def with_stdout(self, stdout=True, verbosity=logging.INFO):
         """Set whether to log to stdout"""
-        self.stdout = stdout
-        # Add stdout handler to any loggers created before enabling stdout.
-        for logger in self.loggers.values():
-            if self.stdout:
-                stdout_handler = logging.StreamHandler(stream=sys.stdout)
-                stdout_handler.setFormatter(logging.Formatter('[%(asctime)s] %(levelname)-8s %(name)s - %(message)s (%(filename)s:%(lineno)s)', "%Y-%m-%d %H:%M:%S"))
-                logger.addHandler(stdout_handler)
+        if stdout:
+            # Add stdout handler to any loggers created before enabling stdout.
+            self.stdout = logging.StreamHandler(stream=sys.stdout)
+            self.stdout.setLevel(verbosity)
+            self.stdout.setFormatter(logging.Formatter('[%(asctime)s] %(levelname)-8s %(name)s - %(message)s (%(filename)s:%(lineno)s)', "%Y-%m-%d %H:%M:%S"))
+            for logger in self.loggers.values():
+                if self.stdout:
+                    logger.addHandler(self.stdout)
         return self
 
-    def with_appinsights(self):
+    def with_appinsights(self, verbosity=logging.INFO):
         """Set whether to log track_* events to appinsights"""
         if telemetry_enabled:
             import atexit
 
             self.appinsights = get_telemetry_log_handler(component_name=COMPONENT_NAME, path=telemetry_config_path)
             atexit.register(self.appinsights.flush)
 
-    def get_logger(self, name, level=logging.INFO):
+    def get_logger(self, name, level=None):
         """Get a logger with the given name and level"""
         if name not in self.loggers:
             logger = logging.getLogger(f'azureml.rag.{name}')
-            logger.setLevel(level)
+            if level is not None:
+                logger.setLevel(level)
+            else:
+                logger.setLevel(self.verbosity)
             if self.stdout:
-                stdout_handler = logging.StreamHandler(stream=sys.stdout)
-                stdout_handler.setFormatter(logging.Formatter('[%(asctime)s] %(levelname)-8s %(name)s - %(message)s (%(filename)s:%(lineno)s)', "%Y-%m-%d %H:%M:%S"))
-                logger.addHandler(stdout_handler)
+                logger.addHandler(self.stdout)
             self.loggers[name] = logger
         return self.loggers[name]
 
     def track_activity(self, logger, name, activity_type=DEFAULT_ACTIVITY_TYPE, custom_dimensions={}):
         """Track the activity of the given logger"""
         if self.appinsights:
             stack = self.get_stack()
@@ -221,17 +219,17 @@
             pass
         return info
 
 
 _logger_factory = LoggerFactory()
 
 
-def enable_stdout_logging():
+def enable_stdout_logging(verbosity=logging.INFO):
     """Enable logging to stdout"""
-    _logger_factory.with_stdout(True)
+    _logger_factory.with_stdout(True, verbosity)
 
 
 def enable_appinsights_logging():
     """Enable logging to appinsights"""
     _logger_factory.with_appinsights()
 
 
@@ -249,7 +247,17 @@
     """Track info with given logger"""
     return _logger_factory.telemetry_info(logger, message, custom_dimensions)
 
 
 def track_error(logger, message, custom_dimensions={}):
     """Track error with given logger"""
     return _logger_factory.telemetry_error(logger, message, custom_dimensions)
+
+
+def disable_mlflow():
+    """Disable mlflow logging"""
+    _logger_factory.mlflow = False
+
+
+def mlflow_enabled():
+    """Check if mlflow logging is enabled"""
+    return _logger_factory.mlflow
```

## Comparing `azureml_rag-0.1.8.dist-info/LICENSE.txt` & `azureml_rag-0.1.9.dist-info/LICENSE.txt`

 * *Files identical despite different names*

## Comparing `azureml_rag-0.1.8.dist-info/METADATA` & `azureml_rag-0.1.9.dist-info/METADATA`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: azureml-rag
-Version: 0.1.8
+Version: 0.1.9
 Summary: Contains Retrieval Augmented Generation related utilities for Azure Machine Learning and OSS interoperability.
 Home-page: https://docs.microsoft.com/python/api/overview/azure/ml/?view=azure-ml-py
 Author: Microsoft Corporation
 License: Proprietary https://aka.ms/azureml-preview-sdk-license 
 Platform: UNKNOWN
 Classifier: Development Status :: 5 - Production/Stable
 Classifier: Intended Audience :: Developers
@@ -28,18 +28,24 @@
 Requires-Dist: azureml-mlflow
 Requires-Dist: azureml-fsspec
 Requires-Dist: fsspec (~=2023.3)
 Requires-Dist: openai (~=0.27.4)
 Requires-Dist: tiktoken (~=0.3.0)
 Requires-Dist: langchain (!=0.0.174,>=0.0.149)
 Requires-Dist: cloudpickle
+Requires-Dist: mmh3
 Requires-Dist: msrest (>=0.6.18)
 Requires-Dist: pyyaml (<7.0.0,>=5.1.0)
 Provides-Extra: cognitive_search
 Requires-Dist: azure-search-documents (~=11.4.0b3) ; extra == 'cognitive_search'
+Provides-Extra: data_generation
+Requires-Dist: pandas (>=1) ; extra == 'data_generation'
+Requires-Dist: beautifulsoup4 (~=4.11.2) ; extra == 'data_generation'
+Requires-Dist: lxml (~=4.9.2) ; extra == 'data_generation'
+Requires-Dist: azure-ai-ml ; extra == 'data_generation'
 Provides-Extra: document_parsing
 Requires-Dist: pandas (>=1) ; extra == 'document_parsing'
 Requires-Dist: nltk (~=3.8.1) ; extra == 'document_parsing'
 Requires-Dist: markdown ; extra == 'document_parsing'
 Requires-Dist: beautifulsoup4 (~=4.11.2) ; extra == 'document_parsing'
 Requires-Dist: tika (~=2.6.0) ; extra == 'document_parsing'
 Requires-Dist: pypdf (~=3.7.0) ; extra == 'document_parsing'
@@ -117,14 +123,20 @@
 retriever = MLIndex(uri_to_folder_with_mlindex).as_langchain_retriever()
 retriever.get_relevant_documents('What is an AzureML Compute Instance?')
 ```
 
 
 # Changelog
 
+## 0.1.9 (2023-06-22)
+
+- Add `azureml.rag.data_generation` module.
+- Fixed bug that would cause crack_and_chunk to fail for documents that contain non-utf-8 characters. Currently these characters will be ignored.
+- Improved heading extraction from Markdown files. When `use_rcts=False` Markdown files will be split on headings and each chunk with have the heading context up to the root as a prefix (e.g. `# Heading 1\n## Heading 2\n# Heading 3\n{content}`)
+
 ## 0.1.8 (2023-06-21)
 
 - Add deployment inferring util for use in azureml-insider notebooks.
 
 ## 0.1.7 (2023-06-08)
 
 - Improved telemetry for tasks (used in RAG Pipeline Components)
```

## Comparing `azureml_rag-0.1.8.dist-info/RECORD` & `azureml_rag-0.1.9.dist-info/RECORD`

 * *Files 27% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 azureml/rag/__init__.py,sha256=zgRl-az58He0no_ZUWab0AuVQSOTmgCLlfwbEcnwKS0,246
-azureml/rag/documents.py,sha256=PTxrleeRwsp_fQd7wCNpNWbtGSh5PBGNCyw97If7bUE,39881
-azureml/rag/embeddings.py,sha256=LPJo0dcNxPsM4Z6XWaROddyK-ERIWVAVe9C7T9mi56U,33664
-azureml/rag/mlindex.py,sha256=DNb-Z15nTzjmGZH5h0RGjqxQRvnIDg756W8gJDbObhA,5058
-azureml/rag/models.py,sha256=Ga9tvV0FsVhXprEeWpPVakNJ9q96byc0OjDESqkMuCw,4149
+azureml/rag/documents.py,sha256=e5k8Aw9dS6LZGOrcU2Y9m4Ba_JEDGVeKTRAxrZryZDY,40590
+azureml/rag/embeddings.py,sha256=zXuMM6Kni30MrNRFk4iqAndbfPuVO-67ipVPX9TPjS8,32799
+azureml/rag/mlindex.py,sha256=y0g25ADWs_dOXSu0xJkc7Axas9TGtbNwo0JVBR_gdBA,6356
+azureml/rag/models.py,sha256=Tm-yhcmW6xA5rZ06gUS3r5ZNlP002wvp5qLY_hQ4P9s,7895
 azureml/rag/_asset_client/client.py,sha256=LpsDsedlQRxytezdvnxx1zfEQUb9DcYDY6RfkRSlXT0,4646
 azureml/rag/_asset_client/_restclient/__init__.py,sha256=38lKUIqL59KqhES7ZGBUGcrELWICWet0VFLxwY4W0fo,893
 azureml/rag/_asset_client/_restclient/_azure_machine_learning_workspaces.py,sha256=7XqTcSuvXHcKXu4FE71E4rBQAQ4Fmpdq8SXrhU7_AAg,4381
 azureml/rag/_asset_client/_restclient/_configuration.py,sha256=E52K3BmA4ZqAE6oP5VjRK32HsBrl6W9Y1oOsWCJu8Ig,3538
 azureml/rag/_asset_client/_restclient/_patch.py,sha256=wuqrJGWK488sJvWwSq6iwPTqil7TPaRadoxE7BMK0tA,1561
 azureml/rag/_asset_client/_restclient/_version.py,sha256=72yoX3gRVc4OFrnlCjEq_1cS0FLSIvofYIXtMN1ElvI,495
 azureml/rag/_asset_client/_restclient/models.py,sha256=rethYtGgAYZ6YZ51EbJFIS62FN1sJLUSGNjDvMdueCg,399
@@ -32,28 +32,36 @@
 azureml/rag/_asset_client/_restclient/runhistory/_version.py,sha256=72yoX3gRVc4OFrnlCjEq_1cS0FLSIvofYIXtMN1ElvI,495
 azureml/rag/_asset_client/_restclient/runhistory/models/__init__.py,sha256=RSjYy-jYXXp31fA5tEprD1uPTdaoCJsE4IKfDBlg8a0,11705
 azureml/rag/_asset_client/_restclient/runhistory/models/_azure_machine_learning_workspaces_enums.py,sha256=cyvWolLt_kCOalE8BvtnI_msfmAAlTYzdob0QDp1KZ0,2566
 azureml/rag/_asset_client/_restclient/runhistory/models/_models.py,sha256=CqQccTVrcm5_9uBZOJ6AMubF9K8GphuIzmF1j46G84Q,175170
 azureml/rag/_asset_client/_restclient/runhistory/models/_models_py3.py,sha256=G5x7IlIzWN98TsXpY80M5nyLjXIwBWWeL9Fk6Piwpzw,188776
 azureml/rag/_asset_client/_restclient/runhistory/operations/__init__.py,sha256=FFrjEyJdO53e7YDI8o_8W9BgTTEVt1ZzS8FwwnNp7-g,563
 azureml/rag/_asset_client/_restclient/runhistory/operations/_runs_operations.py,sha256=1nl-ETYrSQSZH3B-a2QXTKIUCx-Hx3XeQPmgAwKSGoA,162796
+azureml/rag/data_generation/qa.py,sha256=t2liHy1CMYbZ3ml6gMsLJAT4sVJaR0dkz8R_flqosxQ,12423
+azureml/rag/data_generation/prompts/prompt_qa_boolean.txt,sha256=1NYImxdLvrkK_BEdlSqWGq0yYYVCGHy0TbaJs9vEfWA,3081
+azureml/rag/data_generation/prompts/prompt_qa_long_answer.txt,sha256=8Trhq5vJaZCRqPlRJrlnigVM8qNeB_NccOAgbd7hO2A,3324
+azureml/rag/data_generation/prompts/prompt_qa_short_answer.txt,sha256=ieBy6nbhi8oOfhEJBSpOxmIOj-99f-0HMRr3e-LPjS0,3070
+azureml/rag/data_generation/prompts/prompt_qa_summary.txt,sha256=6zB7xSS0bddEEf5eMlrWl4Q-vn9GivAgSdJPEII2j8U,2181
 azureml/rag/langchain/acs.py,sha256=xyJ5u7RlZ8FUVAQZg7z4LJv5pXACddLcRLihUL6sEQI,10785
+azureml/rag/parsers/__init__.py,sha256=NAASVAI2bv3CXFRc3FAuwHSIFc3Ujw3yVTOMr38_-kE,198
+azureml/rag/parsers/markdown.py,sha256=o9Cl6kHkbh_K--D6JQQIKd9UCQXjEZSBZI-RDkUC4Hw,646
 azureml/rag/tasks/__init__.py,sha256=JlCCObuOLZyNhIZlsoL51KtFxiY4xKIIzIRDBcdeu6Y,183
-azureml/rag/tasks/build_faiss.py,sha256=MLgVOgg2vpfq9BgFJL3O69oN2G3mD3R2sNqpASrmllM,2289
-azureml/rag/tasks/crack_and_chunk.py,sha256=pord6kqwlxlwJ5U-kIlbewMSvAnm-HJ3jkx-P8BfqSo,9822
-azureml/rag/tasks/embed.py,sha256=gCjmL9-CG3ygpwxDZX1_8MtfnUwYGMHWnb1kicwPsAQ,7502
-azureml/rag/tasks/embed_prs.py,sha256=h1liQenbyr5pynrQjwoRDqkF5WqRI_gWrW8oyMBUMrg,6121
-azureml/rag/tasks/git_clone.py,sha256=T81sYAIh82DIZYIJVCAZB4NY-FsBf3qqswYM3pw8CxY,3239
-azureml/rag/tasks/register_mlindex.py,sha256=7favzOb15gFuh1Cg8RE1igLGQpcujOxMRaxIT4Nh6hc,3327
-azureml/rag/tasks/update_acs.py,sha256=Z8ii7Xh5MVShkVRFW-2dbR8F8CN7Eue-6zcIMn2iayQ,21619
+azureml/rag/tasks/build_faiss.py,sha256=voKXHjlPQFx6XKQgp5P0dbxMGV36RoK1uS69fJF8pX8,3032
+azureml/rag/tasks/crack_and_chunk.py,sha256=wY4_T99cCl-LI51APSMfJAdfUJ2rvJmFgNe0WttvBpk,7884
+azureml/rag/tasks/embed.py,sha256=l4NziSQ8Irt59o8GkplwJdyDfkdM1gWRopnYbywwrME,10408
+azureml/rag/tasks/embed_prs.py,sha256=CW94M7jZzGsWEr6Mg7UwQgsr-nT9wBhZveT6XgCcbI4,6919
+azureml/rag/tasks/generate_qa.py,sha256=UITFUseiq_q4ZeGnMMCntAehWHsTeGUtPmQPH33PJ6k,9629
+azureml/rag/tasks/git_clone.py,sha256=-cP9G42JJcP7QdWS5qMor8BBvvYArv8HXQQz2ibszYw,3678
+azureml/rag/tasks/register_mlindex.py,sha256=30K97sd7GBgg0WG33kRrzVwI8r-Gr3FcgbXXbtuIL2A,3842
+azureml/rag/tasks/update_acs.py,sha256=0MXAVVQ6WVo1NsVcbXcVDkqpsb8DQBOIWksyEloBkco,21540
 azureml/rag/utils/__init__.py,sha256=FTY5BaTKeythd7R1SXsf3midWHdshZj-bdFZLmZ7J50,208
 azureml/rag/utils/_telemetry.json,sha256=VlnC81iHUOx9NYj_BNpMzI2Y8Q3D8z832w31TQl9qI4,98
 azureml/rag/utils/azureml.py,sha256=dOfvDTgqjTWHg-I9Ho7_o6e7I8kNcVJdl53O0Ts7xQs,1588
 azureml/rag/utils/connections.py,sha256=zInLr6twjfHW4fF2zgChtNSI7YqnxY3eOVnu74xpq-Y,7435
 azureml/rag/utils/deployment.py,sha256=4gF7PCE-B0bU_WlBH3ZSLAg9WglItySMbh9hvUWcw7o,1506
 azureml/rag/utils/git.py,sha256=oumlex0PWWt8ppmL-0Sxli6cpRWpBR9THR0fqN2thFo,2634
-azureml/rag/utils/logging.py,sha256=TR6B6GssB3G-BSfpSXhIka3Ru73R4NgpR2k3F0mBwew,10082
-azureml_rag-0.1.8.dist-info/LICENSE.txt,sha256=FOfkEEz4uS7g278F9Rq12rqKF3lLyBUZhVpLetuZrTg,1021
-azureml_rag-0.1.8.dist-info/METADATA,sha256=-tf5ChiCZGSRS-89XbyP7gCIzKP5NMHP2CMOQF6SUnQ,5744
-azureml_rag-0.1.8.dist-info/WHEEL,sha256=YUYzQ6UQdoqxXjimOitTqynltBCkwY6qlTfTh2IzqQU,97
-azureml_rag-0.1.8.dist-info/top_level.txt,sha256=ZOeEa0TAXo6i5wOjwBoqfIGEuxOcKuscGgNSpizqREY,8
-azureml_rag-0.1.8.dist-info/RECORD,,
+azureml/rag/utils/logging.py,sha256=Kp3Gj9DJ1plV0nT5iKFXP5Dvy5DJduchnngVdbv3E28,10151
+azureml_rag-0.1.9.dist-info/LICENSE.txt,sha256=FOfkEEz4uS7g278F9Rq12rqKF3lLyBUZhVpLetuZrTg,1021
+azureml_rag-0.1.9.dist-info/METADATA,sha256=8GOxpmFxHu6mzS7CoYHXvlw8Gbis10vFwaaS9w00cfw,6497
+azureml_rag-0.1.9.dist-info/WHEEL,sha256=YUYzQ6UQdoqxXjimOitTqynltBCkwY6qlTfTh2IzqQU,97
+azureml_rag-0.1.9.dist-info/top_level.txt,sha256=ZOeEa0TAXo6i5wOjwBoqfIGEuxOcKuscGgNSpizqREY,8
+azureml_rag-0.1.9.dist-info/RECORD,,
```

